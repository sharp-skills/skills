{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kiyanwang"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "Learnings from Running 1000s of <em>Production</em> <em>RabbitMQ</em> Clusters"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.youtube.com/watch?v=nxQrpLfX3rs"}}, "_tags": ["story", "author_kiyanwang", "story_27351479"], "author": "kiyanwang", "created_at": "2021-06-01T07:12:21Z", "created_at_i": 1622531541, "num_comments": 0, "objectID": "27351479", "points": 4, "story_id": 27351479, "title": "Learnings from Running 1000s of Production RabbitMQ Clusters", "updated_at": "2024-09-20T08:43:36Z", "url": "https://www.youtube.com/watch?v=nxQrpLfX3rs"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "iamtrk"}, "story_text": {"matchLevel": "none", "matchedWords": [], "value": ""}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "<em>Production</em> level Node.js <em>RabbitMQ</em> client based on amqlib"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["rabbitmq"], "value": "https://github.com/iamtrk/<em>rabbitMQ</em>"}}, "_tags": ["story", "author_iamtrk", "story_9334609"], "author": "iamtrk", "created_at": "2015-04-07T15:15:38Z", "created_at_i": 1428419738, "num_comments": 0, "objectID": "9334609", "points": 1, "story_id": 9334609, "story_text": "", "title": "Production level Node.js RabbitMQ client based on amqlib", "updated_at": "2023-09-07T02:28:15Z", "url": "https://github.com/iamtrk/rabbitMQ"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "DjGilcrease"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "The Problem\nEnterprise Go apps typically use multiple message queues: <em>RabbitMQ</em> for reliability, Kafka for streaming, SQS for cloud, NATS for microservices, Redis for caching. Each has different APIs, error handling, and testing strategies.<p>Result: Teams spend months learning 6+ SDKs instead of building features. Migration = complete rewrites.<p>The Solution\nmqutils provides one unified API for 6 major message queue systems. Same code, different URL:<p>// Switch systems by changing URL only\nconsumer, err := mqutils.NewConsumer(&quot;amqp://localhost:5672/orders&quot;)\nconsumer, err := mqutils.NewConsumer(&quot;kafka://localhost:9092/orders&quot;) \nconsumer, err := mqutils.NewConsumer(&quot;sqs://us-east-1/orders&quot;)<p>// Same handler code works everywhere\nconsumer.RegisterHandler(&quot;process&quot;, func(msg types.Message) error {\n    return processOrder(msg.Body())\n})\nWhy It's Different\nNot just another wrapper. <em>Production</em>-grade features built-in:<p>Health monitoring across all 6 systems\nBatch processing with configurable timeouts\nGraceful shutdown with proper cleanup\nContext propagation for distributed tracing\nThread-safe operations with race condition prevention\nPerformance: 100K+ msg/sec, &lt;10ms P99 latency, 99.9% uptime in <em>production</em>.<p>Supported Systems\nAMQP/<em>RabbitMQ</em> (amqp://)\nApache Kafka (kafka://)\nNATS Core/JetStream (nats://, jetstream://)\nAWS SQS (sqs://)\nGCP Pub/Sub (pubsub://)\nRedis Pub/Sub &amp; Streams (redis://, redisstream://)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Mqutils \u2013 Universal Go message queue library"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://mqutils.dev/"}}, "_tags": ["story", "author_DjGilcrease", "story_44350790", "show_hn"], "author": "DjGilcrease", "created_at": "2025-06-22T22:22:32Z", "created_at_i": 1750630952, "num_comments": 0, "objectID": "44350790", "points": 6, "story_id": 44350790, "story_text": "The Problem\nEnterprise Go apps typically use multiple message queues: RabbitMQ for reliability, Kafka for streaming, SQS for cloud, NATS for microservices, Redis for caching. Each has different APIs, error handling, and testing strategies.<p>Result: Teams spend months learning 6+ SDKs instead of building features. Migration = complete rewrites.<p>The Solution\nmqutils provides one unified API for 6 major message queue systems. Same code, different URL:<p>&#x2F;&#x2F; Switch systems by changing URL only\nconsumer, err := mqutils.NewConsumer(&quot;amqp:&#x2F;&#x2F;localhost:5672&#x2F;orders&quot;)\nconsumer, err := mqutils.NewConsumer(&quot;kafka:&#x2F;&#x2F;localhost:9092&#x2F;orders&quot;) \nconsumer, err := mqutils.NewConsumer(&quot;sqs:&#x2F;&#x2F;us-east-1&#x2F;orders&quot;)<p>&#x2F;&#x2F; Same handler code works everywhere\nconsumer.RegisterHandler(&quot;process&quot;, func(msg types.Message) error {\n    return processOrder(msg.Body())\n})\nWhy It&#x27;s Different\nNot just another wrapper. Production-grade features built-in:<p>Health monitoring across all 6 systems\nBatch processing with configurable timeouts\nGraceful shutdown with proper cleanup\nContext propagation for distributed tracing\nThread-safe operations with race condition prevention\nPerformance: 100K+ msg&#x2F;sec, &lt;10ms P99 latency, 99.9% uptime in production.<p>Supported Systems\nAMQP&#x2F;RabbitMQ (amqp:&#x2F;&#x2F;)\nApache Kafka (kafka:&#x2F;&#x2F;)\nNATS Core&#x2F;JetStream (nats:&#x2F;&#x2F;, jetstream:&#x2F;&#x2F;)\nAWS SQS (sqs:&#x2F;&#x2F;)\nGCP Pub&#x2F;Sub (pubsub:&#x2F;&#x2F;)\nRedis Pub&#x2F;Sub &amp; Streams (redis:&#x2F;&#x2F;, redisstream:&#x2F;&#x2F;)", "title": "Show HN: Mqutils \u2013 Universal Go message queue library", "updated_at": "2025-06-23T08:30:16Z", "url": "https://mqutils.dev/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mindaugas"}, "story_text": {"matchLevel": "none", "matchedWords": [], "value": ""}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["rabbitmq"], "value": "Startup logbook - using Clojure, Ruby/Rails, <em>RabbitMQ</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "http://s-expressions.com/2009/01/28/startup-logbook-clojure-in-<em>production</em>-release-v01/"}}, "_tags": ["story", "author_mindaugas", "story_466364"], "author": "mindaugas", "children": [466792], "created_at": "2009-02-04T19:54:19Z", "created_at_i": 1233777259, "num_comments": 1, "objectID": "466364", "points": 5, "story_id": 466364, "story_text": "", "title": "Startup logbook - using Clojure, Ruby/Rails, RabbitMQ", "updated_at": "2024-09-19T16:33:01Z", "url": "http://s-expressions.com/2009/01/28/startup-logbook-clojure-in-production-release-v01/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "alexhutcheson"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "In the late 2000s and early 2010s, I remember seeing lots of hype around building distributed systems using message queues (e.g. Amazon SQS, <em>RabbitMQ</em>, ZeroMQ, etc.) A lot of companies had blog posts highlighting their use of message queues for asynchronous communication between nodes, and IIRC the official AWS design recommendations at the time pushed SQS pretty heavily.<p>Now, I almost never see engineering blog posts or HN posts highlighting use of message queues. I see occasional content related to Kafka, but nothing like the hype that message queues used to have.<p>What changed? Possible theories I'm aware of:<p>* Redis tackled most of the use-case, plus caching, so it no longer made sense to pay the operational cost of running a separate message broker. Kafka picked up the really high-scale applications.<p>* Databases (broadly defined) got a lot better at handling high scale, so system designers moved more of the &quot;transient&quot; application state into the main data stores.<p>* We collectively realize that message queues-based architectures don't work as well as we hoped, so we build most things in other ways now.<p>* The technology just got mature enough that it's not exciting to write about, but it's still really widely used.<p>If people have experience designing or implementing greenfield systems based on message queues, I'd be curious to hear about it. I'd also be interested in understanding any war stories or pain points people have had from using message queues in <em>production</em> systems."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Why do message queue-based architectures seem less popular now?"}}, "_tags": ["story", "author_alexhutcheson", "story_40723302", "ask_hn"], "author": "alexhutcheson", "children": [40723582, 40723585, 40723586, 40723598, 40723616, 40723618, 40723621, 40723639, 40723642, 40723650, 40723651, 40723655, 40723658, 40723677, 40723682, 40723688, 40723692, 40723699, 40723700, 40723706, 40723716, 40723723, 40723738, 40723752, 40723799, 40723808, 40723816, 40723843, 40723850, 40723856, 40723876, 40723880, 40723884, 40723901, 40723903, 40723922, 40723933, 40723937, 40723962, 40724291, 40724365, 40724408, 40724429, 40724453, 40724482, 40724551, 40724556, 40724580, 40724605, 40724614, 40724678, 40724736, 40724761, 40724798, 40724894, 40724934, 40725004, 40725020, 40725028, 40725060, 40725079, 40725124, 40725207, 40725291, 40725298, 40725342, 40725390, 40725398, 40725418, 40725434, 40725443, 40725450, 40725501, 40725511, 40725577, 40725600, 40725688, 40725751, 40725852, 40725878, 40725962, 40725995, 40726023, 40726092, 40726094, 40726121, 40726163, 40726205, 40726255, 40726581, 40726756, 40726847, 40726852, 40726885, 40727105, 40727237, 40727262, 40727609, 40727691, 40728351, 40728540, 40728663, 40728910, 40731596, 40732075, 40732941, 40733005, 40733155, 40733905, 40734337, 40734701, 40736565, 40737722, 40738950, 40739146, 40739901, 40747439, 40750786, 40755103, 40757561, 40758576, 40759422, 40773835, 40776035, 40801431], "created_at": "2024-06-18T23:50:39Z", "created_at_i": 1718754639, "num_comments": 364, "objectID": "40723302", "points": 389, "story_id": 40723302, "story_text": "In the late 2000s and early 2010s, I remember seeing lots of hype around building distributed systems using message queues (e.g. Amazon SQS, RabbitMQ, ZeroMQ, etc.) A lot of companies had blog posts highlighting their use of message queues for asynchronous communication between nodes, and IIRC the official AWS design recommendations at the time pushed SQS pretty heavily.<p>Now, I almost never see engineering blog posts or HN posts highlighting use of message queues. I see occasional content related to Kafka, but nothing like the hype that message queues used to have.<p>What changed? Possible theories I&#x27;m aware of:<p>* Redis tackled most of the use-case, plus caching, so it no longer made sense to pay the operational cost of running a separate message broker. Kafka picked up the really high-scale applications.<p>* Databases (broadly defined) got a lot better at handling high scale, so system designers moved more of the &quot;transient&quot; application state into the main data stores.<p>* We collectively realize that message queues-based architectures don&#x27;t work as well as we hoped, so we build most things in other ways now.<p>* The technology just got mature enough that it&#x27;s not exciting to write about, but it&#x27;s still really widely used.<p>If people have experience designing or implementing greenfield systems based on message queues, I&#x27;d be curious to hear about it. I&#x27;d also be interested in understanding any war stories or pain points people have had from using message queues in production systems.", "title": "Ask HN: Why do message queue-based architectures seem less popular now?", "updated_at": "2025-12-12T08:45:49Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "skull8888888"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "Hey HN, we\u2019re Robert, Din and Temirlan from Laminar (<a href=\"https://www.lmnr.ai\">https://www.lmnr.ai</a>), an open-source observability and analytics platform for complex LLM apps. It\u2019s designed to be fast, reliable, and scalable. The stack is <em>RabbitMQ</em> for message queues, Postgres for storage, Clickhouse for analytics, Qdrant for semantic search - all powered by Rust.<p>How is Laminar different from the swarm of other \u201cLLM observability\u201d platforms?<p>On the observability part, we\u2019re focused on handling full execution traces, not just LLM calls. We built a Rust ingestor for OpenTelemetry (Otel) spans with GenAI semantic conventions. As LLM apps get more complex (think Agents with hundreds of LLM and function calls, or complex RAG pipelines), full tracing is critical. With Otel spans, we can: 1. Cover the entire execution trace. 2. Keep the platform future-proof 3. Leverage an amazing OpenLLMetry (<a href=\"https://github.com/traceloop/openllmetry\">https://github.com/traceloop/openllmetry</a>), open-source package for span <em>production</em>.<p>The key difference is that we tie text analytics directly to execution traces. Rich text data makes LLM traces unique, so we let you track \u201csemantic metrics\u201d (like what your AI agent is actually saying) and connect those metrics to where they happen in the trace. If you want to know if your AI drive-through agent made an upsell, you can design an LLM extraction pipeline in our builder (more on it later), host it on Laminar, and handle everything from event requests to output logging. Processing requests simply come as events in the Otel span.<p>We think it\u2019s a win to separate core app logic from LLM event processing. Most devs don\u2019t want to manage background queues for LLM analytics processing but still want insights into how their Agents or RAGs are working.<p>Our Pipeline Builder uses graph UI where nodes are LLM and util functions, and edges showing data flow. We built a custom task execution engine with support of parallel branch executions, cycles and branches (it\u2019s overkill for simple pipelines, but it\u2019s extremely cool and we\u2019ve spent a lot of time designing a robust engine). You can also call pipelines directly as API endpoints. We found them to be extremely useful for iterating on and separating LLM logic. Laminar also traces pipeline directly, which removes the overhead of sending large outputs over the network.<p>One thing missing from all LLM observability platforms right now is an adequate search over traces. We\u2019re attacking this problem by indexing each span in a vector DB and performing hybrid search at query time. This feature is still in beta, but we think it\u2019s gonna be crucial part of our platform going forward.<p>We also support evaluations. We loved the \u201crun everything locally, send results to a server\u201d approach from Braintrust and Weights &amp; Biases, so we did that too: a simple SDK and nice dashboards to track everything. Evals are still early, but we\u2019re pushing hard on them.<p>Our goal is to make Laminar the Supabase for LLMOps - the go-to open-source comprehensive platform for all things LLMs / GenAI. In it\u2019s current shape, Laminar is just few weeks old and developing rapidly, we\u2019d love any feedback or for you to give Laminar a try in your LLM projects!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Laminar \u2013 Open-Source DataDog + PostHog for LLM Apps, Built in Rust"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/lmnr-ai/lmnr"}}, "_tags": ["story", "author_skull8888888", "story_41451698", "show_hn"], "author": "skull8888888", "children": [41452424, 41452994, 41453001, 41453058, 41453087, 41453241, 41453566, 41453832, 41455101, 41455103, 41456518, 41459224, 41459548, 41459620, 41462581], "created_at": "2024-09-04T22:52:19Z", "created_at_i": 1725490339, "num_comments": 45, "objectID": "41451698", "points": 203, "story_id": 41451698, "story_text": "Hey HN, we\u2019re Robert, Din and Temirlan from Laminar (<a href=\"https:&#x2F;&#x2F;www.lmnr.ai\">https:&#x2F;&#x2F;www.lmnr.ai</a>), an open-source observability and analytics platform for complex LLM apps. It\u2019s designed to be fast, reliable, and scalable. The stack is RabbitMQ for message queues, Postgres for storage, Clickhouse for analytics, Qdrant for semantic search - all powered by Rust.<p>How is Laminar different from the swarm of other \u201cLLM observability\u201d platforms?<p>On the observability part, we\u2019re focused on handling full execution traces, not just LLM calls. We built a Rust ingestor for OpenTelemetry (Otel) spans with GenAI semantic conventions. As LLM apps get more complex (think Agents with hundreds of LLM and function calls, or complex RAG pipelines), full tracing is critical. With Otel spans, we can: 1. Cover the entire execution trace. 2. Keep the platform future-proof 3. Leverage an amazing OpenLLMetry (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry\">https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry</a>), open-source package for span production.<p>The key difference is that we tie text analytics directly to execution traces. Rich text data makes LLM traces unique, so we let you track \u201csemantic metrics\u201d (like what your AI agent is actually saying) and connect those metrics to where they happen in the trace. If you want to know if your AI drive-through agent made an upsell, you can design an LLM extraction pipeline in our builder (more on it later), host it on Laminar, and handle everything from event requests to output logging. Processing requests simply come as events in the Otel span.<p>We think it\u2019s a win to separate core app logic from LLM event processing. Most devs don\u2019t want to manage background queues for LLM analytics processing but still want insights into how their Agents or RAGs are working.<p>Our Pipeline Builder uses graph UI where nodes are LLM and util functions, and edges showing data flow. We built a custom task execution engine with support of parallel branch executions, cycles and branches (it\u2019s overkill for simple pipelines, but it\u2019s extremely cool and we\u2019ve spent a lot of time designing a robust engine). You can also call pipelines directly as API endpoints. We found them to be extremely useful for iterating on and separating LLM logic. Laminar also traces pipeline directly, which removes the overhead of sending large outputs over the network.<p>One thing missing from all LLM observability platforms right now is an adequate search over traces. We\u2019re attacking this problem by indexing each span in a vector DB and performing hybrid search at query time. This feature is still in beta, but we think it\u2019s gonna be crucial part of our platform going forward.<p>We also support evaluations. We loved the \u201crun everything locally, send results to a server\u201d approach from Braintrust and Weights &amp; Biases, so we did that too: a simple SDK and nice dashboards to track everything. Evals are still early, but we\u2019re pushing hard on them.<p>Our goal is to make Laminar the Supabase for LLMOps - the go-to open-source comprehensive platform for all things LLMs &#x2F; GenAI. In it\u2019s current shape, Laminar is just few weeks old and developing rapidly, we\u2019d love any feedback or for you to give Laminar a try in your LLM projects!", "title": "Show HN: Laminar \u2013 Open-Source DataDog + PostHog for LLM Apps, Built in Rust", "updated_at": "2026-02-18T07:06:12Z", "url": "https://github.com/lmnr-ai/lmnr"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "dsies"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "Hello HN!<p>We are Ustin and Daniel, co-founders of Batch (<a href=\"https://batch.sh\" rel=\"nofollow\">https://batch.sh</a>) - an event replay platform. You can think of us as version control for data passing through your messaging systems. With Batch, a company is able to go back in time, see what data looked like at a certain point and if it makes sense, replay that piece of data back into the company's systems.<p>This idea was born out of getting annoyed by what an unwieldy blackbox Kafka is. While many folks use Kafka for streaming, there is an equal number of Kafka users that use it as a traditional messaging system. Historically, these systems have offered very poor visibility into what's going on inside them and offer (at best) a poor replay experience. This problem is prevalent pretty much across every messaging system. Especially if the messages on the bus are serialized, it is almost guaranteed that you will have to write custom, one-off scripts when working with these systems.<p>This &quot;visibility&quot; pain point is exacerbated tenfold if you are working with event driven architectures and/or event sourcing - you must have a way to search and replay events as you will need to rebuild state in order to bring up new data stores and services. That may sound straightforward, but it's actually really involved. You have to figure out how and where to store your events, how to serialize them, search them, play them back, and how/when/if to prune, delete or archive them.<p>Rather than spending a ton of money on building such a replay platform in-house, we decided to build a generic one and hopefully save everyone a bunch of time and money. We are 100% believers in &quot;buy&quot; (vs &quot;build&quot;) - companies should focus on building their core product and not waste time on sidequests. We've worked on these systems before at our previous gigs and decided to put our combined experience into building Batch.<p>A friend of mine shared this bit of insight with me (that he heard from Dave Cheney, I think?) - &quot;Is this what you want to spend your innovation tokens on?&quot; (referring to building something in-house) - and the answer is probably... no. So this is how we got here!<p>In practical terms, we give you a &quot;connector&quot; (in the form of a Docker image) that hooks into your messaging system as a consumer and begins copying all data that it sees on a topic/exchange to Batch. Alternatively, you can pump data into our platform via a generic HTTP or gRPC API. Once the messages reach Batch, we index them and write them to a long-term store (we use <a href=\"https://www.elassandra.io\" rel=\"nofollow\">https://www.elassandra.io</a>). At that point, you can use either our UI or HTTP API to search and replay a subset of the messages to an HTTP destination or into another messaging system.<p>Right now, our platform is able to ingest data from Kafka, <em>RabbitMQ</em> and GCP PubSub, and we've got SQS on the roadmap. Really, we're cool with adding support for whatever messaging system you need as long as it solves a problem for you.<p>One super cool thing is that if you are encoding your events in protobuf, we are able to decode them upon arrival on our platform, so that we can index them and let you search for data within them. In fact, we think this functionality is so cool that we really wanted to share it - surely there are other folks that need to quickly read/write encoded data to various messaging systems. We wrote <a href=\"https://github.com/batchcorp/plumber\" rel=\"nofollow\">https://github.com/batchcorp/plumber</a> for that purpose. It's like curl for messaging systems and currently supports Kafka, <em>RabbitMQ</em> and GCP PubSub. It's a port from an internal tool we used when interacting with our own Kafka and <em>RabbitMQ</em> instances.<p>In closing, we would love for you to check out <a href=\"https://batch.sh\" rel=\"nofollow\">https://batch.sh</a> and tell us what you think. Our initial thinking is to allow folks to pump their data into us for free with 1-3 days of retention. If you need more retention, that'll require $ (we're leaning towards a usage-based pricing model).<p>We envision Batch becoming a foundational component of your system architecture, but right now, our #1 goal is to lower the barrier to entry for event sourcing and we think that offering &quot;out-of-the-box&quot; replay functionality is the first step towards making this happen.<p>.. And if event sourcing is not your cup of tea - then you can get us in your stack to gain visibility and a peace of mind.<p>OK that's it! Thank you for checking us out!<p>~Dan &amp; Ustin<p>P.S. Forgot about our creds:<p>I (Dan), spent a large chunk of my career working at data centers doing systems integration work. I got exposed to all kinds of esoteric things like how to integrate diesel generators into CMSs and automate VLAN provisioning for customers. I also learned that &quot;move fast and break things&quot; does not apply to data centers haha. After data centers, I went to work for New Relic, followed by InVision, Digital Ocean and most recently, Community (which is where I met Ustin). I work primarily in Go, consider myself a generalist, prefer light beers over IPAs and dabble in metal (music) <em>production</em>.<p>Ustin is a physicist turned computer scientist and worked towards a PhD on distributed storage over lossy networks. He has spent most of his career working as a founding engineer at startups like Community. He has a lot of experience working in Elixir and Go and working on large, complex systems."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Batch (YC S20) \u2013 Replays for event-driven systems"}}, "_tags": ["story", "author_dsies", "story_24188214", "launch_hn"], "author": "dsies", "children": [24188810, 24188880, 24188927, 24188940, 24188976, 24188993, 24189195, 24190214, 24190223, 24190282, 24190418, 24190483, 24192233, 24193320, 24193504, 24194568, 24195309, 24195510, 24195927, 24196558, 24212098, 24230595], "created_at": "2020-08-17T15:33:49Z", "created_at_i": 1597678429, "num_comments": 58, "objectID": "24188214", "points": 154, "story_id": 24188214, "story_text": "Hello HN!<p>We are Ustin and Daniel, co-founders of Batch (<a href=\"https:&#x2F;&#x2F;batch.sh\" rel=\"nofollow\">https:&#x2F;&#x2F;batch.sh</a>) - an event replay platform. You can think of us as version control for data passing through your messaging systems. With Batch, a company is able to go back in time, see what data looked like at a certain point and if it makes sense, replay that piece of data back into the company&#x27;s systems.<p>This idea was born out of getting annoyed by what an unwieldy blackbox Kafka is. While many folks use Kafka for streaming, there is an equal number of Kafka users that use it as a traditional messaging system. Historically, these systems have offered very poor visibility into what&#x27;s going on inside them and offer (at best) a poor replay experience. This problem is prevalent pretty much across every messaging system. Especially if the messages on the bus are serialized, it is almost guaranteed that you will have to write custom, one-off scripts when working with these systems.<p>This &quot;visibility&quot; pain point is exacerbated tenfold if you are working with event driven architectures and&#x2F;or event sourcing - you must have a way to search and replay events as you will need to rebuild state in order to bring up new data stores and services. That may sound straightforward, but it&#x27;s actually really involved. You have to figure out how and where to store your events, how to serialize them, search them, play them back, and how&#x2F;when&#x2F;if to prune, delete or archive them.<p>Rather than spending a ton of money on building such a replay platform in-house, we decided to build a generic one and hopefully save everyone a bunch of time and money. We are 100% believers in &quot;buy&quot; (vs &quot;build&quot;) - companies should focus on building their core product and not waste time on sidequests. We&#x27;ve worked on these systems before at our previous gigs and decided to put our combined experience into building Batch.<p>A friend of mine shared this bit of insight with me (that he heard from Dave Cheney, I think?) - &quot;Is this what you want to spend your innovation tokens on?&quot; (referring to building something in-house) - and the answer is probably... no. So this is how we got here!<p>In practical terms, we give you a &quot;connector&quot; (in the form of a Docker image) that hooks into your messaging system as a consumer and begins copying all data that it sees on a topic&#x2F;exchange to Batch. Alternatively, you can pump data into our platform via a generic HTTP or gRPC API. Once the messages reach Batch, we index them and write them to a long-term store (we use <a href=\"https:&#x2F;&#x2F;www.elassandra.io\" rel=\"nofollow\">https:&#x2F;&#x2F;www.elassandra.io</a>). At that point, you can use either our UI or HTTP API to search and replay a subset of the messages to an HTTP destination or into another messaging system.<p>Right now, our platform is able to ingest data from Kafka, RabbitMQ and GCP PubSub, and we&#x27;ve got SQS on the roadmap. Really, we&#x27;re cool with adding support for whatever messaging system you need as long as it solves a problem for you.<p>One super cool thing is that if you are encoding your events in protobuf, we are able to decode them upon arrival on our platform, so that we can index them and let you search for data within them. In fact, we think this functionality is so cool that we really wanted to share it - surely there are other folks that need to quickly read&#x2F;write encoded data to various messaging systems. We wrote <a href=\"https:&#x2F;&#x2F;github.com&#x2F;batchcorp&#x2F;plumber\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;batchcorp&#x2F;plumber</a> for that purpose. It&#x27;s like curl for messaging systems and currently supports Kafka, RabbitMQ and GCP PubSub. It&#x27;s a port from an internal tool we used when interacting with our own Kafka and RabbitMQ instances.<p>In closing, we would love for you to check out <a href=\"https:&#x2F;&#x2F;batch.sh\" rel=\"nofollow\">https:&#x2F;&#x2F;batch.sh</a> and tell us what you think. Our initial thinking is to allow folks to pump their data into us for free with 1-3 days of retention. If you need more retention, that&#x27;ll require $ (we&#x27;re leaning towards a usage-based pricing model).<p>We envision Batch becoming a foundational component of your system architecture, but right now, our #1 goal is to lower the barrier to entry for event sourcing and we think that offering &quot;out-of-the-box&quot; replay functionality is the first step towards making this happen.<p>.. And if event sourcing is not your cup of tea - then you can get us in your stack to gain visibility and a peace of mind.<p>OK that&#x27;s it! Thank you for checking us out!<p>~Dan &amp; Ustin<p>P.S. Forgot about our creds:<p>I (Dan), spent a large chunk of my career working at data centers doing systems integration work. I got exposed to all kinds of esoteric things like how to integrate diesel generators into CMSs and automate VLAN provisioning for customers. I also learned that &quot;move fast and break things&quot; does not apply to data centers haha. After data centers, I went to work for New Relic, followed by InVision, Digital Ocean and most recently, Community (which is where I met Ustin). I work primarily in Go, consider myself a generalist, prefer light beers over IPAs and dabble in metal (music) production.<p>Ustin is a physicist turned computer scientist and worked towards a PhD on distributed storage over lossy networks. He has spent most of his career working as a founding engineer at startups like Community. He has a lot of experience working in Elixir and Go and working on large, complex systems.", "title": "Launch HN: Batch (YC S20) \u2013 Replays for event-driven systems", "updated_at": "2024-09-20T06:50:41Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "enether"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "The space is confusing to say the least.<p>Message queues are usually a core part of any distributed architecture, and the options are endless: Kafka, <em>RabbitMQ</em>, NATS, Redis Streams, SQS, ZeroMQ... and then there's the \u201cjust use Postgres\u201d camp for simpler use cases.<p>I\u2019m trying to make sense of the tradeoffs between:<p>- async fire-and-forget pub/sub vs. sync RPC-like point to point communication<p>- simple FIFO vs. priority queues and delay queues<p>- intelligent brokers (e.g. <em>RabbitMQ</em>, NATS with filters) vs. minimal brokers (e.g. Kafka\u2019s client-driven model)<p>There's also a fair amount of ideology/emotional attachment - some folks root for underdogs written in their favorite programming language, others reflexively dismiss anything that's not &quot;enterprise-grade&quot;. And of course, vendors are always in the mix trying to steer the conversation toward their own solution.<p>If you\u2019ve built a <em>production</em> system in the last few years:<p>1. What queue did you choose?<p>2. What didn't work out?<p>3. Where did you regret adding complexity?<p>4. And if you stuck with a DB-based queue \u2014 did it scale?<p>I\u2019d love to hear war stories, regrets, and opinions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: What's your go-to message queue in 2025?"}}, "_tags": ["story", "author_enether", "story_43993982", "ask_hn"], "author": "enether", "children": [43994032, 43996677, 43998178, 43998310, 43998576, 43998606, 44003001, 44003058, 44003712, 44006992, 44018500, 44019235, 44019252, 44019276, 44019291, 44019296, 44019303, 44019341, 44019347, 44019353, 44019361, 44019373, 44019403, 44019456, 44019457, 44019464, 44019470, 44019489, 44019630, 44019727, 44020161, 44020950, 44021272, 44021616, 44022087, 44024498, 44025183, 44030371, 44036233], "created_at": "2025-05-15T11:43:06Z", "created_at_i": 1747309386, "num_comments": 97, "objectID": "43993982", "points": 66, "story_id": 43993982, "story_text": "The space is confusing to say the least.<p>Message queues are usually a core part of any distributed architecture, and the options are endless: Kafka, RabbitMQ, NATS, Redis Streams, SQS, ZeroMQ... and then there&#x27;s the \u201cjust use Postgres\u201d camp for simpler use cases.<p>I\u2019m trying to make sense of the tradeoffs between:<p>- async fire-and-forget pub&#x2F;sub vs. sync RPC-like point to point communication<p>- simple FIFO vs. priority queues and delay queues<p>- intelligent brokers (e.g. RabbitMQ, NATS with filters) vs. minimal brokers (e.g. Kafka\u2019s client-driven model)<p>There&#x27;s also a fair amount of ideology&#x2F;emotional attachment - some folks root for underdogs written in their favorite programming language, others reflexively dismiss anything that&#x27;s not &quot;enterprise-grade&quot;. And of course, vendors are always in the mix trying to steer the conversation toward their own solution.<p>If you\u2019ve built a production system in the last few years:<p>1. What queue did you choose?<p>2. What didn&#x27;t work out?<p>3. Where did you regret adding complexity?<p>4. And if you stuck with a DB-based queue \u2014 did it scale?<p>I\u2019d love to hear war stories, regrets, and opinions.", "title": "Ask HN: What's your go-to message queue in 2025?", "updated_at": "2026-02-21T12:44:22Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "millchristian"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "SoundCloud is the world\u2019s leading social audio platform, allowing everyone to discover unique content anytime, anywhere. We enrich people's lives through the shared love of sound.<p>We reach a rapidly growing global user-base and are constantly pushing the boundaries of creativity and technology, on web and mobile applications.\nWe\u2019re looking for someone passionate about development tools, techniques and methodologies, to help build the SoundCloud platform.<p>With hacker time and tailored professional development, you\u2019ll grow alongside industry leaders while helping to shape the future of audio on the web.<p>Your key responsibilities:<p>Work on a growing set of back-end services for internal and external consumption, from product definition to <em>production</em> and maintenance\nOptimize the backend of existing features for performance and stability\nFind and resolve issues with the platform in close communication with our users\nRequired skills and experience:<p>You need to have solid experience in at least two languages like Scala, Clojure, Go or Ruby.\nExtensive experience with systems integration using HTTP and queues (<em>RabbitMQ</em> and ProtocolBuffers are a plus)\nGood understanding of storage systems, like MySQL and Cassandra, including complex queries and optimization\nExperience in software design techniques, Test-Driven Development and distributed architectures\nBonus:<p>Experience developing popular public APIs\nMRI/JVM tuning and profiling<p>Please apply at : http://soundcloud.com/jobs/2014-11-18-engineer-backend-new-york-city-ny-united-states"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "SoundCloud-NYC: Engineer,Backend"}, "url": {"matchLevel": "none", "matchedWords": [], "value": ""}}, "_tags": ["story", "author_millchristian", "story_8630464", "ask_hn"], "author": "millchristian", "created_at": "2014-11-19T15:20:35Z", "created_at_i": 1416410435, "num_comments": 0, "objectID": "8630464", "points": 3, "story_id": 8630464, "story_text": "SoundCloud is the world\u2019s leading social audio platform, allowing everyone to discover unique content anytime, anywhere. We enrich people&#x27;s lives through the shared love of sound.<p>We reach a rapidly growing global user-base and are constantly pushing the boundaries of creativity and technology, on web and mobile applications.\nWe\u2019re looking for someone passionate about development tools, techniques and methodologies, to help build the SoundCloud platform.<p>With hacker time and tailored professional development, you\u2019ll grow alongside industry leaders while helping to shape the future of audio on the web.<p>Your key responsibilities:<p>Work on a growing set of back-end services for internal and external consumption, from product definition to production and maintenance\nOptimize the backend of existing features for performance and stability\nFind and resolve issues with the platform in close communication with our users\nRequired skills and experience:<p>You need to have solid experience in at least two languages like Scala, Clojure, Go or Ruby.\nExtensive experience with systems integration using HTTP and queues (RabbitMQ and ProtocolBuffers are a plus)\nGood understanding of storage systems, like MySQL and Cassandra, including complex queries and optimization\nExperience in software design techniques, Test-Driven Development and distributed architectures\nBonus:<p>Experience developing popular public APIs\nMRI&#x2F;JVM tuning and profiling<p>Please apply at : http:&#x2F;&#x2F;soundcloud.com&#x2F;jobs&#x2F;2014-11-18-engineer-backend-new-york-city-ny-united-states", "title": "SoundCloud-NYC: Engineer,Backend", "updated_at": "2024-09-19T21:16:59Z", "url": ""}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "didierbreedt"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "We've spent the last year building RunOS, a platform that spins up <em>production</em>-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu<p>- Solid Go bindings via libvirt<p>- Excellent GPU passthrough for AI workloads like Ollama<p>- Good isolation/performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;<p>2. Backend selects available server agents<p>3. gRPC commands sent to provision VMs<p>4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)<p>5. Node agents install and connect<p>6. Kubernetes bootstrap with kubeadm + Cilium<p>7. WireGuard mesh established between nodes<p>8. Storage configured (OpenEBS + Longhorn)<p>9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access<p>- Nodes communicate securely even if Kubernetes fails<p>- Simpler troubleshooting with separated layers<p>- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>The platform supports one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, <em>RabbitMQ</em>, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>Managed option: Dedicated servers with fixed 8 CPU/16GB instances. KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it's early access.<p>Self-hosted option: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Working on: Self-managed VM hosts with custom sizing.<p>What's Next<p>The agent code will be open source eventually. One company runs three <em>production</em> clusters already. Common feedback: &quot;I can't believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We're planning weekly updates here on HackerNews about new features, technical challenges, and <em>production</em> lessons learned building RunOS.<p>Questions? Happy to discuss architecture in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC"}}, "_tags": ["story", "author_didierbreedt", "story_45936611", "show_hn"], "author": "didierbreedt", "children": [45940027], "created_at": "2025-11-15T11:01:12Z", "created_at_i": 1763204472, "num_comments": 2, "objectID": "45936611", "points": 2, "story_id": 45936611, "story_text": "We&#x27;ve spent the last year building RunOS, a platform that spins up production-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu<p>- Solid Go bindings via libvirt<p>- Excellent GPU passthrough for AI workloads like Ollama<p>- Good isolation&#x2F;performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;<p>2. Backend selects available server agents<p>3. gRPC commands sent to provision VMs<p>4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)<p>5. Node agents install and connect<p>6. Kubernetes bootstrap with kubeadm + Cilium<p>7. WireGuard mesh established between nodes<p>8. Storage configured (OpenEBS + Longhorn)<p>9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access<p>- Nodes communicate securely even if Kubernetes fails<p>- Simpler troubleshooting with separated layers<p>- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>The platform supports one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>Managed option: Dedicated servers with fixed 8 CPU&#x2F;16GB instances. KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it&#x27;s early access.<p>Self-hosted option: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Working on: Self-managed VM hosts with custom sizing.<p>What&#x27;s Next<p>The agent code will be open source eventually. One company runs three production clusters already. Common feedback: &quot;I can&#x27;t believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We&#x27;re planning weekly updates here on HackerNews about new features, technical challenges, and production lessons learned building RunOS.<p>Questions? Happy to discuss architecture in the comments.", "title": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC", "updated_at": "2025-11-16T05:21:40Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "aitoehigie"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "There are a myriad of message queue systems out there e.g. <em>RabbitMQ</em>, ZeroMQ, RestMS, etc. I would like to know which one HN users might be using in <em>production</em> and would recommend?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: what message queue should I use?"}, "url": {"matchLevel": "none", "matchedWords": [], "value": ""}}, "_tags": ["story", "author_aitoehigie", "story_1912425", "ask_hn"], "author": "aitoehigie", "children": [1912467, 1912471], "created_at": "2010-11-17T00:47:28Z", "created_at_i": 1289954848, "num_comments": 2, "objectID": "1912425", "points": 2, "story_id": 1912425, "story_text": "There are a myriad of message queue systems out there e.g. RabbitMQ, ZeroMQ, RestMS, etc. I would like to know which one HN users might be using in production and would recommend?", "title": "Ask HN: what message queue should I use?", "updated_at": "2024-09-19T17:18:34Z", "url": ""}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "sdevonoes"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "Not sure how to explain it, but I'll write down a couple of examples to show what I mean.<p>1. Developer X knows how to write decent code. Readability, maintainability, dependency injection, good patterns, solid principles, etc.; his code is easy to read and elegant, and more importantly it works as expected (at least based on the tests on staging environments). Now, the code goes to <em>production</em> and fails because: the number of, let's say, customers the code was loading into memory was just too much for the server to handle (the code was using a simple `for` loop + array variable to keep the customers). The server's memory was exhausted. One solution was to use <em>rabbitmq</em> instead of a simple array.<p>2. Developer X writes a `select` query to perform a search. The query works fine and is fast... but in <em>production</em> it takes much more time than expected (because the database has to handle load coming from other parts of the system as well and because the tables in <em>production</em> are way bigger than on dev or staging). Besides, the `select` query wasn't being run against the slave database, but against master. Developer X didn't think about these two topics when writing the query.<p>Basically, developer X has read all the books that appear in Google when one types `Top 10 Books That Every Programmer Must Read` (and more), and has a solide background in theoretical computer science (networking/databases/operating systems/compilers/automata theory), but has a hard time dealing with so-called &quot;common&quot; patterns/solutions in the real world (master/slave replication, async task processing via queues, issues that appear in <em>production</em> due to a huge amount of data, etc.)<p>Developer X is learning the &quot;hard way&quot; (through experience at his work place), but he finds this way of learning really slow (that's why he read all the top +10 software engineer books in the past).<p>How can developer X learn faster the topics he lacks experience with? Any really good books out there?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: How to fill this knowlegde gap?"}}, "_tags": ["story", "author_sdevonoes", "story_24689554", "ask_hn"], "author": "sdevonoes", "children": [24689658], "created_at": "2020-10-05T16:58:23Z", "created_at_i": 1601917103, "num_comments": 1, "objectID": "24689554", "points": 2, "story_id": 24689554, "story_text": "Not sure how to explain it, but I&#x27;ll write down a couple of examples to show what I mean.<p>1. Developer X knows how to write decent code. Readability, maintainability, dependency injection, good patterns, solid principles, etc.; his code is easy to read and elegant, and more importantly it works as expected (at least based on the tests on staging environments). Now, the code goes to production and fails because: the number of, let&#x27;s say, customers the code was loading into memory was just too much for the server to handle (the code was using a simple `for` loop + array variable to keep the customers). The server&#x27;s memory was exhausted. One solution was to use rabbitmq instead of a simple array.<p>2. Developer X writes a `select` query to perform a search. The query works fine and is fast... but in production it takes much more time than expected (because the database has to handle load coming from other parts of the system as well and because the tables in production are way bigger than on dev or staging). Besides, the `select` query wasn&#x27;t being run against the slave database, but against master. Developer X didn&#x27;t think about these two topics when writing the query.<p>Basically, developer X has read all the books that appear in Google when one types `Top 10 Books That Every Programmer Must Read` (and more), and has a solide background in theoretical computer science (networking&#x2F;databases&#x2F;operating systems&#x2F;compilers&#x2F;automata theory), but has a hard time dealing with so-called &quot;common&quot; patterns&#x2F;solutions in the real world (master&#x2F;slave replication, async task processing via queues, issues that appear in production due to a huge amount of data, etc.)<p>Developer X is learning the &quot;hard way&quot; (through experience at his work place), but he finds this way of learning really slow (that&#x27;s why he read all the top +10 software engineer books in the past).<p>How can developer X learn faster the topics he lacks experience with? Any really good books out there?", "title": "Ask HN: How to fill this knowlegde gap?", "updated_at": "2024-09-20T07:10:14Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "dib85"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["rabbitmq", "production"], "value": "We've spent the last year building RunOS, a platform that spins up <em>production</em>-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu\n- Solid Go bindings via libvirt\n- Excellent GPU passthrough for AI workloads like Ollama\n- Good isolation/performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;\n2. Backend selects available server agents\n3. gRPC commands sent to provision VMs\n4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)\n5. Node agents install and connect\n6. Kubernetes bootstrap with kubeadm + Cilium\n7. WireGuard mesh established between nodes\n8. Storage configured (OpenEBS + Longhorn)\n9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access\n- Nodes communicate securely even if Kubernetes fails\n- Simpler troubleshooting with separated layers\n- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>We offer one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, <em>RabbitMQ</em>, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>RunOS Cloud: Managed dedicated servers with fixed 8 CPU/16GB instances (free trial credits available). KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it's early access.<p>Bring Your Own Node: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Coming soon: Self-managed VM hosts with custom sizing.<p>What's Next<p>Agent code will be open source. One company runs three <em>production</em> clusters already. Common feedback: &quot;I can't believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We're planning weekly updates here on HackerNews about new features, technical challenges, and <em>production</em> lessons.<p>Try it at runos.com - free trial credits for 8 CPU threads and 16GB memory.<p>Questions? Happy to discuss architecture in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC"}}, "_tags": ["story", "author_dib85", "story_45927988", "show_hn"], "author": "dib85", "created_at": "2025-11-14T15:50:14Z", "created_at_i": 1763135414, "num_comments": 0, "objectID": "45927988", "points": 2, "story_id": 45927988, "story_text": "We&#x27;ve spent the last year building RunOS, a platform that spins up production-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu\n- Solid Go bindings via libvirt\n- Excellent GPU passthrough for AI workloads like Ollama\n- Good isolation&#x2F;performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;\n2. Backend selects available server agents\n3. gRPC commands sent to provision VMs\n4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)\n5. Node agents install and connect\n6. Kubernetes bootstrap with kubeadm + Cilium\n7. WireGuard mesh established between nodes\n8. Storage configured (OpenEBS + Longhorn)\n9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access\n- Nodes communicate securely even if Kubernetes fails\n- Simpler troubleshooting with separated layers\n- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>We offer one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>RunOS Cloud: Managed dedicated servers with fixed 8 CPU&#x2F;16GB instances (free trial credits available). KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it&#x27;s early access.<p>Bring Your Own Node: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Coming soon: Self-managed VM hosts with custom sizing.<p>What&#x27;s Next<p>Agent code will be open source. One company runs three production clusters already. Common feedback: &quot;I can&#x27;t believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We&#x27;re planning weekly updates here on HackerNews about new features, technical challenges, and production lessons.<p>Try it at runos.com - free trial credits for 8 CPU threads and 16GB memory.<p>Questions? Happy to discuss architecture in the comments.", "title": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC", "updated_at": "2025-11-14T18:11:05Z"}], "hitsPerPage": 15, "nbHits": 13, "nbPages": 1, "page": 0, "params": "query=rabbitmq+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 13, "processingTimingsMS": {"_request": {"roundTrip": 15}, "afterFetch": {"format": {"highlighting": 1, "total": 1}, "merge": {"total": 1}, "total": 1}, "fetch": {"query": 8, "scanning": 1, "total": 10}, "total": 13}, "query": "rabbitmq production", "serverTimeMS": 15}}