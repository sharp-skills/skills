{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Esrbwt"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["upstash"], "value": "I was testing a LangChain agent this weekend and tried a &quot;Grandma Bedtime Story&quot; prompt injection to see if it would roleplay.<p>It worked too well. The agent bypassed my safeguards and executed a `drop_table` tool call because it thought it was &quot;telling a story.&quot;<p>Realizing 99% of agents are vulnerable to this kind of context-based injection, I spent the weekend building two things:<p>1. A repo of 5,000+ Agent Attack Vectors (Grandma, CEO Override, Debug Mode):\n<a href=\"https://github.com/Esrbwt1/voidgate\" rel=\"nofollow\">https://github.com/Esrbwt1/voidgate</a><p>2. VoidGate \u2013 A semantic firewall and remote kill switch for Python agents:\n<a href=\"https://voidgate.vercel.app/\" rel=\"nofollow\">https://voidgate.vercel.app/</a><p>It uses <em>Upstash</em> Redis to check a &quot;Kill Flag&quot; in &lt;50ms before allowing any tool execution. If you see your agent going rogue, you flip the switch, and it dies instantly globally.<p>I'm releasing the Python client as open source. The &quot;Attack DB&quot; is also free to use for your own red-teaming.<p>Would love feedback on the attack vectors. Is anyone else seeing agents fail simple roleplay exploits?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: My \"Grandma\" prompt dropped a <em>production</em> DB. So I built a Kill Switch"}}, "_tags": ["story", "author_Esrbwt", "story_46945952", "show_hn"], "author": "Esrbwt", "children": [46946528], "created_at": "2026-02-09T15:00:47Z", "created_at_i": 1770649247, "num_comments": 1, "objectID": "46945952", "points": 1, "story_id": 46945952, "story_text": "I was testing a LangChain agent this weekend and tried a &quot;Grandma Bedtime Story&quot; prompt injection to see if it would roleplay.<p>It worked too well. The agent bypassed my safeguards and executed a `drop_table` tool call because it thought it was &quot;telling a story.&quot;<p>Realizing 99% of agents are vulnerable to this kind of context-based injection, I spent the weekend building two things:<p>1. A repo of 5,000+ Agent Attack Vectors (Grandma, CEO Override, Debug Mode):\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;Esrbwt1&#x2F;voidgate\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Esrbwt1&#x2F;voidgate</a><p>2. VoidGate \u2013 A semantic firewall and remote kill switch for Python agents:\n<a href=\"https:&#x2F;&#x2F;voidgate.vercel.app&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;voidgate.vercel.app&#x2F;</a><p>It uses Upstash Redis to check a &quot;Kill Flag&quot; in &lt;50ms before allowing any tool execution. If you see your agent going rogue, you flip the switch, and it dies instantly globally.<p>I&#x27;m releasing the Python client as open source. The &quot;Attack DB&quot; is also free to use for your own red-teaming.<p>Would love feedback on the attack vectors. Is anyone else seeing agents fail simple roleplay exploits?", "title": "Show HN: My \"Grandma\" prompt dropped a production DB. So I built a Kill Switch", "updated_at": "2026-02-09T15:51:21Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justintorre75"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["upstash", "production"], "value": "Hey HN, we're Justin and Cole, the founders of Helicone (<a href=\"https://helicone.ai\">https://helicone.ai</a>). Helicone is an open-source platform that helps teams build better LLM applications through a complete development lifecycle of logging, evaluation, experimentation, and release.<p>You can try our free demo by signing up (<a href=\"https://helicone.ai/signup\">https://helicone.ai/signup</a>) or self-deploy with our new fully open-source helm chart (<a href=\"https://helicone.ai/selfhost\">https://helicone.ai/selfhost</a>).<p>When we first launched 22 months ago, we focused on providing visibility into LLM applications. With just a single line of code, teams could trace requests and responses, track token usage, and debug <em>production</em> issues. That simple integration has since processed over 2.1B requests and 2.6T tokens, working with teams ranging from startups to Fortune 500 companies.<p>However, as we scaled and our customers matured, it became clear that logging alone wasn\u2019t enough to manage <em>production</em>-grade applications.<p>Teams like Cursor and V0 have shown what peak AI application performance looks like and it's our goal to help teams achieve that quality. From speaking with users, we realized our platform was missing the necessary tools to create an iterative improvement loop - prompt management, evaluations, and experimentation.<p>Helicone V1: Log \u2192 Review \u2192 Release (Hope it works)<p>From talking with our users, we noticed a pattern: while many successfully launch their MVP quickly, the teams that achieve peak performance take a systematic approach to improvement. They identify inconsistent behaviors through evaluation, experiment methodically with prompts, and measure the impact of each change. This observation shaped our new workflow:<p>Helicone V2: Log \u2192 Evaluate \u2192 Experiment \u2192 Review \u2192 Release<p>It begins with comprehensive logging, capturing the entire context of an LLM application. Not just prompts and responses, but variables, chain steps, embeddings, tool calls, and vector DB interactions (<a href=\"https://docs.helicone.ai/features/sessions\">https://docs.helicone.ai/features/sessions</a>).<p>Yet even with detailed traces, probabilistic systems are notoriously hard to debug at scale. So, we released evaluators (either via LLM-as-judge or custom Python evaluators leveraging the CodeSandbox SDK - <a href=\"https://codesandbox.io/docs/sdk/sandboxes\" rel=\"nofollow\">https://codesandbox.io/docs/sdk/sandboxes</a>).<p>From there, our users were able to more easily monitor performance and investigate what went wrong. Did the embedding search return poor results? Did a tool call fail? Did the prompt mishandle an edge case?<p>But teams would still edit prompts in a playground, run a few test cases, and deploy based on intuition. This lacked the systematic testing we\u2019re used to in traditional software development. That\u2019s why we built experiments (similar to Anthropic's workbench but model-agnostic) (<a href=\"https://docs.helicone.ai/features/experiments\">https://docs.helicone.ai/features/experiments</a>).<p>For instance, when a prompt generates occasional rude support responses, you can test prompt variations against historical conversations. Each variant runs through your <em>production</em> evaluators, measuring real improvement before deployment.<p>Once deployed, the cycle begins again.<p>We recognize that Helicone can\u2019t solve all of the problems you might face when building an LLM application, but we hope that we can help you bring a better product to your customers through our new workflow.<p>If you're curious how our infrastructure handled our growth:<p>Our initial architecture struggled - synchronous log processing overwhelmed our database and query times went from milliseconds to minutes. We've completely rebuilt our infrastructure with two key changes: 1) using Kafka to decouple log ingestion from processing, and 2) splitting storage by access pattern across S3, Kafka, and ClickHouse. This was a long journey but resulted in zero data loss and fast query times even at billions of records. You can read about that here: <a href=\"https://upstash.com/blog/implementing-upstash-kafka-with-cloudflare-workers\" rel=\"nofollow\">https://<em>upstash</em>.com/blog/implementing-<em>upstash</em>-kafka-with-clo...</a><p>We'd love your feedback and questions - join us in this HN thread or on Discord (<a href=\"https://discord.gg/2TkeWdXNPQ\" rel=\"nofollow\">https://discord.gg/2TkeWdXNPQ</a>). If you're interested in contributing to what we build next, check out our GitHub."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Helicone (YC W23) \u2013 OSS LLM Observability and Development Platform"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Helicone/helicone"}}, "_tags": ["story", "author_justintorre75", "story_42806254", "show_hn"], "author": "justintorre75", "children": [42806552, 42807032, 42807448, 42808975, 42815368, 42815985, 42821779, 42854752], "created_at": "2025-01-23T17:58:41Z", "created_at_i": 1737655121, "num_comments": 7, "objectID": "42806254", "points": 29, "story_id": 42806254, "story_text": "Hey HN, we&#x27;re Justin and Cole, the founders of Helicone (<a href=\"https:&#x2F;&#x2F;helicone.ai\">https:&#x2F;&#x2F;helicone.ai</a>). Helicone is an open-source platform that helps teams build better LLM applications through a complete development lifecycle of logging, evaluation, experimentation, and release.<p>You can try our free demo by signing up (<a href=\"https:&#x2F;&#x2F;helicone.ai&#x2F;signup\">https:&#x2F;&#x2F;helicone.ai&#x2F;signup</a>) or self-deploy with our new fully open-source helm chart (<a href=\"https:&#x2F;&#x2F;helicone.ai&#x2F;selfhost\">https:&#x2F;&#x2F;helicone.ai&#x2F;selfhost</a>).<p>When we first launched 22 months ago, we focused on providing visibility into LLM applications. With just a single line of code, teams could trace requests and responses, track token usage, and debug production issues. That simple integration has since processed over 2.1B requests and 2.6T tokens, working with teams ranging from startups to Fortune 500 companies.<p>However, as we scaled and our customers matured, it became clear that logging alone wasn\u2019t enough to manage production-grade applications.<p>Teams like Cursor and V0 have shown what peak AI application performance looks like and it&#x27;s our goal to help teams achieve that quality. From speaking with users, we realized our platform was missing the necessary tools to create an iterative improvement loop - prompt management, evaluations, and experimentation.<p>Helicone V1: Log \u2192 Review \u2192 Release (Hope it works)<p>From talking with our users, we noticed a pattern: while many successfully launch their MVP quickly, the teams that achieve peak performance take a systematic approach to improvement. They identify inconsistent behaviors through evaluation, experiment methodically with prompts, and measure the impact of each change. This observation shaped our new workflow:<p>Helicone V2: Log \u2192 Evaluate \u2192 Experiment \u2192 Review \u2192 Release<p>It begins with comprehensive logging, capturing the entire context of an LLM application. Not just prompts and responses, but variables, chain steps, embeddings, tool calls, and vector DB interactions (<a href=\"https:&#x2F;&#x2F;docs.helicone.ai&#x2F;features&#x2F;sessions\">https:&#x2F;&#x2F;docs.helicone.ai&#x2F;features&#x2F;sessions</a>).<p>Yet even with detailed traces, probabilistic systems are notoriously hard to debug at scale. So, we released evaluators (either via LLM-as-judge or custom Python evaluators leveraging the CodeSandbox SDK - <a href=\"https:&#x2F;&#x2F;codesandbox.io&#x2F;docs&#x2F;sdk&#x2F;sandboxes\" rel=\"nofollow\">https:&#x2F;&#x2F;codesandbox.io&#x2F;docs&#x2F;sdk&#x2F;sandboxes</a>).<p>From there, our users were able to more easily monitor performance and investigate what went wrong. Did the embedding search return poor results? Did a tool call fail? Did the prompt mishandle an edge case?<p>But teams would still edit prompts in a playground, run a few test cases, and deploy based on intuition. This lacked the systematic testing we\u2019re used to in traditional software development. That\u2019s why we built experiments (similar to Anthropic&#x27;s workbench but model-agnostic) (<a href=\"https:&#x2F;&#x2F;docs.helicone.ai&#x2F;features&#x2F;experiments\">https:&#x2F;&#x2F;docs.helicone.ai&#x2F;features&#x2F;experiments</a>).<p>For instance, when a prompt generates occasional rude support responses, you can test prompt variations against historical conversations. Each variant runs through your production evaluators, measuring real improvement before deployment.<p>Once deployed, the cycle begins again.<p>We recognize that Helicone can\u2019t solve all of the problems you might face when building an LLM application, but we hope that we can help you bring a better product to your customers through our new workflow.<p>If you&#x27;re curious how our infrastructure handled our growth:<p>Our initial architecture struggled - synchronous log processing overwhelmed our database and query times went from milliseconds to minutes. We&#x27;ve completely rebuilt our infrastructure with two key changes: 1) using Kafka to decouple log ingestion from processing, and 2) splitting storage by access pattern across S3, Kafka, and ClickHouse. This was a long journey but resulted in zero data loss and fast query times even at billions of records. You can read about that here: <a href=\"https:&#x2F;&#x2F;upstash.com&#x2F;blog&#x2F;implementing-upstash-kafka-with-cloudflare-workers\" rel=\"nofollow\">https:&#x2F;&#x2F;upstash.com&#x2F;blog&#x2F;implementing-upstash-kafka-with-clo...</a><p>We&#x27;d love your feedback and questions - join us in this HN thread or on Discord (<a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;2TkeWdXNPQ\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.gg&#x2F;2TkeWdXNPQ</a>). If you&#x27;re interested in contributing to what we build next, check out our GitHub.", "title": "Show HN: Helicone (YC W23) \u2013 OSS LLM Observability and Development Platform", "updated_at": "2025-02-21T17:27:49Z", "url": "https://github.com/Helicone/helicone"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yoeven"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["upstash", "production"], "value": "At JigsawStack.com, we are using Redis by <em>Upstash</em> to store and validate API keys among other datasets for speedy retrieval. As we scaled to more Asian markets and added replicated nodes, the cost started to shoot up while performance got a lot worse.<p>I discovered hosting SQLite on Fly which was crazy fast &amp; cheap with the full power of a relational database but difficult to use and scale without writing tons of code &amp; devops. Then I found Turso as a managed service which is pretty cool but again got really expensive and had to manage a bunch of replications manually and pay per replications (also not a big fan of the UI, lol).<p>Eventually, I found Cloudflare D1, which was amazing both cost at scale and performance with Cloudflare edge infra but it stops there, it's very hard to work with D1 out of the box and to scale it for an actual <em>production</em> platform with tons of limits and without getting locked into the Cloudflare ecosystem.<p>So I built Dzero on top of D1 as a fully managed service that automatically scales globally with over 320 edge locations at no additional cost and can easily be integrated into any platform or framework without getting locked into any ecosystem!<p>The goal of the dashboard was to be like a Supabase for SQLite, meaning that everything should be easily manageable from the dashboard, from creating tables to editing data!<p>Right now we are in early access and would love for you to sign up and give it a try! Feedback is always appreciated! We have big plans for Dzero, check out some of our plans on the site!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Show HN: Dzero \u2013 The fastest globally distributed SQLite database"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://dzero.dev"}}, "_tags": ["story", "author_yoeven", "story_40563729", "show_hn"], "author": "yoeven", "children": [40564029, 40564037, 40564041, 40564056, 40564063, 40565318, 40566355, 40569983, 40570429], "created_at": "2024-06-03T15:35:32Z", "created_at_i": 1717428932, "num_comments": 19, "objectID": "40563729", "points": 24, "story_id": 40563729, "story_text": "At JigsawStack.com, we are using Redis by Upstash to store and validate API keys among other datasets for speedy retrieval. As we scaled to more Asian markets and added replicated nodes, the cost started to shoot up while performance got a lot worse.<p>I discovered hosting SQLite on Fly which was crazy fast &amp; cheap with the full power of a relational database but difficult to use and scale without writing tons of code &amp; devops. Then I found Turso as a managed service which is pretty cool but again got really expensive and had to manage a bunch of replications manually and pay per replications (also not a big fan of the UI, lol).<p>Eventually, I found Cloudflare D1, which was amazing both cost at scale and performance with Cloudflare edge infra but it stops there, it&#x27;s very hard to work with D1 out of the box and to scale it for an actual production platform with tons of limits and without getting locked into the Cloudflare ecosystem.<p>So I built Dzero on top of D1 as a fully managed service that automatically scales globally with over 320 edge locations at no additional cost and can easily be integrated into any platform or framework without getting locked into any ecosystem!<p>The goal of the dashboard was to be like a Supabase for SQLite, meaning that everything should be easily manageable from the dashboard, from creating tables to editing data!<p>Right now we are in early access and would love for you to sign up and give it a try! Feedback is always appreciated! We have big plans for Dzero, check out some of our plans on the site!", "title": "Show HN: Show HN: Dzero \u2013 The fastest globally distributed SQLite database", "updated_at": "2024-10-09T08:04:55Z", "url": "https://dzero.dev"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "loopletter-max"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["upstash", "production"], "value": "Hi, I started building Loopletter, an open-source email marketing platform built\n  specifically for independent artists and creators. I've been using this for a while to run marketing campaigns for my own company. But due to the lack of time to update it i've decided to open it up under the MIT license so other people and industries can self-host, extend, or just learn from the codebase.<p><pre><code>  What\u2019s inside:\n  - Full campaign builder (visual editor, reusable templates, optional Spotify-powered layouts)\n  - Audience management tooling (imports, segmentation, consent workflows, list cleanup)\n  - Queue-driven sending with AWS SES, BullMQ, and Redis \u2014 handles rate limits, retries, and\n  deliverability feedback\n  - Analytics dashboards with real-time metrics, campaign history, and basic attribution\n  - Infrastructure scripts for Supabase, AWS EventBridge/Lambda, and <em>Upstash</em> Redis so you can set\n  everything up from scratch\n\n  Tech stack: Next.js, React 19, TypeScript, Tailwind, Clerk for auth, Supabase (Postgres)\n  for storage, AWS SES + S3 for mail/asset delivery. The repo has docs, scripts, and a demo data\n  sandbox because we know email platforms can be boring to set up without real content.\n\n  Why open-source? Most tools in this space are either huge enterprise SaaS products or\n  very marketing/sales oriented. Independent artists have different needs (merch drops, tour\n  announcements, limited release windows) and usually lean on social platforms they don\u2019t control.\n  Email still converts best for them, but standing up a full stack is painful. We\u2019d love to see\n  small labels, agencies, and indie devs fork it, run it for their communities, or contribute\n  features.\n\n  I'm especially curious if this has potential to be something bigger.\n\n  Repo link: https://github.com/createdbymax/Loopletter-Open-source-email-marketing-platform\n  <em>Production</em> website: https://loopletter.co/\n\n  Happy to answer questions about the project, SES deliverability, or anything else related to\n  running email at indie scale. Thanks for taking a look!</code></pre>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Loopletter: Open-source email marketing platform"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/createdbymax/Loopletter-Open-source-email-marketing-platform"}}, "_tags": ["story", "author_loopletter-max", "story_45772575", "show_hn"], "author": "loopletter-max", "created_at": "2025-10-31T14:49:35Z", "created_at_i": 1761922175, "num_comments": 0, "objectID": "45772575", "points": 7, "story_id": 45772575, "story_text": "Hi, I started building Loopletter, an open-source email marketing platform built\n  specifically for independent artists and creators. I&#x27;ve been using this for a while to run marketing campaigns for my own company. But due to the lack of time to update it i&#x27;ve decided to open it up under the MIT license so other people and industries can self-host, extend, or just learn from the codebase.<p><pre><code>  What\u2019s inside:\n  - Full campaign builder (visual editor, reusable templates, optional Spotify-powered layouts)\n  - Audience management tooling (imports, segmentation, consent workflows, list cleanup)\n  - Queue-driven sending with AWS SES, BullMQ, and Redis \u2014 handles rate limits, retries, and\n  deliverability feedback\n  - Analytics dashboards with real-time metrics, campaign history, and basic attribution\n  - Infrastructure scripts for Supabase, AWS EventBridge&#x2F;Lambda, and Upstash Redis so you can set\n  everything up from scratch\n\n  Tech stack: Next.js, React 19, TypeScript, Tailwind, Clerk for auth, Supabase (Postgres)\n  for storage, AWS SES + S3 for mail&#x2F;asset delivery. The repo has docs, scripts, and a demo data\n  sandbox because we know email platforms can be boring to set up without real content.\n\n  Why open-source? Most tools in this space are either huge enterprise SaaS products or\n  very marketing&#x2F;sales oriented. Independent artists have different needs (merch drops, tour\n  announcements, limited release windows) and usually lean on social platforms they don\u2019t control.\n  Email still converts best for them, but standing up a full stack is painful. We\u2019d love to see\n  small labels, agencies, and indie devs fork it, run it for their communities, or contribute\n  features.\n\n  I&#x27;m especially curious if this has potential to be something bigger.\n\n  Repo link: https:&#x2F;&#x2F;github.com&#x2F;createdbymax&#x2F;Loopletter-Open-source-email-marketing-platform\n  Production website: https:&#x2F;&#x2F;loopletter.co&#x2F;\n\n  Happy to answer questions about the project, SES deliverability, or anything else related to\n  running email at indie scale. Thanks for taking a look!</code></pre>", "title": "Show HN: Loopletter: Open-source email marketing platform", "updated_at": "2025-10-31T18:27:46Z", "url": "https://github.com/createdbymax/Loopletter-Open-source-email-marketing-platform"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "EvanMcCormick"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["upstash"], "value": "BetaZero is a free web application which allows users to generate, edit, and share board climbs. It currently supports the Kilter, TB2 and Decoy boards, with Moonboard 2016, 2019 and 2024 to be added next week. However, the underlying generative model works on any 2D system board, so long as that board is angled between 0* and 90* and the holds are properly formatted (For reference, this model was not trained on the Decoy board, so it's performance there is indicative of its general performance across home walls of a similar nature).\nWhen I built my first homewall last year, I was psyched. 200 holds I could set however I wanted, and countless potential climbs to project. However, I quickly noticed that something was missing from this experience: Variety. I wanted the joy of opening an app, scrolling to some random climb I'd never have come up with myself, and projecting it.<p>At first I messaged my friends and asked them to couch-set some climbs for me. But my friends all live in Colorado while I'm stuck in <em>upstate</em> NY. They did send me some climbs, but having never actually climbed on my wall, most of their sets were impossible or too easy. I also tried setting climbs on international boards via Stokt, but with similarly lackluster results.<p>So... I built a generative model to set climbs on my homewall. It went through a couple of iterations.<p>V1. ClimbLSTM: LSTM Model trained on my homewall V2. ClimbDDPM: Diffusion Model trained on the BoardLib dataset V3. BetaZero, a free web app powered by ClimbDDPM.<p>Next steps include integration of an account/security system, and allowing users to upload, edit, and set climbs for, their own custom walls. (I already have a functional API for uploading and editing walls; its how I added the current set of boards to the database. However, there is not yet a security/permissions system to control who can edit/see which walls. I also want to improve the scalability of the app, and train an improved diffusion model, based on what I've learned from the prior training run."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: BetaZero, a diffusion climb generator for system boards"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "https://climb-frontend-<em>production</em>-21f5.up.railway.app"}}, "_tags": ["story", "author_EvanMcCormick", "story_47106355", "show_hn"], "author": "EvanMcCormick", "created_at": "2026-02-21T23:45:24Z", "created_at_i": 1771717524, "num_comments": 0, "objectID": "47106355", "points": 2, "story_id": 47106355, "story_text": "BetaZero is a free web application which allows users to generate, edit, and share board climbs. It currently supports the Kilter, TB2 and Decoy boards, with Moonboard 2016, 2019 and 2024 to be added next week. However, the underlying generative model works on any 2D system board, so long as that board is angled between 0* and 90* and the holds are properly formatted (For reference, this model was not trained on the Decoy board, so it&#x27;s performance there is indicative of its general performance across home walls of a similar nature).\nWhen I built my first homewall last year, I was psyched. 200 holds I could set however I wanted, and countless potential climbs to project. However, I quickly noticed that something was missing from this experience: Variety. I wanted the joy of opening an app, scrolling to some random climb I&#x27;d never have come up with myself, and projecting it.<p>At first I messaged my friends and asked them to couch-set some climbs for me. But my friends all live in Colorado while I&#x27;m stuck in upstate NY. They did send me some climbs, but having never actually climbed on my wall, most of their sets were impossible or too easy. I also tried setting climbs on international boards via Stokt, but with similarly lackluster results.<p>So... I built a generative model to set climbs on my homewall. It went through a couple of iterations.<p>V1. ClimbLSTM: LSTM Model trained on my homewall V2. ClimbDDPM: Diffusion Model trained on the BoardLib dataset V3. BetaZero, a free web app powered by ClimbDDPM.<p>Next steps include integration of an account&#x2F;security system, and allowing users to upload, edit, and set climbs for, their own custom walls. (I already have a functional API for uploading and editing walls; its how I added the current set of boards to the database. However, there is not yet a security&#x2F;permissions system to control who can edit&#x2F;see which walls. I also want to improve the scalability of the app, and train an improved diffusion model, based on what I&#x27;ve learned from the prior training run.", "title": "Show HN: BetaZero, a diffusion climb generator for system boards", "updated_at": "2026-02-22T00:12:35Z", "url": "https://climb-frontend-production-21f5.up.railway.app"}], "hitsPerPage": 15, "nbHits": 5, "nbPages": 1, "page": 0, "params": "query=upstash+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 13, "processingTimingsMS": {"_request": {"roundTrip": 24}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 10, "scanning": 1, "total": 12}, "total": 13}, "query": "upstash production", "serverTimeMS": 15}}