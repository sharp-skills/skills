{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"fullyHighlighted": true, "matchLevel": "partial", "matchedWords": ["pydantic"], "value": "<em>pydantic</em>"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "<em>Pydantic</em>-AI-<em>production</em>-ready-template"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "https://github.com/m7mdhka/<em>pydantic</em>-ai-<em>production</em>-ready-template"}}, "_tags": ["story", "author_pydantic", "story_46111230"], "author": "pydantic", "children": [46111231, 46111261], "created_at": "2025-12-01T18:42:45Z", "created_at_i": 1764614565, "num_comments": 2, "objectID": "46111230", "points": 1, "story_id": 46111230, "title": "Pydantic-AI-production-ready-template", "updated_at": "2025-12-04T20:32:21Z", "url": "https://github.com/m7mdhka/pydantic-ai-production-ready-template"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Aherontas"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "Hey all!\nI recently gave a workshop talk at PyCon Greece 2025 about building <em>production</em>-ready agent systems.<p>To check the workshop, I put together a demo repo: (I will add the slides too soon in my blog: <a href=\"https://www.petrostechchronicles.com/\" rel=\"nofollow\">https://www.petrostechchronicles.com/</a>)\n<a href=\"https://github.com/Aherontas/Pycon_Greece_2025_Presentation_Agents\" rel=\"nofollow\">https://github.com/Aherontas/Pycon_Greece_2025_Presentation_...</a><p>The idea was to show how multiple AI agents can collaborate using FastAPI + <em>Pydantic</em>-AI, with protocols like MCP (Model Context Protocol) and A2A (Agent-to-Agent) for safe communication and orchestration.<p>Features:<p>- Multiple agents running in containers<p>- MCP servers (Brave search, GitHub, filesystem, etc.) as tools<p>- A2A communication between services<p>- Minimal UI for experimentation for Tech Trend - repo analysis<p>I built this repo because most agent frameworks look great in isolated demos, but fall apart when you try to glue agents together into a real application. My goal was to help people experiment with these patterns and move closer to real-world use cases.<p>It\u2019s not <em>production</em>-grade, but would love feedback, criticism, or war stories from anyone who\u2019s tried building actual multi-agent systems.\nBig questions:<p>Do you think agent-to-agent protocols like MCP/A2A will stick?<p>Or will the future be mostly single powerful LLMs with plugin stacks?<p>Thanks \u2014 excited to hear what the HN crowd thinks!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pydantic"], "value": "Show HN: AI-powered web service combining FastAPI, <em>Pydantic</em>-AI, and MCP servers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Aherontas/Pycon_Greece_2025_Presentation_Agents"}}, "_tags": ["story", "author_Aherontas", "story_45243320", "show_hn"], "author": "Aherontas", "children": [45128860, 45243325, 45253092, 45253248, 45253999, 45254599, 45257839, 45260699, 45288417, 45332320], "created_at": "2025-09-14T21:17:56Z", "created_at_i": 1757884676, "num_comments": 24, "objectID": "45243320", "points": 46, "story_id": 45243320, "story_text": "Hey all!\nI recently gave a workshop talk at PyCon Greece 2025 about building production-ready agent systems.<p>To check the workshop, I put together a demo repo: (I will add the slides too soon in my blog: <a href=\"https:&#x2F;&#x2F;www.petrostechchronicles.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.petrostechchronicles.com&#x2F;</a>)\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;Aherontas&#x2F;Pycon_Greece_2025_Presentation_Agents\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Aherontas&#x2F;Pycon_Greece_2025_Presentation_...</a><p>The idea was to show how multiple AI agents can collaborate using FastAPI + Pydantic-AI, with protocols like MCP (Model Context Protocol) and A2A (Agent-to-Agent) for safe communication and orchestration.<p>Features:<p>- Multiple agents running in containers<p>- MCP servers (Brave search, GitHub, filesystem, etc.) as tools<p>- A2A communication between services<p>- Minimal UI for experimentation for Tech Trend - repo analysis<p>I built this repo because most agent frameworks look great in isolated demos, but fall apart when you try to glue agents together into a real application. My goal was to help people experiment with these patterns and move closer to real-world use cases.<p>It\u2019s not production-grade, but would love feedback, criticism, or war stories from anyone who\u2019s tried building actual multi-agent systems.\nBig questions:<p>Do you think agent-to-agent protocols like MCP&#x2F;A2A will stick?<p>Or will the future be mostly single powerful LLMs with plugin stacks?<p>Thanks \u2014 excited to hear what the HN crowd thinks!", "title": "Show HN: AI-powered web service combining FastAPI, Pydantic-AI, and MCP servers", "updated_at": "2025-09-23T17:02:14Z", "url": "https://github.com/Aherontas/Pycon_Greece_2025_Presentation_Agents"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "larrykubin"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pydantic"], "value": "I'm currently evaluating Python frameworks for a new project. I've built a number of applications with Flask / Django over the years, but also recently discovered FastAPI, which is built on Starlette. I tried it out for a small hobby app locally and enjoyed the experience so far. I liked how you could define types with <em>Pydantic</em>, the dependency injection, and how it seemed a little easier to bootstrap and form a project structure  vs. flask. The auto API docs are great as well. Also the FastAPI documentation itself seemed very thorough and had great examples for authentication, docker images, boilerplates, etc. The project and docs feel mature.<p>That said, I don't see as much written about FastAPI and Starlette. I want to recommend it for a major project. Is there a large community around Starlette/FastAPI? Have you used it for large mission critical projects with many users? Would love to hear more about real world usage."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: What are your experiences with using Starlette or FastAPI in <em>production</em>?"}}, "_tags": ["story", "author_larrykubin", "story_22776339", "ask_hn"], "author": "larrykubin", "children": [22778862, 22779231, 22779949], "created_at": "2020-04-04T05:15:34Z", "created_at_i": 1585977334, "num_comments": 11, "objectID": "22776339", "points": 19, "story_id": 22776339, "story_text": "I&#x27;m currently evaluating Python frameworks for a new project. I&#x27;ve built a number of applications with Flask &#x2F; Django over the years, but also recently discovered FastAPI, which is built on Starlette. I tried it out for a small hobby app locally and enjoyed the experience so far. I liked how you could define types with Pydantic, the dependency injection, and how it seemed a little easier to bootstrap and form a project structure  vs. flask. The auto API docs are great as well. Also the FastAPI documentation itself seemed very thorough and had great examples for authentication, docker images, boilerplates, etc. The project and docs feel mature.<p>That said, I don&#x27;t see as much written about FastAPI and Starlette. I want to recommend it for a major project. Is there a large community around Starlette&#x2F;FastAPI? Have you used it for large mission critical projects with many users? Would love to hear more about real world usage.", "title": "Ask HN: What are your experiences with using Starlette or FastAPI in production?", "updated_at": "2026-01-25T12:39:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "samj1912"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "Show HN: CloudCoil \u2013 <em>Production</em>-ready Python client for the cloud-native ecosystem<p>I built CloudCoil (<a href=\"https://github.com/cloudcoil/cloudcoil\">https://github.com/cloudcoil/cloudcoil</a>) to make cloud-native development in Python feel first-class, starting with a modern async Kubernetes client. Frustrated with existing tools that felt like awkward ports from Go/Java, I focused on creating an API that Python developers would actually enjoy using.<p>Installation is as simple as:\n    uv add cloudcoil[kubernetes]  # Using uv (recommended)\n    pip install cloudcoil[kubernetes]  # Using pip<p>Key features:\n-  Elegant, truly Pythonic API that follows Python idioms\n-  Async-first with native async/await (but sync works too!)\n-  Full type safety with MyPy + <em>Pydantic</em>\n-  Zero-config pytest fixtures for K8s integration testing<p>Quick taste of the API:<p><pre><code>    # It's this simple to work with resources\n    service = k8s.core.v1.Service.get(&quot;kubernetes&quot;)\n\n    # Async iteration feels natural\n    async for pod in await k8s.core.v1.Pod.async_list():\n        print(f&quot;Found pod: {pod.metadata.name}&quot;)\n\n    # Create resources with pure Python syntax\n    deployment = k8s.apps.v1.Deployment(\n        metadata=dict(name=&quot;web&quot;),\n        spec=dict(replicas=3)\n    ).create()\n</code></pre>\nThe ecosystem is growing! We already have first-class integrations for:<p>- cert-manager (cloudcoil.models.cert_manager)\n- FluxCD (cloudcoil.models.fluxcd)\n- Kyverno (cloudcoil.models.kyverno)<p>Missing your favorite operator? I've made it super easy to add new integrations using our cookiecutter template and codegen tools.<p>I'd especially love feedback on:\n1. The API design - does it feel natural to Python devs?\n2. Testing features - what else would make k8s testing easier?\n3. Which operators/CRDs you'd most like to see integrated next<p>Check out <a href=\"https://github.com/cloudcoil/cloudcoil\">https://github.com/cloudcoil/cloudcoil</a> or try it out with PyPI: cloudcoil"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: CloudCoil \u2013 <em>Production</em>-ready Python client for cloud-native ecosystem"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/cloudcoil/cloudcoil"}}, "_tags": ["story", "author_samj1912", "story_42785596", "show_hn"], "author": "samj1912", "created_at": "2025-01-21T21:56:04Z", "created_at_i": 1737496564, "num_comments": 0, "objectID": "42785596", "points": 5, "story_id": 42785596, "story_text": "Show HN: CloudCoil \u2013 Production-ready Python client for the cloud-native ecosystem<p>I built CloudCoil (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;cloudcoil&#x2F;cloudcoil\">https:&#x2F;&#x2F;github.com&#x2F;cloudcoil&#x2F;cloudcoil</a>) to make cloud-native development in Python feel first-class, starting with a modern async Kubernetes client. Frustrated with existing tools that felt like awkward ports from Go&#x2F;Java, I focused on creating an API that Python developers would actually enjoy using.<p>Installation is as simple as:\n    uv add cloudcoil[kubernetes]  # Using uv (recommended)\n    pip install cloudcoil[kubernetes]  # Using pip<p>Key features:\n-  Elegant, truly Pythonic API that follows Python idioms\n-  Async-first with native async&#x2F;await (but sync works too!)\n-  Full type safety with MyPy + Pydantic\n-  Zero-config pytest fixtures for K8s integration testing<p>Quick taste of the API:<p><pre><code>    # It&#x27;s this simple to work with resources\n    service = k8s.core.v1.Service.get(&quot;kubernetes&quot;)\n\n    # Async iteration feels natural\n    async for pod in await k8s.core.v1.Pod.async_list():\n        print(f&quot;Found pod: {pod.metadata.name}&quot;)\n\n    # Create resources with pure Python syntax\n    deployment = k8s.apps.v1.Deployment(\n        metadata=dict(name=&quot;web&quot;),\n        spec=dict(replicas=3)\n    ).create()\n</code></pre>\nThe ecosystem is growing! We already have first-class integrations for:<p>- cert-manager (cloudcoil.models.cert_manager)\n- FluxCD (cloudcoil.models.fluxcd)\n- Kyverno (cloudcoil.models.kyverno)<p>Missing your favorite operator? I&#x27;ve made it super easy to add new integrations using our cookiecutter template and codegen tools.<p>I&#x27;d especially love feedback on:\n1. The API design - does it feel natural to Python devs?\n2. Testing features - what else would make k8s testing easier?\n3. Which operators&#x2F;CRDs you&#x27;d most like to see integrated next<p>Check out <a href=\"https:&#x2F;&#x2F;github.com&#x2F;cloudcoil&#x2F;cloudcoil\">https:&#x2F;&#x2F;github.com&#x2F;cloudcoil&#x2F;cloudcoil</a> or try it out with PyPI: cloudcoil", "title": "Show HN: CloudCoil \u2013 Production-ready Python client for cloud-native ecosystem", "updated_at": "2025-01-24T23:11:31Z", "url": "https://github.com/cloudcoil/cloudcoil"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "nbrad"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pydantic"], "value": "I\u2019m hoping for something battle-tested that supports async/await and migrations. If your answer is some variation of \u201creal programmers don\u2019t use ORMs\u201d... fair enough, I suppose. But here are the options as I see them:<p>- Django: very mature ORM, migration support is great. But it doesn\u2019t support async/await (https://docs.djangoproject.com/en/5.0/topics/async/), and these days FastAPI seems like a better option. FastAPI just needs an ORM, so =&gt;<p>- SQLModel: from @tiangolo who created FastAPI, clean and good <em>Pydantic</em> support, so this would be my default option, but even though it seems to support async/await the doc page is blank (https://sqlmodel.tiangolo.com/advanced/), and for migrations you resort to the underlying =&gt;<p>- SQLAlchemy: seems by far the largest/best supported, has async/await, has migrations through Alembic (it seems not as fully-featured as Django\u2019s?), but trying to use it has felt very kludgy/painful/verbose.<p>- Tortoise: the README claims to fulfill my dreams, but I haven\u2019t met anyone yet using it in prod? Anyone have experience they can share?<p>- Any others I missed?<p>Meanwhile in the JS/TS ecosystem, Prisma just added <i>preview</i> support for JOINs 2 weeks ago (https://github.com/prisma/prisma/releases/tag/5.7.0 !?!) and yet it seems ubiquitous (though I hear everyone\u2019s moving to Drizzle).<p>Advice greatly appreciated"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Any Python ORMs worthy of <em>production</em>?"}}, "_tags": ["story", "author_nbrad", "story_38689836", "ask_hn"], "author": "nbrad", "children": [38689939, 38690028, 38690114, 38690916, 38692665], "created_at": "2023-12-18T23:27:45Z", "created_at_i": 1702942065, "num_comments": 8, "objectID": "38689836", "points": 2, "story_id": 38689836, "story_text": "I\u2019m hoping for something battle-tested that supports async&#x2F;await and migrations. If your answer is some variation of \u201creal programmers don\u2019t use ORMs\u201d... fair enough, I suppose. But here are the options as I see them:<p>- Django: very mature ORM, migration support is great. But it doesn\u2019t support async&#x2F;await (https:&#x2F;&#x2F;docs.djangoproject.com&#x2F;en&#x2F;5.0&#x2F;topics&#x2F;async&#x2F;), and these days FastAPI seems like a better option. FastAPI just needs an ORM, so =&gt;<p>- SQLModel: from @tiangolo who created FastAPI, clean and good Pydantic support, so this would be my default option, but even though it seems to support async&#x2F;await the doc page is blank (https:&#x2F;&#x2F;sqlmodel.tiangolo.com&#x2F;advanced&#x2F;), and for migrations you resort to the underlying =&gt;<p>- SQLAlchemy: seems by far the largest&#x2F;best supported, has async&#x2F;await, has migrations through Alembic (it seems not as fully-featured as Django\u2019s?), but trying to use it has felt very kludgy&#x2F;painful&#x2F;verbose.<p>- Tortoise: the README claims to fulfill my dreams, but I haven\u2019t met anyone yet using it in prod? Anyone have experience they can share?<p>- Any others I missed?<p>Meanwhile in the JS&#x2F;TS ecosystem, Prisma just added <i>preview</i> support for JOINs 2 weeks ago (https:&#x2F;&#x2F;github.com&#x2F;prisma&#x2F;prisma&#x2F;releases&#x2F;tag&#x2F;5.7.0 !?!) and yet it seems ubiquitous (though I hear everyone\u2019s moving to Drizzle).<p>Advice greatly appreciated", "title": "Any Python ORMs worthy of production?", "updated_at": "2025-08-11T11:34:41Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "arizen"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pydantic"], "value": "Show HN: Open-source reference architecture for AI Agents (LangGraph, <em>Pydantic</em>)"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "https://github.com/ai-builders-group/build-<em>production</em>-ai-agents"}}, "_tags": ["story", "author_arizen", "story_46206479", "show_hn"], "author": "arizen", "children": [46206488], "created_at": "2025-12-09T16:03:58Z", "created_at_i": 1765296238, "num_comments": 1, "objectID": "46206479", "points": 2, "story_id": 46206479, "title": "Show HN: Open-source reference architecture for AI Agents (LangGraph, Pydantic)", "updated_at": "2025-12-12T14:02:20Z", "url": "https://github.com/ai-builders-group/build-production-ai-agents"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "codezerox"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "PydanticAI: Model-Agnostic, <em>Production</em>-Ready Agent Framework for LLM"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pydantic"], "value": "https://www.kifinity.com/github/<em>pydantic</em>/<em>pydantic</em>-ai"}}, "_tags": ["story", "author_codezerox", "story_42343447"], "author": "codezerox", "created_at": "2024-12-06T19:38:12Z", "created_at_i": 1733513892, "num_comments": 0, "objectID": "42343447", "points": 2, "story_id": 42343447, "title": "PydanticAI: Model-Agnostic, Production-Ready Agent Framework for LLM", "updated_at": "2024-12-06T19:56:24Z", "url": "https://www.kifinity.com/github/pydantic/pydantic-ai"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "openforgeai"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "I've been building a <em>production</em> SaaS (AI-native nurturing platform) as a solo engineer for the past year. No team, no funding \u2014 14 AI agent skills handling WhatsApp automation, payment\n   processing, CRM, webinar funnels, lifecycle management.<p><pre><code>  The biggest lesson: AI writing code is the easy part. The hard part is architecture that lets AI agents collaborate reliably. Without it, agents silently fail \u2014 no errors, no logs,\n  just events disappearing into the void.\n\n  I extracted the core patterns into OpenForgeAI:\n\n  - EventBus: Singleton pub/sub with auto-wiring (PascalCase event \u2192 on_snake_case handler convention)\n  - SagaCoordinator: Multi-step workflows with automatic compensation (rollback)\n  - Skill Protocol: Composable, idempotent agent skills with typed results\n  - ContractValidator: Catches broken agent wiring at deploy time, not runtime\n  - HandlerContext: Async-safe per-invocation context via ContextVar (multi-tenant safe)\n\n  The &quot;17 Laws&quot; came from <em>production</em> failures. Example: Law 4 says &quot;no orphan events&quot; \u2014 every emitted event must have a subscriber. I learned this when customer invoices silently stopped\n   sending because a skill emitted InvoiceGenerated but nothing consumed it. Zero errors logged.\n\n  Single dependency (<em>pydantic</em>). Python 3.10+.\n\n      pip install openforgeai\n\n  GitHub: https://github.com/openforgeai/openforgeai\n  Docs: https://github.com/openforgeai/openforgeai/blob/main/docs/architecture.md\n  The 17 Laws: https://github.com/openforgeai/openforgeai/blob/main/docs/17-laws.md\n\n  Happy to answer questions about the architecture or the experience of building <em>production</em> systems this way.</code></pre>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "OpenForgeAI \u2013 <em>Production</em> agentic architecture I used to build a SaaS alone"}}, "_tags": ["story", "author_openforgeai", "story_47069079", "ask_hn"], "author": "openforgeai", "children": [47069469], "created_at": "2026-02-19T02:15:04Z", "created_at_i": 1771467304, "num_comments": 1, "objectID": "47069079", "points": 1, "story_id": 47069079, "story_text": "I&#x27;ve been building a production SaaS (AI-native nurturing platform) as a solo engineer for the past year. No team, no funding \u2014 14 AI agent skills handling WhatsApp automation, payment\n   processing, CRM, webinar funnels, lifecycle management.<p><pre><code>  The biggest lesson: AI writing code is the easy part. The hard part is architecture that lets AI agents collaborate reliably. Without it, agents silently fail \u2014 no errors, no logs,\n  just events disappearing into the void.\n\n  I extracted the core patterns into OpenForgeAI:\n\n  - EventBus: Singleton pub&#x2F;sub with auto-wiring (PascalCase event \u2192 on_snake_case handler convention)\n  - SagaCoordinator: Multi-step workflows with automatic compensation (rollback)\n  - Skill Protocol: Composable, idempotent agent skills with typed results\n  - ContractValidator: Catches broken agent wiring at deploy time, not runtime\n  - HandlerContext: Async-safe per-invocation context via ContextVar (multi-tenant safe)\n\n  The &quot;17 Laws&quot; came from production failures. Example: Law 4 says &quot;no orphan events&quot; \u2014 every emitted event must have a subscriber. I learned this when customer invoices silently stopped\n   sending because a skill emitted InvoiceGenerated but nothing consumed it. Zero errors logged.\n\n  Single dependency (pydantic). Python 3.10+.\n\n      pip install openforgeai\n\n  GitHub: https:&#x2F;&#x2F;github.com&#x2F;openforgeai&#x2F;openforgeai\n  Docs: https:&#x2F;&#x2F;github.com&#x2F;openforgeai&#x2F;openforgeai&#x2F;blob&#x2F;main&#x2F;docs&#x2F;architecture.md\n  The 17 Laws: https:&#x2F;&#x2F;github.com&#x2F;openforgeai&#x2F;openforgeai&#x2F;blob&#x2F;main&#x2F;docs&#x2F;17-laws.md\n\n  Happy to answer questions about the architecture or the experience of building production systems this way.</code></pre>", "title": "OpenForgeAI \u2013 Production agentic architecture I used to build a SaaS alone", "updated_at": "2026-02-19T03:19:38Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "galsapir"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "We're evaluating agent frameworks for a health AI product and leaning toward Anthropic's Claude Agent SDK. Did a quick POC and liked the simplicity: clean @tool decorator, native MCP support, flat mental model.\nBut I'm finding fewer <em>production</em> case studies compared to LangGraph or similar. Curious about:<p>Multi-turn conversation handling, does it manage state well or do you thread history manually?\nLong-running tasks (minutes/hours), any gotchas with timeouts or checkpointing?\nThe latency overhead people mention (~12s per query per one github issue). is this still an issue or has it improved?\nGeneral <em>production</em> rough edges we should know about?<p>For context: most of our context is pre-computed, occasional JIT tool calls. Comparing against <em>Pydantic</em> AI and LangGraph but trying to avoid over-engineering.\nAppreciate any war stories."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: Anyone using Claude Agent SDK in <em>production</em>?"}}, "_tags": ["story", "author_galsapir", "story_46679473", "ask_hn"], "author": "galsapir", "children": [46740737], "created_at": "2026-01-19T14:38:31Z", "created_at_i": 1768833511, "num_comments": 1, "objectID": "46679473", "points": 1, "story_id": 46679473, "story_text": "We&#x27;re evaluating agent frameworks for a health AI product and leaning toward Anthropic&#x27;s Claude Agent SDK. Did a quick POC and liked the simplicity: clean @tool decorator, native MCP support, flat mental model.\nBut I&#x27;m finding fewer production case studies compared to LangGraph or similar. Curious about:<p>Multi-turn conversation handling, does it manage state well or do you thread history manually?\nLong-running tasks (minutes&#x2F;hours), any gotchas with timeouts or checkpointing?\nThe latency overhead people mention (~12s per query per one github issue). is this still an issue or has it improved?\nGeneral production rough edges we should know about?<p>For context: most of our context is pre-computed, occasional JIT tool calls. Comparing against Pydantic AI and LangGraph but trying to avoid over-engineering.\nAppreciate any war stories.", "title": "Ask HN: Anyone using Claude Agent SDK in production?", "updated_at": "2026-01-24T03:19:29Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bloppe"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "I've been working with the Featureform team on their new open-source project, [EnrichMCP][1], a Python ORM framework that helps AI agents understand and interact with your data in a structured, semantic way.<p>EnrichMCP is built on top of [MCP][2] and acts like an ORM, but for agents instead of humans. You define your data model using SQLAlchemy, APIs, or custom logic, and EnrichMCP turns it into a type-safe, introspectable interface that agents can discover, traverse, and invoke.<p>It auto-generates tools from your models, validates all I/O with <em>Pydantic</em>, handles relationships, and supports schema discovery. Agents can go from user \u2192 orders \u2192 product naturally, just like a developer navigating an ORM.<p>We use this internally to let agents query <em>production</em> systems, call APIs, apply business logic, and even integrate ML models. It works out of the box with SQLAlchemy and is easy to extend to any data source.<p>If you're building agentic systems or anything AI-native, I'd love your feedback. Code and docs are here: <a href=\"https://github.com/featureform/enrichmcp\">https://github.com/featureform/enrichmcp</a>. Happy to answer any questions.<p>[1]: <a href=\"https://github.com/featureform/enrichmcp\">https://github.com/featureform/enrichmcp</a><p>[2]: <a href=\"https://modelcontextprotocol.io/introduction\" rel=\"nofollow\">https://modelcontextprotocol.io/introduction</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: EnrichMCP \u2013 A Python ORM for Agents"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/featureform/enrichmcp"}}, "_tags": ["story", "author_bloppe", "story_44320772", "show_hn"], "author": "bloppe", "children": [44321352, 44321365, 44321614, 44321844, 44322851, 44324981, 44325537, 44326757], "created_at": "2025-06-19T17:32:21Z", "created_at_i": 1750354341, "num_comments": 33, "objectID": "44320772", "points": 133, "story_id": 44320772, "story_text": "I&#x27;ve been working with the Featureform team on their new open-source project, [EnrichMCP][1], a Python ORM framework that helps AI agents understand and interact with your data in a structured, semantic way.<p>EnrichMCP is built on top of [MCP][2] and acts like an ORM, but for agents instead of humans. You define your data model using SQLAlchemy, APIs, or custom logic, and EnrichMCP turns it into a type-safe, introspectable interface that agents can discover, traverse, and invoke.<p>It auto-generates tools from your models, validates all I&#x2F;O with Pydantic, handles relationships, and supports schema discovery. Agents can go from user \u2192 orders \u2192 product naturally, just like a developer navigating an ORM.<p>We use this internally to let agents query production systems, call APIs, apply business logic, and even integrate ML models. It works out of the box with SQLAlchemy and is easy to extend to any data source.<p>If you&#x27;re building agentic systems or anything AI-native, I&#x27;d love your feedback. Code and docs are here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;featureform&#x2F;enrichmcp\">https:&#x2F;&#x2F;github.com&#x2F;featureform&#x2F;enrichmcp</a>. Happy to answer any questions.<p>[1]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;featureform&#x2F;enrichmcp\">https:&#x2F;&#x2F;github.com&#x2F;featureform&#x2F;enrichmcp</a><p>[2]: <a href=\"https:&#x2F;&#x2F;modelcontextprotocol.io&#x2F;introduction\" rel=\"nofollow\">https:&#x2F;&#x2F;modelcontextprotocol.io&#x2F;introduction</a>", "title": "Show HN: EnrichMCP \u2013 A Python ORM for Agents", "updated_at": "2025-08-14T22:03:30Z", "url": "https://github.com/featureform/enrichmcp"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "itssimon"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "G\u2019day Hacker News, I\u2019m Simon Gurcke, the sole founder of Apitally (<a href=\"https://apitally.io\" rel=\"nofollow\">https://apitally.io</a>).<p>I\u2019m building a simple API monitoring and analytics tool for Python / Node.js apps. It helps users understand API usage and performance, spot issues early and troubleshoot effectively when something goes wrong.<p>Features include:<p>- <i>Dashboards:</i> Provide insights into API traffic, errors, performance and consumers.<p>- <i>Request logging:</i> Opt-in and highly configurable in terms of what data is logged. Users can drill down from aggregated metrics to individual requests (proven to be super helpful when troubleshooting issues).<p>- <i>Custom alerts:</i> Based on 14 different API metrics with notifications delivered via email, Slack or Microsoft Teams.<p>- <i>Validation error tracking:</i> Captures metrics about which fields failed validation and why. Works for web frameworks with built-in validation (e.g. FastAPI with <em>pydantic</em>), or that integrate with popular third-party validation libraries (e.g. Zod for Hono).<p>- <i>Server error tracking:</i> Captures exception details and stack traces for 500 error responses. An integration with the Sentry SDK also captures event IDs, allowing users to click through to the relevant Sentry issue for more context.<p>I first started developing Apitally to scratch my own itch. While working at a health tech company where I was responsible for API-based software products, I became frustrated with the monitoring tools we had in place - Datadog and the ELK stack. They were too complex for my API-centric use cases, and often a pain to use.<p>As a result, I focused on making Apitally as simple as possible. This involved not just refining the UX of the dashboard, but also optimizing the developer experience with the open-source SDKs:<p>- <a href=\"https://github.com/apitally/apitally-py\">https://github.com/apitally/apitally-py</a> - Python SDK (supports FastAPI, Flask, Django, Litestar, Starlette)<p>- <a href=\"https://github.com/apitally/apitally-js\">https://github.com/apitally/apitally-js</a> - Node.js SDK (supports Express, NestJS, Fastify, Koa, Hono)<p>My other focus was on data privacy, as that is a strict requirement in the healthcare industry. By default, Apitally doesn\u2019t capture any sensitive data - metrics are aggregated on the client side (similar to Prometheus) and sent in the background in regular intervals.<p>The hardest part has been implementing integrations for various web frameworks and supporting a wide range of versions. I learned a lot about the inner workings of web frameworks in the process. Good test coverage and an extensive test matrix were really important to not break people\u2019s <em>production</em> APIs with buggy middleware.<p>Apitally\u2019s backend is built in Python and runs on a small Kubernetes cluster on DigitalOcean. It uses PostgreSQL and ClickHouse to store data and NATS JetStream as a message queue. I chose NATS for being lightweight and its exactly-once processing capabilities. I\u2019m also impressed by ClickHouse\u2019s performance given the low hardware specs of my server (4 vCPUs, 8 GB RAM).<p>Apitally is free to use for small hobby projects (with limitations), and I offer two paid tiers for $39 and $119 (USD) per month. The dashboard has a demo mode, allowing people to explore the product without having to set up their own app first.<p>Thank you for reading about my bootstrapped indie product. Please let me know your thoughts and questions in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Apitally \u2013 A simple, privacy-focused API monitoring and analytics tool"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://apitally.io"}}, "_tags": ["story", "author_itssimon", "story_42915435", "show_hn"], "author": "itssimon", "children": [42915477, 42915683, 42929740, 42949349, 42949483, 42959486, 42960715], "created_at": "2025-02-03T06:07:00Z", "created_at_i": 1738562820, "num_comments": 21, "objectID": "42915435", "points": 46, "story_id": 42915435, "story_text": "G\u2019day Hacker News, I\u2019m Simon Gurcke, the sole founder of Apitally (<a href=\"https:&#x2F;&#x2F;apitally.io\" rel=\"nofollow\">https:&#x2F;&#x2F;apitally.io</a>).<p>I\u2019m building a simple API monitoring and analytics tool for Python &#x2F; Node.js apps. It helps users understand API usage and performance, spot issues early and troubleshoot effectively when something goes wrong.<p>Features include:<p>- <i>Dashboards:</i> Provide insights into API traffic, errors, performance and consumers.<p>- <i>Request logging:</i> Opt-in and highly configurable in terms of what data is logged. Users can drill down from aggregated metrics to individual requests (proven to be super helpful when troubleshooting issues).<p>- <i>Custom alerts:</i> Based on 14 different API metrics with notifications delivered via email, Slack or Microsoft Teams.<p>- <i>Validation error tracking:</i> Captures metrics about which fields failed validation and why. Works for web frameworks with built-in validation (e.g. FastAPI with pydantic), or that integrate with popular third-party validation libraries (e.g. Zod for Hono).<p>- <i>Server error tracking:</i> Captures exception details and stack traces for 500 error responses. An integration with the Sentry SDK also captures event IDs, allowing users to click through to the relevant Sentry issue for more context.<p>I first started developing Apitally to scratch my own itch. While working at a health tech company where I was responsible for API-based software products, I became frustrated with the monitoring tools we had in place - Datadog and the ELK stack. They were too complex for my API-centric use cases, and often a pain to use.<p>As a result, I focused on making Apitally as simple as possible. This involved not just refining the UX of the dashboard, but also optimizing the developer experience with the open-source SDKs:<p>- <a href=\"https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-py\">https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-py</a> - Python SDK (supports FastAPI, Flask, Django, Litestar, Starlette)<p>- <a href=\"https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-js\">https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-js</a> - Node.js SDK (supports Express, NestJS, Fastify, Koa, Hono)<p>My other focus was on data privacy, as that is a strict requirement in the healthcare industry. By default, Apitally doesn\u2019t capture any sensitive data - metrics are aggregated on the client side (similar to Prometheus) and sent in the background in regular intervals.<p>The hardest part has been implementing integrations for various web frameworks and supporting a wide range of versions. I learned a lot about the inner workings of web frameworks in the process. Good test coverage and an extensive test matrix were really important to not break people\u2019s production APIs with buggy middleware.<p>Apitally\u2019s backend is built in Python and runs on a small Kubernetes cluster on DigitalOcean. It uses PostgreSQL and ClickHouse to store data and NATS JetStream as a message queue. I chose NATS for being lightweight and its exactly-once processing capabilities. I\u2019m also impressed by ClickHouse\u2019s performance given the low hardware specs of my server (4 vCPUs, 8 GB RAM).<p>Apitally is free to use for small hobby projects (with limitations), and I offer two paid tiers for $39 and $119 (USD) per month. The dashboard has a demo mode, allowing people to explore the product without having to set up their own app first.<p>Thank you for reading about my bootstrapped indie product. Please let me know your thoughts and questions in the comments.", "title": "Show HN: Apitally \u2013 A simple, privacy-focused API monitoring and analytics tool", "updated_at": "2025-11-19T05:50:23Z", "url": "https://apitally.io"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "art049"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "Hi HN! We\u2019re Arthur and Adrien from CodSpeed. We\u2019re building a tool measuring software performance before any <em>production</em> deployment, catching performance regressions before they hit <em>production</em> environments and reporting performance changes directly in Pull Request comments. It\u2019s kind of like Codecov but for performance measurement.<p>Today, the go to solution to measure performance is probably to use an APM(DataDog, Sentry, \u2026), continuously analyzing your <em>production</em> environment. However, since those solutions are operating on real environments they need real users to experience poor performance in order to report issues and unfortunately, performance remains an afterthought appearing only at the end of the development cycle.<p>Another possibility to measure performance is to create benchmarks while developing and to run them on a regular basis to have an idea of the performance trend of your project. However, with this approach, the variance in the results creates a lot of noise and it\u2019s rarely possible to compare your results with the ones from a co-worker or a <em>production</em> environment.<p>To make consistent performance measurement as easy as unit testing and fully integrated in CI workflows, we chose a benchmark based solution. And, to eliminate the usual variance associated with running them, we measure the number of instructions and memory/cache accesses through CPU instrumentation performed with Valgrind. This approach gives repeatable and consistent results that couldn\u2019t be obtained with a time based statistical approach, especially in extremely noisy CI and cloud environments.<p>We have been in closed beta for a few months, already being used by popular open-source projects such as Prisma and <em>Pydantic</em>. Notably, CodSpeed helped <em>Pydantic</em> through their Rust migration, empowering them to make the library 17x faster: <a href=\"https://docs.pydantic.dev/latest/blog/pydantic-v2/#performance\" rel=\"nofollow noreferrer\">https://docs.<em>pydantic</em>.dev/latest/blog/<em>pydantic</em>-v2/#performan...</a><p>Today, we\u2019re super excited to finally make the product available to everyone. We currently support Python, Node.js and Rust and are looking forward to integrate with more languages soon.<p>The product is and will be free forever for open-source projects. Also, we have a per-seat pricing for private repository usage.\nWe have a lot of exciting features planned regarding additional integrations, such as Database and GPU integrations that should come in upcoming months.<p>Don\u2019t hesitate to try out the product and give your honest feedback. We\u2019re looking forward to your comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: CodSpeed \u2013 Continuous Performance Measurement"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://codspeed.io/"}}, "_tags": ["story", "author_art049", "story_36682012", "show_hn"], "author": "art049", "children": [36682013, 36682668, 36683693, 36715126], "created_at": "2023-07-11T15:02:20Z", "created_at_i": 1689087740, "num_comments": 3, "objectID": "36682012", "points": 32, "story_id": 36682012, "story_text": "Hi HN! We\u2019re Arthur and Adrien from CodSpeed. We\u2019re building a tool measuring software performance before any production deployment, catching performance regressions before they hit production environments and reporting performance changes directly in Pull Request comments. It\u2019s kind of like Codecov but for performance measurement.<p>Today, the go to solution to measure performance is probably to use an APM(DataDog, Sentry, \u2026), continuously analyzing your production environment. However, since those solutions are operating on real environments they need real users to experience poor performance in order to report issues and unfortunately, performance remains an afterthought appearing only at the end of the development cycle.<p>Another possibility to measure performance is to create benchmarks while developing and to run them on a regular basis to have an idea of the performance trend of your project. However, with this approach, the variance in the results creates a lot of noise and it\u2019s rarely possible to compare your results with the ones from a co-worker or a production environment.<p>To make consistent performance measurement as easy as unit testing and fully integrated in CI workflows, we chose a benchmark based solution. And, to eliminate the usual variance associated with running them, we measure the number of instructions and memory&#x2F;cache accesses through CPU instrumentation performed with Valgrind. This approach gives repeatable and consistent results that couldn\u2019t be obtained with a time based statistical approach, especially in extremely noisy CI and cloud environments.<p>We have been in closed beta for a few months, already being used by popular open-source projects such as Prisma and Pydantic. Notably, CodSpeed helped Pydantic through their Rust migration, empowering them to make the library 17x faster: <a href=\"https:&#x2F;&#x2F;docs.pydantic.dev&#x2F;latest&#x2F;blog&#x2F;pydantic-v2&#x2F;#performance\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;docs.pydantic.dev&#x2F;latest&#x2F;blog&#x2F;pydantic-v2&#x2F;#performan...</a><p>Today, we\u2019re super excited to finally make the product available to everyone. We currently support Python, Node.js and Rust and are looking forward to integrate with more languages soon.<p>The product is and will be free forever for open-source projects. Also, we have a per-seat pricing for private repository usage.\nWe have a lot of exciting features planned regarding additional integrations, such as Database and GPU integrations that should come in upcoming months.<p>Don\u2019t hesitate to try out the product and give your honest feedback. We\u2019re looking forward to your comments!", "title": "Show HN: CodSpeed \u2013 Continuous Performance Measurement", "updated_at": "2024-09-20T14:39:18Z", "url": "https://codspeed.io/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "damacaner"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "Because I think like that.<p>I tried Django, Flask, FastAPI till now, maybe there is other frameworks that is good enough, but I only had experience with &quot;<em>production</em>-ready&quot; frameworks.<p>And it feels... wrong? I don't know why, but it feels like something is wrong.<p>In Django, I barely ever feel like coding. It was just writing config files, like I didn't feel like I was coding.<p>Same in Flask and FastAPI. Development is so quick that I literally don't feel like coding. I mainly worked with GoLang till now, reinvented so many wheels, ventured into so many libraries, it felt &quot;fuck yeah I am codin we are ballin&quot;.<p>Python seems... too easy? You barely ever reinvent wheels, you have something in your head? Some freak made it a feature of FastAPI. &quot;hey caner, can we do th....&quot;, yes you fucking can I don't even need to hear your question because a dude created a feature you wanted in Starlette. It is madness.<p>Another example, you need to write lines of lines comments and markdown files for Swagger documentation in GoLang, FastAPI does that for you.<p>Validators? <em>Pydantic</em> holds your hand.<p>Yes, I am literally bickering about writing less code feels so wrong.<p>But am I the only one?<p>edit: I don't say fuck python and it's frameworks, I have tons of fun with it. and it pays the bills right now, so.... just a discuss and steam off post."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Do you think writing backend with Python feels wrong?"}}, "_tags": ["story", "author_damacaner", "story_36345700", "ask_hn"], "author": "damacaner", "children": [36345826, 36345930, 36345941, 36350538, 36359123, 36360682], "created_at": "2023-06-15T20:00:08Z", "created_at_i": 1686859208, "num_comments": 16, "objectID": "36345700", "points": 12, "story_id": 36345700, "story_text": "Because I think like that.<p>I tried Django, Flask, FastAPI till now, maybe there is other frameworks that is good enough, but I only had experience with &quot;production-ready&quot; frameworks.<p>And it feels... wrong? I don&#x27;t know why, but it feels like something is wrong.<p>In Django, I barely ever feel like coding. It was just writing config files, like I didn&#x27;t feel like I was coding.<p>Same in Flask and FastAPI. Development is so quick that I literally don&#x27;t feel like coding. I mainly worked with GoLang till now, reinvented so many wheels, ventured into so many libraries, it felt &quot;fuck yeah I am codin we are ballin&quot;.<p>Python seems... too easy? You barely ever reinvent wheels, you have something in your head? Some freak made it a feature of FastAPI. &quot;hey caner, can we do th....&quot;, yes you fucking can I don&#x27;t even need to hear your question because a dude created a feature you wanted in Starlette. It is madness.<p>Another example, you need to write lines of lines comments and markdown files for Swagger documentation in GoLang, FastAPI does that for you.<p>Validators? Pydantic holds your hand.<p>Yes, I am literally bickering about writing less code feels so wrong.<p>But am I the only one?<p>edit: I don&#x27;t say fuck python and it&#x27;s frameworks, I have tons of fun with it. and it pays the bills right now, so.... just a discuss and steam off post.", "title": "Do you think writing backend with Python feels wrong?", "updated_at": "2024-12-08T05:37:44Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "shashstormer"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "https://github.com/shashstormer/authtuna<p>Hey everyone, creator of AuthTuna here.<p>For years, I've been building complex, multi-tenant APIs with FastAPI, and I've always found that implementing robust, granular, and truly async security is a huge pain point. You either roll your own and risk vulnerabilities, or you wrestle with frameworks that aren't async-native, causing performance bottlenecks.<p>I built AuthTuna to solve this. It's the async-first security framework I always wanted:<p>Truly Async Core: Built on asyncio and SQLAlchemy 2.0. No part of your security logic will block the event loop.<p>Granular Hierarchical Permissions (RBAC): Go beyond simple roles. You can define permissions like Organization -&gt; Project -&gt; Resource and check them with a simple Depends(PermissionChecker(...)).<p>Advanced Server-Side Sessions: It provides the security of server-side sessions (with hijack detection) without sacrificing the performance you'd expect from JWTs.<p>Great Developer Experience: Comes with <em>Pydantic</em> models, pre-built routers for auth flows, and clear dependencies to get you started in minutes.<p>I use it in my own <em>production</em> systems, so it's been well tested. The goal is to make robust security the easy path, not an afterthought.<p>I'd love to hear your thoughts and get your feedback. What are you currently using to handle auth in your async Python projects?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "AuthTuna \u2013 A modern, async security framework for FastAPI"}}, "_tags": ["story", "author_shashstormer", "story_45249930", "ask_hn"], "author": "shashstormer", "children": [45250017], "created_at": "2025-09-15T14:06:57Z", "created_at_i": 1757945217, "num_comments": 1, "objectID": "45249930", "points": 3, "story_id": 45249930, "story_text": "https:&#x2F;&#x2F;github.com&#x2F;shashstormer&#x2F;authtuna<p>Hey everyone, creator of AuthTuna here.<p>For years, I&#x27;ve been building complex, multi-tenant APIs with FastAPI, and I&#x27;ve always found that implementing robust, granular, and truly async security is a huge pain point. You either roll your own and risk vulnerabilities, or you wrestle with frameworks that aren&#x27;t async-native, causing performance bottlenecks.<p>I built AuthTuna to solve this. It&#x27;s the async-first security framework I always wanted:<p>Truly Async Core: Built on asyncio and SQLAlchemy 2.0. No part of your security logic will block the event loop.<p>Granular Hierarchical Permissions (RBAC): Go beyond simple roles. You can define permissions like Organization -&gt; Project -&gt; Resource and check them with a simple Depends(PermissionChecker(...)).<p>Advanced Server-Side Sessions: It provides the security of server-side sessions (with hijack detection) without sacrificing the performance you&#x27;d expect from JWTs.<p>Great Developer Experience: Comes with Pydantic models, pre-built routers for auth flows, and clear dependencies to get you started in minutes.<p>I use it in my own production systems, so it&#x27;s been well tested. The goal is to make robust security the easy path, not an afterthought.<p>I&#x27;d love to hear your thoughts and get your feedback. What are you currently using to handle auth in your async Python projects?", "title": "AuthTuna \u2013 A modern, async security framework for FastAPI", "updated_at": "2025-10-26T15:22:28Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "wirehack"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pydantic", "production"], "value": "Hi HN! We are Klavis AI (<a href=\"https://www.klavis.ai/\">https://www.klavis.ai/</a>) and we are launching a managed MCP Sandbox-as-a-Service for RL training on tool use.<p>If you want a model to learn tool use through RL, you need realistic environments where the model can take actions, you can observe the resulting state, and compute a reward. For SaaS tools, this means managing dozens of test accounts, handling OAuth and token refresh, seeding realistic data for each episode, resetting state between runs, and ensuring isolation when you're running concurrent training sessions. Most research teams spend months building this plumbing per integration.<p>Klavis is a managed sandbox service that handles all of that. You call our API to get an isolated sandbox backed by a real service instance (not a mock), initialize it with whatever data state you need, let your model interact via MCP, then dump the final state to compute your reward. One more API call resets everything for the next episode.<p>The key thing is these are real services, not static mocks. When your model creates a calendar event or updates a Salesforce record, that action actually executes against real infrastructure. The state changes are real. This matters because you want training to reflect <em>production</em> behavior exactly.<p>We currently support 50+ integrations across productivity tools (Google Calendar, Outlook, Slack), CRM (Salesforce, HubSpot), dev tools (GitHub, Jira, Linear), databases (Postgres, Snowflake), and others. We handle the account pooling, auth management, and lifecycle orchestration so researchers can focus on the actual training.<p>Technically, the workflow is: create a sandbox, call initialize API with a JSON payload defining your starting state, let the model interact via standard MCP tools, call dump API to get a typed snapshot of the final state, compare against your target for reward calculation, then call reset or delete. We use strict <em>Pydantic</em> schemas for all inputs and outputs so malformed data gets rejected immediately rather than causing silent failures mid-training.<p>Here is a quick demo: <a href=\"https://youtu.be/10C18rpCYcA\" rel=\"nofollow\">https://youtu.be/10C18rpCYcA</a>.<p>We look forward to your comments. Thanks for reading!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Managed MCP Sandbox Environments for RL Training on Tool Use"}}, "_tags": ["story", "author_wirehack", "story_46247531", "show_hn"], "author": "wirehack", "created_at": "2025-12-12T19:08:09Z", "created_at_i": 1765566489, "num_comments": 0, "objectID": "46247531", "points": 3, "story_id": 46247531, "story_text": "Hi HN! We are Klavis AI (<a href=\"https:&#x2F;&#x2F;www.klavis.ai&#x2F;\">https:&#x2F;&#x2F;www.klavis.ai&#x2F;</a>) and we are launching a managed MCP Sandbox-as-a-Service for RL training on tool use.<p>If you want a model to learn tool use through RL, you need realistic environments where the model can take actions, you can observe the resulting state, and compute a reward. For SaaS tools, this means managing dozens of test accounts, handling OAuth and token refresh, seeding realistic data for each episode, resetting state between runs, and ensuring isolation when you&#x27;re running concurrent training sessions. Most research teams spend months building this plumbing per integration.<p>Klavis is a managed sandbox service that handles all of that. You call our API to get an isolated sandbox backed by a real service instance (not a mock), initialize it with whatever data state you need, let your model interact via MCP, then dump the final state to compute your reward. One more API call resets everything for the next episode.<p>The key thing is these are real services, not static mocks. When your model creates a calendar event or updates a Salesforce record, that action actually executes against real infrastructure. The state changes are real. This matters because you want training to reflect production behavior exactly.<p>We currently support 50+ integrations across productivity tools (Google Calendar, Outlook, Slack), CRM (Salesforce, HubSpot), dev tools (GitHub, Jira, Linear), databases (Postgres, Snowflake), and others. We handle the account pooling, auth management, and lifecycle orchestration so researchers can focus on the actual training.<p>Technically, the workflow is: create a sandbox, call initialize API with a JSON payload defining your starting state, let the model interact via standard MCP tools, call dump API to get a typed snapshot of the final state, compare against your target for reward calculation, then call reset or delete. We use strict Pydantic schemas for all inputs and outputs so malformed data gets rejected immediately rather than causing silent failures mid-training.<p>Here is a quick demo: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;10C18rpCYcA\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;10C18rpCYcA</a>.<p>We look forward to your comments. Thanks for reading!", "title": "Show HN: Managed MCP Sandbox Environments for RL Training on Tool Use", "updated_at": "2025-12-12T19:31:22Z"}], "hitsPerPage": 15, "nbHits": 24, "nbPages": 2, "page": 0, "params": "query=pydantic+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 14, "processingTimingsMS": {"_request": {"roundTrip": 13}, "afterFetch": {"format": {"highlighting": 2, "total": 2}}, "fetch": {"query": 10, "scanning": 2, "total": 13}, "total": 14}, "query": "pydantic production", "serverTimeMS": 18}}