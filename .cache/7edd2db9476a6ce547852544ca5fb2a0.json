{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jqphu"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "When doing a refactor I always wanted to make sure I didn't introduce any behavioral changes. Tests are great, but they sometimes don't cover all the cases you're looking for.<p>Shadow Traffic sets up a <em>Cloudflare worker</em> that mirrors <em>production</em> traffic to a staging deployment and checks if the responses are identical. This requires minimal changes in your infrastructure, just point traffic to your <em>production</em> and staging environments."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Shadow Traffic \u2013 Test staging on <em>production</em> workloads"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/jqphu/shadow-traffic"}}, "_tags": ["story", "author_jqphu", "story_37461282", "show_hn"], "author": "jqphu", "created_at": "2023-09-10T23:58:55Z", "created_at_i": 1694390335, "num_comments": 0, "objectID": "37461282", "points": 1, "story_id": 37461282, "story_text": "When doing a refactor I always wanted to make sure I didn&#x27;t introduce any behavioral changes. Tests are great, but they sometimes don&#x27;t cover all the cases you&#x27;re looking for.<p>Shadow Traffic sets up a Cloudflare worker that mirrors production traffic to a staging deployment and checks if the responses are identical. This requires minimal changes in your infrastructure, just point traffic to your production and staging environments.", "title": "Show HN: Shadow Traffic \u2013 Test staging on production workloads", "updated_at": "2024-09-20T15:04:28Z", "url": "https://github.com/jqphu/shadow-traffic"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "vhsdev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "This is an educational reference implementation showing how to build reasonably secure, standards-compliant authentication from first principles on <em>Cloudflare Workers</em>.<p>Stack: Hono, Turso (libSQL), PBKDF2-SHA384 + normalization + common-password checks, JWT access + refresh tokens with revocation support, HTTP-only SameSite cookies, device tracking.<p>It's deliberately minimal \u2014 no OAuth, no passkeys, no magic links, no rate limiting \u2014 because the goal is clarity and auditability.<p>I wrote it mainly to deeply understand edge-runtime auth constraints and to have a clean Apache-2.0 example that follows NIST SP 800-63B / SP 800-132 and OWASP guidance.<p>For <em>production</em> I'd almost always reach for Better Auth instead (<a href=\"https://www.better-auth.com\">https://www.better-auth.com</a>) \u2014 this repo is not trying to compete with it.<p>Live demo: <a href=\"https://private-landing.vhsdev.workers.dev/\" rel=\"nofollow\">https://private-landing.vhsdev.workers.dev/</a><p>Repo: <a href=\"https://github.com/vhscom/private-landing\" rel=\"nofollow\">https://github.com/vhscom/private-landing</a><p>Happy to answer questions about the crypto choices, the refresh token revocation pattern, Turso schema, constant-time comparison, unicode pitfalls, etc."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["cloudflare", "workers"], "value": "Show HN: Minimal NIST/OWASP-compliant auth implementation for <em>Cloudflare Workers</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/vhscom/private-landing"}}, "_tags": ["story", "author_vhsdev", "story_46944084", "show_hn"], "author": "vhsdev", "children": [46944095, 46944778, 46944804, 46949106], "created_at": "2026-02-09T11:30:06Z", "created_at_i": 1770636606, "num_comments": 10, "objectID": "46944084", "points": 33, "story_id": 46944084, "story_text": "This is an educational reference implementation showing how to build reasonably secure, standards-compliant authentication from first principles on Cloudflare Workers.<p>Stack: Hono, Turso (libSQL), PBKDF2-SHA384 + normalization + common-password checks, JWT access + refresh tokens with revocation support, HTTP-only SameSite cookies, device tracking.<p>It&#x27;s deliberately minimal \u2014 no OAuth, no passkeys, no magic links, no rate limiting \u2014 because the goal is clarity and auditability.<p>I wrote it mainly to deeply understand edge-runtime auth constraints and to have a clean Apache-2.0 example that follows NIST SP 800-63B &#x2F; SP 800-132 and OWASP guidance.<p>For production I&#x27;d almost always reach for Better Auth instead (<a href=\"https:&#x2F;&#x2F;www.better-auth.com\">https:&#x2F;&#x2F;www.better-auth.com</a>) \u2014 this repo is not trying to compete with it.<p>Live demo: <a href=\"https:&#x2F;&#x2F;private-landing.vhsdev.workers.dev&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;private-landing.vhsdev.workers.dev&#x2F;</a><p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;vhscom&#x2F;private-landing\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vhscom&#x2F;private-landing</a><p>Happy to answer questions about the crypto choices, the refresh token revocation pattern, Turso schema, constant-time comparison, unicode pitfalls, etc.", "title": "Show HN: Minimal NIST/OWASP-compliant auth implementation for Cloudflare Workers", "updated_at": "2026-02-11T13:51:28Z", "url": "https://github.com/vhscom/private-landing"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "burcs"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Every time I had an idea for a small online shop, it felt harder than it should be. The tools were either big and expensive, or they locked you into their way of doing things.<p>I wanted something simpler. Something I could launch and manage myself. And to be 100% honest I wanted to prove it could be done.<p>All I really needed was an API that handled the boring parts of a store and then stayed out of the way. Products, inventory, carts, checkout, orders, etc...<p>So I built Merchant.<p>Merchant is an open-source e-commerce backend that runs on <em>Cloudflare Workers</em>. It uses D1 for the database and Stripe for payments. It supports outbound webhooks so it can be extended without much effort. There\u2019s also an admin dashboard if you don\u2019t want to manage everything through the API.<p>It\u2019s still early and there are rough edges, shipping is basic, it only supports Stripe and it's fairly US-centric. I probably wouldn\u2019t move your <em>production</em> store to it yet.<p>I\u2019d love to hear what you think. What would make something like this useful to you?<p><a href=\"https://github.com/ygwyg/merchant\" rel=\"nofollow\">https://github.com/ygwyg/merchant</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["cloudflare", "workers"], "value": "Show HN: Merchant, a lightweight ecommerce back end on <em>Cloudflare Workers</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://merchant.dev"}}, "_tags": ["story", "author_burcs", "story_46454481", "show_hn"], "author": "burcs", "children": [46454666], "created_at": "2026-01-01T14:40:32Z", "created_at_i": 1767278432, "num_comments": 1, "objectID": "46454481", "points": 5, "story_id": 46454481, "story_text": "Every time I had an idea for a small online shop, it felt harder than it should be. The tools were either big and expensive, or they locked you into their way of doing things.<p>I wanted something simpler. Something I could launch and manage myself. And to be 100% honest I wanted to prove it could be done.<p>All I really needed was an API that handled the boring parts of a store and then stayed out of the way. Products, inventory, carts, checkout, orders, etc...<p>So I built Merchant.<p>Merchant is an open-source e-commerce backend that runs on Cloudflare Workers. It uses D1 for the database and Stripe for payments. It supports outbound webhooks so it can be extended without much effort. There\u2019s also an admin dashboard if you don\u2019t want to manage everything through the API.<p>It\u2019s still early and there are rough edges, shipping is basic, it only supports Stripe and it&#x27;s fairly US-centric. I probably wouldn\u2019t move your production store to it yet.<p>I\u2019d love to hear what you think. What would make something like this useful to you?<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;ygwyg&#x2F;merchant\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ygwyg&#x2F;merchant</a>", "title": "Show HN: Merchant, a lightweight ecommerce back end on Cloudflare Workers", "updated_at": "2026-01-14T09:39:53Z", "url": "https://merchant.dev"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rubenfiszel"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Ruben here, software engineer, long-time lurker of Hacker News and founder of Windmill. Windmill is a fully open-source self-hostable platform and runtime to build complex workflows, internal apps and integrations using any scripts in Python or Typescript-deno. I am back after having been revealed a bit too soon on HN and miraculously getting into YC (<a href=\"https://news.ycombinator.com/item?id=31272793\" rel=\"nofollow\">https://news.ycombinator.com/item?id=31272793</a>).<p>To build internal apps for ops, integrations between services that cannot talk to each other directly, or to run background jobs that run your business logic and analytics, the two main options today are no-code solutions and old-fashioned, roll-your-own scripting. Both have problems, and our goal with Windmill is to find a new sweet spot between the two. No-code solutions are productive <i>if</i> your problem matches the tool exactly - but it not, they are rigid, hard to extend and quickly become tech debt, annihilating their initial time advantage. Indeed, no-code is just code but made by an opinionated someone else and hidden as a blackbox with an UI.<p>The alternative is to do it the old-fashioned way, writing everything from scratch, both backend and frontend, perhaps deploying it on the latest flavor of serverless, and pray to never have to touch it again because that took way too much time and it has now became a burden that the ops and business team might poke you about regularly.<p>Furthermore, the landscape of SaaS is specialized tools for everything\u2014alerting, data analytics, administration panels, support management, integration between services\u2014when it feels like a few scripts would have been as good or even better and spared you the need of depending on one yet another tool. This could be even further facilitated if there was a way to import the right bunch of scripts from a fellow community of engineers, tweak it and deploy it like you can do in communities where automation can be shared as simple JSON files, for instance in the node-red or home assistant community.. That\u2019s the idea of Windmill: to bring back the power of scripting in an easy way.<p>With Windmill, you write normal scripts, or reuse ones made by others, and we make them <em>production</em>-grade and composable. You shouldn\u2019t have to worry about things like http requests or scheduling jobs. We abstract much of that away, making your scripts be both more focused and more composable. You end up doing things the right way but much quicker.<p>We reduce the complexity of workflows, integrations and internal apps by uniting them all under one banner. At the heart, they mostly have the same needs: workflows with a UI or a schedule. One tool that does it all out-of-the-box offers greater consistency and allows you to grow the complexity of your toolset at your own pace.<p>I have an academic background in compilers and industry experience in distributed systems. My compiler work made me wary of solving every problem with a domain-specific-language (DSL) or complex frameworks. We can just do more with the well-crafted existing languages like Python or Typescript.  Rolling up your own DSL is nice in theory, you can make it very ergonomic and focused on the task at hand, but then you start adding features and either reinvent existing \u2013 albeit worse \u2013 programming language or decide to stop there. In the very large majority of cases, a well crafted library is vastly superior to any DSL. By being able to use any library of Python and Typescript, we stand on the shoulders of giants.<p>I have also observed that the best distributed systems are often the most simple as they are more predictable and have invariants that are easier to reason with and scale horizontally. This is why for Windmill, we rely solely on Postgres + our native workers + our http REST api layer. Later on, we plan to build adapters to host the workers on AWS lambda or <em>Cloudflare workers</em>, and the queue on Kafka if your needs are exceptionally high.<p>At the heart of what we have built is a queue implemented in Postgres and workers implemented in Rust that create a sandbox (using nsjail), fetch dependencies, and execute  scripts. Every script can be triggered through its name  with an HTTP POST by passing a JSON payload in which every field corresponds 1:1 to an argument of the script\u2019s main parameters. Most primitive types in Python or Typescript have a natural corresponding type in JSON so the conversion is always what you would expect.  We then execute the script inside a new sandbox and then store the results in the same Postgres DB at the end of the job execution.<p>The HTTP payload can be sent from your own frontend or you can use our automatically generated UI. Indeed, we do a simple, yet effective analysis of the parameters of your script, and from it, generate the jsonschema corresponding to your parameters. That schema is what enables us to convert any script into a no-code like module for flows, or a standalone internal app with its auto-generated UI. In the case of Python, we also look at the imports to deduce the Pypi dependencies without you having to declare them.<p>For flows, we defined an open spec for building them out of those scripts we call OpenFlow: <a href=\"https://docs.windmill.dev/docs/openflow\" rel=\"nofollow\">https://docs.windmill.dev/docs/openflow</a>. It  is essentially a json format for describing a sequence of steps with for loops and soon branching. The most interesting bit here is that each input of each step can define its input as a javascript expression that refers to and transforms the output of any previous step. We make it fast by leveraging native v8 integration in Rust (thanks to the deno team) for executing those expressions. This makes this apparently linear sequence a flexible DAG in which one can express complex workflows.<p>Then on top of that we have an UI builder for flows that hides most of the complexity to give an experience that is similar to a low-code platform where every step is treated as a blackbox. The platform itself offers all the features that you would expect: a variable and object store for storing states, plain values and credentials; a cron scheduler, tight permissioning for the sensitive credentials, groups, a webeditor with smart assistant to edit the scripts directly in the platform etc. Finally, we made a hub (<a href=\"https://hub.windmill.dev\" rel=\"nofollow\">https://hub.windmill.dev</a>) to share flows and scripts with everyone. The goal is to grow over time an exhaustive library of pre-made modules and flows to tweak from so that you can focus on what is actually custom to you.<p>Windmill is open-source and self-hostable. You can think of it as a superset of both Pipedream and Airplane.dev. Compared to Temporal, the scripts themselves are agnostic of the flow in which they are embedded, which has the benefit of making it easier to build a hub of reusable modules. We are the only ones as far as we know to convert script parameters to UI automatically. We see ourselves as complementary to UI builder solutions like Retool or Tooljet as we do not want to focus too much on the auto-generated UI and could be used solely as the backend part of the two aforementioned tools.<p>We are now a team of 3 senior engineers and the product is progressing faster than ever with a public roadmap: <a href=\"https://github.com/orgs/windmill-labs/projects/2\" rel=\"nofollow\">https://github.com/orgs/windmill-labs/projects/2</a><p>We make money from commercial licenses, support and team plans on the hosted solution.<p>You can self-host it or try it  <a href=\"https://app.windmill.dev\" rel=\"nofollow\">https://app.windmill.dev</a>, the free tier is generous (and the paid one is not enforced yet). Our landing page is: <a href=\"https://windmill.dev\" rel=\"nofollow\">https://windmill.dev</a>. We would appreciate your feedback and ideas and look forward to all your comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Windmill (YC S22) \u2013 Turn scripts into internal apps and workflows"}}, "_tags": ["story", "author_rubenfiszel", "story_32400849", "launch_hn"], "author": "rubenfiszel", "children": [32401024, 32401130, 32401179, 32401382, 32401671, 32401979, 32402106, 32402484, 32402797, 32403674, 32405249, 32405271, 32405426, 32405497, 32407247, 32407428, 32408428, 32408482, 32409571, 32411171, 32411397, 32411697, 32413950, 32429873], "created_at": "2022-08-09T17:19:18Z", "created_at_i": 1660065558, "num_comments": 79, "objectID": "32400849", "points": 212, "story_id": 32400849, "story_text": "Ruben here, software engineer, long-time lurker of Hacker News and founder of Windmill. Windmill is a fully open-source self-hostable platform and runtime to build complex workflows, internal apps and integrations using any scripts in Python or Typescript-deno. I am back after having been revealed a bit too soon on HN and miraculously getting into YC (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31272793\" rel=\"nofollow\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31272793</a>).<p>To build internal apps for ops, integrations between services that cannot talk to each other directly, or to run background jobs that run your business logic and analytics, the two main options today are no-code solutions and old-fashioned, roll-your-own scripting. Both have problems, and our goal with Windmill is to find a new sweet spot between the two. No-code solutions are productive <i>if</i> your problem matches the tool exactly - but it not, they are rigid, hard to extend and quickly become tech debt, annihilating their initial time advantage. Indeed, no-code is just code but made by an opinionated someone else and hidden as a blackbox with an UI.<p>The alternative is to do it the old-fashioned way, writing everything from scratch, both backend and frontend, perhaps deploying it on the latest flavor of serverless, and pray to never have to touch it again because that took way too much time and it has now became a burden that the ops and business team might poke you about regularly.<p>Furthermore, the landscape of SaaS is specialized tools for everything\u2014alerting, data analytics, administration panels, support management, integration between services\u2014when it feels like a few scripts would have been as good or even better and spared you the need of depending on one yet another tool. This could be even further facilitated if there was a way to import the right bunch of scripts from a fellow community of engineers, tweak it and deploy it like you can do in communities where automation can be shared as simple JSON files, for instance in the node-red or home assistant community.. That\u2019s the idea of Windmill: to bring back the power of scripting in an easy way.<p>With Windmill, you write normal scripts, or reuse ones made by others, and we make them production-grade and composable. You shouldn\u2019t have to worry about things like http requests or scheduling jobs. We abstract much of that away, making your scripts be both more focused and more composable. You end up doing things the right way but much quicker.<p>We reduce the complexity of workflows, integrations and internal apps by uniting them all under one banner. At the heart, they mostly have the same needs: workflows with a UI or a schedule. One tool that does it all out-of-the-box offers greater consistency and allows you to grow the complexity of your toolset at your own pace.<p>I have an academic background in compilers and industry experience in distributed systems. My compiler work made me wary of solving every problem with a domain-specific-language (DSL) or complex frameworks. We can just do more with the well-crafted existing languages like Python or Typescript.  Rolling up your own DSL is nice in theory, you can make it very ergonomic and focused on the task at hand, but then you start adding features and either reinvent existing \u2013 albeit worse \u2013 programming language or decide to stop there. In the very large majority of cases, a well crafted library is vastly superior to any DSL. By being able to use any library of Python and Typescript, we stand on the shoulders of giants.<p>I have also observed that the best distributed systems are often the most simple as they are more predictable and have invariants that are easier to reason with and scale horizontally. This is why for Windmill, we rely solely on Postgres + our native workers + our http REST api layer. Later on, we plan to build adapters to host the workers on AWS lambda or Cloudflare workers, and the queue on Kafka if your needs are exceptionally high.<p>At the heart of what we have built is a queue implemented in Postgres and workers implemented in Rust that create a sandbox (using nsjail), fetch dependencies, and execute  scripts. Every script can be triggered through its name  with an HTTP POST by passing a JSON payload in which every field corresponds 1:1 to an argument of the script\u2019s main parameters. Most primitive types in Python or Typescript have a natural corresponding type in JSON so the conversion is always what you would expect.  We then execute the script inside a new sandbox and then store the results in the same Postgres DB at the end of the job execution.<p>The HTTP payload can be sent from your own frontend or you can use our automatically generated UI. Indeed, we do a simple, yet effective analysis of the parameters of your script, and from it, generate the jsonschema corresponding to your parameters. That schema is what enables us to convert any script into a no-code like module for flows, or a standalone internal app with its auto-generated UI. In the case of Python, we also look at the imports to deduce the Pypi dependencies without you having to declare them.<p>For flows, we defined an open spec for building them out of those scripts we call OpenFlow: <a href=\"https:&#x2F;&#x2F;docs.windmill.dev&#x2F;docs&#x2F;openflow\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.windmill.dev&#x2F;docs&#x2F;openflow</a>. It  is essentially a json format for describing a sequence of steps with for loops and soon branching. The most interesting bit here is that each input of each step can define its input as a javascript expression that refers to and transforms the output of any previous step. We make it fast by leveraging native v8 integration in Rust (thanks to the deno team) for executing those expressions. This makes this apparently linear sequence a flexible DAG in which one can express complex workflows.<p>Then on top of that we have an UI builder for flows that hides most of the complexity to give an experience that is similar to a low-code platform where every step is treated as a blackbox. The platform itself offers all the features that you would expect: a variable and object store for storing states, plain values and credentials; a cron scheduler, tight permissioning for the sensitive credentials, groups, a webeditor with smart assistant to edit the scripts directly in the platform etc. Finally, we made a hub (<a href=\"https:&#x2F;&#x2F;hub.windmill.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;hub.windmill.dev</a>) to share flows and scripts with everyone. The goal is to grow over time an exhaustive library of pre-made modules and flows to tweak from so that you can focus on what is actually custom to you.<p>Windmill is open-source and self-hostable. You can think of it as a superset of both Pipedream and Airplane.dev. Compared to Temporal, the scripts themselves are agnostic of the flow in which they are embedded, which has the benefit of making it easier to build a hub of reusable modules. We are the only ones as far as we know to convert script parameters to UI automatically. We see ourselves as complementary to UI builder solutions like Retool or Tooljet as we do not want to focus too much on the auto-generated UI and could be used solely as the backend part of the two aforementioned tools.<p>We are now a team of 3 senior engineers and the product is progressing faster than ever with a public roadmap: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;windmill-labs&#x2F;projects&#x2F;2\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;windmill-labs&#x2F;projects&#x2F;2</a><p>We make money from commercial licenses, support and team plans on the hosted solution.<p>You can self-host it or try it  <a href=\"https:&#x2F;&#x2F;app.windmill.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;app.windmill.dev</a>, the free tier is generous (and the paid one is not enforced yet). Our landing page is: <a href=\"https:&#x2F;&#x2F;windmill.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;windmill.dev</a>. We would appreciate your feedback and ideas and look forward to all your comments!", "title": "Launch HN: Windmill (YC S22) \u2013 Turn scripts into internal apps and workflows", "updated_at": "2024-09-20T11:44:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justintorre75"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Hi HN - Justin, Scott, and Barak here. We're excited to introduce Helicone (<a href=\"https://www.helicone.ai\">https://www.helicone.ai</a>) an open-source logging solution for OpenAi applications. Helicone's one-line integration logs the prompts, completions, latencies, and costs of your OpenAI requests. It currently works with GPT, and can be integrated with one line of code. There\u2019s a demo at <a href=\"https://www.helicone.ai/video\">https://www.helicone.ai/video</a>.<p>Helicone's core technology is a proxy that routes all your OpenAI requests through our edge-deployed <em>Cloudflare Workers</em>. These workers are incredibly reliable and cause no discernible latency impact in <em>production</em> environments. As a proxy, we offer more than just observability: we provide caching and prompt formatting, and we'll soon add user rate limiting and model provider back off to make sure your app is still up when OpenAI is down.<p>Our web application then provides insights into key metrics, such as which users are disproportionately driving costs and what is the token usage broken down by prompts. You can filter this data based on custom logic and export it to other destinations.<p>Getting started with Helicone is quick and easy, regardless of the OpenAI SDK you use. Our proxy-based solution does not require a third party package\u2014simply change your request's base URL from <a href=\"https://api.openai.com/v1\" rel=\"nofollow\">https://api.openai.com/v1</a> to <a href=\"https://oai.hconeai.com/v1\" rel=\"nofollow\">https://oai.hconeai.com/v1</a>. Helicone can be integrated with LangChain, LLama Index, and all other OpenAI native libraries. (<a href=\"https://docs.helicone.ai/quickstart/integrate-in-one-line-of-code\">https://docs.helicone.ai/quickstart/integrate-in-one-line-of...</a>)<p>We have exciting new features coming up, one of which is an API to log user feedback. For instance, if you're developing a tool like GitHub Copilot, you can log when a user accepted or rejected a suggestion. Helicone will then aggregate your result quality into metrics and make finetuning suggestions for when you can save costs or improve performance.<p>Before launching Helicone, we developed several projects with GPT-3, including airapbattle.com, tabletalk.ai, and dreamsubmarine.com. For each project, we used a beta version of Helicone which gave us instant visibility into user engagement and result quality issues. As we talked to more builders and companies, we realized they were spending too much time building in-house solutions like this and that existing analytics products were not tailored to inference endpoints like GPT-3.<p>Helicone is developed under the Common Clause V1.0 w/ Apache 2.0 license so that you can use Helicone within your own infrastructure. If you do not want to self-host, we provide a hosted solution with 1k requests free per month to try our product. If you exceed that we offer a paid subscription as well, and you can view our pricing at <a href=\"https://www.helicone.ai/pricing\">https://www.helicone.ai/pricing</a>.<p>We're thrilled to introduce Helicone to the HackerNews community and would love to hear your thoughts, ideas, and experiences related to LLM logging and analytics. We're eager to engage in meaningful discussions, so please don't hesitate to share your insights and feedback with us!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Helicone.ai (YC W23) \u2013 Open-source logging for OpenAI"}}, "_tags": ["story", "author_justintorre75", "story_35279155", "launch_hn"], "author": "justintorre75", "children": [35279926, 35279945, 35279965, 35279980, 35280417, 35280653, 35280758, 35281022, 35281129, 35281256, 35281382, 35281593, 35282424, 35282515, 35282945, 35283225, 35283256, 35283690, 35283860, 35284214, 35284226, 35284442, 35284726, 35284729, 35285199, 35285816, 35286211, 35286461], "created_at": "2023-03-23T18:25:45Z", "created_at_i": 1679595945, "num_comments": 72, "objectID": "35279155", "points": 166, "story_id": 35279155, "story_text": "Hi HN - Justin, Scott, and Barak here. We&#x27;re excited to introduce Helicone (<a href=\"https:&#x2F;&#x2F;www.helicone.ai\">https:&#x2F;&#x2F;www.helicone.ai</a>) an open-source logging solution for OpenAi applications. Helicone&#x27;s one-line integration logs the prompts, completions, latencies, and costs of your OpenAI requests. It currently works with GPT, and can be integrated with one line of code. There\u2019s a demo at <a href=\"https:&#x2F;&#x2F;www.helicone.ai&#x2F;video\">https:&#x2F;&#x2F;www.helicone.ai&#x2F;video</a>.<p>Helicone&#x27;s core technology is a proxy that routes all your OpenAI requests through our edge-deployed Cloudflare Workers. These workers are incredibly reliable and cause no discernible latency impact in production environments. As a proxy, we offer more than just observability: we provide caching and prompt formatting, and we&#x27;ll soon add user rate limiting and model provider back off to make sure your app is still up when OpenAI is down.<p>Our web application then provides insights into key metrics, such as which users are disproportionately driving costs and what is the token usage broken down by prompts. You can filter this data based on custom logic and export it to other destinations.<p>Getting started with Helicone is quick and easy, regardless of the OpenAI SDK you use. Our proxy-based solution does not require a third party package\u2014simply change your request&#x27;s base URL from <a href=\"https:&#x2F;&#x2F;api.openai.com&#x2F;v1\" rel=\"nofollow\">https:&#x2F;&#x2F;api.openai.com&#x2F;v1</a> to <a href=\"https:&#x2F;&#x2F;oai.hconeai.com&#x2F;v1\" rel=\"nofollow\">https:&#x2F;&#x2F;oai.hconeai.com&#x2F;v1</a>. Helicone can be integrated with LangChain, LLama Index, and all other OpenAI native libraries. (<a href=\"https:&#x2F;&#x2F;docs.helicone.ai&#x2F;quickstart&#x2F;integrate-in-one-line-of-code\">https:&#x2F;&#x2F;docs.helicone.ai&#x2F;quickstart&#x2F;integrate-in-one-line-of...</a>)<p>We have exciting new features coming up, one of which is an API to log user feedback. For instance, if you&#x27;re developing a tool like GitHub Copilot, you can log when a user accepted or rejected a suggestion. Helicone will then aggregate your result quality into metrics and make finetuning suggestions for when you can save costs or improve performance.<p>Before launching Helicone, we developed several projects with GPT-3, including airapbattle.com, tabletalk.ai, and dreamsubmarine.com. For each project, we used a beta version of Helicone which gave us instant visibility into user engagement and result quality issues. As we talked to more builders and companies, we realized they were spending too much time building in-house solutions like this and that existing analytics products were not tailored to inference endpoints like GPT-3.<p>Helicone is developed under the Common Clause V1.0 w&#x2F; Apache 2.0 license so that you can use Helicone within your own infrastructure. If you do not want to self-host, we provide a hosted solution with 1k requests free per month to try our product. If you exceed that we offer a paid subscription as well, and you can view our pricing at <a href=\"https:&#x2F;&#x2F;www.helicone.ai&#x2F;pricing\">https:&#x2F;&#x2F;www.helicone.ai&#x2F;pricing</a>.<p>We&#x27;re thrilled to introduce Helicone to the HackerNews community and would love to hear your thoughts, ideas, and experiences related to LLM logging and analytics. We&#x27;re eager to engage in meaningful discussions, so please don&#x27;t hesitate to share your insights and feedback with us!", "title": "Launch HN: Helicone.ai (YC W23) \u2013 Open-source logging for OpenAI", "updated_at": "2024-09-20T13:41:41Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yoavm"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Hi HN! We're Yair and Yo'av of Zaraz (<a href=\"https://zaraz.com\" rel=\"nofollow\">https://zaraz.com</a>). We make websites faster by loading their third-party stack in an optimized way. By \u201cthird-party\u201d we mean utilities or additional products you add to your website (eg. analytics), not things you build your website with (eg. React).<p>Before we started this we worked on opposite sides of this battle for third-party inclusion: Yair was working for the folks asking to implement just-one-more analytics tool, while Yo'av was a developer trying (and often failing) to push back. Avoiding bloat to begin with would be preferable, but anyone working for even a medium-sized company knows how hard that is - usually when a higher up agrees to try or add a new tool, resistance is futile. Hence the question becomes, can you do it without harming your performance?<p>The average US top 5,000 website loads 22 different third-party tools - analytics, customer success, marketing and whatnot. We wrote a bot that scanned these websites and discovered that third-parties account for 40% of their \u201cTime to Interactive\u201d, and other metrics like TBT, FCP, FID and CLS were hurt in a similar way. From the user perspective, the page usually behaves exactly the same without these tools (...except 40% faster).<p>These new metrics are becoming more popular for two reasons. Firstly, users actually feel them - unlike events such as &quot;DOMContentLoad&quot; &amp; &quot;Load&quot; that can be triggered long before the user can actually do anything, these metrics provide a much better proxy to the real user experience. Secondly, with Google soon penalizing slow websites, they're becoming more and more important for SEO. We see the growing popularity of these metrics as a good thing. We want a faster web.<p>Nowadays, the most common way to integrate a third-party into your website is either to just paste its `&lt;script&gt;` snippet somewhere in your code, or use some &quot;Tag Management&quot; software (awful name!) like Google Tag Manager, Tealium, or even a tool like Segment. All these options pretty much come with the same cost - everything loads by default together with your page, and users just have to wait and wait. If all this slowness doesn't feel so bad on your devices now, remember that much of the world accesses the internet through devices that are probably a lot slower than yours.<p>We built Zaraz to be a performance-first third-party manager. Each tool is different, but the concept is to run whatever we can on our backend instead of in the browser, leaving it to focus on loading your website. While other solutions serve all your visitors with the same script and then evaluate it in the browser (should we run this conversion pixel? Should we load this analytics tool?) - we do this on our backend. But the real magic is that we created an environment living inside a <em>Cloudflare Worker</em>, that executes the actual third-party scripts instead of having them run in the browser. Google Analytics, Reddit conversion pixel, LinkedIn Insight, you name it - we\u2019re turning all those things into miniature server-side applications that your visitors\u2019 browsers need not to worry about. If a certain tool still needs to fire a request from the browser (eg. it needs to set a cookie), only the resulting URL from evaluating its script will be sent back to the user browser. It\u2019s a server side environment executing third-party code, that you have 100% control over. When we measure the speed of a website optimized with Zaraz, third-parties have close to zero effect on it, because the browser almost does nothing.<p>We are already serving a few customers in <em>production</em>, and we\u2019re seeing huge improvements in speed (and revenues!) with all of them. Zaraz is probably the easiest way you can make your website faster, today (try our analyzer to see how we can improve your website: <a href=\"https://zaraz.com/analyze\" rel=\"nofollow\">https://zaraz.com/analyze</a>). Aside from performance, since we have total control on what data is revealed to our isolated environment, we are using it to help companies protect the privacy of their visitors by masking PIIs, hiding IP addresses, disabling fingerprinting etc. We designed our infrastructure as a set of serverless, storageless and stateless functions - to make sure your visitors data is never saved, not even by accident.<p>We are currently onboarding mostly enterprises and high-traffic websites, but we plan to introduce a free tier after we are done creating a self-onboarding flow. We are on a mission to make the web faster and we want all websites to benefit from it!<p>We would be thrilled to hear what you think, and if you have more ideas on how to make websites faster please do share them with us. Thank you!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Zaraz (YC W20) \u2013 Use third-party tools without slowing your website"}}, "_tags": ["story", "author_yoavm", "story_26002657", "launch_hn"], "author": "yoavm", "children": [26003049, 26003087, 26003230, 26003268, 26003276, 26003602, 26003621, 26003654, 26003755, 26004230, 26004284, 26004530, 26004854, 26005036, 26005245, 26005408, 26005652, 26006214, 26006341, 26006452, 26006634, 26008058, 26008194, 26008457, 26009148, 26009970, 26010355, 26010600, 26010791, 26011576, 26012463, 26012540, 26035462], "created_at": "2021-02-02T16:08:30Z", "created_at_i": 1612282110, "num_comments": 84, "objectID": "26002657", "points": 145, "story_id": 26002657, "story_text": "Hi HN! We&#x27;re Yair and Yo&#x27;av of Zaraz (<a href=\"https:&#x2F;&#x2F;zaraz.com\" rel=\"nofollow\">https:&#x2F;&#x2F;zaraz.com</a>). We make websites faster by loading their third-party stack in an optimized way. By \u201cthird-party\u201d we mean utilities or additional products you add to your website (eg. analytics), not things you build your website with (eg. React).<p>Before we started this we worked on opposite sides of this battle for third-party inclusion: Yair was working for the folks asking to implement just-one-more analytics tool, while Yo&#x27;av was a developer trying (and often failing) to push back. Avoiding bloat to begin with would be preferable, but anyone working for even a medium-sized company knows how hard that is - usually when a higher up agrees to try or add a new tool, resistance is futile. Hence the question becomes, can you do it without harming your performance?<p>The average US top 5,000 website loads 22 different third-party tools - analytics, customer success, marketing and whatnot. We wrote a bot that scanned these websites and discovered that third-parties account for 40% of their \u201cTime to Interactive\u201d, and other metrics like TBT, FCP, FID and CLS were hurt in a similar way. From the user perspective, the page usually behaves exactly the same without these tools (...except 40% faster).<p>These new metrics are becoming more popular for two reasons. Firstly, users actually feel them - unlike events such as &quot;DOMContentLoad&quot; &amp; &quot;Load&quot; that can be triggered long before the user can actually do anything, these metrics provide a much better proxy to the real user experience. Secondly, with Google soon penalizing slow websites, they&#x27;re becoming more and more important for SEO. We see the growing popularity of these metrics as a good thing. We want a faster web.<p>Nowadays, the most common way to integrate a third-party into your website is either to just paste its `&lt;script&gt;` snippet somewhere in your code, or use some &quot;Tag Management&quot; software (awful name!) like Google Tag Manager, Tealium, or even a tool like Segment. All these options pretty much come with the same cost - everything loads by default together with your page, and users just have to wait and wait. If all this slowness doesn&#x27;t feel so bad on your devices now, remember that much of the world accesses the internet through devices that are probably a lot slower than yours.<p>We built Zaraz to be a performance-first third-party manager. Each tool is different, but the concept is to run whatever we can on our backend instead of in the browser, leaving it to focus on loading your website. While other solutions serve all your visitors with the same script and then evaluate it in the browser (should we run this conversion pixel? Should we load this analytics tool?) - we do this on our backend. But the real magic is that we created an environment living inside a Cloudflare Worker, that executes the actual third-party scripts instead of having them run in the browser. Google Analytics, Reddit conversion pixel, LinkedIn Insight, you name it - we\u2019re turning all those things into miniature server-side applications that your visitors\u2019 browsers need not to worry about. If a certain tool still needs to fire a request from the browser (eg. it needs to set a cookie), only the resulting URL from evaluating its script will be sent back to the user browser. It\u2019s a server side environment executing third-party code, that you have 100% control over. When we measure the speed of a website optimized with Zaraz, third-parties have close to zero effect on it, because the browser almost does nothing.<p>We are already serving a few customers in production, and we\u2019re seeing huge improvements in speed (and revenues!) with all of them. Zaraz is probably the easiest way you can make your website faster, today (try our analyzer to see how we can improve your website: <a href=\"https:&#x2F;&#x2F;zaraz.com&#x2F;analyze\" rel=\"nofollow\">https:&#x2F;&#x2F;zaraz.com&#x2F;analyze</a>). Aside from performance, since we have total control on what data is revealed to our isolated environment, we are using it to help companies protect the privacy of their visitors by masking PIIs, hiding IP addresses, disabling fingerprinting etc. We designed our infrastructure as a set of serverless, storageless and stateless functions - to make sure your visitors data is never saved, not even by accident.<p>We are currently onboarding mostly enterprises and high-traffic websites, but we plan to introduce a free tier after we are done creating a self-onboarding flow. We are on a mission to make the web faster and we want all websites to benefit from it!<p>We would be thrilled to hear what you think, and if you have more ideas on how to make websites faster please do share them with us. Thank you!", "title": "Launch HN: Zaraz (YC W20) \u2013 Use third-party tools without slowing your website", "updated_at": "2024-09-20T07:47:11Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pmig"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Hi HN, my name is Philip, I\u2019m the co-founder of Glasskube and one of the creators of HyprMCP.<p>This project started when we did what everyone was doing \u2014 building a remote MCP server and launching it. Building the first local MCP server for testing was quite simple, and we had our first tools ready within a day. The next step was turning that into a <em>production</em>-ready remote MCP server.<p>As we exposed the MCP server to our users, we wanted to authenticate them with our existing authentication methods. We dove deep into authentication. Our approach was to build an auth proxy and plug it in front of our MCP. It took a while to figure out Dynamic Client Registration (DCR) and the OAuth spec, and especially the gaps between existing OIDC IDPs and what LLM clients needed.<p>We thought authentication would be the hard part \u2014 but it wasn\u2019t. When we shared the MCP server with a few friendly startups, we realized that different MCP clients behave differently. Especially if something didn't work, it was hard to figure out the root cause. We ended up storing all the raw gRPC method calls to see if the initialization and subsequent requests worked. This is especially useful if you are on a serverless environment with limited debugging functionality, like <em>Cloudflare Workers</em>.<p>Once we solved auth and compatibility, we launched to a small customer base \u2014 done, right? Unfortunately, not quite. Technically everything was working, but when we started talking to users, they told us the MCP server didn\u2019t always respond with the right tools for their prompts. We had a working enterprise-grade MCP server \u2014 but it wasn\u2019t very smart. After talking to some startup friends, we realized we needed an evaluation layer. That\u2019s when we added prompt analytics \u2014 letting us see which prompts triggered which tools and how well they performed. That alone dramatically improved our MCP\u2019s behavior and overall user experience.<p>After building all of this into our proxy, we realized that everyone building a remote MCP was facing the same challenges. So we decided to package it all up and release it to the community.<p>We\u2019re thrilled to launch and open-source HyprMCP.\nIt acts as a proxy that you can plug in front of your MCP server(s) with zero code changes. You get authentication, logging and debugging, prompt analytics, and an MCP connection instructions generator.<p>Under the hood, HyprMCP leverages dynamic Kubernetes Operators (Metacontroller) to automate infrastructure provisioning.<p>On the roadmap: MCP aggregation \u2014 combining multiple MCP servers under one single remote URL for large organizations running servers with different lifecycles.\nAll of it without storing end user credentials on the server and connecting the MCP to the organizations existing authentication methods.<p>You can check the project out on GitHub: <a href=\"https://github.com/hyprmcp/jetski\" rel=\"nofollow\">https://github.com/hyprmcp/jetski</a><p>For testing, we also have a hosted version here: <a href=\"https://app.hyprmcp.com\" rel=\"nofollow\">https://app.hyprmcp.com</a><p>We even created a demo video on YouTube: <a href=\"https://www.youtube.com/watch?v=m2-YyfjXap4\" rel=\"nofollow\">https://www.youtube.com/watch?v=m2-YyfjXap4</a><p>We\u2019d love to get your feedback, hear what features are missing, and learn how you\u2019re building and running your own MCP servers."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: HyprMCP \u2013 Analytics, logs and auth for MCP servers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/hyprmcp/jetski"}}, "_tags": ["story", "author_pmig", "story_45521788", "show_hn"], "author": "pmig", "children": [45523623, 45524058, 45525004, 45541392, 45545190], "created_at": "2025-10-08T23:27:23Z", "created_at_i": 1759966043, "num_comments": 8, "objectID": "45521788", "points": 59, "story_id": 45521788, "story_text": "Hi HN, my name is Philip, I\u2019m the co-founder of Glasskube and one of the creators of HyprMCP.<p>This project started when we did what everyone was doing \u2014 building a remote MCP server and launching it. Building the first local MCP server for testing was quite simple, and we had our first tools ready within a day. The next step was turning that into a production-ready remote MCP server.<p>As we exposed the MCP server to our users, we wanted to authenticate them with our existing authentication methods. We dove deep into authentication. Our approach was to build an auth proxy and plug it in front of our MCP. It took a while to figure out Dynamic Client Registration (DCR) and the OAuth spec, and especially the gaps between existing OIDC IDPs and what LLM clients needed.<p>We thought authentication would be the hard part \u2014 but it wasn\u2019t. When we shared the MCP server with a few friendly startups, we realized that different MCP clients behave differently. Especially if something didn&#x27;t work, it was hard to figure out the root cause. We ended up storing all the raw gRPC method calls to see if the initialization and subsequent requests worked. This is especially useful if you are on a serverless environment with limited debugging functionality, like Cloudflare Workers.<p>Once we solved auth and compatibility, we launched to a small customer base \u2014 done, right? Unfortunately, not quite. Technically everything was working, but when we started talking to users, they told us the MCP server didn\u2019t always respond with the right tools for their prompts. We had a working enterprise-grade MCP server \u2014 but it wasn\u2019t very smart. After talking to some startup friends, we realized we needed an evaluation layer. That\u2019s when we added prompt analytics \u2014 letting us see which prompts triggered which tools and how well they performed. That alone dramatically improved our MCP\u2019s behavior and overall user experience.<p>After building all of this into our proxy, we realized that everyone building a remote MCP was facing the same challenges. So we decided to package it all up and release it to the community.<p>We\u2019re thrilled to launch and open-source HyprMCP.\nIt acts as a proxy that you can plug in front of your MCP server(s) with zero code changes. You get authentication, logging and debugging, prompt analytics, and an MCP connection instructions generator.<p>Under the hood, HyprMCP leverages dynamic Kubernetes Operators (Metacontroller) to automate infrastructure provisioning.<p>On the roadmap: MCP aggregation \u2014 combining multiple MCP servers under one single remote URL for large organizations running servers with different lifecycles.\nAll of it without storing end user credentials on the server and connecting the MCP to the organizations existing authentication methods.<p>You can check the project out on GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;hyprmcp&#x2F;jetski\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;hyprmcp&#x2F;jetski</a><p>For testing, we also have a hosted version here: <a href=\"https:&#x2F;&#x2F;app.hyprmcp.com\" rel=\"nofollow\">https:&#x2F;&#x2F;app.hyprmcp.com</a><p>We even created a demo video on YouTube: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=m2-YyfjXap4\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=m2-YyfjXap4</a><p>We\u2019d love to get your feedback, hear what features are missing, and learn how you\u2019re building and running your own MCP servers.", "title": "Show HN: HyprMCP \u2013 Analytics, logs and auth for MCP servers", "updated_at": "2025-10-13T22:57:43Z", "url": "https://github.com/hyprmcp/jetski"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "brendonmatos"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "I built this as a small side project to learn and experiment, and I ended up with this!<p>I used a subdomain from my personal portfolio, and everything else runs on free tiers.<p>The project uses Nuxt, SVG, <em>Cloudflare Workers</em>, D1 (SQL), KV, Terraform, and some agentic coding with OpenAI Codex and Claude Code.<p>What started as a joke among friends turned into a fun excuse to build something end to end, from zero to <em>production</em>, and to explore a few things \nI\u2019d never touched before.<p>I\u2019d really appreciate any feedback or suggestions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Open-source certificate from GitHub activity"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://certificate.brendonmatos.com"}}, "_tags": ["story", "author_brendonmatos", "story_46668780", "show_hn"], "author": "brendonmatos", "children": [46669074, 46671733, 46674481, 46682870, 46715792, 46720413, 46727102], "created_at": "2026-01-18T15:52:05Z", "created_at_i": 1768751525, "num_comments": 14, "objectID": "46668780", "points": 43, "story_id": 46668780, "story_text": "I built this as a small side project to learn and experiment, and I ended up with this!<p>I used a subdomain from my personal portfolio, and everything else runs on free tiers.<p>The project uses Nuxt, SVG, Cloudflare Workers, D1 (SQL), KV, Terraform, and some agentic coding with OpenAI Codex and Claude Code.<p>What started as a joke among friends turned into a fun excuse to build something end to end, from zero to production, and to explore a few things \nI\u2019d never touched before.<p>I\u2019d really appreciate any feedback or suggestions.", "title": "Show HN: Open-source certificate from GitHub activity", "updated_at": "2026-01-29T10:16:49Z", "url": "https://certificate.brendonmatos.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "NathanFlurry"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "I\u2019m tired of getting screwed by game backends like Microsoft PlayFab and Unity Gaming Services. To get a simple cross-platform friends system, we\u2019ve bent over backward fighting against their limited APIs to jerry-rig the behavior we need.<p>These backends \u201csolved\u201d their platform\u2019s rigidity by tacking on a clunky scripting layer [1] [3] with a proprietary KV database [2] [4] (look ma, no transactions!). They proceed to claim you can build a LiveOps game on this rubbish without testing, database migrations, monitoring, BI tooling, etc.<p>We got fed up and built an open-source backend engine as an alternative: Open Game Backend.<p>OpenGB is comprised of four components: the engine, modules, scripts, &amp; registries.<p>Modules &amp; Registries<p>No two games have identical use cases, but these game backends try to provide the same API to everyone. For example, PlayFab has 44 API endpoints for their economy API  [5], yet every developer I know still finds it too restrictive to use in a serious game.<p>Instead, OpenGB assumes game developers will start with off-the-shelf modules, then fork &amp; write new modules as their needs grow. Everything down to auth, tokens, &amp; rate limiting are implemented as modules so they can be modified. Forking a module is as simple as `opengb fork &lt;module&gt;` to pull code in to the repo.<p>Scripts<p>I love niche game engines &amp; languages, so I\u2019ve seen a lot of weird tooling. But these cloud scripts are some of the most tedious platforms I\u2019ve written code for. Even their example code has race conditions [6] and requires 306 LOC to implement a simple transaction [7].<p>We built the scripting API to feel as much like a game engine as possible. All of the complexities of database queries/migrations, schema validation, and networking are abstracted away in a cozy engine-like scripting environment where game devs feel at home. It's dead simple to write a leaderboard module from scratch in a couple minutes: https://opengb.dev/build/crash-course<p>Official Registry<p>We don\u2019t intend to ship a truckload of half-baked services like PlayFab &amp; UGS does. We want to build a solid engine that others can extend with modules.<p>To help developers bootstrap their backends, we maintain an official registry of modules (written by us &amp; external contributors) that are thoroughly reviewed for performance, testing, &amp; docs. We hope other devs also share their registries, similar to the Unity Asset Store community. See: https://opengb.dev/modules/overview<p>We\u2019re currently working with a few studios rolling OpenGB out to <em>production</em>. If you give it a spin, we\u2019d love to hear your thoughts below or in our Discord: https://rivet.gg/discord<p>Documentation: https://opengb.dev<p>GitHub: https://github.com/rivet-gg/opengb<p>A few more notes:<p>- Licensed permissively under Apache 2.0, go nuts<p>- Leverages PostgreSQL\u2019s features &amp; extensions, similar to Supabase<p>- Generates type-safe client SDKs &amp; Open API specs for your modules<p>- Supports Deno, NodeJS, <em>Cloudflare Workers</em><p>[1] https://learn.microsoft.com/en-us/gaming/playfab/features/automation/cloudscript/writing-custom-cloudscript<p>[2] https://learn.microsoft.com/en-us/gaming/playfab/features/entities/entity-objects<p>[3] https://docs.unity.com/ugs/en-us/manual/cloud-code/manual/scripts<p>[4] https://docs.unity.com/ugs/en-us/manual/cloud-save/manual<p>[5] https://learn.microsoft.com/en-us/rest/api/playfab/economy/catalog<p>[6] https://github.com/Unity-Technologies/com.unity.services.samples.use-cases/blob/main/Assets/Use%20Case%20Samples/Daily%20Rewards/Config%20as%20Code/DailyRewards_Claim.js#L38<p>[7] https://github.com/Unity-Technologies/com.unity.services.samples.use-cases/blob/main/Assets/Use%20Case%20Samples/Command%20Batching/Config%20as%20Code/CommandBatching_ProcessBatch.js"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: OpenGB \u2013 modular game back end engine with first-class scripting"}}, "_tags": ["story", "author_NathanFlurry", "story_39716400", "show_hn"], "author": "NathanFlurry", "children": [39718057, 39719048], "created_at": "2024-03-15T14:50:08Z", "created_at_i": 1710514208, "num_comments": 4, "objectID": "39716400", "points": 20, "story_id": 39716400, "story_text": "I\u2019m tired of getting screwed by game backends like Microsoft PlayFab and Unity Gaming Services. To get a simple cross-platform friends system, we\u2019ve bent over backward fighting against their limited APIs to jerry-rig the behavior we need.<p>These backends \u201csolved\u201d their platform\u2019s rigidity by tacking on a clunky scripting layer [1] [3] with a proprietary KV database [2] [4] (look ma, no transactions!). They proceed to claim you can build a LiveOps game on this rubbish without testing, database migrations, monitoring, BI tooling, etc.<p>We got fed up and built an open-source backend engine as an alternative: Open Game Backend.<p>OpenGB is comprised of four components: the engine, modules, scripts, &amp; registries.<p>Modules &amp; Registries<p>No two games have identical use cases, but these game backends try to provide the same API to everyone. For example, PlayFab has 44 API endpoints for their economy API  [5], yet every developer I know still finds it too restrictive to use in a serious game.<p>Instead, OpenGB assumes game developers will start with off-the-shelf modules, then fork &amp; write new modules as their needs grow. Everything down to auth, tokens, &amp; rate limiting are implemented as modules so they can be modified. Forking a module is as simple as `opengb fork &lt;module&gt;` to pull code in to the repo.<p>Scripts<p>I love niche game engines &amp; languages, so I\u2019ve seen a lot of weird tooling. But these cloud scripts are some of the most tedious platforms I\u2019ve written code for. Even their example code has race conditions [6] and requires 306 LOC to implement a simple transaction [7].<p>We built the scripting API to feel as much like a game engine as possible. All of the complexities of database queries&#x2F;migrations, schema validation, and networking are abstracted away in a cozy engine-like scripting environment where game devs feel at home. It&#x27;s dead simple to write a leaderboard module from scratch in a couple minutes: https:&#x2F;&#x2F;opengb.dev&#x2F;build&#x2F;crash-course<p>Official Registry<p>We don\u2019t intend to ship a truckload of half-baked services like PlayFab &amp; UGS does. We want to build a solid engine that others can extend with modules.<p>To help developers bootstrap their backends, we maintain an official registry of modules (written by us &amp; external contributors) that are thoroughly reviewed for performance, testing, &amp; docs. We hope other devs also share their registries, similar to the Unity Asset Store community. See: https:&#x2F;&#x2F;opengb.dev&#x2F;modules&#x2F;overview<p>We\u2019re currently working with a few studios rolling OpenGB out to production. If you give it a spin, we\u2019d love to hear your thoughts below or in our Discord: https:&#x2F;&#x2F;rivet.gg&#x2F;discord<p>Documentation: https:&#x2F;&#x2F;opengb.dev<p>GitHub: https:&#x2F;&#x2F;github.com&#x2F;rivet-gg&#x2F;opengb<p>A few more notes:<p>- Licensed permissively under Apache 2.0, go nuts<p>- Leverages PostgreSQL\u2019s features &amp; extensions, similar to Supabase<p>- Generates type-safe client SDKs &amp; Open API specs for your modules<p>- Supports Deno, NodeJS, Cloudflare Workers<p>[1] https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;gaming&#x2F;playfab&#x2F;features&#x2F;automation&#x2F;cloudscript&#x2F;writing-custom-cloudscript<p>[2] https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;gaming&#x2F;playfab&#x2F;features&#x2F;entities&#x2F;entity-objects<p>[3] https:&#x2F;&#x2F;docs.unity.com&#x2F;ugs&#x2F;en-us&#x2F;manual&#x2F;cloud-code&#x2F;manual&#x2F;scripts<p>[4] https:&#x2F;&#x2F;docs.unity.com&#x2F;ugs&#x2F;en-us&#x2F;manual&#x2F;cloud-save&#x2F;manual<p>[5] https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;rest&#x2F;api&#x2F;playfab&#x2F;economy&#x2F;catalog<p>[6] https:&#x2F;&#x2F;github.com&#x2F;Unity-Technologies&#x2F;com.unity.services.samples.use-cases&#x2F;blob&#x2F;main&#x2F;Assets&#x2F;Use%20Case%20Samples&#x2F;Daily%20Rewards&#x2F;Config%20as%20Code&#x2F;DailyRewards_Claim.js#L38<p>[7] https:&#x2F;&#x2F;github.com&#x2F;Unity-Technologies&#x2F;com.unity.services.samples.use-cases&#x2F;blob&#x2F;main&#x2F;Assets&#x2F;Use%20Case%20Samples&#x2F;Command%20Batching&#x2F;Config%20as%20Code&#x2F;CommandBatching_ProcessBatch.js", "title": "Show HN: OpenGB \u2013 modular game back end engine with first-class scripting", "updated_at": "2024-09-20T16:34:54Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "elawler24"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Today we\u2019re launching Velvet, an AI gateway for warehousing OpenAI and Anthropic requests to your PostgreSQL instance.<p>We originally built an AI SQL editor, but realized that customers were using it to monitor their AI requests in <em>production</em>. We had already built an AI request warehousing tool internally to debug our SQL editor and gave some customers access.<p>A few days into testing this idea, our pilot customer launched [1] and we began warehousing 1,500 requests per second. We worked closely with their engineering team in the following weeks, completely re-architecting Velvet for scale and additional features (such as Batch support). Along the way, other companies began seeking out Velvet to get visibility into their own LLM requests.<p>We\u2019re launching our AI gateway as a self-serve product today, but our pilot customers are already warehousing over 3 million requests per week - so the system is stable and performant.<p>What makes Velvet unique is that you own the data in your own database. Also, we\u2019re the first proxy that gives visibility into OpenAI batch calls - so you can observe and monitor async calls that save you money.<p>Some technical notes:<p>- Supports OpenAI and Anthropic endpoints<p>- Data is formatted as JSON and logged to your own PostgreSQL instance (can add support for other databases for paying customers).<p>- You can include queryable metadata in the header, such as user ID, org ID, model ID, and version ID.<p>- Built on <em>Cloudflare workers</em>, which keeps latency minimal (using our caching feature will reduce latency overall)<p>- Built for security + starting process of SOC II soon<p>Why warehouse your requests?<p>- Understand where money is spent. Use custom headers to calculate the cost per customer, model, or service.<p>- Download real request/response data, so you can evaluate new models (e.g., re-running requests with a cheaper mini model)<p>- Monitor time to completion of batch jobs. (e.g., OpenAI says 24 hours, but our customers average 3-4 hours)<p>- Export a subset of example requests for fine-tuning<p>It\u2019s just a 2 line code change to get started.<p>Try a sandbox demoing the logging proxy here: <a href=\"https://usevelvet.com/sandbox\" rel=\"nofollow\">https://usevelvet.com/sandbox</a><p>More details in our docs <a href=\"https://docs.usevelvet.com\" rel=\"nofollow\">https://docs.usevelvet.com</a><p>[1] <a href=\"https://news.ycombinator.com/item?id=40801494\">https://news.ycombinator.com/item?id=40801494</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Warehouse OpenAI requests to your own database"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.usevelvet.com"}}, "_tags": ["story", "author_elawler24", "story_41381498", "show_hn"], "author": "elawler24", "children": [41381525, 41381951, 41385826], "created_at": "2024-08-28T16:51:45Z", "created_at_i": 1724863905, "num_comments": 6, "objectID": "41381498", "points": 18, "story_id": 41381498, "story_text": "Today we\u2019re launching Velvet, an AI gateway for warehousing OpenAI and Anthropic requests to your PostgreSQL instance.<p>We originally built an AI SQL editor, but realized that customers were using it to monitor their AI requests in production. We had already built an AI request warehousing tool internally to debug our SQL editor and gave some customers access.<p>A few days into testing this idea, our pilot customer launched [1] and we began warehousing 1,500 requests per second. We worked closely with their engineering team in the following weeks, completely re-architecting Velvet for scale and additional features (such as Batch support). Along the way, other companies began seeking out Velvet to get visibility into their own LLM requests.<p>We\u2019re launching our AI gateway as a self-serve product today, but our pilot customers are already warehousing over 3 million requests per week - so the system is stable and performant.<p>What makes Velvet unique is that you own the data in your own database. Also, we\u2019re the first proxy that gives visibility into OpenAI batch calls - so you can observe and monitor async calls that save you money.<p>Some technical notes:<p>- Supports OpenAI and Anthropic endpoints<p>- Data is formatted as JSON and logged to your own PostgreSQL instance (can add support for other databases for paying customers).<p>- You can include queryable metadata in the header, such as user ID, org ID, model ID, and version ID.<p>- Built on Cloudflare workers, which keeps latency minimal (using our caching feature will reduce latency overall)<p>- Built for security + starting process of SOC II soon<p>Why warehouse your requests?<p>- Understand where money is spent. Use custom headers to calculate the cost per customer, model, or service.<p>- Download real request&#x2F;response data, so you can evaluate new models (e.g., re-running requests with a cheaper mini model)<p>- Monitor time to completion of batch jobs. (e.g., OpenAI says 24 hours, but our customers average 3-4 hours)<p>- Export a subset of example requests for fine-tuning<p>It\u2019s just a 2 line code change to get started.<p>Try a sandbox demoing the logging proxy here: <a href=\"https:&#x2F;&#x2F;usevelvet.com&#x2F;sandbox\" rel=\"nofollow\">https:&#x2F;&#x2F;usevelvet.com&#x2F;sandbox</a><p>More details in our docs <a href=\"https:&#x2F;&#x2F;docs.usevelvet.com\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.usevelvet.com</a><p>[1] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40801494\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40801494</a>", "title": "Show HN: Warehouse OpenAI requests to your own database", "updated_at": "2024-09-20T17:43:03Z", "url": "https://www.usevelvet.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "markogg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Hello everyone!<p>To briefly introduce myself, I'm mainly a backend developer, working in the industry for the past 7 years.<p>On one of the products I was working on we were heavily using a custom feature flagging system. Basically, any functionality we were building was starting with a new feature flag. It was useful because:<p>1. We were able to silently deploy a feature to <em>production</em> for real environment testing\n2. We would enable feature flag for specific users across our team, like marketing, sales, so everyone can check it out, test and prepare for the release\n3. We were restricting certain features across pricing plans or organizations\n4. We were sometimes initially only releasing to customers that requested the feature, or a small group<p>There were more use cases, but all in all, feature flagging was deeply integrated in the product.<p>As someone who has a lot of experience with this topic, I decided to build a new feature flagging tool in a way how I would imagine it to work.<p>Additionally, I always wanted to go on a journey with my own product, going through the entire process from seeing an empty page on my localhost to having a finished, well documented and working product.<p>The product is now live and working, although still with a &quot;Beta&quot; badge as it lacks some real-world testing.<p>The stack I'm using:\n1. Laravel framework for the main app with Interia.js and Tailwind CSS for styling\n2. Postgres/Redis for data storage\n3. Astro for the marketing website and (Starlight) for the documentation<p>The main app is deployed on Hetzner with Kubernetes.<p>One of the main issues I was worried about is the latency, as flags evaluation must be fast otherwise it is not very appealing to use an external tool for that. I solved it by caching evaluations on Cloudflare Edge network close to users around the world. I've set up a worker that is responsible for managing the flag evaluation content. I have some ideas to improve this approach in future, but for v1 I think it does a pretty good job.<p>Marketing website is also on <em>Cloudflare worker</em>.<p>I would like to invite you to try it out and provide some feedback if you have, at <a href=\"https://fecusio.com/\" rel=\"nofollow\">https://fecusio.com/</a><p>To understand how the product is intended to be used, you can read the Quickstart guide: <a href=\"https://fecusio.com/docs/quickstart/quickstart/\" rel=\"nofollow\">https://fecusio.com/docs/quickstart/quickstart/</a><p>Thanks!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Fecusio \u2013 Feature flagging and release management tool"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://fecusio.com"}}, "_tags": ["story", "author_markogg", "story_44915159", "show_hn"], "author": "markogg", "children": [44915596], "created_at": "2025-08-15T17:34:22Z", "created_at_i": 1755279262, "num_comments": 2, "objectID": "44915159", "points": 8, "story_id": 44915159, "story_text": "Hello everyone!<p>To briefly introduce myself, I&#x27;m mainly a backend developer, working in the industry for the past 7 years.<p>On one of the products I was working on we were heavily using a custom feature flagging system. Basically, any functionality we were building was starting with a new feature flag. It was useful because:<p>1. We were able to silently deploy a feature to production for real environment testing\n2. We would enable feature flag for specific users across our team, like marketing, sales, so everyone can check it out, test and prepare for the release\n3. We were restricting certain features across pricing plans or organizations\n4. We were sometimes initially only releasing to customers that requested the feature, or a small group<p>There were more use cases, but all in all, feature flagging was deeply integrated in the product.<p>As someone who has a lot of experience with this topic, I decided to build a new feature flagging tool in a way how I would imagine it to work.<p>Additionally, I always wanted to go on a journey with my own product, going through the entire process from seeing an empty page on my localhost to having a finished, well documented and working product.<p>The product is now live and working, although still with a &quot;Beta&quot; badge as it lacks some real-world testing.<p>The stack I&#x27;m using:\n1. Laravel framework for the main app with Interia.js and Tailwind CSS for styling\n2. Postgres&#x2F;Redis for data storage\n3. Astro for the marketing website and (Starlight) for the documentation<p>The main app is deployed on Hetzner with Kubernetes.<p>One of the main issues I was worried about is the latency, as flags evaluation must be fast otherwise it is not very appealing to use an external tool for that. I solved it by caching evaluations on Cloudflare Edge network close to users around the world. I&#x27;ve set up a worker that is responsible for managing the flag evaluation content. I have some ideas to improve this approach in future, but for v1 I think it does a pretty good job.<p>Marketing website is also on Cloudflare worker.<p>I would like to invite you to try it out and provide some feedback if you have, at <a href=\"https:&#x2F;&#x2F;fecusio.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;fecusio.com&#x2F;</a><p>To understand how the product is intended to be used, you can read the Quickstart guide: <a href=\"https:&#x2F;&#x2F;fecusio.com&#x2F;docs&#x2F;quickstart&#x2F;quickstart&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;fecusio.com&#x2F;docs&#x2F;quickstart&#x2F;quickstart&#x2F;</a><p>Thanks!", "title": "Show HN: Fecusio \u2013 Feature flagging and release management tool", "updated_at": "2025-08-26T13:16:36Z", "url": "https://fecusio.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yutakobayashi"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Hey HN!<p>Hanabi.rest is a web application that utilizes AI technology to generate REST APIs and SQL from prompts or screenshots, significantly speeding up backend development. This application allows for checking the functionality of APIs directly connected to the data layer through a browser, thereby greatly improving the development process.<p>Hono.js is optimized for web standards and Edge runtimes, enabling deployment in various environments such as Cloudflare, Fastly, Deno, Bun, Lagon, AWS, and Node.js. The APIs created can be shared through links, allowing community members to contribute to the improvement of applications through stars and forks.<p>Support is also provided for importing npm packages, automatically adding types to the editor. Furthermore, it is possible to automatically generate APIs with AI using links to interfaces generated from Vercel's v0.dev. Additionally, the generated APIs can be deployed directly to <em>Cloudflare Workers</em>, and D1 databases can be created, migrated, and worker scripts deployed with just a few clicks from the browser, making it easy to introduce them into <em>production</em> environments. A dedicated CLI is also available for configuring projects and advancing local development.<p>We\u2019re eager to hear your thoughts and feedback. Give Hanabi.rest a try and let us know what you think!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Hanabi.rest \u2013 AI-based API building platform"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://hanabi.rest"}}, "_tags": ["story", "author_yutakobayashi", "story_40520912", "show_hn"], "author": "yutakobayashi", "created_at": "2024-05-30T06:54:34Z", "created_at_i": 1717052074, "num_comments": 0, "objectID": "40520912", "points": 3, "story_id": 40520912, "story_text": "Hey HN!<p>Hanabi.rest is a web application that utilizes AI technology to generate REST APIs and SQL from prompts or screenshots, significantly speeding up backend development. This application allows for checking the functionality of APIs directly connected to the data layer through a browser, thereby greatly improving the development process.<p>Hono.js is optimized for web standards and Edge runtimes, enabling deployment in various environments such as Cloudflare, Fastly, Deno, Bun, Lagon, AWS, and Node.js. The APIs created can be shared through links, allowing community members to contribute to the improvement of applications through stars and forks.<p>Support is also provided for importing npm packages, automatically adding types to the editor. Furthermore, it is possible to automatically generate APIs with AI using links to interfaces generated from Vercel&#x27;s v0.dev. Additionally, the generated APIs can be deployed directly to Cloudflare Workers, and D1 databases can be created, migrated, and worker scripts deployed with just a few clicks from the browser, making it easy to introduce them into production environments. A dedicated CLI is also available for configuring projects and advancing local development.<p>We\u2019re eager to hear your thoughts and feedback. Give Hanabi.rest a try and let us know what you think!", "title": "Show HN: Hanabi.rest \u2013 AI-based API building platform", "updated_at": "2024-09-20T17:09:13Z", "url": "https://hanabi.rest"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ankrgyl"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Hi HN,<p>We're excited to open source our AI Proxy which supports Mistral, LLaMa2, OpenAI, Azure, Anthropic, and more through vanilla OpenAI SDKs. The proxy also supports configurable caching, API key management, and load balancing across multiple providers.<p>The proxy code also includes multiple deployment options: <em>Cloudflare workers</em>, Vercel, AWS Lambda, and plain-old Express. We're open sourcing this because our customers are starting to run their <em>production</em> workloads through it, and we believe that critical-path-of-<em>production</em> tools should be open source.<p>To play around with a hosted version of the proxy, you can simply set the base URL in your OpenAI libs to <a href=\"https://braintrustproxy.com/v1\" rel=\"nofollow noreferrer\">https://braintrustproxy.com/v1</a> (more detailed instructions in the repo[1] and our docs[2]).<p>We'd love your feedback -- on use cases we may not have thought of, models we should support, or other features that would be useful to include in this layer of abstraction.<p>[1]: <a href=\"https://github.com/braintrustdata/braintrust-proxy\">https://github.com/braintrustdata/braintrust-proxy</a>\n[2]: <a href=\"http://braintrustdata.com/docs/guides/proxy\" rel=\"nofollow noreferrer\">http://braintrustdata.com/docs/guides/proxy</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: AI Proxy with support for multiple providers, caching"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/braintrustdata/braintrust-proxy"}}, "_tags": ["story", "author_ankrgyl", "story_38434510", "show_hn"], "author": "ankrgyl", "created_at": "2023-11-27T16:43:54Z", "created_at_i": 1701103434, "num_comments": 0, "objectID": "38434510", "points": 3, "story_id": 38434510, "story_text": "Hi HN,<p>We&#x27;re excited to open source our AI Proxy which supports Mistral, LLaMa2, OpenAI, Azure, Anthropic, and more through vanilla OpenAI SDKs. The proxy also supports configurable caching, API key management, and load balancing across multiple providers.<p>The proxy code also includes multiple deployment options: Cloudflare workers, Vercel, AWS Lambda, and plain-old Express. We&#x27;re open sourcing this because our customers are starting to run their production workloads through it, and we believe that critical-path-of-production tools should be open source.<p>To play around with a hosted version of the proxy, you can simply set the base URL in your OpenAI libs to <a href=\"https:&#x2F;&#x2F;braintrustproxy.com&#x2F;v1\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;braintrustproxy.com&#x2F;v1</a> (more detailed instructions in the repo[1] and our docs[2]).<p>We&#x27;d love your feedback -- on use cases we may not have thought of, models we should support, or other features that would be useful to include in this layer of abstraction.<p>[1]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;braintrustdata&#x2F;braintrust-proxy\">https:&#x2F;&#x2F;github.com&#x2F;braintrustdata&#x2F;braintrust-proxy</a>\n[2]: <a href=\"http:&#x2F;&#x2F;braintrustdata.com&#x2F;docs&#x2F;guides&#x2F;proxy\" rel=\"nofollow noreferrer\">http:&#x2F;&#x2F;braintrustdata.com&#x2F;docs&#x2F;guides&#x2F;proxy</a>", "title": "Show HN: AI Proxy with support for multiple providers, caching", "updated_at": "2024-09-20T15:47:26Z", "url": "https://github.com/braintrustdata/braintrust-proxy"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "lienid"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "Hi HN,<p>Wanted to share something I've been tinkering on for a bit.<p>The Problem:<p>Crypto is notorious for its hacks and uncanny ability to put user money at risk. There's a million companies focusing on securing the smart contract piece of the puzzle through audits (CodeArena, Sherlock, etc) and on-chain monitoring (Hypernative), but a glaring ignorance to frontend attacks.<p>This is a bit concerning given that we've seen millions of dollars go up in smoke through frontend attacks on BadgerDAO (<em>Cloudflare worker</em>), Curve (DNS), Sushi Miso (supply chain), and others.<p>The problem is historically there have been no good frameworks for E2E testing where those that did exist were either extremely convoluted to use, extremely flaky, or both. There have also been no systems for monitoring these kinds of E2E tests like there are in Checkly, LambdaTest, etc for traditional apps.<p>The Solution:<p>I built a E2E testing framework and a system for monitoring these E2E tests against <em>production</em> sites to alleviate this.<p>The framework - GuardianTest - is open source, has minimal dependencies, has no brittleness around wallet interactions, can perform actions against network forks (Ethereum, Arbitrum, Optimism, Polygon), can easily mock token balances, can validate the contract target of an app interaction, can make assertions about on-chain state, and has all the validation and action capabilities of Playwright<p>Link to the code: <a href=\"https://github.com/GuardianUI/GuardianTest/tree/v1.0.3\">https://github.com/GuardianUI/GuardianTest/tree/v1.0.3</a><p>Link to NPM: <a href=\"https://www.npmjs.com/package/@guardianui/test?activeTab=readme\" rel=\"nofollow\">https://www.npmjs.com/package/@guardianui/test?activeTab=rea...</a><p>The monitoring - Once these tests are written they can be submitted to be monitored as frequently as every 5 min, or as infrequently as once a day, will provide alerts are sent if something goes wrong, and alerts are categorized by their significance so there should be no false positives for critical issues<p>General docs: <a href=\"https://docs.guardianui.com/overview/readme\" rel=\"nofollow\">https://docs.guardianui.com/overview/readme</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: GuardianUI \u2013 Simplify crypto E2E testing and monitoring"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.guardianui.com"}}, "_tags": ["story", "author_lienid", "story_36156052", "show_hn"], "author": "lienid", "created_at": "2023-06-01T19:21:08Z", "created_at_i": 1685647268, "num_comments": 0, "objectID": "36156052", "points": 3, "story_id": 36156052, "story_text": "Hi HN,<p>Wanted to share something I&#x27;ve been tinkering on for a bit.<p>The Problem:<p>Crypto is notorious for its hacks and uncanny ability to put user money at risk. There&#x27;s a million companies focusing on securing the smart contract piece of the puzzle through audits (CodeArena, Sherlock, etc) and on-chain monitoring (Hypernative), but a glaring ignorance to frontend attacks.<p>This is a bit concerning given that we&#x27;ve seen millions of dollars go up in smoke through frontend attacks on BadgerDAO (Cloudflare worker), Curve (DNS), Sushi Miso (supply chain), and others.<p>The problem is historically there have been no good frameworks for E2E testing where those that did exist were either extremely convoluted to use, extremely flaky, or both. There have also been no systems for monitoring these kinds of E2E tests like there are in Checkly, LambdaTest, etc for traditional apps.<p>The Solution:<p>I built a E2E testing framework and a system for monitoring these E2E tests against production sites to alleviate this.<p>The framework - GuardianTest - is open source, has minimal dependencies, has no brittleness around wallet interactions, can perform actions against network forks (Ethereum, Arbitrum, Optimism, Polygon), can easily mock token balances, can validate the contract target of an app interaction, can make assertions about on-chain state, and has all the validation and action capabilities of Playwright<p>Link to the code: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;GuardianUI&#x2F;GuardianTest&#x2F;tree&#x2F;v1.0.3\">https:&#x2F;&#x2F;github.com&#x2F;GuardianUI&#x2F;GuardianTest&#x2F;tree&#x2F;v1.0.3</a><p>Link to NPM: <a href=\"https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@guardianui&#x2F;test?activeTab=readme\" rel=\"nofollow\">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@guardianui&#x2F;test?activeTab=rea...</a><p>The monitoring - Once these tests are written they can be submitted to be monitored as frequently as every 5 min, or as infrequently as once a day, will provide alerts are sent if something goes wrong, and alerts are categorized by their significance so there should be no false positives for critical issues<p>General docs: <a href=\"https:&#x2F;&#x2F;docs.guardianui.com&#x2F;overview&#x2F;readme\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.guardianui.com&#x2F;overview&#x2F;readme</a>", "title": "Show HN: GuardianUI \u2013 Simplify crypto E2E testing and monitoring", "updated_at": "2024-09-20T14:15:36Z", "url": "https://www.guardianui.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "alexgarden"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cloudflare", "workers", "production"], "value": "I run multi-agent teams in high-consequence scenarios. Read: fuckups at 3 AM = I'm awake.<p>I kept hitting the same issue. I couldn't get a rules-based system to enforce behavior <i>and</i> I had no real way to prove that agents really did what they said they did. I can log and monitor them - set up (a million) Slack alerts but none of these things are PROOF. Logs are mutable. And that matters more every day as agents get more powerful (take THAT, @meta)<p><i>So I went down the rabbit hole.</i><p>The obvious answer is zero-knowledge proofs. Prove behavior cryptographically. Except proving an LLM inference in a zkVM is computationally Star Trek. Lagrange proved GPT-2 E2E, Polyhedra can do Llama-3 at 150 seconds <i>per token</i> \u2014 <em>production</em>-scale is still hours, not seconds.<p>The a-ha: I don't need to prove the <i>model</i> is correct. I need to prove the <i>auditor</i> is honest.<p>My system intercepts agent thinking blocks (Claude, OpenAI, Gemini), analyzes them against a behavioral contract, and produces a verdict: clear, review needed, or boundary violation. That derivation is deterministic \u2014 ~10,000 RISC-V cycles. Provable <i>today</i>.<p>So I built a guest program inside SP1's zkVM (on Modal) that re-derives the verdict from scratch, ignoring what the auditor claimed, and generates a STARK proof. If the auditor said &quot;clear&quot; but the evidence warranted &quot;violation,&quot; the proof fails. The auditor cannot lie.<p><i>Quis custodiet ipsos custodes</i> (Who watches the watchmen?) \u2014 answered with math. Sub-second on GPU.<p><i>OK... pretty cool, but what ELSE can you do with it?</i><p>Great question! Once I had provable individual verdicts: what about teams? Can I prove the <i>group</i> is safe?<p>I ended up applying financial risk theory to AI agent fleets (things I never expected to be doing with my life). CoVaR for tail risk \u2014 one bad agent in a group of four good ones doesn't average out to &quot;fine.&quot; Markowitz portfolio theory for coherence \u2014 treating value alignment like diversification. DebtRank for contagion \u2014 if Agent A fails, who's exposed? Originally designed for bank failures. Works disturbingly well for agents.<p>Then I needed Shapley attribution for individual risk. Except real Shapley is exponential (2^n subsets), and Monte Carlo introduces randomness. Randomness = non-determinism = unprovable in a zkVM. Leave-One-Out approximation: deterministic, O(n\u00b2), the only Shapley variant that works inside a prover.<p>Oh, and all of it runs in Q16.16 fixed-point arithmetic (i32) because floating-point produces different results on different architectures, and &quot;different results&quot; inside a zkVM = worthless proof. I implemented exp, sqrt, and clamp from scratch in integer math. Casting spells at 2 AM in the dark again.<p>The whole stack \u2014 CoVaR, Markowitz, DebtRank, Shapley, circuit breakers \u2014 computes in TypeScript on <em>Cloudflare Workers</em> (instant), then re-derives in Rust inside the zkVM (provable). Both produce identical results. If they don't, something is very wrong.<p><i>So what?</i><p>Every agent accumulates cryptographically attested checkpoints \u2014 Ed25519 signatures, SHA-256 hash chains, Merkle trees, STARK proofs \u2014 and earns a Trust Score. Credit rating for AI agents, AAA to CCC. The score isn't an opinion. It's a computation over evidence anyone can independently verify. FICO computes scores from data you can't inspect. This computes scores from data anyone can cryptographically verify.<p><i>Everything I described here is live code.</i> Four agents handling a <em>production</em> incident \u2014 coherence matrix, trust topology, Merkle visualization, drift detection: <a href=\"https://mnemom.ai/showcase\" rel=\"nofollow\">https://mnemom.ai/showcase</a><p>Apache-licensed. Zero-code gateway: npm install -g @mnemom/smoltbot &amp;&amp; smoltbot register<p>GitHub: github.com/mnemom | Docs: docs.mnemom.ai"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: I applied Markowitz port. theory to agent teams / proved it in a zkVM"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.mnemom.ai/showcase"}}, "_tags": ["story", "author_alexgarden", "story_47141753", "show_hn"], "author": "alexgarden", "created_at": "2026-02-24T19:41:04Z", "created_at_i": 1771962064, "num_comments": 0, "objectID": "47141753", "points": 2, "story_id": 47141753, "story_text": "I run multi-agent teams in high-consequence scenarios. Read: fuckups at 3 AM = I&#x27;m awake.<p>I kept hitting the same issue. I couldn&#x27;t get a rules-based system to enforce behavior <i>and</i> I had no real way to prove that agents really did what they said they did. I can log and monitor them - set up (a million) Slack alerts but none of these things are PROOF. Logs are mutable. And that matters more every day as agents get more powerful (take THAT, @meta)<p><i>So I went down the rabbit hole.</i><p>The obvious answer is zero-knowledge proofs. Prove behavior cryptographically. Except proving an LLM inference in a zkVM is computationally Star Trek. Lagrange proved GPT-2 E2E, Polyhedra can do Llama-3 at 150 seconds <i>per token</i> \u2014 production-scale is still hours, not seconds.<p>The a-ha: I don&#x27;t need to prove the <i>model</i> is correct. I need to prove the <i>auditor</i> is honest.<p>My system intercepts agent thinking blocks (Claude, OpenAI, Gemini), analyzes them against a behavioral contract, and produces a verdict: clear, review needed, or boundary violation. That derivation is deterministic \u2014 ~10,000 RISC-V cycles. Provable <i>today</i>.<p>So I built a guest program inside SP1&#x27;s zkVM (on Modal) that re-derives the verdict from scratch, ignoring what the auditor claimed, and generates a STARK proof. If the auditor said &quot;clear&quot; but the evidence warranted &quot;violation,&quot; the proof fails. The auditor cannot lie.<p><i>Quis custodiet ipsos custodes</i> (Who watches the watchmen?) \u2014 answered with math. Sub-second on GPU.<p><i>OK... pretty cool, but what ELSE can you do with it?</i><p>Great question! Once I had provable individual verdicts: what about teams? Can I prove the <i>group</i> is safe?<p>I ended up applying financial risk theory to AI agent fleets (things I never expected to be doing with my life). CoVaR for tail risk \u2014 one bad agent in a group of four good ones doesn&#x27;t average out to &quot;fine.&quot; Markowitz portfolio theory for coherence \u2014 treating value alignment like diversification. DebtRank for contagion \u2014 if Agent A fails, who&#x27;s exposed? Originally designed for bank failures. Works disturbingly well for agents.<p>Then I needed Shapley attribution for individual risk. Except real Shapley is exponential (2^n subsets), and Monte Carlo introduces randomness. Randomness = non-determinism = unprovable in a zkVM. Leave-One-Out approximation: deterministic, O(n\u00b2), the only Shapley variant that works inside a prover.<p>Oh, and all of it runs in Q16.16 fixed-point arithmetic (i32) because floating-point produces different results on different architectures, and &quot;different results&quot; inside a zkVM = worthless proof. I implemented exp, sqrt, and clamp from scratch in integer math. Casting spells at 2 AM in the dark again.<p>The whole stack \u2014 CoVaR, Markowitz, DebtRank, Shapley, circuit breakers \u2014 computes in TypeScript on Cloudflare Workers (instant), then re-derives in Rust inside the zkVM (provable). Both produce identical results. If they don&#x27;t, something is very wrong.<p><i>So what?</i><p>Every agent accumulates cryptographically attested checkpoints \u2014 Ed25519 signatures, SHA-256 hash chains, Merkle trees, STARK proofs \u2014 and earns a Trust Score. Credit rating for AI agents, AAA to CCC. The score isn&#x27;t an opinion. It&#x27;s a computation over evidence anyone can independently verify. FICO computes scores from data you can&#x27;t inspect. This computes scores from data anyone can cryptographically verify.<p><i>Everything I described here is live code.</i> Four agents handling a production incident \u2014 coherence matrix, trust topology, Merkle visualization, drift detection: <a href=\"https:&#x2F;&#x2F;mnemom.ai&#x2F;showcase\" rel=\"nofollow\">https:&#x2F;&#x2F;mnemom.ai&#x2F;showcase</a><p>Apache-licensed. Zero-code gateway: npm install -g @mnemom&#x2F;smoltbot &amp;&amp; smoltbot register<p>GitHub: github.com&#x2F;mnemom | Docs: docs.mnemom.ai", "title": "Show HN: I applied Markowitz port. theory to agent teams / proved it in a zkVM", "updated_at": "2026-02-24T19:51:30Z", "url": "https://www.mnemom.ai/showcase"}], "hitsPerPage": 15, "nbHits": 21, "nbPages": 2, "page": 0, "params": "query=cloudflare-workers+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 16, "processingTimingsMS": {"_request": {"roundTrip": 18}, "afterFetch": {"format": {"highlighting": 3, "total": 3}}, "fetch": {"query": 12, "scanning": 2, "total": 15}, "total": 16}, "query": "cloudflare-workers production", "serverTimeMS": 21}}