{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "matesz"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Replicate my *entire* <em>production</em> database? You must be mad"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "https://blog.<em>turso</em>.tech/replicate-my-entire-<em>production</em>-database-you-must-be-mad-641d3f1ed9d8"}}, "_tags": ["story", "author_matesz", "story_36810244"], "author": "matesz", "created_at": "2023-07-21T05:16:52Z", "created_at_i": 1689916612, "num_comments": 0, "objectID": "36810244", "points": 1, "story_id": 36810244, "title": "Replicate my *entire* production database? You must be mad", "updated_at": "2024-09-20T14:34:01Z", "url": "https://blog.turso.tech/replicate-my-entire-production-database-you-must-be-mad-641d3f1ed9d8"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "vhsdev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "This is an educational reference implementation showing how to build reasonably secure, standards-compliant authentication from first principles on Cloudflare Workers.<p>Stack: Hono, <em>Turso</em> (libSQL), PBKDF2-SHA384 + normalization + common-password checks, JWT access + refresh tokens with revocation support, HTTP-only SameSite cookies, device tracking.<p>It's deliberately minimal \u2014 no OAuth, no passkeys, no magic links, no rate limiting \u2014 because the goal is clarity and auditability.<p>I wrote it mainly to deeply understand edge-runtime auth constraints and to have a clean Apache-2.0 example that follows NIST SP 800-63B / SP 800-132 and OWASP guidance.<p>For <em>production</em> I'd almost always reach for Better Auth instead (<a href=\"https://www.better-auth.com\">https://www.better-auth.com</a>) \u2014 this repo is not trying to compete with it.<p>Live demo: <a href=\"https://private-landing.vhsdev.workers.dev/\" rel=\"nofollow\">https://private-landing.vhsdev.workers.dev/</a><p>Repo: <a href=\"https://github.com/vhscom/private-landing\" rel=\"nofollow\">https://github.com/vhscom/private-landing</a><p>Happy to answer questions about the crypto choices, the refresh token revocation pattern, <em>Turso</em> schema, constant-time comparison, unicode pitfalls, etc."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Minimal NIST/OWASP-compliant auth implementation for Cloudflare Workers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/vhscom/private-landing"}}, "_tags": ["story", "author_vhsdev", "story_46944084", "show_hn"], "author": "vhsdev", "children": [46944095, 46944778, 46944804, 46949106], "created_at": "2026-02-09T11:30:06Z", "created_at_i": 1770636606, "num_comments": 10, "objectID": "46944084", "points": 33, "story_id": 46944084, "story_text": "This is an educational reference implementation showing how to build reasonably secure, standards-compliant authentication from first principles on Cloudflare Workers.<p>Stack: Hono, Turso (libSQL), PBKDF2-SHA384 + normalization + common-password checks, JWT access + refresh tokens with revocation support, HTTP-only SameSite cookies, device tracking.<p>It&#x27;s deliberately minimal \u2014 no OAuth, no passkeys, no magic links, no rate limiting \u2014 because the goal is clarity and auditability.<p>I wrote it mainly to deeply understand edge-runtime auth constraints and to have a clean Apache-2.0 example that follows NIST SP 800-63B &#x2F; SP 800-132 and OWASP guidance.<p>For production I&#x27;d almost always reach for Better Auth instead (<a href=\"https:&#x2F;&#x2F;www.better-auth.com\">https:&#x2F;&#x2F;www.better-auth.com</a>) \u2014 this repo is not trying to compete with it.<p>Live demo: <a href=\"https:&#x2F;&#x2F;private-landing.vhsdev.workers.dev&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;private-landing.vhsdev.workers.dev&#x2F;</a><p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;vhscom&#x2F;private-landing\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vhscom&#x2F;private-landing</a><p>Happy to answer questions about the crypto choices, the refresh token revocation pattern, Turso schema, constant-time comparison, unicode pitfalls, etc.", "title": "Show HN: Minimal NIST/OWASP-compliant auth implementation for Cloudflare Workers", "updated_at": "2026-02-11T13:51:28Z", "url": "https://github.com/vhscom/private-landing"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "thenorthbay"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "Wouldn't it be cool to have a Supabase for SQLite?<p>The core idea here is: let SQLite run next to your application on the server; but have all features a client-server database give you.<p>What's the spec for this?<p>- SQLite runs next to server as <em>production</em> database<p>- That way, reads and writes are very fast<p>- In dev, some sort of worker auto-copies the prod DB to the local repo. <em>Production</em> bugs can be reproduced easily and code fixed quickly<p>- Have an interface that lets you access, view, and modify data in the <em>production</em> DB, kind of like Firebase. Might need a server of its own... or couldn't that just be the app server itself?<p>- SQLite auto-backs up to a bucket (like Litestream)<p>I kinda really want this, but haven't found anything quite like it. I've seen <em>Turso</em>, but it seems they focus more on global replication instead of the OSS developer experience I'm looking for.<p>What do you think? What am I missing?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Wouldn't it be cool to have a Supabase for SQLite?"}}, "_tags": ["story", "author_thenorthbay", "story_40156219", "ask_hn"], "author": "thenorthbay", "children": [40156302, 40157229, 40157624, 40157757, 40157970, 40158110, 40158120, 40158209, 40158323, 40158501, 40159135, 40159461, 40160588, 40161131, 40162783, 40166440, 40166558, 40166702, 40166784, 40174540, 40190772], "created_at": "2024-04-25T11:41:52Z", "created_at_i": 1714045312, "num_comments": 26, "objectID": "40156219", "points": 26, "story_id": 40156219, "story_text": "Wouldn&#x27;t it be cool to have a Supabase for SQLite?<p>The core idea here is: let SQLite run next to your application on the server; but have all features a client-server database give you.<p>What&#x27;s the spec for this?<p>- SQLite runs next to server as production database<p>- That way, reads and writes are very fast<p>- In dev, some sort of worker auto-copies the prod DB to the local repo. Production bugs can be reproduced easily and code fixed quickly<p>- Have an interface that lets you access, view, and modify data in the production DB, kind of like Firebase. Might need a server of its own... or couldn&#x27;t that just be the app server itself?<p>- SQLite auto-backs up to a bucket (like Litestream)<p>I kinda really want this, but haven&#x27;t found anything quite like it. I&#x27;ve seen Turso, but it seems they focus more on global replication instead of the OSS developer experience I&#x27;m looking for.<p>What do you think? What am I missing?", "title": "Ask HN: Wouldn't it be cool to have a Supabase for SQLite?", "updated_at": "2025-12-04T16:34:20Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yoeven"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "At JigsawStack.com, we are using Redis by Upstash to store and validate API keys among other datasets for speedy retrieval. As we scaled to more Asian markets and added replicated nodes, the cost started to shoot up while performance got a lot worse.<p>I discovered hosting SQLite on Fly which was crazy fast &amp; cheap with the full power of a relational database but difficult to use and scale without writing tons of code &amp; devops. Then I found <em>Turso</em> as a managed service which is pretty cool but again got really expensive and had to manage a bunch of replications manually and pay per replications (also not a big fan of the UI, lol).<p>Eventually, I found Cloudflare D1, which was amazing both cost at scale and performance with Cloudflare edge infra but it stops there, it's very hard to work with D1 out of the box and to scale it for an actual <em>production</em> platform with tons of limits and without getting locked into the Cloudflare ecosystem.<p>So I built Dzero on top of D1 as a fully managed service that automatically scales globally with over 320 edge locations at no additional cost and can easily be integrated into any platform or framework without getting locked into any ecosystem!<p>The goal of the dashboard was to be like a Supabase for SQLite, meaning that everything should be easily manageable from the dashboard, from creating tables to editing data!<p>Right now we are in early access and would love for you to sign up and give it a try! Feedback is always appreciated! We have big plans for Dzero, check out some of our plans on the site!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Show HN: Dzero \u2013 The fastest globally distributed SQLite database"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://dzero.dev"}}, "_tags": ["story", "author_yoeven", "story_40563729", "show_hn"], "author": "yoeven", "children": [40564029, 40564037, 40564041, 40564056, 40564063, 40565318, 40566355, 40569983, 40570429], "created_at": "2024-06-03T15:35:32Z", "created_at_i": 1717428932, "num_comments": 19, "objectID": "40563729", "points": 24, "story_id": 40563729, "story_text": "At JigsawStack.com, we are using Redis by Upstash to store and validate API keys among other datasets for speedy retrieval. As we scaled to more Asian markets and added replicated nodes, the cost started to shoot up while performance got a lot worse.<p>I discovered hosting SQLite on Fly which was crazy fast &amp; cheap with the full power of a relational database but difficult to use and scale without writing tons of code &amp; devops. Then I found Turso as a managed service which is pretty cool but again got really expensive and had to manage a bunch of replications manually and pay per replications (also not a big fan of the UI, lol).<p>Eventually, I found Cloudflare D1, which was amazing both cost at scale and performance with Cloudflare edge infra but it stops there, it&#x27;s very hard to work with D1 out of the box and to scale it for an actual production platform with tons of limits and without getting locked into the Cloudflare ecosystem.<p>So I built Dzero on top of D1 as a fully managed service that automatically scales globally with over 320 edge locations at no additional cost and can easily be integrated into any platform or framework without getting locked into any ecosystem!<p>The goal of the dashboard was to be like a Supabase for SQLite, meaning that everything should be easily manageable from the dashboard, from creating tables to editing data!<p>Right now we are in early access and would love for you to sign up and give it a try! Feedback is always appreciated! We have big plans for Dzero, check out some of our plans on the site!", "title": "Show HN: Show HN: Dzero \u2013 The fastest globally distributed SQLite database", "updated_at": "2024-10-09T08:04:55Z", "url": "https://dzero.dev"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yoeven"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "I am personally looking for something like this, a small, fast, cheap and globally distributed SQLite service with great DX with tools like branching, performance checks, AI to write queries etc.<p>The closest I have found is https://<em>turso</em>.tech but its no where close to the experience you get with planet scale.<p>I'm not sure if this is something people would want or have a large use case for in <em>production</em>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Is PlanetScale for SQLite something people would want?"}}, "_tags": ["story", "author_yoeven", "story_39930489", "ask_hn"], "author": "yoeven", "children": [39938679, 39949816], "created_at": "2024-04-04T13:59:29Z", "created_at_i": 1712239169, "num_comments": 2, "objectID": "39930489", "points": 4, "story_id": 39930489, "story_text": "I am personally looking for something like this, a small, fast, cheap and globally distributed SQLite service with great DX with tools like branching, performance checks, AI to write queries etc.<p>The closest I have found is https:&#x2F;&#x2F;turso.tech but its no where close to the experience you get with planet scale.<p>I&#x27;m not sure if this is something people would want or have a large use case for in production", "title": "Ask HN: Is PlanetScale for SQLite something people would want?", "updated_at": "2024-09-20T16:47:50Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ludo_dev"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "What is Symfony-UX/<em>Turbo</em> worth in <em>production</em>?"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["turso"], "value": "https://en.developpeur-freelance.io/symfony-ux-<em>turbo</em>/"}}, "_tags": ["story", "author_ludo_dev", "story_39508457"], "author": "ludo_dev", "created_at": "2024-02-26T07:23:17Z", "created_at_i": 1708932197, "num_comments": 0, "objectID": "39508457", "points": 1, "story_id": 39508457, "title": "What is Symfony-UX/Turbo worth in production?", "updated_at": "2024-09-20T16:24:50Z", "url": "https://en.developpeur-freelance.io/symfony-ux-turbo/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "samizdis"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "Porsche Taycan <em>Turbo</em> S Sets Fastest <em>Production</em> EV N\u00fcrburgring Lap Record"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "https://www.thedrive.com/news/porsche-taycan-<em>turbo</em>-s-fastest-<em>production</em>-ev-nurburgring-lap-record"}}, "_tags": ["story", "author_samizdis", "story_32417081"], "author": "samizdis", "created_at": "2022-08-10T19:17:16Z", "created_at_i": 1660159036, "num_comments": 0, "objectID": "32417081", "points": 3, "story_id": 32417081, "title": "Porsche Taycan Turbo S Sets Fastest Production EV N\u00fcrburgring Lap Record", "updated_at": "2024-09-20T11:46:50Z", "url": "https://www.thedrive.com/news/porsche-taycan-turbo-s-fastest-production-ev-nurburgring-lap-record"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ij23"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "Hello Hacker News,<p>Stop relying on benchmarks and easily test LLMs in <em>production</em>. \nTry it here: <a href=\"https://admin.litellm.ai/\" rel=\"nofollow noreferrer\">https://admin.litellm.ai/</a><p>LiteLLM allows you to simplify calling any LLM as a drop in replacement for gpt-3.5-<em>turbo</em><p>We're launching `completion_with_split_tests` to easily A/B test all LLMs.<p>Example usage - 1 function:\ncompletion_with_split_tests(<p><pre><code>  models={\n &quot;claude-2&quot;: 0.4, \n &quot;gpt-3.5-<em>turbo</em>&quot;: 0.6\n  }, \n\n  messages=messages,\n\n  temperature=temperature</code></pre>\n)<p>For each completion call we allow you to:<p>- Control/Modify LLM configs (prompt, temperature, max_tokens etc without needing to edit code)<p>- Easily swap in/out 100+ LLMs without redeploying code<p>- View Input/Outputs for each LLM on our UI<p>- Retry requests with an alternate LLM<p>Happy completion()!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: LiteLLM - Open source library A/B test LLMs in <em>Production</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://litellm.vercel.app/docs/tutorials/ab_test_llms"}}, "_tags": ["story", "author_ij23", "story_37362967", "show_hn"], "author": "ij23", "created_at": "2023-09-02T16:25:38Z", "created_at_i": 1693671938, "num_comments": 0, "objectID": "37362967", "points": 2, "story_id": 37362967, "story_text": "Hello Hacker News,<p>Stop relying on benchmarks and easily test LLMs in production. \nTry it here: <a href=\"https:&#x2F;&#x2F;admin.litellm.ai&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;admin.litellm.ai&#x2F;</a><p>LiteLLM allows you to simplify calling any LLM as a drop in replacement for gpt-3.5-turbo<p>We&#x27;re launching `completion_with_split_tests` to easily A&#x2F;B test all LLMs.<p>Example usage - 1 function:\ncompletion_with_split_tests(<p><pre><code>  models={\n &quot;claude-2&quot;: 0.4, \n &quot;gpt-3.5-turbo&quot;: 0.6\n  }, \n\n  messages=messages,\n\n  temperature=temperature</code></pre>\n)<p>For each completion call we allow you to:<p>- Control&#x2F;Modify LLM configs (prompt, temperature, max_tokens etc without needing to edit code)<p>- Easily swap in&#x2F;out 100+ LLMs without redeploying code<p>- View Input&#x2F;Outputs for each LLM on our UI<p>- Retry requests with an alternate LLM<p>Happy completion()!", "title": "Show HN: LiteLLM - Open source library A/B test LLMs in Production", "updated_at": "2024-09-20T15:03:05Z", "url": "https://litellm.vercel.app/docs/tutorials/ab_test_llms"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ashvardanian"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "A few months ago, I benchmarked FastAPI on an i9 MacBook Pro. I couldn't believe my eyes. A primary REST endpoint to `sum` two integers took 6 milliseconds to evaluate. It is okay if you are targeting a server in another city, but it should be less when your client and server apps are running on the same machine.<p>FastAPI would have bottleneck-ed the inference of our lightweight UForm neural networks recently trending on HN under the title &quot;Beating OpenAI CLIP with 100x less data and compute&quot;. (Thank you all for the kind words!) So I wrote another library.<p>It has been a while since I have written networking libraries, so I was eager to try the newer io_uring networking functionality added by Jens Axboe in kernel 5.19. TLDR: It's excellent! We used pre-registered buffers and re-allocated file descriptors from a managed pool. Some other parts, like multi-shot requests, also look intriguing, but we couldn't see a flawless way to integrate them into UJRPC. Maybe next time.<p>Like a parent with two kids, we tell everyone we love Kernel Bypass and SIMD equally. So I decided to combine the two, potentially implementing one of the fastest implementations of the most straightforward RPC protocol - JSON-RPC. ~~Healthy and Fun~~ Efficient and Simple, what can be better?<p>By now, you may already guess at least one of the dependencies - `simdjson` by Daniel Lemiere, that has become the industry standard. io_uring is generally very fast, even with a single core. Adding more polling threads may only increase congestion. We needed to continue using no more than one thread, but parsing messages may involve more work than just invoking a JSON parser.<p>JSON-RPC is transport agnostic. The incoming requests can be sent over HTTP, pre-pended by rows of headers. Those would have to be POSTs and generally contain Content-Length and Content-Type. There is a SIMD-accelerated library for that as well. It is called `picohttpparser`, uses SSE, and is maintained by H2O.<p>The story doesn't end there. JSON is limited. Passing binary strings is a nightmare. The most common approach is to encode them with base-64. So we took the <em>Turbo</em>-Base64 from the PowTurbo project to decode those binary strings.<p>The core implementation of UJRPC is under 2000 lines of C++. Knowing that those lines connect 3 great libraries with the newest and coolest parts of Linux is enough to put a smile on my face. Most people are more rational, so here is another reason to be cheerful.<p>- FastAPI throughput: 3'184 rps.\n- Python gRPC throughput: 9'849 rps.\n- UJRPC throughput:\n-- Python server with io_uring: 43'000 rps.\n-- C server with POSIX: 79'000 rps.\n-- C server with io_uring: 231'000 rps.<p>Granted, this is yet to be your batteries-included server. It can't balance the load, manage threads, spell S in HTTPS, or call parents when you misbehave in school. But at least part of it you shouldn't expect from a web server.<p>After following the standardization process of executors in C++ for the last N+1 years, we adapted the &quot;bring your runtime&quot; and &quot;bring your thread-pool&quot; policies. HTTPS support, however, is our next primary objective.<p>---<p>Of course, it is a pre-<em>production</em> project and must have a lot of bugs. Don't hesitate to report them. We have huge plans for this tiny package and will potentially make it the default transport of UKV: <a href=\"https://github.com/unum-cloud/ukv\">https://github.com/unum-cloud/ukv</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Faster FastAPI with simdjson and io_uring on Linux 5.19"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/unum-cloud/ujrpc"}}, "_tags": ["story", "author_ashvardanian", "story_35042316", "show_hn"], "author": "ashvardanian", "children": [35042609, 35042699, 35042709, 35042733, 35042807, 35042895, 35043092, 35043349, 35044940, 35045476, 35046029, 35049669], "created_at": "2023-03-06T15:41:23Z", "created_at_i": 1678117283, "num_comments": 90, "objectID": "35042316", "points": 290, "story_id": 35042316, "story_text": "A few months ago, I benchmarked FastAPI on an i9 MacBook Pro. I couldn&#x27;t believe my eyes. A primary REST endpoint to `sum` two integers took 6 milliseconds to evaluate. It is okay if you are targeting a server in another city, but it should be less when your client and server apps are running on the same machine.<p>FastAPI would have bottleneck-ed the inference of our lightweight UForm neural networks recently trending on HN under the title &quot;Beating OpenAI CLIP with 100x less data and compute&quot;. (Thank you all for the kind words!) So I wrote another library.<p>It has been a while since I have written networking libraries, so I was eager to try the newer io_uring networking functionality added by Jens Axboe in kernel 5.19. TLDR: It&#x27;s excellent! We used pre-registered buffers and re-allocated file descriptors from a managed pool. Some other parts, like multi-shot requests, also look intriguing, but we couldn&#x27;t see a flawless way to integrate them into UJRPC. Maybe next time.<p>Like a parent with two kids, we tell everyone we love Kernel Bypass and SIMD equally. So I decided to combine the two, potentially implementing one of the fastest implementations of the most straightforward RPC protocol - JSON-RPC. ~~Healthy and Fun~~ Efficient and Simple, what can be better?<p>By now, you may already guess at least one of the dependencies - `simdjson` by Daniel Lemiere, that has become the industry standard. io_uring is generally very fast, even with a single core. Adding more polling threads may only increase congestion. We needed to continue using no more than one thread, but parsing messages may involve more work than just invoking a JSON parser.<p>JSON-RPC is transport agnostic. The incoming requests can be sent over HTTP, pre-pended by rows of headers. Those would have to be POSTs and generally contain Content-Length and Content-Type. There is a SIMD-accelerated library for that as well. It is called `picohttpparser`, uses SSE, and is maintained by H2O.<p>The story doesn&#x27;t end there. JSON is limited. Passing binary strings is a nightmare. The most common approach is to encode them with base-64. So we took the Turbo-Base64 from the PowTurbo project to decode those binary strings.<p>The core implementation of UJRPC is under 2000 lines of C++. Knowing that those lines connect 3 great libraries with the newest and coolest parts of Linux is enough to put a smile on my face. Most people are more rational, so here is another reason to be cheerful.<p>- FastAPI throughput: 3&#x27;184 rps.\n- Python gRPC throughput: 9&#x27;849 rps.\n- UJRPC throughput:\n-- Python server with io_uring: 43&#x27;000 rps.\n-- C server with POSIX: 79&#x27;000 rps.\n-- C server with io_uring: 231&#x27;000 rps.<p>Granted, this is yet to be your batteries-included server. It can&#x27;t balance the load, manage threads, spell S in HTTPS, or call parents when you misbehave in school. But at least part of it you shouldn&#x27;t expect from a web server.<p>After following the standardization process of executors in C++ for the last N+1 years, we adapted the &quot;bring your runtime&quot; and &quot;bring your thread-pool&quot; policies. HTTPS support, however, is our next primary objective.<p>---<p>Of course, it is a pre-production project and must have a lot of bugs. Don&#x27;t hesitate to report them. We have huge plans for this tiny package and will potentially make it the default transport of UKV: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;ukv\">https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;ukv</a>", "title": "Show HN: Faster FastAPI with simdjson and io_uring on Linux 5.19", "updated_at": "2025-04-27T23:00:46Z", "url": "https://github.com/unum-cloud/ujrpc"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mriolfi"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "Hi HN! We\u2019re Matheus &amp; J\u00e9r\u00f4me and we\u2019re the co-founders of Tint(<a href=\"http://www.tint.ai\" rel=\"nofollow\">http://www.tint.ai</a>). We help companies add insurance to their products.<p>Many companies, such as marketplaces, merchants, and travel agents could include insurance as part of their products and services to make them more valuable to their customers. For example, insurance will be included when you rent a campervan for a weekend trip at Outdoorsy, to protect you if anything goes wrong. Our platform provides everything that is needed: software, access to insurers, compliance\u2014everything required to manage risk and protect users, profitably.<p>We met in 2014 when we were early employees at <em>Turo</em>, the car-sharing startup. While there, we saw the potential that insurance products have and also saw how hard it was to fully capitalize on it. <em>Turo</em> has an obvious and pressing need for insurance, but to fill it, they had to build their own systems, find insurers to back the program, and ensure compliance with state laws. None of this was their core business. We got inspired by the problem and by the opportunity to solve it, so we decided to create Tint.<p>Here is a real example from Riders Share, one of our clients: you go to their website/app to rent a motorbike for the weekend and find an awesome Harley Davidson. You proceed to checkout, see a few protection/insurance options, select one, and book the trip. You won't notice, but Riders Share's app has used Tint to risk-score the transaction, decide if it should be confirmed, and calculate how much the protection should cost.<p>Now, imagine you are a developer working on this project and need to add insurance to the product. What do you do? Instead of reinventing the wheel and adding more lines of code to maintain, you can leverage our APIs to integrate all the touchpoints required to sell insurance to your users (risk selection, quotes, issuing policy, claims, \u2026). All the logic for the API responses is configured from our app so your insurance team can easily iterate on the next versions of your insurance product. Oh, and we also train machine learning models so we can recommend ways to improve its performance.<p>We're live in <em>production</em> and have helped our clients embed hundreds of thousands of insurance policies. While our tech applies to any insurance use case, we are initially targeting marketplaces that embed insurance.<p>We'd love to hear any of your ideas or experiences in this space.<p>Thanks, Matheus + J\u00e9r\u00f4me"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Tint (YC W21) \u2013 Embed insurance into any product"}}, "_tags": ["story", "author_mriolfi", "story_26026526", "launch_hn"], "author": "mriolfi", "children": [26026583, 26026612, 26026784, 26027159, 26027197, 26028057, 26028568, 26028687, 26029095, 26029451, 26030435, 26030898, 26033270, 26033885, 26034459, 26035807, 26036570, 26036785], "created_at": "2021-02-04T16:02:28Z", "created_at_i": 1612454548, "num_comments": 48, "objectID": "26026526", "points": 133, "story_id": 26026526, "story_text": "Hi HN! We\u2019re Matheus &amp; J\u00e9r\u00f4me and we\u2019re the co-founders of Tint(<a href=\"http:&#x2F;&#x2F;www.tint.ai\" rel=\"nofollow\">http:&#x2F;&#x2F;www.tint.ai</a>). We help companies add insurance to their products.<p>Many companies, such as marketplaces, merchants, and travel agents could include insurance as part of their products and services to make them more valuable to their customers. For example, insurance will be included when you rent a campervan for a weekend trip at Outdoorsy, to protect you if anything goes wrong. Our platform provides everything that is needed: software, access to insurers, compliance\u2014everything required to manage risk and protect users, profitably.<p>We met in 2014 when we were early employees at Turo, the car-sharing startup. While there, we saw the potential that insurance products have and also saw how hard it was to fully capitalize on it. Turo has an obvious and pressing need for insurance, but to fill it, they had to build their own systems, find insurers to back the program, and ensure compliance with state laws. None of this was their core business. We got inspired by the problem and by the opportunity to solve it, so we decided to create Tint.<p>Here is a real example from Riders Share, one of our clients: you go to their website&#x2F;app to rent a motorbike for the weekend and find an awesome Harley Davidson. You proceed to checkout, see a few protection&#x2F;insurance options, select one, and book the trip. You won&#x27;t notice, but Riders Share&#x27;s app has used Tint to risk-score the transaction, decide if it should be confirmed, and calculate how much the protection should cost.<p>Now, imagine you are a developer working on this project and need to add insurance to the product. What do you do? Instead of reinventing the wheel and adding more lines of code to maintain, you can leverage our APIs to integrate all the touchpoints required to sell insurance to your users (risk selection, quotes, issuing policy, claims, \u2026). All the logic for the API responses is configured from our app so your insurance team can easily iterate on the next versions of your insurance product. Oh, and we also train machine learning models so we can recommend ways to improve its performance.<p>We&#x27;re live in production and have helped our clients embed hundreds of thousands of insurance policies. While our tech applies to any insurance use case, we are initially targeting marketplaces that embed insurance.<p>We&#x27;d love to hear any of your ideas or experiences in this space.<p>Thanks, Matheus + J\u00e9r\u00f4me", "title": "Launch HN: Tint (YC W21) \u2013 Embed insurance into any product", "updated_at": "2024-09-20T07:50:08Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "gavinuhma"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "We\u2019ve built the Cape API so developers can keep sensitive data private while prompting LLMs like GPT-4 and GPT 3.5 <em>Turbo</em>.<p>With Cape, you can easily de-identify sensitive data before sending it to OpenAI. In addition, you can create embeddings from sensitive text and documents and perform vector searches to improve your prompt context all while keeping the data confidential.<p>Developers are using Cape with data like financial statements, legal contracts, and internal/proprietary knowledge that would otherwise be too sensitive to process with the ChatGPT API.<p>You can try CapeChat, our playground for the API at <a href=\"https://chat.capeprivacy.com/\" rel=\"nofollow noreferrer\">https://chat.capeprivacy.com/</a><p>The Cape API is self-serve, and has a free tier. The main features of the API are:<p>De-identification \u2014 Redacts sensitive data like PII, PCI, and PHI from your text and documents.<p>Re-identification \u2014 Reverts de-identified data back to the original form.<p>Upload documents \u2014 Converts sensitive documents to embeddings (supports PDF, Excel, Word, CSV, TXT, PowerPoint, and Markdown).<p>Vector Search \u2014 Performs a vector search on your embeddings to augment your prompts with context.<p>To do all this, we work with a number of privacy and security techniques.<p>First of all, we process data within a secure enclave, which is an isolated VM with in-memory encryption. The data remains confidential. No human, including our team at Cape or the underlying cloud provider, can see the data.<p>Secondly, within the secure enclave, Cape de-identifies your data by removing PII, PCI, and PHI before it is processed by OpenAI. As GPT-4 generates and streams back the response tokens, we re-identify the data so it becomes readable again.<p>In addition to de-identification, Cape also has API endpoints for embeddings, vector search, and document uploads, which all operate entirely within the secure enclave (no external calls and no sub-processors).<p>Why did we build this?<p>Developers asked us for help! We've been working at the intersection of privacy and AI since 2017, and with the explosion of interest in LLMs we've had a lot of questions from developers.<p>Privacy and security remain one of the biggest barriers to adopting AI like LLMs, particularly for sensitive data.<p>We\u2019ve spoken with many companies who have been experimenting with ChatGPT or the GPT-4 API and they are extremely excited about the potential, however they find taking an LLM powered feature from PoC to <em>production</em> is a major lift, and it\u2019s uncharted territory for many teams. Developers have questions like:<p>- How do we ensure the privacy of our customer\u2019s data if we\u2019re sending it to OpenAI?<p>- How can we securely feed large bodies of internal, proprietary data into GPT-4?<p>- How can we mitigate hallucinations and bias so that we have higher trust in AI generated text?<p>The features of the Cape API are designed to help solve these problems for developers, and we have a number of early customers using the API in <em>production</em> already.<p>To get started, checkout our docs: <a href=\"https://docs.capeprivacy.com/\" rel=\"nofollow noreferrer\">https://docs.capeprivacy.com/</a><p>View the API reference: <a href=\"https://api.capeprivacy.com/v1/redoc\" rel=\"nofollow noreferrer\">https://api.capeprivacy.com/v1/redoc</a><p>Join the discussion on our Discord: <a href=\"https://discord.gg/nQW7YxUYjh\" rel=\"nofollow noreferrer\">https://discord.gg/nQW7YxUYjh</a><p>And of course try the CapeChat playground at <a href=\"https://chat.capeprivacy.com/\" rel=\"nofollow noreferrer\">https://chat.capeprivacy.com/</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Cape API \u2013 Keep your sensitive data private while using GPT-4"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://capeprivacy.com"}}, "_tags": ["story", "author_gavinuhma", "story_36492149", "show_hn"], "author": "gavinuhma", "children": [36492994, 36493126, 36493255, 36493259, 36493722, 36493946, 36494930, 36500840], "created_at": "2023-06-27T13:04:08Z", "created_at_i": 1687871048, "num_comments": 29, "objectID": "36492149", "points": 29, "story_id": 36492149, "story_text": "We\u2019ve built the Cape API so developers can keep sensitive data private while prompting LLMs like GPT-4 and GPT 3.5 Turbo.<p>With Cape, you can easily de-identify sensitive data before sending it to OpenAI. In addition, you can create embeddings from sensitive text and documents and perform vector searches to improve your prompt context all while keeping the data confidential.<p>Developers are using Cape with data like financial statements, legal contracts, and internal&#x2F;proprietary knowledge that would otherwise be too sensitive to process with the ChatGPT API.<p>You can try CapeChat, our playground for the API at <a href=\"https:&#x2F;&#x2F;chat.capeprivacy.com&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;chat.capeprivacy.com&#x2F;</a><p>The Cape API is self-serve, and has a free tier. The main features of the API are:<p>De-identification \u2014 Redacts sensitive data like PII, PCI, and PHI from your text and documents.<p>Re-identification \u2014 Reverts de-identified data back to the original form.<p>Upload documents \u2014 Converts sensitive documents to embeddings (supports PDF, Excel, Word, CSV, TXT, PowerPoint, and Markdown).<p>Vector Search \u2014 Performs a vector search on your embeddings to augment your prompts with context.<p>To do all this, we work with a number of privacy and security techniques.<p>First of all, we process data within a secure enclave, which is an isolated VM with in-memory encryption. The data remains confidential. No human, including our team at Cape or the underlying cloud provider, can see the data.<p>Secondly, within the secure enclave, Cape de-identifies your data by removing PII, PCI, and PHI before it is processed by OpenAI. As GPT-4 generates and streams back the response tokens, we re-identify the data so it becomes readable again.<p>In addition to de-identification, Cape also has API endpoints for embeddings, vector search, and document uploads, which all operate entirely within the secure enclave (no external calls and no sub-processors).<p>Why did we build this?<p>Developers asked us for help! We&#x27;ve been working at the intersection of privacy and AI since 2017, and with the explosion of interest in LLMs we&#x27;ve had a lot of questions from developers.<p>Privacy and security remain one of the biggest barriers to adopting AI like LLMs, particularly for sensitive data.<p>We\u2019ve spoken with many companies who have been experimenting with ChatGPT or the GPT-4 API and they are extremely excited about the potential, however they find taking an LLM powered feature from PoC to production is a major lift, and it\u2019s uncharted territory for many teams. Developers have questions like:<p>- How do we ensure the privacy of our customer\u2019s data if we\u2019re sending it to OpenAI?<p>- How can we securely feed large bodies of internal, proprietary data into GPT-4?<p>- How can we mitigate hallucinations and bias so that we have higher trust in AI generated text?<p>The features of the Cape API are designed to help solve these problems for developers, and we have a number of early customers using the API in production already.<p>To get started, checkout our docs: <a href=\"https:&#x2F;&#x2F;docs.capeprivacy.com&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;docs.capeprivacy.com&#x2F;</a><p>View the API reference: <a href=\"https:&#x2F;&#x2F;api.capeprivacy.com&#x2F;v1&#x2F;redoc\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;api.capeprivacy.com&#x2F;v1&#x2F;redoc</a><p>Join the discussion on our Discord: <a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;nQW7YxUYjh\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;discord.gg&#x2F;nQW7YxUYjh</a><p>And of course try the CapeChat playground at <a href=\"https:&#x2F;&#x2F;chat.capeprivacy.com&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;chat.capeprivacy.com&#x2F;</a>", "title": "Show HN: Cape API \u2013 Keep your sensitive data private while using GPT-4", "updated_at": "2024-09-20T14:31:04Z", "url": "https://capeprivacy.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "aithrowaway"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "I\u2019ve been playing around with the OpenAI APIs for an AI project. My GPT4 limit is 10k tokens per minute, which seems to be the current default for new accounts. I\u2019m running into it constantly just in development, and that\u2019s for a pretty low-key use case.<p>It seems like to use it in <em>production</em> for even a modestly successful product would require like 1000x higher limits.<p>According to OpenAI\u2019s docs, they are not considering rate limit increases at all for GPT4. Azure OpenAI has 2x higher starting limits, but that\u2019s still not close to enough, and they\u2019re not considering rate limit increases currently either.<p>I\u2019m sure this will change with time, and I understand OpenAI\u2019s reasons for rolling out access gradually. Still, I see some services out there that appear to be leaning heavily on GPT4, or are at least finding ways of getting comparable quality, that seem like they shouldn\u2019t be possible without far higher limits. I\u2019m curious if anyone can speak in general terms on how this is being accomplished? Do they have higher limits from earlier stages of the rollout? Or special deals with OpenAI/MSFT? Or are they using fine-tuning and other strategies to make 3.5 (or other models) reach near 4-level quality? Or using TOS-violating hacks like rotating requests across many API keys?<p>Building in a way that allows users to make any required GPT4 calls locally with their own API key seems like a possibility as well depending on the app, but that obviously limits the audience and isn\u2019t great for ux or onboarding.<p>For my use case, GPT4 is just barely reaching the point of viability\u2014not perfect but good enough to provide significant value, whereas 3.5 <em>turbo</em> is woefully inadequate. While it doesn\u2019t seem like a bad idea necessarily to build it out for now within the limits and then be well-positioned when they finally get increased, I\u2019m mainly just wondering whether everyone\u2019s in the same boat on this or if people are finding legitimate workarounds that don\u2019t require some insider connection."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: How are people getting around GPT4 rate limits?"}}, "_tags": ["story", "author_aithrowaway", "story_37426999", "ask_hn"], "author": "aithrowaway", "children": [37427198, 37427864, 37441756], "created_at": "2023-09-07T23:17:34Z", "created_at_i": 1694128654, "num_comments": 13, "objectID": "37426999", "points": 13, "story_id": 37426999, "story_text": "I\u2019ve been playing around with the OpenAI APIs for an AI project. My GPT4 limit is 10k tokens per minute, which seems to be the current default for new accounts. I\u2019m running into it constantly just in development, and that\u2019s for a pretty low-key use case.<p>It seems like to use it in production for even a modestly successful product would require like 1000x higher limits.<p>According to OpenAI\u2019s docs, they are not considering rate limit increases at all for GPT4. Azure OpenAI has 2x higher starting limits, but that\u2019s still not close to enough, and they\u2019re not considering rate limit increases currently either.<p>I\u2019m sure this will change with time, and I understand OpenAI\u2019s reasons for rolling out access gradually. Still, I see some services out there that appear to be leaning heavily on GPT4, or are at least finding ways of getting comparable quality, that seem like they shouldn\u2019t be possible without far higher limits. I\u2019m curious if anyone can speak in general terms on how this is being accomplished? Do they have higher limits from earlier stages of the rollout? Or special deals with OpenAI&#x2F;MSFT? Or are they using fine-tuning and other strategies to make 3.5 (or other models) reach near 4-level quality? Or using TOS-violating hacks like rotating requests across many API keys?<p>Building in a way that allows users to make any required GPT4 calls locally with their own API key seems like a possibility as well depending on the app, but that obviously limits the audience and isn\u2019t great for ux or onboarding.<p>For my use case, GPT4 is just barely reaching the point of viability\u2014not perfect but good enough to provide significant value, whereas 3.5 turbo is woefully inadequate. While it doesn\u2019t seem like a bad idea necessarily to build it out for now within the limits and then be well-positioned when they finally get increased, I\u2019m mainly just wondering whether everyone\u2019s in the same boat on this or if people are finding legitimate workarounds that don\u2019t require some insider connection.", "title": "Ask HN: How are people getting around GPT4 rate limits?", "updated_at": "2024-09-20T15:00:07Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "shensations"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "I'm the head of product at ROOST, a nonprofit focused on democratizing online safety technology through open source software, and one of the people behind Osprey, an open-source rules engine for fighting abuse at scale. We just released v1.0, which is what's running in <em>production</em> at Discord and Bluesky (and soon Matrix!), processing hundreds of millions of events daily.<p>Most platforms face a bad choice: build custom abuse-fighting infrastructure from scratch, or use closed vendor solutions. We built Osprey to give smaller platforms and communities access to the same quality of tooling that large companies build internally. All of our projects are self-hosted, which means data stays in your own system.<p>v1.0 is our proven <em>production</em>-ready release. It includes:<p>- Core rules engine: The rules engine itself lets you write detection logic. Rules can extract features, mutate labels, emit verdicts, and trigger downstream actions.<p>- Interface for open-ended investigation: The open-ended interface means you can identify new abuse patterns as they emerge. The whole system is designed for high throughput and horizontal scaling.<p>- Labeling service: human and automated labels on entities (users, IPs, emails) that feed into rules and workflows<p>- Rust coordinator: horizontal scaling and load balancing across workers via gRPC streams and service discovery<p>- Event stream: real-time investigation UI with searchable history using our SML (custom python language for safety teams to use if they don't know python or programming languages) query language<p>License: Apache 2.0<p>Repo: <a href=\"https://github.com/roostorg/osprey\" rel=\"nofollow\">https://github.com/roostorg/osprey</a><p>v1.0 Release notes: <a href=\"https://github.com/roostorg/osprey/releases/tag/1.0\" rel=\"nofollow\">https://github.com/roostorg/osprey/releases/tag/1.0</a><p>Github Codespace demo (note: if this is not running when you click on it, it probably idled out): <a href=\"https://turbo-fiesta-p46xvxwp4vfwqv-5002.app.github.dev/?start=2026-02-01T18%3A58%3A24Z&amp;end=2026-02-02T18%3A58%3A24Z&amp;queryFilter=SuspiciousDisplayName+%3D%3D+True&amp;interval=day&amp;customSummaryFeatures=AccountCreatedAt&amp;customSummaryFeatures=FollowerCount&amp;customSummaryFeatures=PostCount&amp;customSummaryFeatures=ActionName&amp;customSummaryFeatures=EventType&amp;customSummaryFeatures=UserId&amp;customSummaryFeatures=DisplayName&amp;customSummaryFeatures=IpAddress&amp;customSummaryFeatures=PostText&amp;topn=DisplayName&amp;chart=\" rel=\"nofollow\">https://<em>turbo</em>-fiesta-p46xvxwp4vfwqv-5002.app.github.dev/?sta...</a><p>Our FOSDEM talk from last week: <a href=\"https://fosdem.org/2026/schedule/event/U7ABHE-roost-osprey/\" rel=\"nofollow\">https://fosdem.org/2026/schedule/event/U7ABHE-roost-osprey/</a><p>Thanks for reading! I'm happy to answer questions, get feedback, or discuss things."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Osprey 1.0, open-source rules engine for online safety"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://roost.tools/blog/introducing-osprey-v1-0-open-source-infrastructure-for-real-time-abuse-mitigation/"}}, "_tags": ["story", "author_shensations", "story_46860249", "show_hn"], "author": "shensations", "created_at": "2026-02-02T19:37:52Z", "created_at_i": 1770061072, "num_comments": 0, "objectID": "46860249", "points": 3, "story_id": 46860249, "story_text": "I&#x27;m the head of product at ROOST, a nonprofit focused on democratizing online safety technology through open source software, and one of the people behind Osprey, an open-source rules engine for fighting abuse at scale. We just released v1.0, which is what&#x27;s running in production at Discord and Bluesky (and soon Matrix!), processing hundreds of millions of events daily.<p>Most platforms face a bad choice: build custom abuse-fighting infrastructure from scratch, or use closed vendor solutions. We built Osprey to give smaller platforms and communities access to the same quality of tooling that large companies build internally. All of our projects are self-hosted, which means data stays in your own system.<p>v1.0 is our proven production-ready release. It includes:<p>- Core rules engine: The rules engine itself lets you write detection logic. Rules can extract features, mutate labels, emit verdicts, and trigger downstream actions.<p>- Interface for open-ended investigation: The open-ended interface means you can identify new abuse patterns as they emerge. The whole system is designed for high throughput and horizontal scaling.<p>- Labeling service: human and automated labels on entities (users, IPs, emails) that feed into rules and workflows<p>- Rust coordinator: horizontal scaling and load balancing across workers via gRPC streams and service discovery<p>- Event stream: real-time investigation UI with searchable history using our SML (custom python language for safety teams to use if they don&#x27;t know python or programming languages) query language<p>License: Apache 2.0<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;roostorg&#x2F;osprey\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;roostorg&#x2F;osprey</a><p>v1.0 Release notes: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;roostorg&#x2F;osprey&#x2F;releases&#x2F;tag&#x2F;1.0\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;roostorg&#x2F;osprey&#x2F;releases&#x2F;tag&#x2F;1.0</a><p>Github Codespace demo (note: if this is not running when you click on it, it probably idled out): <a href=\"https:&#x2F;&#x2F;turbo-fiesta-p46xvxwp4vfwqv-5002.app.github.dev&#x2F;?start=2026-02-01T18%3A58%3A24Z&amp;end=2026-02-02T18%3A58%3A24Z&amp;queryFilter=SuspiciousDisplayName+%3D%3D+True&amp;interval=day&amp;customSummaryFeatures=AccountCreatedAt&amp;customSummaryFeatures=FollowerCount&amp;customSummaryFeatures=PostCount&amp;customSummaryFeatures=ActionName&amp;customSummaryFeatures=EventType&amp;customSummaryFeatures=UserId&amp;customSummaryFeatures=DisplayName&amp;customSummaryFeatures=IpAddress&amp;customSummaryFeatures=PostText&amp;topn=DisplayName&amp;chart=\" rel=\"nofollow\">https:&#x2F;&#x2F;turbo-fiesta-p46xvxwp4vfwqv-5002.app.github.dev&#x2F;?sta...</a><p>Our FOSDEM talk from last week: <a href=\"https:&#x2F;&#x2F;fosdem.org&#x2F;2026&#x2F;schedule&#x2F;event&#x2F;U7ABHE-roost-osprey&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;fosdem.org&#x2F;2026&#x2F;schedule&#x2F;event&#x2F;U7ABHE-roost-osprey&#x2F;</a><p>Thanks for reading! I&#x27;m happy to answer questions, get feedback, or discuss things.", "title": "Show HN: Osprey 1.0, open-source rules engine for online safety", "updated_at": "2026-02-21T19:07:30Z", "url": "https://roost.tools/blog/introducing-osprey-v1-0-open-source-infrastructure-for-real-time-abuse-mitigation/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "behnamoh"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "Gemini 1.0 Ultra and Gemini 1.5 Advanced are all the rage right now, but I wanted to say that in <em>production</em>, you're not going to use vanilla free-form text LLMs; you use LLMs that can reliably generate JSON. OpenAI knew this months ago and built this feature into their models (not as an afterthought like grammars in local models). I still haven't seen any other model get close to GPT-4-<em>turbo</em>'s ability to call functions and return JSON output. If I'm building an app using LLMs, right now it's an obvious decision to choose GPT models, not Gemini or local models (I love local models and I'm a contributor to some OSS projects, but function calling is not there yet).<p>Another thing people often neglect is that OpenAI basically owns the API design right now. The fact that there are several drop-in replacements for their API shows how much apps depend on the specific structure of their API. For comparison, this is Gemini's REST API usage:<p>```\n    echo '{\n      &quot;contents&quot;:[\n        {\n          &quot;parts&quot;:[\n            {&quot;text&quot;: &quot;What is this picture?&quot;},\n            {\n              &quot;inline_data&quot;: {\n                &quot;mime_type&quot;:&quot;image/jpeg&quot;,\n                &quot;data&quot;: &quot;'$(base64 -w0 image.jpg)'&quot;\n              }\n            }\n          ]\n        }\n      ]\n    }' &gt; request.json<p><pre><code>    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro-vision:generateContent?key=${API_KEY} \\\n            -H 'Content-Type: application/json' \\\n            -d @request.json 2&gt; /dev/null | grep &quot;text&quot;</code></pre>\n```<p>And here's another one:<p>```\n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n        -H 'Content-Type: application/json' \\\n        -X POST \\\n        -d '{\n            &quot;contents&quot;: [{\n                &quot;parts&quot;:[\n                    {&quot;text&quot;: &quot;Write a story about a magic backpack.&quot;}\n                ]\n            }],\n            &quot;safetySettings&quot;: [\n                {\n                    &quot;category&quot;: &quot;HARM_CATEGORY_DANGEROUS_CONTENT&quot;,\n                    &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;\n                }\n            ],\n            &quot;generationConfig&quot;: {\n                &quot;stopSequences&quot;: [\n                    &quot;Title&quot;\n                ],\n                &quot;temperature&quot;: 1.0,\n                &quot;maxOutputTokens&quot;: 800,\n                &quot;topP&quot;: 0.8,\n                &quot;topK&quot;: 10\n            }\n        }'  2&gt; /dev/null | grep &quot;text&quot;\n```"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Tell HN: OpenAI still has a moat, it's called function calling and its API"}}, "_tags": ["story", "author_behnamoh", "story_39457299", "ask_hn"], "author": "behnamoh", "children": [39457331, 39457551, 39457861, 39462815], "created_at": "2024-02-21T18:10:39Z", "created_at_i": 1708539039, "num_comments": 4, "objectID": "39457299", "points": 2, "story_id": 39457299, "story_text": "Gemini 1.0 Ultra and Gemini 1.5 Advanced are all the rage right now, but I wanted to say that in production, you&#x27;re not going to use vanilla free-form text LLMs; you use LLMs that can reliably generate JSON. OpenAI knew this months ago and built this feature into their models (not as an afterthought like grammars in local models). I still haven&#x27;t seen any other model get close to GPT-4-turbo&#x27;s ability to call functions and return JSON output. If I&#x27;m building an app using LLMs, right now it&#x27;s an obvious decision to choose GPT models, not Gemini or local models (I love local models and I&#x27;m a contributor to some OSS projects, but function calling is not there yet).<p>Another thing people often neglect is that OpenAI basically owns the API design right now. The fact that there are several drop-in replacements for their API shows how much apps depend on the specific structure of their API. For comparison, this is Gemini&#x27;s REST API usage:<p>```\n    echo &#x27;{\n      &quot;contents&quot;:[\n        {\n          &quot;parts&quot;:[\n            {&quot;text&quot;: &quot;What is this picture?&quot;},\n            {\n              &quot;inline_data&quot;: {\n                &quot;mime_type&quot;:&quot;image&#x2F;jpeg&quot;,\n                &quot;data&quot;: &quot;&#x27;$(base64 -w0 image.jpg)&#x27;&quot;\n              }\n            }\n          ]\n        }\n      ]\n    }&#x27; &gt; request.json<p><pre><code>    curl https:&#x2F;&#x2F;generativelanguage.googleapis.com&#x2F;v1beta&#x2F;models&#x2F;gemini-pro-vision:generateContent?key=${API_KEY} \\\n            -H &#x27;Content-Type: application&#x2F;json&#x27; \\\n            -d @request.json 2&gt; &#x2F;dev&#x2F;null | grep &quot;text&quot;</code></pre>\n```<p>And here&#x27;s another one:<p>```\n    curl https:&#x2F;&#x2F;generativelanguage.googleapis.com&#x2F;v1beta&#x2F;models&#x2F;gemini-pro:generateContent?key=$API_KEY \\\n        -H &#x27;Content-Type: application&#x2F;json&#x27; \\\n        -X POST \\\n        -d &#x27;{\n            &quot;contents&quot;: [{\n                &quot;parts&quot;:[\n                    {&quot;text&quot;: &quot;Write a story about a magic backpack.&quot;}\n                ]\n            }],\n            &quot;safetySettings&quot;: [\n                {\n                    &quot;category&quot;: &quot;HARM_CATEGORY_DANGEROUS_CONTENT&quot;,\n                    &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;\n                }\n            ],\n            &quot;generationConfig&quot;: {\n                &quot;stopSequences&quot;: [\n                    &quot;Title&quot;\n                ],\n                &quot;temperature&quot;: 1.0,\n                &quot;maxOutputTokens&quot;: 800,\n                &quot;topP&quot;: 0.8,\n                &quot;topK&quot;: 10\n            }\n        }&#x27;  2&gt; &#x2F;dev&#x2F;null | grep &quot;text&quot;\n```", "title": "Tell HN: OpenAI still has a moat, it's called function calling and its API", "updated_at": "2024-09-20T16:27:37Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rodrigogs"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["turso", "production"], "value": "Built a platform where AI-generated Italian brain-rot characters chat with users through other AIs \u2013 creating a perfectly absurd meta-loop.<p>What makes this interesting: 100% AI-coded project. I didn't write a single line of TypeScript \u2013 just acted as &quot;AI whisperer&quot; guiding Claude 3.7 and Claude 4 through architecture decisions and debugging.<p>This project isn't supposed to be useful \u2013 it's an experiment to test AI agents to their limits and see what emerges from pure human-AI collaboration.<p>Key highlights:<p>182/182 tests passing across monorepo packages\nDual build system (ESM + CJS)\nReal-time AI character conversations with TTS\nGlass morphism design that exceeded expectations\nMulti-language support (6 languages)\nThe journey showed how AI coding evolved: Claude 3.7 needed constant supervision (creative but prone to &quot;hallucinations&quot;), while Claude 4 became a trusted partner delivering <em>production</em>-ready features autonomously.<p>Tech stack: Next.js 15, TypeScript monorepo with <em>Turbo</em>, LangChain, OpenAI integration, GitHub OAuth, Vercel deployment.<p>Source: <a href=\"https://github.com/rodrigogs/vibecode-playground\">https://github.com/rodrigogs/vibecode-playground</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Brain-rot Factory \u2013 Italian meme characters powered by AI (Next.js 15)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://brain-rot-factory.vercel.app/en/about"}}, "_tags": ["story", "author_rodrigogs", "story_44642702", "show_hn"], "author": "rodrigogs", "created_at": "2025-07-22T02:32:43Z", "created_at_i": 1753151563, "num_comments": 0, "objectID": "44642702", "points": 2, "story_id": 44642702, "story_text": "Built a platform where AI-generated Italian brain-rot characters chat with users through other AIs \u2013 creating a perfectly absurd meta-loop.<p>What makes this interesting: 100% AI-coded project. I didn&#x27;t write a single line of TypeScript \u2013 just acted as &quot;AI whisperer&quot; guiding Claude 3.7 and Claude 4 through architecture decisions and debugging.<p>This project isn&#x27;t supposed to be useful \u2013 it&#x27;s an experiment to test AI agents to their limits and see what emerges from pure human-AI collaboration.<p>Key highlights:<p>182&#x2F;182 tests passing across monorepo packages\nDual build system (ESM + CJS)\nReal-time AI character conversations with TTS\nGlass morphism design that exceeded expectations\nMulti-language support (6 languages)\nThe journey showed how AI coding evolved: Claude 3.7 needed constant supervision (creative but prone to &quot;hallucinations&quot;), while Claude 4 became a trusted partner delivering production-ready features autonomously.<p>Tech stack: Next.js 15, TypeScript monorepo with Turbo, LangChain, OpenAI integration, GitHub OAuth, Vercel deployment.<p>Source: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;rodrigogs&#x2F;vibecode-playground\">https:&#x2F;&#x2F;github.com&#x2F;rodrigogs&#x2F;vibecode-playground</a>", "title": "Show HN: Brain-rot Factory \u2013 Italian meme characters powered by AI (Next.js 15)", "updated_at": "2025-07-22T05:30:38Z", "url": "https://brain-rot-factory.vercel.app/en/about"}], "hitsPerPage": 15, "nbHits": 21, "nbPages": 2, "page": 0, "params": "query=turso+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 9, "processingTimingsMS": {"_request": {"roundTrip": 15}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 5, "scanning": 2, "total": 8}, "total": 9}, "query": "turso production", "serverTimeMS": 12}}