{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rjenkins"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Project Page: <a href=\"https://rotel.dev\" rel=\"nofollow\">https://rotel.dev</a>\nGithub: <a href=\"https://github.com/streamfold/rotel\" rel=\"nofollow\">https://github.com/streamfold/rotel</a><p>Hi HN, Ray and Mike here! We\u2019re building Rotel, a new high performance, resource efficient approach to <em>OpenTelemetry</em> collection. Rotel is open source (Apache License 2.0) and runs as a standalone process and collects telemetry from external processes or other collection agents. It consumes 75% less memory and 50% less CPU in benchmarks <a href=\"https://streamfold.github.io/rotel-otel-loadtests/benchmarks\" rel=\"nofollow\">https://streamfold.github.io/rotel-otel-loadtests/benchmarks</a>, so it is particularly well-suited for environments where resource optimization is paramount.<p>Mike and I love working on Observability solutions. We\u2019ve built and optimized telemetry data planes for products such as Librato, Papertrail and AppOptics. As we\u2019ve scaled high volume data processing systems over the years, we\u2019ve often found ourselves under pressure to improve SaaS margins or the resilience of complex distributed systems. This has led us to the same question again and again: How can we do this more reliably and efficiently?<p>We believe telemetry collection demands high levels of efficiency, and Rotel is our first step toward addressing that need. Rotel\u2019s Lambda extension <a href=\"https://github.com/streamfold/rotel-lambda-extension\" rel=\"nofollow\">https://github.com/streamfold/rotel-lambda-extension</a> has already unlocked the promise of <em>OpenTelemetry</em> in <em>production</em> serverless environments, where resource constraints previously made this nearly impossible.<p>With Rotel we\u2019re also rethinking DX for <em>OpenTelemetry</em> collection. Its lightweight footprint allows for packaging directly in runtimes like Python and Node.js, eliminating the need for additional configuration and deployment of sidecars. Rotel\u2019s Python Processor SDK enables you to build telemetry processors right from your favorite IDE with the ease and expressiveness of Python and the performance of Rust.<p>Getting started with Rotel is easy using prebuilt Docker containers, just run\u2026<p>`docker run -ti -p 4317-4318:4317-4318 streamfold/rotel --debug-log traces --exporter blackhole`<p>to start Rotel listening for OTLP traffic on ports 4317 &amp; 4318.<p>Check out the Getting Started guide for more install and config instructions: <a href=\"https://rotel.dev/docs/setup/getting-started\" rel=\"nofollow\">https://rotel.dev/docs/setup/getting-started</a>.<p>We\u2019re super excited to share Rotel with you and would love to hear what you think!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "Show HN: Rotel \u2013 Fast and Efficient <em>OpenTelemetry</em> Collection in Rust"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "https://rotel.dev/blog/rotel-fast-and-efficient-<em>opentelemetry</em>-collection-in-rust/"}}, "_tags": ["story", "author_rjenkins", "story_44800235", "show_hn"], "author": "rjenkins", "created_at": "2025-08-05T16:31:23Z", "created_at_i": 1754411483, "num_comments": 0, "objectID": "44800235", "points": 9, "story_id": 44800235, "story_text": "Project Page: <a href=\"https:&#x2F;&#x2F;rotel.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;rotel.dev</a>\nGithub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;streamfold&#x2F;rotel\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;streamfold&#x2F;rotel</a><p>Hi HN, Ray and Mike here! We\u2019re building Rotel, a new high performance, resource efficient approach to OpenTelemetry collection. Rotel is open source (Apache License 2.0) and runs as a standalone process and collects telemetry from external processes or other collection agents. It consumes 75% less memory and 50% less CPU in benchmarks <a href=\"https:&#x2F;&#x2F;streamfold.github.io&#x2F;rotel-otel-loadtests&#x2F;benchmarks\" rel=\"nofollow\">https:&#x2F;&#x2F;streamfold.github.io&#x2F;rotel-otel-loadtests&#x2F;benchmarks</a>, so it is particularly well-suited for environments where resource optimization is paramount.<p>Mike and I love working on Observability solutions. We\u2019ve built and optimized telemetry data planes for products such as Librato, Papertrail and AppOptics. As we\u2019ve scaled high volume data processing systems over the years, we\u2019ve often found ourselves under pressure to improve SaaS margins or the resilience of complex distributed systems. This has led us to the same question again and again: How can we do this more reliably and efficiently?<p>We believe telemetry collection demands high levels of efficiency, and Rotel is our first step toward addressing that need. Rotel\u2019s Lambda extension <a href=\"https:&#x2F;&#x2F;github.com&#x2F;streamfold&#x2F;rotel-lambda-extension\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;streamfold&#x2F;rotel-lambda-extension</a> has already unlocked the promise of OpenTelemetry in production serverless environments, where resource constraints previously made this nearly impossible.<p>With Rotel we\u2019re also rethinking DX for OpenTelemetry collection. Its lightweight footprint allows for packaging directly in runtimes like Python and Node.js, eliminating the need for additional configuration and deployment of sidecars. Rotel\u2019s Python Processor SDK enables you to build telemetry processors right from your favorite IDE with the ease and expressiveness of Python and the performance of Rust.<p>Getting started with Rotel is easy using prebuilt Docker containers, just run\u2026<p>`docker run -ti -p 4317-4318:4317-4318 streamfold&#x2F;rotel --debug-log traces --exporter blackhole`<p>to start Rotel listening for OTLP traffic on ports 4317 &amp; 4318.<p>Check out the Getting Started guide for more install and config instructions: <a href=\"https:&#x2F;&#x2F;rotel.dev&#x2F;docs&#x2F;setup&#x2F;getting-started\" rel=\"nofollow\">https:&#x2F;&#x2F;rotel.dev&#x2F;docs&#x2F;setup&#x2F;getting-started</a>.<p>We\u2019re super excited to share Rotel with you and would love to hear what you think!", "title": "Show HN: Rotel \u2013 Fast and Efficient OpenTelemetry Collection in Rust", "updated_at": "2025-08-19T00:00:01Z", "url": "https://rotel.dev/blog/rotel-fast-and-efficient-opentelemetry-collection-in-rust/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "gkop"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "Check out <em>OpenTelemetry</em>: <a href=\"https://opentelemetry.io/\" rel=\"nofollow\">https://<em>opentelemetry</em>.io/</a><p>&gt; <em>OpenTelemetry</em> is in beta across several languages and is suitable for use. We anticipate general availability soon.<p>Can folks share their early experience with Open Telemetry? Specifically, how are the integrations for major runtimes like Java and Node, when compared with, say, the fairly mature commercial integrations of New Relic?"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Ask HN: Is <em>OpenTelemetry</em> ready for <em>production</em> use?"}}, "_tags": ["story", "author_gkop", "story_29245670", "ask_hn"], "author": "gkop", "created_at": "2021-11-16T20:00:37Z", "created_at_i": 1637092837, "num_comments": 0, "objectID": "29245670", "points": 6, "story_id": 29245670, "story_text": "Check out OpenTelemetry: <a href=\"https:&#x2F;&#x2F;opentelemetry.io&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;opentelemetry.io&#x2F;</a><p>&gt; OpenTelemetry is in beta across several languages and is suitable for use. We anticipate general availability soon.<p>Can folks share their early experience with Open Telemetry? Specifically, how are the integrations for major runtimes like Java and Node, when compared with, say, the fairly mature commercial integrations of New Relic?", "title": "Ask HN: Is OpenTelemetry ready for production use?", "updated_at": "2024-09-20T09:51:24Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ericpsimon"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Hi HN, I'm Eric and I'm a recovering data engineer. Recently I have worked on the data platforms for multiple YC backed start-ups Kable (YC W22) and Finch (YC S20).<p>Every data team I've worked with struggles with data quality validation. Current solutions like Apache Deequ require spinning up entire Spark clusters just to check if your data meets basic quality constraints.<p>When I found Apache DataFusion, it was love at first sight - it provided the ergonomics of Apache Spark, without the overhead, JVM, etc. That is what led me to build Term. It is able to take advantage of the ergonomics of Spark without the overhead.<p>Term is a Rust library that provides Deequ-style data validation using Apache DataFusion. You can run comprehensive data quality checks anywhere - from your laptop to CI/CD pipelines - without any JVM or cluster setup. On a 1M row dataset with 20 constraints, Term completes validation in 0.21 seconds (vs 3.2 seconds without optimization) by intelligently batching operations into just 2 scans instead of 20.<p>The technical approach: Term leverages DataFusion's columnar processing engine to efficiently validate data in Arrow format. Validation rules compile directly to DataFusion's physical plans, and Rust's zero-cost abstractions mean the overhead is minimal. You get 100MB/s single-core throughput, which often outperforms distributed solutions for datasets under 100GB.<p>Term supports all the validation patterns you'd expect - completeness checks, uniqueness validation, statistical analysis (mean, correlation, standard deviation), pattern matching, custom SQL expressions, and built-in <em>OpenTelemetry</em> integration for <em>production</em> observability. The entire setup takes less than 5 minutes - just `cargo add term-guard` and you're validating data.<p>GitHub: <a href=\"https://github.com/withterm/term\">https://github.com/withterm/term</a><p>I built this because I was tired of seeing teams skip data validation entirely rather than deal with Spark infrastructure. With Term, you can add validation to any Rust data pipeline with minimal overhead and zero operational complexity.<p>Coming next: Python/Node.js bindings, streaming support, and database connectivity. I'm particularly excited about making this accessible beyond the Rust ecosystem.<p>I'd love feedback on:<p>- The validation API - does it cover your use cases?<p>- Performance on your real-world datasets<p>- What validation patterns you need that aren't supported yet<p>- Ideas for the Python/Node.js API design<p>Happy to dive into technical details about DataFusion integration, performance optimizations, or anything else!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "Show HN: Term \u2013 Rust-based data validation with <em>OpenTelemetry</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/withterm/term"}}, "_tags": ["story", "author_ericpsimon", "story_44735703", "show_hn"], "author": "ericpsimon", "created_at": "2025-07-30T15:44:45Z", "created_at_i": 1753890285, "num_comments": 0, "objectID": "44735703", "points": 3, "story_id": 44735703, "story_text": "Hi HN, I&#x27;m Eric and I&#x27;m a recovering data engineer. Recently I have worked on the data platforms for multiple YC backed start-ups Kable (YC W22) and Finch (YC S20).<p>Every data team I&#x27;ve worked with struggles with data quality validation. Current solutions like Apache Deequ require spinning up entire Spark clusters just to check if your data meets basic quality constraints.<p>When I found Apache DataFusion, it was love at first sight - it provided the ergonomics of Apache Spark, without the overhead, JVM, etc. That is what led me to build Term. It is able to take advantage of the ergonomics of Spark without the overhead.<p>Term is a Rust library that provides Deequ-style data validation using Apache DataFusion. You can run comprehensive data quality checks anywhere - from your laptop to CI&#x2F;CD pipelines - without any JVM or cluster setup. On a 1M row dataset with 20 constraints, Term completes validation in 0.21 seconds (vs 3.2 seconds without optimization) by intelligently batching operations into just 2 scans instead of 20.<p>The technical approach: Term leverages DataFusion&#x27;s columnar processing engine to efficiently validate data in Arrow format. Validation rules compile directly to DataFusion&#x27;s physical plans, and Rust&#x27;s zero-cost abstractions mean the overhead is minimal. You get 100MB&#x2F;s single-core throughput, which often outperforms distributed solutions for datasets under 100GB.<p>Term supports all the validation patterns you&#x27;d expect - completeness checks, uniqueness validation, statistical analysis (mean, correlation, standard deviation), pattern matching, custom SQL expressions, and built-in OpenTelemetry integration for production observability. The entire setup takes less than 5 minutes - just `cargo add term-guard` and you&#x27;re validating data.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;withterm&#x2F;term\">https:&#x2F;&#x2F;github.com&#x2F;withterm&#x2F;term</a><p>I built this because I was tired of seeing teams skip data validation entirely rather than deal with Spark infrastructure. With Term, you can add validation to any Rust data pipeline with minimal overhead and zero operational complexity.<p>Coming next: Python&#x2F;Node.js bindings, streaming support, and database connectivity. I&#x27;m particularly excited about making this accessible beyond the Rust ecosystem.<p>I&#x27;d love feedback on:<p>- The validation API - does it cover your use cases?<p>- Performance on your real-world datasets<p>- What validation patterns you need that aren&#x27;t supported yet<p>- Ideas for the Python&#x2F;Node.js API design<p>Happy to dive into technical details about DataFusion integration, performance optimizations, or anything else!", "title": "Show HN: Term \u2013 Rust-based data validation with OpenTelemetry", "updated_at": "2025-08-02T12:39:51Z", "url": "https://github.com/withterm/term"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "royal0203"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Hi HN, we\u2019re Royal and Vedant, co-founders of CodeParrot (<a href=\"https://www.codeparrot.ai/\">https://www.codeparrot.ai/</a>). CodeParrot automates API testing so developers can speed up release cycles and increase test coverage. It captures <em>production</em> traffic and database state to generate test cases that update with every release.<p>Here\u2019s a short video that shows how it works: <a href=\"https://www.loom.com/share/dd6c12e23ceb43f587814a2fbc165c1f\" rel=\"nofollow\">https://www.loom.com/share/dd6c12e23ceb43f587814a2fbc165c1f</a> .<p>As managers of engineering teams (I was CTO of an ed-tech startup, Vedant was the founding engineer of a unicorn company) both of us faced challenges in enforcing high test coverage. We ended up relying a lot on manual testing but it became hard to scale, and led to reduced velocity and higher <em>production</em> bugs. This motivated us to build CodeParrot.<p>How it works: we auto-instrument backend services to capture <em>production</em> traffic. Requests and responses coming to your backend service, as well as the downstream calls made by it like DB calls are stored. As part of your CI pipeline, we replay the captured requests whenever your service is updated. The responses are compared with the responses from <em>production</em> env and regressions are highlighted to the developers. To ensure that the same codebase gives the same response in CI environment and <em>production</em>, we mock all downstream calls with the values from <em>production</em>.<p>Most tools to record and replay <em>production</em> traffic for the purpose of testing capture traffic on the network layer (as sidecar or through load balancer), CodeParrot instead relies on an instrumentation agent (built on top of <em>OpenTelemetry</em>) to capture traffic, enabling us to capture downstream request/response like database responses which are otherwise encrypted on network layer. This helps us mock downstream calls and compare the response from CI environment vs <em>production</em> environment. Additionally, this helps us sample requests based on code flow and downstream responses which provide better test coverage compared to just relying on API headers &amp; parameters.<p>Our self-serve product will be out in a few weeks. Meanwhile, we can help you integrate CodeParrot, please reach out at royal@codeparrot.ai or you can choose a slot here - <a href=\"https://tidycal.com/royal1/schedule-demo\" rel=\"nofollow\">https://tidycal.com/royal1/schedule-demo</a>. We\u2019ll be selling CodeParrot via a subscription model but the details are TBD. In addition, we will be open sourcing the project soon.<p>If you\u2019ve already tried or are thinking of using tools in this space, we\u2019d love to hear your experience and what you care about most. We look forward to everyone\u2019s comments!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Launch HN: Codeparrot (YC W23) \u2013 Automated API testing using <em>production</em> traffic"}}, "_tags": ["story", "author_royal0203", "story_35201036", "launch_hn"], "author": "royal0203", "children": [35201199, 35201214, 35201522, 35201547, 35202532, 35202563, 35202570, 35202669, 35202720, 35203067, 35203177, 35203281, 35203298, 35203330, 35203871, 35204126, 35204461, 35204796, 35208135, 35209788, 35209990], "created_at": "2023-03-17T18:35:44Z", "created_at_i": 1679078144, "num_comments": 63, "objectID": "35201036", "points": 127, "story_id": 35201036, "story_text": "Hi HN, we\u2019re Royal and Vedant, co-founders of CodeParrot (<a href=\"https:&#x2F;&#x2F;www.codeparrot.ai&#x2F;\">https:&#x2F;&#x2F;www.codeparrot.ai&#x2F;</a>). CodeParrot automates API testing so developers can speed up release cycles and increase test coverage. It captures production traffic and database state to generate test cases that update with every release.<p>Here\u2019s a short video that shows how it works: <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;dd6c12e23ceb43f587814a2fbc165c1f\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;dd6c12e23ceb43f587814a2fbc165c1f</a> .<p>As managers of engineering teams (I was CTO of an ed-tech startup, Vedant was the founding engineer of a unicorn company) both of us faced challenges in enforcing high test coverage. We ended up relying a lot on manual testing but it became hard to scale, and led to reduced velocity and higher production bugs. This motivated us to build CodeParrot.<p>How it works: we auto-instrument backend services to capture production traffic. Requests and responses coming to your backend service, as well as the downstream calls made by it like DB calls are stored. As part of your CI pipeline, we replay the captured requests whenever your service is updated. The responses are compared with the responses from production env and regressions are highlighted to the developers. To ensure that the same codebase gives the same response in CI environment and production, we mock all downstream calls with the values from production.<p>Most tools to record and replay production traffic for the purpose of testing capture traffic on the network layer (as sidecar or through load balancer), CodeParrot instead relies on an instrumentation agent (built on top of OpenTelemetry) to capture traffic, enabling us to capture downstream request&#x2F;response like database responses which are otherwise encrypted on network layer. This helps us mock downstream calls and compare the response from CI environment vs production environment. Additionally, this helps us sample requests based on code flow and downstream responses which provide better test coverage compared to just relying on API headers &amp; parameters.<p>Our self-serve product will be out in a few weeks. Meanwhile, we can help you integrate CodeParrot, please reach out at royal@codeparrot.ai or you can choose a slot here - <a href=\"https:&#x2F;&#x2F;tidycal.com&#x2F;royal1&#x2F;schedule-demo\" rel=\"nofollow\">https:&#x2F;&#x2F;tidycal.com&#x2F;royal1&#x2F;schedule-demo</a>. We\u2019ll be selling CodeParrot via a subscription model but the details are TBD. In addition, we will be open sourcing the project soon.<p>If you\u2019ve already tried or are thinking of using tools in this space, we\u2019d love to hear your experience and what you care about most. We look forward to everyone\u2019s comments!", "title": "Launch HN: Codeparrot (YC W23) \u2013 Automated API testing using production traffic", "updated_at": "2025-12-07T15:15:30Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "GalKlm"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Hey everyone, we are Nir and Gal from Traceloop (<a href=\"https://www.traceloop.com\">https://www.traceloop.com</a>). We help teams understand when their LLM apps are failing or hallucinating at scale. See a demo: <a href=\"https://www.traceloop.com/video\">https://www.traceloop.com/video</a> or try it yourself at <a href=\"https://www.traceloop.com/docs/demo\">https://www.traceloop.com/docs/demo</a>.<p>When moving your LLM app to <em>production</em>, significant scale makes it harder for engineers and data scientists alike to understand when their LLM is hallucinating or returning malformed responses. When you get to millions of calls to OpenAI a month, methods like \u201cLLM as a judge\u201d can\u2019t work at a reasonable cost or latency. So, what most people we talked to usually do is sample some generations by hand, maybe for some specific important customers, and manually look for errors or hallucinations.<p>Traceloop is a monitoring platform that detects when your LLM app fails. Under the hood, we built real-time versions of known metrics like faithfulness, relevancy, redundancy, and many others. These are loosely based on some well-known NLP metrics that work well for LLM-generated texts. We correlate them with changes we detect in your system - like updates to prompts or to the model you\u2019re using - to detect regressions automatically.<p>Here are some cool examples we\u2019ve seen with our customers -<p>1. Applying our QA relevancy metric to an entity extraction task, we managed to discover issues where the model was not extracting the right entities (like an address instead of a person\u2019s name); or returning random answers like \u201cI\u2019m here! What can I help you with today?\u201d.<p>2. Our soft-faithfulness metric was able to detect cases in summarization tasks where a model was completely making up stuff that never appeared in the original text.<p>One of the challenges we faced was figuring out how to collect the data that we need from our customers' LLM apps. That\u2019s where <em>OpenTelemetry</em> came in handy. We built OpenLLMetry (<a href=\"https://github.com/traceloop/openllmetry\">https://github.com/traceloop/openllmetry</a>), and announced it here almost a year ago. It standardized the use of <em>OpenTelemetry</em> to observe LLM apps. We realized that the concepts of traces, spans, metrics, and logs that were standardized with <em>OpenTelemetry</em> can easily extend to gen AI. We partnered with 20+ observability platforms to make sure that OpenLLMetry becomes the standard for GenAI observability and that the data that we collect can be sent to other platforms as well.<p>We plan to extend the metrics we provide to support agents that use tools, vision models, and other amazing developments in our fast-paced industry.<p>We invite you to give Traceloop a spin and are eager for your feedback! How do you track and debug hallucinations? How much has that been an issue for you? What types of hallucinations have you encountered?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "Launch HN: Traceloop (YC W23) \u2013 Detecting LLM Hallucinations with <em>OpenTelemetry</em>"}}, "_tags": ["story", "author_GalKlm", "story_40985609", "launch_hn"], "author": "GalKlm", "children": [40985705, 40987046, 40987049, 40987070, 40987200, 40988123, 40988854, 40988895, 40989064, 40989609, 40990482, 40991184, 40991724, 40991743, 40992336, 41006263, 41015682], "created_at": "2024-07-17T13:19:03Z", "created_at_i": 1721222343, "num_comments": 72, "objectID": "40985609", "points": 101, "story_id": 40985609, "story_text": "Hey everyone, we are Nir and Gal from Traceloop (<a href=\"https:&#x2F;&#x2F;www.traceloop.com\">https:&#x2F;&#x2F;www.traceloop.com</a>). We help teams understand when their LLM apps are failing or hallucinating at scale. See a demo: <a href=\"https:&#x2F;&#x2F;www.traceloop.com&#x2F;video\">https:&#x2F;&#x2F;www.traceloop.com&#x2F;video</a> or try it yourself at <a href=\"https:&#x2F;&#x2F;www.traceloop.com&#x2F;docs&#x2F;demo\">https:&#x2F;&#x2F;www.traceloop.com&#x2F;docs&#x2F;demo</a>.<p>When moving your LLM app to production, significant scale makes it harder for engineers and data scientists alike to understand when their LLM is hallucinating or returning malformed responses. When you get to millions of calls to OpenAI a month, methods like \u201cLLM as a judge\u201d can\u2019t work at a reasonable cost or latency. So, what most people we talked to usually do is sample some generations by hand, maybe for some specific important customers, and manually look for errors or hallucinations.<p>Traceloop is a monitoring platform that detects when your LLM app fails. Under the hood, we built real-time versions of known metrics like faithfulness, relevancy, redundancy, and many others. These are loosely based on some well-known NLP metrics that work well for LLM-generated texts. We correlate them with changes we detect in your system - like updates to prompts or to the model you\u2019re using - to detect regressions automatically.<p>Here are some cool examples we\u2019ve seen with our customers -<p>1. Applying our QA relevancy metric to an entity extraction task, we managed to discover issues where the model was not extracting the right entities (like an address instead of a person\u2019s name); or returning random answers like \u201cI\u2019m here! What can I help you with today?\u201d.<p>2. Our soft-faithfulness metric was able to detect cases in summarization tasks where a model was completely making up stuff that never appeared in the original text.<p>One of the challenges we faced was figuring out how to collect the data that we need from our customers&#x27; LLM apps. That\u2019s where OpenTelemetry came in handy. We built OpenLLMetry (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry\">https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry</a>), and announced it here almost a year ago. It standardized the use of OpenTelemetry to observe LLM apps. We realized that the concepts of traces, spans, metrics, and logs that were standardized with OpenTelemetry can easily extend to gen AI. We partnered with 20+ observability platforms to make sure that OpenLLMetry becomes the standard for GenAI observability and that the data that we collect can be sent to other platforms as well.<p>We plan to extend the metrics we provide to support agents that use tools, vision models, and other amazing developments in our fast-paced industry.<p>We invite you to give Traceloop a spin and are eager for your feedback! How do you track and debug hallucinations? How much has that been an issue for you? What types of hallucinations have you encountered?", "title": "Launch HN: Traceloop (YC W23) \u2013 Detecting LLM Hallucinations with OpenTelemetry", "updated_at": "2025-08-14T22:35:36Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "GalKlm"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Hey HN, Gal, Nir and Doron here.<p>Over the past 2 years, we've helped teams debug everything from prompt issues to <em>production</em> outages.<p>We kept running into the same problem: Jumping between our IDEs and our observability dashboards.  So, we built an open-source MCP server that connects any <em>OpenTelemetry</em> backend (Grafana, Jaeger, Datadog, Dynatrace, Traceloop) to our dev environment using an MCP.<p>While there are many MCP servers built for specific providers (like Datadog\u2019s - <a href=\"https://docs.datadoghq.com/bits_ai/mcp_server\" rel=\"nofollow\">https://docs.datadoghq.com/bits_ai/mcp_server</a>), they\u2019re closed source (so we can\u2019t easily extend them), and they\u2019re locked to a single platform, so organizations that leverage multiple platforms, where data is scattered in between them, can\u2019t really use them.<p>We\u2019re adding support for more providers every day - feel free to contribute your own.<p>We would love your feedback and opinions - feel free to connect it to Claude or ChatGPT and try to investigate your own <em>production</em> data. What do you think about the set of tools that we currently expose? Do you think we should expose more or others?<p>Github: <a href=\"https://github.com/traceloop/opentelemetry-mcp-server\" rel=\"nofollow\">https://github.com/traceloop/<em>opentelemetry</em>-mcp-server</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "Show HN: MCP Server for <em>OpenTelemetry</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "https://github.com/traceloop/<em>opentelemetry</em>-mcp-server"}}, "_tags": ["story", "author_GalKlm", "story_45967204", "show_hn"], "author": "GalKlm", "children": [45967537], "created_at": "2025-11-18T15:09:15Z", "created_at_i": 1763478555, "num_comments": 2, "objectID": "45967204", "points": 13, "story_id": 45967204, "story_text": "Hey HN, Gal, Nir and Doron here.<p>Over the past 2 years, we&#x27;ve helped teams debug everything from prompt issues to production outages.<p>We kept running into the same problem: Jumping between our IDEs and our observability dashboards.  So, we built an open-source MCP server that connects any OpenTelemetry backend (Grafana, Jaeger, Datadog, Dynatrace, Traceloop) to our dev environment using an MCP.<p>While there are many MCP servers built for specific providers (like Datadog\u2019s - <a href=\"https:&#x2F;&#x2F;docs.datadoghq.com&#x2F;bits_ai&#x2F;mcp_server\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.datadoghq.com&#x2F;bits_ai&#x2F;mcp_server</a>), they\u2019re closed source (so we can\u2019t easily extend them), and they\u2019re locked to a single platform, so organizations that leverage multiple platforms, where data is scattered in between them, can\u2019t really use them.<p>We\u2019re adding support for more providers every day - feel free to contribute your own.<p>We would love your feedback and opinions - feel free to connect it to Claude or ChatGPT and try to investigate your own production data. What do you think about the set of tools that we currently expose? Do you think we should expose more or others?<p>Github: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;opentelemetry-mcp-server\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;opentelemetry-mcp-server</a>", "title": "Show HN: MCP Server for OpenTelemetry", "updated_at": "2025-11-18T17:19:22Z", "url": "https://github.com/traceloop/opentelemetry-mcp-server"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "madhusudancs"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Hello. I am Madhu, a Software Engineer at Resolve AI. We launched our product today and we are thrilled to share it with you all and get feedback: <a href=\"https://resolve.ai/\" rel=\"nofollow\">https://resolve.ai/</a><p>Our team at Resolve AI comes with a wealth of experience in this space. I was an early contributor to Kubernetes at Google where I worked on Kubernetes and associated technologies for ~6 years. More recently, I was the tech lead for the Kubernetes-based compute platform at Robinhood where my teams were in a number of SEVs per year, not necessarily caused by the platform itself but still supported (pretty much the story of life for Infrastructure Engineers everywhere). Our co-founders, Spiros Xanthos and Mayank Agarwal co-created <em>OpenTelemetry</em> at their previous startup Omnition (acquired by Splunk). More recently, Spiros was the GM and Senior Vice President of Splunk Observability and Mayank was the lead architect for all of Splunk's observability product lines. We have all lived the problems we are trying to solve.<p>Resolve is AI for <em>production</em> engineers. <em>Production</em> systems are dynamic and complex. Addressing common <em>production</em> engineering concerns like incident troubleshooting, cloud operations, security, compliance and cost involves painfully piecing together information from many teams (service on-call rotations, Platform, SRE, etc) and multiple (routinely 10+) different tools (observability, CI/CD, infrastructure, paging, chat, etc). These tools were not designed to work together, pushing the complexity on humans.<p>Resolve AI is tackling this challenge by building an AI <em>Production</em> Engineer with the goal of automating the majority of tasks across incident management, cloud operations, security engineering, compliance, and cost management. As the first step in our ambitious journey, we are automating incident troubleshooting as it is the most direct way to prevent outages and improve reliability while relieving engineers from the most stressful part of their job. Our goal is to automate the resolution of 80%+ of alerts and incidents without human involvement.<p>Resolve AI automatically maps and keeps up-to-date a complete knowledge graph of any <em>production</em> environment, without needing any upfront training or user input. It builds knowledge of which tools and signals are relevant for any situation. It comes pre-built with models for various tool categories such as metrics, logs, traces, alerts, seamlessly connecting with category- and vendor-specific products like Prometheus, Splunk, GCP, AWS, Azure and others. These models automatically and continuously adapt to each customer's environment.<p>With the state-of-the-art reasoning engine that\u2019s composed of multiple agents, Resolve AI is able to investigate novel incidents, accurately determine causality, learn and adapt as it encounters new situations and perform various complex actions.<p>Generative AI is inherently probabilistic and not always 100% accurate. Without full context, AI models may hallucinate, potentially misleading users. For an AI that takes actions, building user trust is paramount; it must present clear evidence for any decision or action. We address these challenges by building an interface that supports claims with evidence, present findings with context and allow humans to collaborate with the system so that they can guide the system when needed.<p>Our video demo is on the website. Please take a look. We really appreciate your feedback. We are also happy to hop on a call to show a demo live if you are interested (madhu@resolve.ai)."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Resolve AI \u2013 Your AI <em>Production</em> Engineer"}}, "_tags": ["story", "author_madhusudancs", "story_41712089", "show_hn"], "author": "madhusudancs", "children": [41717514, 41731108], "created_at": "2024-10-01T18:17:23Z", "created_at_i": 1727806643, "num_comments": 2, "objectID": "41712089", "points": 10, "story_id": 41712089, "story_text": "Hello. I am Madhu, a Software Engineer at Resolve AI. We launched our product today and we are thrilled to share it with you all and get feedback: <a href=\"https:&#x2F;&#x2F;resolve.ai&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;resolve.ai&#x2F;</a><p>Our team at Resolve AI comes with a wealth of experience in this space. I was an early contributor to Kubernetes at Google where I worked on Kubernetes and associated technologies for ~6 years. More recently, I was the tech lead for the Kubernetes-based compute platform at Robinhood where my teams were in a number of SEVs per year, not necessarily caused by the platform itself but still supported (pretty much the story of life for Infrastructure Engineers everywhere). Our co-founders, Spiros Xanthos and Mayank Agarwal co-created OpenTelemetry at their previous startup Omnition (acquired by Splunk). More recently, Spiros was the GM and Senior Vice President of Splunk Observability and Mayank was the lead architect for all of Splunk&#x27;s observability product lines. We have all lived the problems we are trying to solve.<p>Resolve is AI for production engineers. Production systems are dynamic and complex. Addressing common production engineering concerns like incident troubleshooting, cloud operations, security, compliance and cost involves painfully piecing together information from many teams (service on-call rotations, Platform, SRE, etc) and multiple (routinely 10+) different tools (observability, CI&#x2F;CD, infrastructure, paging, chat, etc). These tools were not designed to work together, pushing the complexity on humans.<p>Resolve AI is tackling this challenge by building an AI Production Engineer with the goal of automating the majority of tasks across incident management, cloud operations, security engineering, compliance, and cost management. As the first step in our ambitious journey, we are automating incident troubleshooting as it is the most direct way to prevent outages and improve reliability while relieving engineers from the most stressful part of their job. Our goal is to automate the resolution of 80%+ of alerts and incidents without human involvement.<p>Resolve AI automatically maps and keeps up-to-date a complete knowledge graph of any production environment, without needing any upfront training or user input. It builds knowledge of which tools and signals are relevant for any situation. It comes pre-built with models for various tool categories such as metrics, logs, traces, alerts, seamlessly connecting with category- and vendor-specific products like Prometheus, Splunk, GCP, AWS, Azure and others. These models automatically and continuously adapt to each customer&#x27;s environment.<p>With the state-of-the-art reasoning engine that\u2019s composed of multiple agents, Resolve AI is able to investigate novel incidents, accurately determine causality, learn and adapt as it encounters new situations and perform various complex actions.<p>Generative AI is inherently probabilistic and not always 100% accurate. Without full context, AI models may hallucinate, potentially misleading users. For an AI that takes actions, building user trust is paramount; it must present clear evidence for any decision or action. We address these challenges by building an interface that supports claims with evidence, present findings with context and allow humans to collaborate with the system so that they can guide the system when needed.<p>Our video demo is on the website. Please take a look. We really appreciate your feedback. We are also happy to hop on a call to show a demo live if you are interested (madhu@resolve.ai).", "title": "Show HN: Resolve AI \u2013 Your AI Production Engineer", "updated_at": "2025-03-11T21:29:55Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fractalwrench"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "I've been working on an open source Android SDK that supports <em>OpenTelemetry</em> export. I'm excited about this as observability SDKs have traditionally used proprietary schemas that make it hard to export &amp; analyse <em>production</em> data.<p>Would be great to hear any feedback/comments folks might have!<p>Here's the Android SDK: <a href=\"https://github.com/embrace-io/embrace-android-sdk\">https://github.com/embrace-io/embrace-android-sdk</a>\nThere is also an iOS SDK, but I've been less directly involved in that: <a href=\"https://github.com/embrace-io/embrace-apple-sdk\">https://github.com/embrace-io/embrace-apple-sdk</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "Show HN: Android SDK that supports <em>OpenTelemetry</em> export"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/embrace-io/embrace-android-sdk"}}, "_tags": ["story", "author_fractalwrench", "story_40155445", "show_hn"], "author": "fractalwrench", "created_at": "2024-04-25T09:51:46Z", "created_at_i": 1714038706, "num_comments": 0, "objectID": "40155445", "points": 7, "story_id": 40155445, "story_text": "I&#x27;ve been working on an open source Android SDK that supports OpenTelemetry export. I&#x27;m excited about this as observability SDKs have traditionally used proprietary schemas that make it hard to export &amp; analyse production data.<p>Would be great to hear any feedback&#x2F;comments folks might have!<p>Here&#x27;s the Android SDK: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;embrace-io&#x2F;embrace-android-sdk\">https:&#x2F;&#x2F;github.com&#x2F;embrace-io&#x2F;embrace-android-sdk</a>\nThere is also an iOS SDK, but I&#x27;ve been less directly involved in that: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;embrace-io&#x2F;embrace-apple-sdk\">https:&#x2F;&#x2F;github.com&#x2F;embrace-io&#x2F;embrace-apple-sdk</a>", "title": "Show HN: Android SDK that supports OpenTelemetry export", "updated_at": "2024-09-20T16:53:54Z", "url": "https://github.com/embrace-io/embrace-android-sdk"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "nirga"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "For the past few weeks we've been building an engine that can run system integration tests written in Jest against <em>OpenTelemetry</em> traces.<p>If you ever worked on a system with a few tens of microservices you got to a point where making changes become difficult. You always worry something is going to break. Unit tests are not enough. \nYou need to have a way to test if (for example) the HTTP request to register a user will trigger that welcome e-mail at the end of the flow.<p>So we've build an open source framework where you can simply write such tests. \nYou can assert on database queries, on HTTP requests and responses, and more to come. \nWe use traces from <em>OpenTelemetry</em> to verify that the assertions you wrote in your test are true.<p>Our long term goal is to even generate the tests automatically from those traces.<p>We're still early on so would love to get any feedback on our concept and implementation!<p>repo: <a href=\"https://github.com/traceloop/jest-opentelemetry\">https://github.com/traceloop/jest-<em>opentelemetry</em></a><p>docs: <a href=\"https://docs.traceloop.dev/jest-otel/introduction\">https://docs.traceloop.dev/jest-otel/introduction</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Traceloop \u2013 open-source for generating tests from <em>production</em> traffic"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "https://github.com/traceloop/jest-<em>opentelemetry</em>"}}, "_tags": ["story", "author_nirga", "story_34907938", "show_hn"], "author": "nirga", "children": [34925329], "created_at": "2023-02-23T07:41:27Z", "created_at_i": 1677138087, "num_comments": 1, "objectID": "34907938", "points": 3, "story_id": 34907938, "story_text": "For the past few weeks we&#x27;ve been building an engine that can run system integration tests written in Jest against OpenTelemetry traces.<p>If you ever worked on a system with a few tens of microservices you got to a point where making changes become difficult. You always worry something is going to break. Unit tests are not enough. \nYou need to have a way to test if (for example) the HTTP request to register a user will trigger that welcome e-mail at the end of the flow.<p>So we&#x27;ve build an open source framework where you can simply write such tests. \nYou can assert on database queries, on HTTP requests and responses, and more to come. \nWe use traces from OpenTelemetry to verify that the assertions you wrote in your test are true.<p>Our long term goal is to even generate the tests automatically from those traces.<p>We&#x27;re still early on so would love to get any feedback on our concept and implementation!<p>repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;jest-opentelemetry\">https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;jest-opentelemetry</a><p>docs: <a href=\"https:&#x2F;&#x2F;docs.traceloop.dev&#x2F;jest-otel&#x2F;introduction\">https:&#x2F;&#x2F;docs.traceloop.dev&#x2F;jest-otel&#x2F;introduction</a>", "title": "Show HN: Traceloop \u2013 open-source for generating tests from production traffic", "updated_at": "2024-09-20T13:22:10Z", "url": "https://github.com/traceloop/jest-opentelemetry"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "PkLavc"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "I\u2019m a Computer Engineering student from Brazil. I built this to showcase backend patterns that go beyond simple CRUDs: row-level multi-tenancy (Prisma), async processing (BullMQ/Redis), and observability (<em>OpenTelemetry</em>). Looking for feedback on the architecture!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: <em>Production</em>-Ready NestJS Back End (Multi-Tenancy, Event-Driven)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/PkLavc/portfolio"}}, "_tags": ["story", "author_PkLavc", "story_46983399", "show_hn"], "author": "PkLavc", "created_at": "2026-02-12T00:41:09Z", "created_at_i": 1770856869, "num_comments": 0, "objectID": "46983399", "points": 1, "story_id": 46983399, "story_text": "I\u2019m a Computer Engineering student from Brazil. I built this to showcase backend patterns that go beyond simple CRUDs: row-level multi-tenancy (Prisma), async processing (BullMQ&#x2F;Redis), and observability (OpenTelemetry). Looking for feedback on the architecture!", "title": "Show HN: Production-Ready NestJS Back End (Multi-Tenancy, Event-Driven)", "updated_at": "2026-02-12T00:44:01Z", "url": "https://github.com/PkLavc/portfolio"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Evanson"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Hey HN! I built Lumina \u2013 an open-source observability platform for AI/LLM applications. Self-host it in 5 minutes with Docker Compose, all features included.<p>The Problem:<p>I've been building LLM apps for the past year, and I kept running into the same issues:\n- LLM responses would randomly change after prompt tweaks, breaking things.\n- Costs would spike unexpectedly (turns out a bug was hitting GPT-4 instead of 3.5).\n- No easy way to compare &quot;before vs after&quot; when testing prompt changes.\n- Existing tools were either too expensive or missing features in free tiers.<p>What I Built:<p>Lumina is <em>OpenTelemetry</em>-native, meaning:\n- Works with your existing OTEL stack (Datadog, Grafana, etc.).\n- No vendor lock-in, standard trace format.\n- Integrates in 3 lines of code.<p>Key features:\n - Cost &amp; quality monitoring \u2013 Automatic alerts when costs spike, or responses degrade.\n - Replay testing \u2013 Capture <em>production</em> traces, replay them after changes, see diffs.\n - Semantic comparison \u2013 Not just string matching \u2013 uses Claude to judge if responses are &quot;better&quot; or &quot;worse.&quot;\n - Self-hosted tier \u2013 50k traces/day, 7-day retention, ALL features included (alerts, replay, semantic scoring)<p>How it works:<p>```bash\n# Start Lumina\ngit clone <a href=\"https://github.com/use-lumina/Lumina\" rel=\"nofollow\">https://github.com/use-lumina/Lumina</a>\ncd Lumina/infra/docker\ndocker-compose up -d\n```<p>```typescript\n// Add to your app (no API key needed for self-hosted!)\nimport { Lumina } from '@uselumina/sdk';<p>const lumina = new Lumina({\n  endpoint: 'http://localhost:8080/v1/traces',\n});<p>// Wrap your LLM call\nconst response = await lumina.traceLLM(\n  async () =&gt; await openai.chat.completions.create({...}),\n  { provider: 'openai', model: 'gpt-4', prompt: '...' }\n);\n```<p>That's it. Every LLM call is now tracked with cost, latency, tokens, and quality scores.<p>What makes it different:<p>1. Free self-hosted with limits that work \u2013 50k traces/day and 7-day retention (resets daily at midnight UTC). All features included: alerts, replay testing, and semantic scoring. Perfect for most development and small <em>production</em> workloads. Need more? Upgrade to managed cloud.<p>2. <em>OpenTelemetry</em>-native \u2013 Not another proprietary format. Use standard OTEL exporters, works with existing infra. Can send traces to both Lumina AND Datadog simultaneously.<p>3. Replay testing \u2013 The killer feature. Capture 100 <em>production</em> traces, change your prompt, replay them all, and get a semantic diff report. Like snapshot testing for LLMs.<p>4. Fast \u2013 Built with Bun, Postgres, Redis, NATS. Sub-500ms from trace to alert. Handles 10k+ traces/min on a single machine.<p>What I'm looking for:<p>- Feedback on the approach (is OTEL the right foundation?)\n- Bug reports (tested on Mac/Linux/WSL2, but I'm sure there are issues)\n- Ideas for what features matter most (alerts? replay? cost tracking?)\n- Help with the semantic scorer (currently uses Claude, want to make it pluggable)<p>Why open source:<p>I want this to be the standard for LLM observability. That only works if it's:\n- Free to use and modify (Apache 2.0)\n- Easy to self-host (Docker Compose, no cloud dependencies)\n- Open to contributions (good first issues tagged)<p>The business model is managed hosting for teams that don't want to run infrastructure. But the core product is and always will be free.<p>Try it:\n- GitHub: <a href=\"https://github.com/use-lumina/Lumina\" rel=\"nofollow\">https://github.com/use-lumina/Lumina</a>\n- Docs: <a href=\"https://docs.uselumina.io\" rel=\"nofollow\">https://docs.uselumina.io</a>\n- Quick start: 5 minutes from `git clone` to dashboard<p>I'd love to hear what you think! Especially interested in:\n- What observability problems are you hitting with LLMs\n- Missing features that would make this useful for you\n- Any similar tools you're using (and what they do better)<p>Thanks for reading!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "Show HN: Lumina \u2013 Open-source observability for AI systems(<em>OpenTelemetry</em>-native)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/use-lumina/Lumina"}}, "_tags": ["story", "author_Evanson", "story_46781731", "show_hn"], "author": "Evanson", "created_at": "2026-01-27T16:00:06Z", "created_at_i": 1769529606, "num_comments": 0, "objectID": "46781731", "points": 1, "story_id": 46781731, "story_text": "Hey HN! I built Lumina \u2013 an open-source observability platform for AI&#x2F;LLM applications. Self-host it in 5 minutes with Docker Compose, all features included.<p>The Problem:<p>I&#x27;ve been building LLM apps for the past year, and I kept running into the same issues:\n- LLM responses would randomly change after prompt tweaks, breaking things.\n- Costs would spike unexpectedly (turns out a bug was hitting GPT-4 instead of 3.5).\n- No easy way to compare &quot;before vs after&quot; when testing prompt changes.\n- Existing tools were either too expensive or missing features in free tiers.<p>What I Built:<p>Lumina is OpenTelemetry-native, meaning:\n- Works with your existing OTEL stack (Datadog, Grafana, etc.).\n- No vendor lock-in, standard trace format.\n- Integrates in 3 lines of code.<p>Key features:\n - Cost &amp; quality monitoring \u2013 Automatic alerts when costs spike, or responses degrade.\n - Replay testing \u2013 Capture production traces, replay them after changes, see diffs.\n - Semantic comparison \u2013 Not just string matching \u2013 uses Claude to judge if responses are &quot;better&quot; or &quot;worse.&quot;\n - Self-hosted tier \u2013 50k traces&#x2F;day, 7-day retention, ALL features included (alerts, replay, semantic scoring)<p>How it works:<p>```bash\n# Start Lumina\ngit clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\ncd Lumina&#x2F;infra&#x2F;docker\ndocker-compose up -d\n```<p>```typescript\n&#x2F;&#x2F; Add to your app (no API key needed for self-hosted!)\nimport { Lumina } from &#x27;@uselumina&#x2F;sdk&#x27;;<p>const lumina = new Lumina({\n  endpoint: &#x27;http:&#x2F;&#x2F;localhost:8080&#x2F;v1&#x2F;traces&#x27;,\n});<p>&#x2F;&#x2F; Wrap your LLM call\nconst response = await lumina.traceLLM(\n  async () =&gt; await openai.chat.completions.create({...}),\n  { provider: &#x27;openai&#x27;, model: &#x27;gpt-4&#x27;, prompt: &#x27;...&#x27; }\n);\n```<p>That&#x27;s it. Every LLM call is now tracked with cost, latency, tokens, and quality scores.<p>What makes it different:<p>1. Free self-hosted with limits that work \u2013 50k traces&#x2F;day and 7-day retention (resets daily at midnight UTC). All features included: alerts, replay testing, and semantic scoring. Perfect for most development and small production workloads. Need more? Upgrade to managed cloud.<p>2. OpenTelemetry-native \u2013 Not another proprietary format. Use standard OTEL exporters, works with existing infra. Can send traces to both Lumina AND Datadog simultaneously.<p>3. Replay testing \u2013 The killer feature. Capture 100 production traces, change your prompt, replay them all, and get a semantic diff report. Like snapshot testing for LLMs.<p>4. Fast \u2013 Built with Bun, Postgres, Redis, NATS. Sub-500ms from trace to alert. Handles 10k+ traces&#x2F;min on a single machine.<p>What I&#x27;m looking for:<p>- Feedback on the approach (is OTEL the right foundation?)\n- Bug reports (tested on Mac&#x2F;Linux&#x2F;WSL2, but I&#x27;m sure there are issues)\n- Ideas for what features matter most (alerts? replay? cost tracking?)\n- Help with the semantic scorer (currently uses Claude, want to make it pluggable)<p>Why open source:<p>I want this to be the standard for LLM observability. That only works if it&#x27;s:\n- Free to use and modify (Apache 2.0)\n- Easy to self-host (Docker Compose, no cloud dependencies)\n- Open to contributions (good first issues tagged)<p>The business model is managed hosting for teams that don&#x27;t want to run infrastructure. But the core product is and always will be free.<p>Try it:\n- GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\n- Docs: <a href=\"https:&#x2F;&#x2F;docs.uselumina.io\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.uselumina.io</a>\n- Quick start: 5 minutes from `git clone` to dashboard<p>I&#x27;d love to hear what you think! Especially interested in:\n- What observability problems are you hitting with LLMs\n- Missing features that would make this useful for you\n- Any similar tools you&#x27;re using (and what they do better)<p>Thanks for reading!", "title": "Show HN: Lumina \u2013 Open-source observability for AI systems(OpenTelemetry-native)", "updated_at": "2026-01-27T16:02:58Z", "url": "https://github.com/use-lumina/Lumina"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "o11yguru"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "We recently worked with a customer whose log ingestion pipeline (<em>OpenTelemetry</em> Collector + Kafka) was falling behind fast. Throughput capped at ~12K EPS per partition (192K total across 16 partitions), while their <em>production</em> required much more. Consumer lag was accelerating, and the backlog was weeks deep.<p>We spent several weeks digging into the Kafka receiver and testing different configurations. Key findings:<p>Kafka client swap \u2192 Opted into the Franz-Go client (via feature gate), yielding a 25% improvement.<p>Encoding mismatch \u2192 Discovered OTLP JSON was being used for raw app logs; switching to plain JSON nearly tripled throughput.<p>Export protocol choice \u2192 gRPC introduced extra overhead from JSON\u2192protobuf conversion; HTTPS was ~3K EPS faster.<p>Batching strategies \u2192 Placement of the batch processor changed CPU/memory efficiency depending on workload.<p>After applying all fixes, we sustained 30K EPS per partition / 480K EPS total \u2014 a 150% improvement \u2014 and cleared the backlog in under 48 hours.<p>Full use case (with benchmarks, config details, and lessons learned): https://bindplane.com/blog/kafka-performance-crisis-how-we-scaled-<em>opentelemetry</em>-log-ingestion-by-150<p>Has anyone else hit scaling limits with the OTel Kafka receiver? Curious what approaches others have tried."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["opentelemetry"], "value": "Scaling <em>OpenTelemetry</em> Kafka ingestion by 150% from 192K \u2192 480K EPS"}}, "_tags": ["story", "author_o11yguru", "story_44952255", "ask_hn"], "author": "o11yguru", "created_at": "2025-08-19T14:53:52Z", "created_at_i": 1755615232, "num_comments": 0, "objectID": "44952255", "points": 1, "story_id": 44952255, "story_text": "We recently worked with a customer whose log ingestion pipeline (OpenTelemetry Collector + Kafka) was falling behind fast. Throughput capped at ~12K EPS per partition (192K total across 16 partitions), while their production required much more. Consumer lag was accelerating, and the backlog was weeks deep.<p>We spent several weeks digging into the Kafka receiver and testing different configurations. Key findings:<p>Kafka client swap \u2192 Opted into the Franz-Go client (via feature gate), yielding a 25% improvement.<p>Encoding mismatch \u2192 Discovered OTLP JSON was being used for raw app logs; switching to plain JSON nearly tripled throughput.<p>Export protocol choice \u2192 gRPC introduced extra overhead from JSON\u2192protobuf conversion; HTTPS was ~3K EPS faster.<p>Batching strategies \u2192 Placement of the batch processor changed CPU&#x2F;memory efficiency depending on workload.<p>After applying all fixes, we sustained 30K EPS per partition &#x2F; 480K EPS total \u2014 a 150% improvement \u2014 and cleared the backlog in under 48 hours.<p>Full use case (with benchmarks, config details, and lessons learned): https:&#x2F;&#x2F;bindplane.com&#x2F;blog&#x2F;kafka-performance-crisis-how-we-scaled-opentelemetry-log-ingestion-by-150<p>Has anyone else hit scaling limits with the OTel Kafka receiver? Curious what approaches others have tried.", "title": "Scaling OpenTelemetry Kafka ingestion by 150% from 192K \u2192 480K EPS", "updated_at": "2025-08-19T14:59:27Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "skull8888888"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Hey HN, we\u2019re Robert, Din and Temirlan from Laminar (<a href=\"https://www.lmnr.ai\">https://www.lmnr.ai</a>), an open-source observability and analytics platform for complex LLM apps. It\u2019s designed to be fast, reliable, and scalable. The stack is RabbitMQ for message queues, Postgres for storage, Clickhouse for analytics, Qdrant for semantic search - all powered by Rust.<p>How is Laminar different from the swarm of other \u201cLLM observability\u201d platforms?<p>On the observability part, we\u2019re focused on handling full execution traces, not just LLM calls. We built a Rust ingestor for <em>OpenTelemetry</em> (Otel) spans with GenAI semantic conventions. As LLM apps get more complex (think Agents with hundreds of LLM and function calls, or complex RAG pipelines), full tracing is critical. With Otel spans, we can: 1. Cover the entire execution trace. 2. Keep the platform future-proof 3. Leverage an amazing OpenLLMetry (<a href=\"https://github.com/traceloop/openllmetry\">https://github.com/traceloop/openllmetry</a>), open-source package for span <em>production</em>.<p>The key difference is that we tie text analytics directly to execution traces. Rich text data makes LLM traces unique, so we let you track \u201csemantic metrics\u201d (like what your AI agent is actually saying) and connect those metrics to where they happen in the trace. If you want to know if your AI drive-through agent made an upsell, you can design an LLM extraction pipeline in our builder (more on it later), host it on Laminar, and handle everything from event requests to output logging. Processing requests simply come as events in the Otel span.<p>We think it\u2019s a win to separate core app logic from LLM event processing. Most devs don\u2019t want to manage background queues for LLM analytics processing but still want insights into how their Agents or RAGs are working.<p>Our Pipeline Builder uses graph UI where nodes are LLM and util functions, and edges showing data flow. We built a custom task execution engine with support of parallel branch executions, cycles and branches (it\u2019s overkill for simple pipelines, but it\u2019s extremely cool and we\u2019ve spent a lot of time designing a robust engine). You can also call pipelines directly as API endpoints. We found them to be extremely useful for iterating on and separating LLM logic. Laminar also traces pipeline directly, which removes the overhead of sending large outputs over the network.<p>One thing missing from all LLM observability platforms right now is an adequate search over traces. We\u2019re attacking this problem by indexing each span in a vector DB and performing hybrid search at query time. This feature is still in beta, but we think it\u2019s gonna be crucial part of our platform going forward.<p>We also support evaluations. We loved the \u201crun everything locally, send results to a server\u201d approach from Braintrust and Weights &amp; Biases, so we did that too: a simple SDK and nice dashboards to track everything. Evals are still early, but we\u2019re pushing hard on them.<p>Our goal is to make Laminar the Supabase for LLMOps - the go-to open-source comprehensive platform for all things LLMs / GenAI. In it\u2019s current shape, Laminar is just few weeks old and developing rapidly, we\u2019d love any feedback or for you to give Laminar a try in your LLM projects!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Laminar \u2013 Open-Source DataDog + PostHog for LLM Apps, Built in Rust"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/lmnr-ai/lmnr"}}, "_tags": ["story", "author_skull8888888", "story_41451698", "show_hn"], "author": "skull8888888", "children": [41452424, 41452994, 41453001, 41453058, 41453087, 41453241, 41453566, 41453832, 41455101, 41455103, 41456518, 41459224, 41459548, 41459620, 41462581], "created_at": "2024-09-04T22:52:19Z", "created_at_i": 1725490339, "num_comments": 45, "objectID": "41451698", "points": 203, "story_id": 41451698, "story_text": "Hey HN, we\u2019re Robert, Din and Temirlan from Laminar (<a href=\"https:&#x2F;&#x2F;www.lmnr.ai\">https:&#x2F;&#x2F;www.lmnr.ai</a>), an open-source observability and analytics platform for complex LLM apps. It\u2019s designed to be fast, reliable, and scalable. The stack is RabbitMQ for message queues, Postgres for storage, Clickhouse for analytics, Qdrant for semantic search - all powered by Rust.<p>How is Laminar different from the swarm of other \u201cLLM observability\u201d platforms?<p>On the observability part, we\u2019re focused on handling full execution traces, not just LLM calls. We built a Rust ingestor for OpenTelemetry (Otel) spans with GenAI semantic conventions. As LLM apps get more complex (think Agents with hundreds of LLM and function calls, or complex RAG pipelines), full tracing is critical. With Otel spans, we can: 1. Cover the entire execution trace. 2. Keep the platform future-proof 3. Leverage an amazing OpenLLMetry (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry\">https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry</a>), open-source package for span production.<p>The key difference is that we tie text analytics directly to execution traces. Rich text data makes LLM traces unique, so we let you track \u201csemantic metrics\u201d (like what your AI agent is actually saying) and connect those metrics to where they happen in the trace. If you want to know if your AI drive-through agent made an upsell, you can design an LLM extraction pipeline in our builder (more on it later), host it on Laminar, and handle everything from event requests to output logging. Processing requests simply come as events in the Otel span.<p>We think it\u2019s a win to separate core app logic from LLM event processing. Most devs don\u2019t want to manage background queues for LLM analytics processing but still want insights into how their Agents or RAGs are working.<p>Our Pipeline Builder uses graph UI where nodes are LLM and util functions, and edges showing data flow. We built a custom task execution engine with support of parallel branch executions, cycles and branches (it\u2019s overkill for simple pipelines, but it\u2019s extremely cool and we\u2019ve spent a lot of time designing a robust engine). You can also call pipelines directly as API endpoints. We found them to be extremely useful for iterating on and separating LLM logic. Laminar also traces pipeline directly, which removes the overhead of sending large outputs over the network.<p>One thing missing from all LLM observability platforms right now is an adequate search over traces. We\u2019re attacking this problem by indexing each span in a vector DB and performing hybrid search at query time. This feature is still in beta, but we think it\u2019s gonna be crucial part of our platform going forward.<p>We also support evaluations. We loved the \u201crun everything locally, send results to a server\u201d approach from Braintrust and Weights &amp; Biases, so we did that too: a simple SDK and nice dashboards to track everything. Evals are still early, but we\u2019re pushing hard on them.<p>Our goal is to make Laminar the Supabase for LLMOps - the go-to open-source comprehensive platform for all things LLMs &#x2F; GenAI. In it\u2019s current shape, Laminar is just few weeks old and developing rapidly, we\u2019d love any feedback or for you to give Laminar a try in your LLM projects!", "title": "Show HN: Laminar \u2013 Open-Source DataDog + PostHog for LLM Apps, Built in Rust", "updated_at": "2026-02-18T07:06:12Z", "url": "https://github.com/lmnr-ai/lmnr"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "edenfed"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Hi HN! We\u2019re Eden and Ari, co-founders of Odigos (<a href=\"https://github.com/keyval-dev/odigos\">https://github.com/keyval-dev/odigos</a>). Odigos is an open-source project that lets you instantly generate distributed traces for your applications. It works alongside existing monitoring tools and does not require any code changes.<p>Our earlier experiences with monitoring tools were frustrating. Monitoring a distributed system with multiple microservices, we found ourselves spending way too much time trying to locate the specific microservice that was at the root of a problem. For example, we once spent hours debugging an application which we suspected was causing high latency, only to find out that the actual problem was rooted in a completely different application<p>Then we learned about distributed tracing, which solves exactly this problem. Unlike metrics or logs that capture a data point in time in a single application, a distributed trace follows a request as it propagates through a distributed environment by tagging it with a unique ID. This allows developers to understand the context of each request and how their distributed applications work.<p>The downside is that it is difficult to implement. Unlike metrics or logs, the value of distributed tracing is gained only after implementing it across multiple applications. If even one of your applications does not produce distributed tracing, the context propagation is broken and the value of the traces drops significantly.<p>We manually implemented distributed tracing for multiple companies, but found it a challenge to coordinate all the development teams to instrument their applications in order to achieve a complete distributed trace. Once the implementation was finished, we saw great value and fixed <em>production</em> issues much faster. But partial implementation wasn\u2019t worth much.<p>We set out to automate this process. We knew how to do most of it, but the trickiest part was how to automatically instrument programs written in compiled languages (like Go). If we could do that, we would be able to automate the entire process of generating distributed traces. While researching, we realized that eBPF\u2014a technology that allows the Linux kernel to load external programs for execution within the kernel\u2014could be used to develop automatic instrumentation for compiled languages. That was the final piece of the puzzle, and with it we were able to develop Odigos.<p>Odigos first scans and recognizes all your running applications, then recognizes the programming language of each one and auto-instruments it accordingly, using eBPF and <em>OpenTelemetry</em>. In addition, it deploys collectors that buffer, filter, and deliver data to your chosen monitoring tool, and auto scales them according to the amount of traffic. This automation allows developers to enjoy distributed traces within minutes as opposed to manual effort which can take months to implement.<p>Automatic instrumentation across programming languages is not a trivial task, especially when dealing with static binaries (like the ones produced by the Go compiler). We built multiple mechanisms to make sure we inject the relevant headers in a secure and stable way. We developed a system that tracks functions and structs across different versions of open-source libraries. In addition, we developed a system that performs userspace memory management in eBPF. As a result, Odigos is the only solution that is able to automatically generate distributed traces for compiled languages like Go and Rust. While other solutions require users to be experts in <em>OpenTelemetry</em> or eBPF, our solution does not require prior knowledge of observability technologies.<p>Our solution can be installed on any Kubernetes cluster by executing a single command. Once installed, we detect the programming language of every running application and apply the relevant instrumentation. For JIT languages (Java and .NET) or interpreted languages (JavaScript and Python) we deploy <em>OpenTelemetry</em> instrumentation. For compiled languages (Go, Rust, C) we deploy our eBPF-based instrumentation. All of this is abstracted from the user, who only has to: (1) select any or all of their target applications and (2) select a backend to send monitoring data to.<p>In May 2022, we released our first open-source project: automatic instrumentation for Go applications, based on eBPF. We later donated this project to the <em>OpenTelemetry</em> community and it is currently being developed as part of the Go Automatic Instrumentation SIG.<p>We are big believers in open standards, therefore the instrumentation and collectors used by Odigos are all based on open-source projects developed by the <em>OpenTelemetry</em> community. This also enables us to be vendor-agnostic.<p>Currently we are focused on building our open-source project. There are no pricing or paid features yet, but in the future, we are planning to offer a managed version of Odigos that will include enterprise features.<p>If you're interested to learn more, check out our docs (<a href=\"https://docs.odigos.io\" rel=\"nofollow\">https://docs.odigos.io</a>), watch a demo video (<a href=\"https://www.youtube.com/watch?v=9d36AmVtuGU\">https://www.youtube.com/watch?v=9d36AmVtuGU</a>), and visit our website (<a href=\"https://odigos.io\" rel=\"nofollow\">https://odigos.io</a>).<p>We\u2019d love to hear your experiences with tracing and monitoring distributed applications and anything else you\u2019d like to share!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Odigos (YC W23) \u2013 Instant distributed tracing for Kubernetes clusters"}}, "_tags": ["story", "author_edenfed", "story_34442603", "launch_hn"], "author": "edenfed", "children": [34442851, 34442879, 34443073, 34443159, 34443429, 34443443, 34443569, 34443576, 34444153, 34444928, 34445342, 34445716, 34446989, 34447987, 34448488, 34449070, 34449178, 34450141, 34451288, 34451496, 34452580, 34454189, 34456597, 34479359], "created_at": "2023-01-19T16:59:52Z", "created_at_i": 1674147592, "num_comments": 52, "objectID": "34442603", "points": 162, "story_id": 34442603, "story_text": "Hi HN! We\u2019re Eden and Ari, co-founders of Odigos (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;keyval-dev&#x2F;odigos\">https:&#x2F;&#x2F;github.com&#x2F;keyval-dev&#x2F;odigos</a>). Odigos is an open-source project that lets you instantly generate distributed traces for your applications. It works alongside existing monitoring tools and does not require any code changes.<p>Our earlier experiences with monitoring tools were frustrating. Monitoring a distributed system with multiple microservices, we found ourselves spending way too much time trying to locate the specific microservice that was at the root of a problem. For example, we once spent hours debugging an application which we suspected was causing high latency, only to find out that the actual problem was rooted in a completely different application<p>Then we learned about distributed tracing, which solves exactly this problem. Unlike metrics or logs that capture a data point in time in a single application, a distributed trace follows a request as it propagates through a distributed environment by tagging it with a unique ID. This allows developers to understand the context of each request and how their distributed applications work.<p>The downside is that it is difficult to implement. Unlike metrics or logs, the value of distributed tracing is gained only after implementing it across multiple applications. If even one of your applications does not produce distributed tracing, the context propagation is broken and the value of the traces drops significantly.<p>We manually implemented distributed tracing for multiple companies, but found it a challenge to coordinate all the development teams to instrument their applications in order to achieve a complete distributed trace. Once the implementation was finished, we saw great value and fixed production issues much faster. But partial implementation wasn\u2019t worth much.<p>We set out to automate this process. We knew how to do most of it, but the trickiest part was how to automatically instrument programs written in compiled languages (like Go). If we could do that, we would be able to automate the entire process of generating distributed traces. While researching, we realized that eBPF\u2014a technology that allows the Linux kernel to load external programs for execution within the kernel\u2014could be used to develop automatic instrumentation for compiled languages. That was the final piece of the puzzle, and with it we were able to develop Odigos.<p>Odigos first scans and recognizes all your running applications, then recognizes the programming language of each one and auto-instruments it accordingly, using eBPF and OpenTelemetry. In addition, it deploys collectors that buffer, filter, and deliver data to your chosen monitoring tool, and auto scales them according to the amount of traffic. This automation allows developers to enjoy distributed traces within minutes as opposed to manual effort which can take months to implement.<p>Automatic instrumentation across programming languages is not a trivial task, especially when dealing with static binaries (like the ones produced by the Go compiler). We built multiple mechanisms to make sure we inject the relevant headers in a secure and stable way. We developed a system that tracks functions and structs across different versions of open-source libraries. In addition, we developed a system that performs userspace memory management in eBPF. As a result, Odigos is the only solution that is able to automatically generate distributed traces for compiled languages like Go and Rust. While other solutions require users to be experts in OpenTelemetry or eBPF, our solution does not require prior knowledge of observability technologies.<p>Our solution can be installed on any Kubernetes cluster by executing a single command. Once installed, we detect the programming language of every running application and apply the relevant instrumentation. For JIT languages (Java and .NET) or interpreted languages (JavaScript and Python) we deploy OpenTelemetry instrumentation. For compiled languages (Go, Rust, C) we deploy our eBPF-based instrumentation. All of this is abstracted from the user, who only has to: (1) select any or all of their target applications and (2) select a backend to send monitoring data to.<p>In May 2022, we released our first open-source project: automatic instrumentation for Go applications, based on eBPF. We later donated this project to the OpenTelemetry community and it is currently being developed as part of the Go Automatic Instrumentation SIG.<p>We are big believers in open standards, therefore the instrumentation and collectors used by Odigos are all based on open-source projects developed by the OpenTelemetry community. This also enables us to be vendor-agnostic.<p>Currently we are focused on building our open-source project. There are no pricing or paid features yet, but in the future, we are planning to offer a managed version of Odigos that will include enterprise features.<p>If you&#x27;re interested to learn more, check out our docs (<a href=\"https:&#x2F;&#x2F;docs.odigos.io\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.odigos.io</a>), watch a demo video (<a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9d36AmVtuGU\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9d36AmVtuGU</a>), and visit our website (<a href=\"https:&#x2F;&#x2F;odigos.io\" rel=\"nofollow\">https:&#x2F;&#x2F;odigos.io</a>).<p>We\u2019d love to hear your experiences with tracing and monitoring distributed applications and anything else you\u2019d like to share!", "title": "Launch HN: Odigos (YC W23) \u2013 Instant distributed tracing for Kubernetes clusters", "updated_at": "2024-09-20T13:02:44Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fabienpenso"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["opentelemetry", "production"], "value": "Hey HN. I'm Fabien, principal engineer, 25 years shipping <em>production</em> systems (Ruby, Swift, now Rust). I built Moltis because I wanted an AI assistant I could run myself, trust end to end, and make extensible in the Rust way using traits and the type system. It shares some ideas with OpenClaw (same memory approach, Pi-inspired self-extension) but is Rust-native from the ground up. The agent can create its own skills at runtime.<p>Moltis is one Rust binary, 150k lines, ~60MB, web UI included. No Node, no Python, no runtime deps. Multi-provider LLM routing (OpenAI, local GGUF/MLX, Hugging Face), sandboxed execution (Docker/Podman/Apple Containers), hybrid vector + full-text memory, MCP tool servers with auto-restart, and multi-channel (web, Telegram, API) with shared context. MIT licensed. No telemetry phoning home, but full observability built in (<em>OpenTelemetry</em>, Prometheus).<p>I've included 1-click deploys on DigitalOcean and Fly.io, but since a Docker image is provided you can easily run it on your own servers as well. I've written before about owning your content (<a href=\"https://pen.so/2020/11/07/own-your-content/\" rel=\"nofollow\">https://pen.so/2020/11/07/own-your-content/</a>) and owning your email (<a href=\"https://pen.so/2020/12/10/own-your-email/\" rel=\"nofollow\">https://pen.so/2020/12/10/own-your-email/</a>). Same logic here: if something touches your files, credentials, and daily workflow, you should be able to inspect it, audit it, and fork it if the project changes direction.<p>It's alpha. I use it daily and I'm shipping because it's useful, not because it's done.<p>Longer architecture deep-dive: <a href=\"https://pen.so/2026/02/12/moltis-a-personal-ai-assistant-built-in-rust/\" rel=\"nofollow\">https://pen.so/2026/02/12/moltis-a-personal-ai-assistant-bui...</a><p>Happy to discuss the Rust architecture, security model, or local LLM setup. Would love feedback."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Moltis \u2013 AI assistant with memory, tools, and self-extending skills"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.moltis.org"}}, "_tags": ["story", "author_fabienpenso", "story_46993587", "show_hn"], "author": "fabienpenso", "children": [46994819, 46996806, 47007030, 47007174, 47007263, 47007514, 47007525, 47007667, 47007824, 47008032, 47008298, 47008585, 47008614, 47009148, 47009604, 47010364, 47010835, 47011060, 47011375, 47012050, 47013592, 47014345, 47017051, 47018401, 47024452, 47054036, 47165498], "created_at": "2026-02-12T19:15:21Z", "created_at_i": 1770923721, "num_comments": 51, "objectID": "46993587", "points": 131, "story_id": 46993587, "story_text": "Hey HN. I&#x27;m Fabien, principal engineer, 25 years shipping production systems (Ruby, Swift, now Rust). I built Moltis because I wanted an AI assistant I could run myself, trust end to end, and make extensible in the Rust way using traits and the type system. It shares some ideas with OpenClaw (same memory approach, Pi-inspired self-extension) but is Rust-native from the ground up. The agent can create its own skills at runtime.<p>Moltis is one Rust binary, 150k lines, ~60MB, web UI included. No Node, no Python, no runtime deps. Multi-provider LLM routing (OpenAI, local GGUF&#x2F;MLX, Hugging Face), sandboxed execution (Docker&#x2F;Podman&#x2F;Apple Containers), hybrid vector + full-text memory, MCP tool servers with auto-restart, and multi-channel (web, Telegram, API) with shared context. MIT licensed. No telemetry phoning home, but full observability built in (OpenTelemetry, Prometheus).<p>I&#x27;ve included 1-click deploys on DigitalOcean and Fly.io, but since a Docker image is provided you can easily run it on your own servers as well. I&#x27;ve written before about owning your content (<a href=\"https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;11&#x2F;07&#x2F;own-your-content&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;11&#x2F;07&#x2F;own-your-content&#x2F;</a>) and owning your email (<a href=\"https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;12&#x2F;10&#x2F;own-your-email&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;12&#x2F;10&#x2F;own-your-email&#x2F;</a>). Same logic here: if something touches your files, credentials, and daily workflow, you should be able to inspect it, audit it, and fork it if the project changes direction.<p>It&#x27;s alpha. I use it daily and I&#x27;m shipping because it&#x27;s useful, not because it&#x27;s done.<p>Longer architecture deep-dive: <a href=\"https:&#x2F;&#x2F;pen.so&#x2F;2026&#x2F;02&#x2F;12&#x2F;moltis-a-personal-ai-assistant-built-in-rust&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pen.so&#x2F;2026&#x2F;02&#x2F;12&#x2F;moltis-a-personal-ai-assistant-bui...</a><p>Happy to discuss the Rust architecture, security model, or local LLM setup. Would love feedback.", "title": "Show HN: Moltis \u2013 AI assistant with memory, tools, and self-extending skills", "updated_at": "2026-02-27T04:18:37Z", "url": "https://www.moltis.org"}], "hitsPerPage": 15, "nbHits": 43, "nbPages": 3, "page": 0, "params": "query=opentelemetry+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 14, "processingTimingsMS": {"_request": {"roundTrip": 19}, "afterFetch": {"format": {"highlighting": 3, "total": 4}}, "fetch": {"query": 11, "scanning": 1, "total": 13}, "total": 14}, "query": "opentelemetry production", "serverTimeMS": 18}}