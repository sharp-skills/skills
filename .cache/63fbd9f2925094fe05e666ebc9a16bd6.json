{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "devsecopsify"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "Load Testing in <em>Production</em> with Grafana <em>Loki</em>, Kubernetes and Go"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "https://tech.loveholidays.com/load-testing-in-<em>production</em>-with-grafana-<em>loki</em>-kubernetes-and-golang-1699554d2aa3?gi=dce6cd88d60c"}}, "_tags": ["story", "author_devsecopsify", "story_30307904"], "author": "devsecopsify", "created_at": "2022-02-11T22:45:18Z", "created_at_i": 1644619518, "num_comments": 0, "objectID": "30307904", "points": 6, "story_id": 30307904, "title": "Load Testing in Production with Grafana Loki, Kubernetes and Go", "updated_at": "2024-09-20T10:26:39Z", "url": "https://tech.loveholidays.com/load-testing-in-production-with-grafana-loki-kubernetes-and-golang-1699554d2aa3?gi=dce6cd88d60c"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ArtWomb"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "<em>Loki</em>: A Unified Multiphysics Simulation Framework for <em>Production</em> [pdf]"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["loki"], "value": "http://alexey.stomakhin.com/research/siggraph2022_<em>loki</em>.pdf"}}, "_tags": ["story", "author_ArtWomb", "story_34105740"], "author": "ArtWomb", "created_at": "2022-12-23T13:48:53Z", "created_at_i": 1671803333, "num_comments": 0, "objectID": "34105740", "points": 3, "story_id": 34105740, "title": "Loki: A Unified Multiphysics Simulation Framework for Production [pdf]", "updated_at": "2024-09-20T12:47:04Z", "url": "http://alexey.stomakhin.com/research/siggraph2022_loki.pdf"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "valyala"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "I don't like the existing query languages for Elasticsearch and Grafana <em>Loki</em>, because they are too awkward to use for typical logs' investigation cases. So I designed new query language - LogsQL - and wrote reference implementation for it as a part of VictoriaLogs - an open source database for logs. LogsQL is based on the following principles:<p>- Simplicity. It is easy to write typical queries over logs in it. For example, a single word `error` is a valid LogsQL query, which returns all the logs with the `error` word. Another example is `_time:5m error`, which returns all the logs with the 'error' word over the last 5 minutes.<p>- Composable building blocks similar to Unix pipes, which allow powerful filtering, transforming and calculating stats over the selected logs. For example, `_time:5m error | stats count() as rows` returns the number of logs with the `error` word over the last 5 minutes.<p>- Readability. Typical LogsQL queries must be easy to read and understand even for persons unfamiliar with it.<p>Take a look at LogsQL docs [1] and try using VictoriaLogs [2] in <em>production</em>. If you like Unix way and KISS design principle, then you'll enjoy LogsQL :)<p>[1] <a href=\"https://docs.victoriametrics.com/victorialogs/logsql/\" rel=\"nofollow\">https://docs.victoriametrics.com/victorialogs/logsql/</a><p>[2] <a href=\"https://docs.victoriametrics.com/victorialogs/\" rel=\"nofollow\">https://docs.victoriametrics.com/victorialogs/</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LogsQL \u2013 opinionated query language for logs"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://docs.victoriametrics.com/victorialogs/logsql/"}}, "_tags": ["story", "author_valyala", "story_40449299", "show_hn"], "author": "valyala", "children": [40476432, 40476496, 40476900, 40477156, 40477209, 40477781, 40477935], "created_at": "2024-05-23T01:28:28Z", "created_at_i": 1716427708, "num_comments": 29, "objectID": "40449299", "points": 61, "story_id": 40449299, "story_text": "I don&#x27;t like the existing query languages for Elasticsearch and Grafana Loki, because they are too awkward to use for typical logs&#x27; investigation cases. So I designed new query language - LogsQL - and wrote reference implementation for it as a part of VictoriaLogs - an open source database for logs. LogsQL is based on the following principles:<p>- Simplicity. It is easy to write typical queries over logs in it. For example, a single word `error` is a valid LogsQL query, which returns all the logs with the `error` word. Another example is `_time:5m error`, which returns all the logs with the &#x27;error&#x27; word over the last 5 minutes.<p>- Composable building blocks similar to Unix pipes, which allow powerful filtering, transforming and calculating stats over the selected logs. For example, `_time:5m error | stats count() as rows` returns the number of logs with the `error` word over the last 5 minutes.<p>- Readability. Typical LogsQL queries must be easy to read and understand even for persons unfamiliar with it.<p>Take a look at LogsQL docs [1] and try using VictoriaLogs [2] in production. If you like Unix way and KISS design principle, then you&#x27;ll enjoy LogsQL :)<p>[1] <a href=\"https:&#x2F;&#x2F;docs.victoriametrics.com&#x2F;victorialogs&#x2F;logsql&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.victoriametrics.com&#x2F;victorialogs&#x2F;logsql&#x2F;</a><p>[2] <a href=\"https:&#x2F;&#x2F;docs.victoriametrics.com&#x2F;victorialogs&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.victoriametrics.com&#x2F;victorialogs&#x2F;</a>", "title": "Show HN: LogsQL \u2013 opinionated query language for logs", "updated_at": "2025-12-16T15:20:22Z", "url": "https://docs.victoriametrics.com/victorialogs/logsql/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "valyala"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "Why we generate and collect logs? Mostly for post-factum analysis and debugging after some event (incident in <em>production</em>, slow distributed request, security breach, etc). For example:<p>- To find all the error logs with a particular substring, and then to inspect them visually.<p>- To find logs for the particular request_id, user_id or trace_id and then to inspect them visually.<p>- To calculate the number of happy hackers SSHed into your hosts.<p>- To calculate stats over web logs for a particular domain, path, browser, etc.<p>- To calculate the frequency of logs with particular substrings.<p>All these tasks are easy to perform from command-line when logs are stored in plain files. Just start with<p><pre><code>  cat /path/to/log | grep some-substring\n</code></pre>\nThen iteratively apply the needed commands to the selected log lines - wc, awk, grep, less, head, sort, uniq, cut, etc. - until the desired result is obtained.<p>This approach serves great for analyzing locally stored logs on a few hosts. It doesn't scale well for cases when logs should be analyzed across hundreds of hosts and/or  application instances. Of course, there are command-line tools for parallel execution of shell commands across hundreds of hosts, which can help with this case. But we want better open-source solution.<p>So we've got ElasticSearch, OpenSearch and Grafana <em>Loki</em>. These solutions allow collecting logs from hundreds of hosts/applications. But they totally screw up analysis of these logs. While they provide shiny UIs, their usability for typical tasks for logs' analysis and debugging is multiple orders of magnitude lower than good old command-line tools provide. They provide awkward to use query languages with silly limitations (such as the number of returned log lines per query) and very limited integration with existing command-line tools. For example, you cannot easily perform the equivalent of the following command:<p><pre><code>  cat /log/file | grep some-string | my-custom-script-for-analysis\n</code></pre>\nover logs stored in ElasticSearch / Grafana <em>Loki</em> if &quot;some-string&quot; occurs in millions or billions of log lines.<p>ElasticSearch and <em>Loki</em> also need non-trivial configuration, index creation, performance tuning and maintenance. Do we really want paying this price in exchange to get subpar ability to analyze logs collected from hundreds of hosts/applications?<p>Probably, it is time to use better solution, which allows collecting logs from hundreds of sources and then analyzing them with good old command-line tools in the usual ergonomic way? This question was raised many times when I had to analyze logs with modern solutions for logs. I couldn't find the proper solution, so I decided creating it on my own based on my experience with creating VictoriaMetrics. So I created open-source user-friendly database for logs - VictoriaLogs [1]. It accepts structured and unstructured logs from popular log shippers such as Filebeat, Fluentbit, Logstash, Vector, etc., it supports fast full-text search without any configuration / tuning, and it has perfect integration with good old command-line tools [2].<p>Give it a try and share your experience! I hope you'll fall in love with VictoriaLogs, especially if you like command-line because of its' high productivity.<p>[1] https://docs.victoriametrics.com/VictoriaLogs/<p>[2] https://docs.victoriametrics.com/VictoriaLogs/querying/#command-line"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "The usability of modern log management tools is screwed"}}, "_tags": ["story", "author_valyala", "story_36671872", "ask_hn"], "author": "valyala", "children": [36672012, 36672626], "created_at": "2023-07-10T19:41:47Z", "created_at_i": 1689018107, "num_comments": 3, "objectID": "36671872", "points": 8, "story_id": 36671872, "story_text": "Why we generate and collect logs? Mostly for post-factum analysis and debugging after some event (incident in production, slow distributed request, security breach, etc). For example:<p>- To find all the error logs with a particular substring, and then to inspect them visually.<p>- To find logs for the particular request_id, user_id or trace_id and then to inspect them visually.<p>- To calculate the number of happy hackers SSHed into your hosts.<p>- To calculate stats over web logs for a particular domain, path, browser, etc.<p>- To calculate the frequency of logs with particular substrings.<p>All these tasks are easy to perform from command-line when logs are stored in plain files. Just start with<p><pre><code>  cat &#x2F;path&#x2F;to&#x2F;log | grep some-substring\n</code></pre>\nThen iteratively apply the needed commands to the selected log lines - wc, awk, grep, less, head, sort, uniq, cut, etc. - until the desired result is obtained.<p>This approach serves great for analyzing locally stored logs on a few hosts. It doesn&#x27;t scale well for cases when logs should be analyzed across hundreds of hosts and&#x2F;or  application instances. Of course, there are command-line tools for parallel execution of shell commands across hundreds of hosts, which can help with this case. But we want better open-source solution.<p>So we&#x27;ve got ElasticSearch, OpenSearch and Grafana Loki. These solutions allow collecting logs from hundreds of hosts&#x2F;applications. But they totally screw up analysis of these logs. While they provide shiny UIs, their usability for typical tasks for logs&#x27; analysis and debugging is multiple orders of magnitude lower than good old command-line tools provide. They provide awkward to use query languages with silly limitations (such as the number of returned log lines per query) and very limited integration with existing command-line tools. For example, you cannot easily perform the equivalent of the following command:<p><pre><code>  cat &#x2F;log&#x2F;file | grep some-string | my-custom-script-for-analysis\n</code></pre>\nover logs stored in ElasticSearch &#x2F; Grafana Loki if &quot;some-string&quot; occurs in millions or billions of log lines.<p>ElasticSearch and Loki also need non-trivial configuration, index creation, performance tuning and maintenance. Do we really want paying this price in exchange to get subpar ability to analyze logs collected from hundreds of hosts&#x2F;applications?<p>Probably, it is time to use better solution, which allows collecting logs from hundreds of sources and then analyzing them with good old command-line tools in the usual ergonomic way? This question was raised many times when I had to analyze logs with modern solutions for logs. I couldn&#x27;t find the proper solution, so I decided creating it on my own based on my experience with creating VictoriaMetrics. So I created open-source user-friendly database for logs - VictoriaLogs [1]. It accepts structured and unstructured logs from popular log shippers such as Filebeat, Fluentbit, Logstash, Vector, etc., it supports fast full-text search without any configuration &#x2F; tuning, and it has perfect integration with good old command-line tools [2].<p>Give it a try and share your experience! I hope you&#x27;ll fall in love with VictoriaLogs, especially if you like command-line because of its&#x27; high productivity.<p>[1] https:&#x2F;&#x2F;docs.victoriametrics.com&#x2F;VictoriaLogs&#x2F;<p>[2] https:&#x2F;&#x2F;docs.victoriametrics.com&#x2F;VictoriaLogs&#x2F;querying&#x2F;#command-line", "title": "The usability of modern log management tools is screwed", "updated_at": "2025-12-16T15:21:09Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kvaranasi_"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "Hi HN, I've been building RocketLogs, an intelligence observability layer that sits on top of <em>Loki</em>, Tempo, and Prometheus.<p>The problem I kept running into: you get paged at 3 AM, open Grafana, flip between six dashboards, grep through logs, try to figure out which deploy went out, and an hour later you maybe have a root cause. You still need to dig through code and find the culprit. The data is all there, but nothing connects it for you.<p>RocketLogs tries to fix that:<p>VS Code / Cursor Extension - Fetches your slow and errored-out endpoints from <em>production</em> and lists them right in the Cursor chat sidebar with their latencies. It maps them to your codebase using Pyroscope profiling. So you have metrics -&gt; traces -&gt; profiles -&gt; code right inside the codebase. It shows latency and a heat-map of how much your modules/funcs are bleeding your endpoints. So you can jump to the offending function and fix it without context-switching and within minutes.<p>Beyond those, it's a full observability stack: log search, distributed tracing with waterfall views, SLOs with error budget tracking, incident management with AI-generated synopses, smart alerts, and Prometheus dashboards.<p>We're built on the LGTM stack, so if you're already sending data to <em>Loki</em>/Tempo/Prometheus, you can point RocketLogs at your existing infrastructure. Using this as essentially another observability layer that sits on top. If you want to use multiple vendors, you can also do a telemetry fan-out.<p>Would love feedback. What's the most painful part of your current observability workflow?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Cursor for Observability"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://dashboard.rocketgraph.app"}}, "_tags": ["story", "author_kvaranasi_", "story_47053906", "show_hn"], "author": "kvaranasi_", "children": [47131101], "created_at": "2026-02-17T21:50:04Z", "created_at_i": 1771365004, "num_comments": 1, "objectID": "47053906", "points": 3, "story_id": 47053906, "story_text": "Hi HN, I&#x27;ve been building RocketLogs, an intelligence observability layer that sits on top of Loki, Tempo, and Prometheus.<p>The problem I kept running into: you get paged at 3 AM, open Grafana, flip between six dashboards, grep through logs, try to figure out which deploy went out, and an hour later you maybe have a root cause. You still need to dig through code and find the culprit. The data is all there, but nothing connects it for you.<p>RocketLogs tries to fix that:<p>VS Code &#x2F; Cursor Extension - Fetches your slow and errored-out endpoints from production and lists them right in the Cursor chat sidebar with their latencies. It maps them to your codebase using Pyroscope profiling. So you have metrics -&gt; traces -&gt; profiles -&gt; code right inside the codebase. It shows latency and a heat-map of how much your modules&#x2F;funcs are bleeding your endpoints. So you can jump to the offending function and fix it without context-switching and within minutes.<p>Beyond those, it&#x27;s a full observability stack: log search, distributed tracing with waterfall views, SLOs with error budget tracking, incident management with AI-generated synopses, smart alerts, and Prometheus dashboards.<p>We&#x27;re built on the LGTM stack, so if you&#x27;re already sending data to Loki&#x2F;Tempo&#x2F;Prometheus, you can point RocketLogs at your existing infrastructure. Using this as essentially another observability layer that sits on top. If you want to use multiple vendors, you can also do a telemetry fan-out.<p>Would love feedback. What&#x27;s the most painful part of your current observability workflow?", "title": "Show HN: Cursor for Observability", "updated_at": "2026-02-24T00:26:40Z", "url": "https://dashboard.rocketgraph.app"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "shivajikobardan"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "I am a <em>production</em> operator engineer in a Nepali company.<p>Here're the reasons why I know I am in dead end job.<p>- It has already been 1y and nowadays, work feels like a chore. I've been just following my notes that I made earlier and solving tickets based on that.<p>- The job is limiting in its scope. I studied computer science in university. But this job is revolved around escalating issues to developers. The only thing that we do are<p>- changing database flags to make it work.<p>- forward logs to developers(We probably exist only because there is no <em>loki</em> log centralization)<p>- The only &quot;real tech&quot; that we work is when debugging web servers or when some new change request for webservers comes.<p>- Since it's limiting in scope, it's also limiting in salary(which is not necessarily true always, but it's here).<p>- Manager has cleverly overhired so that even if few leave, there will be no issues (more employees, less budget for each)<p>- I am underpaid. This is Nepal and neither development hits here nor recession. We're forever in same state &quot;status quo&quot;. I am making somewhere around 250$/mo and I should be making 350$/mo for the last 6 months.<p>- Performance review is coming soon, but I've heard from reliable sources that the max increment will be 100$/mo. So, I can expect at max 350$/mo after 1.5 yoe.<p>* * *\nMy problem\n* * *<p>I've not been able to find a new job and I feel guilty to even search for a new job. So, I am searching a new job with a heavy heart. While I've applied to couple of places, but they don't even respond (even with multiple serious referrals from known people).<p>Now the thing is linkedin is where employers are finding me and I've put my job title different compared to what it is on CV. I think that's why I'm not getting any responses after I send my CV.\nThe title is like data analytics employee writing data science. LOL. I wrote it just because it is what I want to be and it's how people are searching for that role candidates.<p>* * *<p>I am still investing time in self-learning. Due to my low salary, certification exams(they cost double here in Nepal) are out of my reach(RHCSA/LPIC etc). But I've dedicated my ass off to study and I regularly study ~3hrs. I know that I won't reach that far by self study without a job that challenges me. But I am unable to find a newer challenging job.<p>I learnt loads of Linux and web servers in the current job, which simply would not have been possible without getting this job for me.<p>* * *<p>TLDR:<p>- Dead end job<p>- comfort zone in current job<p>- not finding a new job, hard time building skills w/o a challenging job<p>- don't even search pro-actively for jobs<p>- have a differing title in linkedin and resume, which might be a reason for extremely low interview conversion rates."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: How to gear up from getting comfortable with a dead-end job?"}}, "_tags": ["story", "author_shivajikobardan", "story_40633954", "ask_hn"], "author": "shivajikobardan", "children": [40634454, 40635366], "created_at": "2024-06-10T14:20:31Z", "created_at_i": 1718029231, "num_comments": 5, "objectID": "40633954", "points": 2, "story_id": 40633954, "story_text": "I am a production operator engineer in a Nepali company.<p>Here&#x27;re the reasons why I know I am in dead end job.<p>- It has already been 1y and nowadays, work feels like a chore. I&#x27;ve been just following my notes that I made earlier and solving tickets based on that.<p>- The job is limiting in its scope. I studied computer science in university. But this job is revolved around escalating issues to developers. The only thing that we do are<p>- changing database flags to make it work.<p>- forward logs to developers(We probably exist only because there is no loki log centralization)<p>- The only &quot;real tech&quot; that we work is when debugging web servers or when some new change request for webservers comes.<p>- Since it&#x27;s limiting in scope, it&#x27;s also limiting in salary(which is not necessarily true always, but it&#x27;s here).<p>- Manager has cleverly overhired so that even if few leave, there will be no issues (more employees, less budget for each)<p>- I am underpaid. This is Nepal and neither development hits here nor recession. We&#x27;re forever in same state &quot;status quo&quot;. I am making somewhere around 250$&#x2F;mo and I should be making 350$&#x2F;mo for the last 6 months.<p>- Performance review is coming soon, but I&#x27;ve heard from reliable sources that the max increment will be 100$&#x2F;mo. So, I can expect at max 350$&#x2F;mo after 1.5 yoe.<p>* * *\nMy problem\n* * *<p>I&#x27;ve not been able to find a new job and I feel guilty to even search for a new job. So, I am searching a new job with a heavy heart. While I&#x27;ve applied to couple of places, but they don&#x27;t even respond (even with multiple serious referrals from known people).<p>Now the thing is linkedin is where employers are finding me and I&#x27;ve put my job title different compared to what it is on CV. I think that&#x27;s why I&#x27;m not getting any responses after I send my CV.\nThe title is like data analytics employee writing data science. LOL. I wrote it just because it is what I want to be and it&#x27;s how people are searching for that role candidates.<p>* * *<p>I am still investing time in self-learning. Due to my low salary, certification exams(they cost double here in Nepal) are out of my reach(RHCSA&#x2F;LPIC etc). But I&#x27;ve dedicated my ass off to study and I regularly study ~3hrs. I know that I won&#x27;t reach that far by self study without a job that challenges me. But I am unable to find a newer challenging job.<p>I learnt loads of Linux and web servers in the current job, which simply would not have been possible without getting this job for me.<p>* * *<p>TLDR:<p>- Dead end job<p>- comfort zone in current job<p>- not finding a new job, hard time building skills w&#x2F;o a challenging job<p>- don&#x27;t even search pro-actively for jobs<p>- have a differing title in linkedin and resume, which might be a reason for extremely low interview conversion rates.", "title": "Ask HN: How to gear up from getting comfortable with a dead-end job?", "updated_at": "2024-09-20T17:13:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Mockapapella"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "Hi HN, I've spent the better part of the last year deploying AI inference systems for both personal and work projects. I've noticed a ton of &quot;gotchas&quot; and footguns throughout the process that make doing it right a pain, so I built &quot;Batteries Included AI Deployment&quot;. Not a creative name but it does what it says on the tin.<p>In short, it's a template for deploying AI inference APIs with FastAPI.<p>In long, it uses Docker to encapsulate (almost) the entire development and deployment process. The repo includes:<p>1. A way to download and cache models straight from huggingface<p>2. A way to expose those cached models via a FastAPI server endpoint<p>3. A docker configuration that exposes a `debugpy` port so that you can debug your application within a container<p>4. A way to run tests<p>5. A way to debug tests (using `debugpy` as mentioned above)<p>6. A way to run pre-commits on staged files<p>7. A way to manually run pre-commits on all code in your repository<p>8. CI steps via GitHub Actions<p>9. Full Observability with a Grafana Dashboard<p>10. Metrics via Prometheus<p>11. Tracing via Tempo<p>12. Logs via <em>Loki</em><p>13. GPU monitoring via DCGM<p>14. CD via GitHub actions and a `post-receive` hook on the server<p>15. Alerts that email you when something goes wrong in <em>production</em><p>I say &quot;almost&quot; because you still need a way to attach to the debugger port from outside the docker container and there's some one-time configurations that need to be set up manually, but not anything beyond that.<p>I'd love to hear any feedback you might have :)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Batteries Included AI Deployment"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Mockapapella/batteries-included-ai-deployment"}}, "_tags": ["story", "author_Mockapapella", "story_40760541", "show_hn"], "author": "Mockapapella", "children": [40760589], "created_at": "2024-06-22T17:17:55Z", "created_at_i": 1719076675, "num_comments": 2, "objectID": "40760541", "points": 2, "story_id": 40760541, "story_text": "Hi HN, I&#x27;ve spent the better part of the last year deploying AI inference systems for both personal and work projects. I&#x27;ve noticed a ton of &quot;gotchas&quot; and footguns throughout the process that make doing it right a pain, so I built &quot;Batteries Included AI Deployment&quot;. Not a creative name but it does what it says on the tin.<p>In short, it&#x27;s a template for deploying AI inference APIs with FastAPI.<p>In long, it uses Docker to encapsulate (almost) the entire development and deployment process. The repo includes:<p>1. A way to download and cache models straight from huggingface<p>2. A way to expose those cached models via a FastAPI server endpoint<p>3. A docker configuration that exposes a `debugpy` port so that you can debug your application within a container<p>4. A way to run tests<p>5. A way to debug tests (using `debugpy` as mentioned above)<p>6. A way to run pre-commits on staged files<p>7. A way to manually run pre-commits on all code in your repository<p>8. CI steps via GitHub Actions<p>9. Full Observability with a Grafana Dashboard<p>10. Metrics via Prometheus<p>11. Tracing via Tempo<p>12. Logs via Loki<p>13. GPU monitoring via DCGM<p>14. CD via GitHub actions and a `post-receive` hook on the server<p>15. Alerts that email you when something goes wrong in production<p>I say &quot;almost&quot; because you still need a way to attach to the debugger port from outside the docker container and there&#x27;s some one-time configurations that need to be set up manually, but not anything beyond that.<p>I&#x27;d love to hear any feedback you might have :)", "title": "Show HN: Batteries Included AI Deployment", "updated_at": "2024-09-20T17:17:53Z", "url": "https://github.com/Mockapapella/batteries-included-ai-deployment"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "hkupty"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "Hi!<p>It has been over a year now that I have been slowly but surely investing in this project and I think it has reached a point where it can be <em>production</em> ready now.<p>Penna is a specialized SLF4J backend that aims to target a specific use-case, for a specific deployment type: it writes JSON logs to stdout, to address a common use case for apps deployed in kubernetes, docker or similar, where those logs are captured by agents and shipped to the likes of <em>Loki</em> or Elastic Stack.<p>Penna is meant to be easy setup (essentially works without any necessary configuration) and allows for yaml-based configs for fine tuning, which can be updated while the app is live without a restart.<p>Those scenarios are based on my observations over a few places I worked with, so I'd like to hear from you how do you feel about the problem it tries to solve and, if you try the project, your overall impressions.<p>The source code is here[0] if you want to have a look at it.<p>Thanks in advance!<p>[0]: <a href=\"https://github.com/hkupty/penna\">https://github.com/hkupty/penna</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: I wrote a specialized SLF4J back end, for JSON logs to stdout"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://hkupty.github.io/penna/"}}, "_tags": ["story", "author_hkupty", "story_39932188", "show_hn"], "author": "hkupty", "children": [39932788], "created_at": "2024-04-04T16:03:48Z", "created_at_i": 1712246628, "num_comments": 2, "objectID": "39932188", "points": 2, "story_id": 39932188, "story_text": "Hi!<p>It has been over a year now that I have been slowly but surely investing in this project and I think it has reached a point where it can be production ready now.<p>Penna is a specialized SLF4J backend that aims to target a specific use-case, for a specific deployment type: it writes JSON logs to stdout, to address a common use case for apps deployed in kubernetes, docker or similar, where those logs are captured by agents and shipped to the likes of Loki or Elastic Stack.<p>Penna is meant to be easy setup (essentially works without any necessary configuration) and allows for yaml-based configs for fine tuning, which can be updated while the app is live without a restart.<p>Those scenarios are based on my observations over a few places I worked with, so I&#x27;d like to hear from you how do you feel about the problem it tries to solve and, if you try the project, your overall impressions.<p>The source code is here[0] if you want to have a look at it.<p>Thanks in advance!<p>[0]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;hkupty&#x2F;penna\">https:&#x2F;&#x2F;github.com&#x2F;hkupty&#x2F;penna</a>", "title": "Show HN: I wrote a specialized SLF4J back end, for JSON logs to stdout", "updated_at": "2024-09-20T16:48:05Z", "url": "https://hkupty.github.io/penna/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "shunmugavel2609"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "Engineering leaders Care a lot about MTTR: the faster you mitigate and remediate an outage, the less user pain and revenue hit<p>I\u2019m building an AI-powered SRE agent called Kepler to help engineers diagnose and explain <em>production</em> incidents faster. It integrates with systems like Prometheus, <em>Loki</em>, GitHub, and CI/CD pipelines to understand what changed, what broke, and why \u2014 and summarizes it in plain English.<p>The idea came from years of firefighting outages as a backend engineer and realizing how much time gets lost just figuring out where to look. We\u2019ve all been there \u2014 four engineers, three dashboards, ten tabs open, and two hours gone before someone says: \u201coh, it\u2019s that PR from this morning.\u201d<p>Kepler automates root cause analysis by correlating alerts, logs, metrics, traces, and code diffs. Some of the features we\u2019re experimenting with:<p>GitHub PR blame scoring against error logs<p>Top-log summarization using LLMs<p>Incident replay and RCA report generation<p>Slack-first UX for oncall usage<p>Live demo <a href=\"https://www.loom.com/share/78cdb41f6d504ef8b3457d4279ae170c\" rel=\"nofollow\">https://www.loom.com/share/78cdb41f6d504ef8b3457d4279ae170c</a><p>Website \u2013 <a href=\"https://meetkepler.com\" rel=\"nofollow\">https://meetkepler.com</a> \nWe\u2019re planning to open source the core. Waitlist is open for anyone who wants to try it out.<p>We\u2019re also exploring fine-tuning open-source reasoning models for infrastructure and SRE reasoning \u2014 with incident triage as our first focus.<p>It\u2019s very early and rough around the edges, but I\u2019d love your feedback:<p>What do you wish were automated during incidents?<p>What slows you down during triage?<p>Would you use a system like this during oncall?<p>If you want to jam or chat: <a href=\"https://calendly.com/meetkepler/30min\" rel=\"nofollow\">https://calendly.com/meetkepler/30min</a><p>Thanks!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Kepler SRE agent that does rootcause analysis for Incidents"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.loom.com/share/78cdb41f6d504ef8b3457d4279ae170c"}}, "_tags": ["story", "author_shunmugavel2609", "story_44172141", "show_hn"], "author": "shunmugavel2609", "created_at": "2025-06-03T17:00:36Z", "created_at_i": 1748970036, "num_comments": 0, "objectID": "44172141", "points": 2, "story_id": 44172141, "story_text": "Engineering leaders Care a lot about MTTR: the faster you mitigate and remediate an outage, the less user pain and revenue hit<p>I\u2019m building an AI-powered SRE agent called Kepler to help engineers diagnose and explain production incidents faster. It integrates with systems like Prometheus, Loki, GitHub, and CI&#x2F;CD pipelines to understand what changed, what broke, and why \u2014 and summarizes it in plain English.<p>The idea came from years of firefighting outages as a backend engineer and realizing how much time gets lost just figuring out where to look. We\u2019ve all been there \u2014 four engineers, three dashboards, ten tabs open, and two hours gone before someone says: \u201coh, it\u2019s that PR from this morning.\u201d<p>Kepler automates root cause analysis by correlating alerts, logs, metrics, traces, and code diffs. Some of the features we\u2019re experimenting with:<p>GitHub PR blame scoring against error logs<p>Top-log summarization using LLMs<p>Incident replay and RCA report generation<p>Slack-first UX for oncall usage<p>Live demo <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;78cdb41f6d504ef8b3457d4279ae170c\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;78cdb41f6d504ef8b3457d4279ae170c</a><p>Website \u2013 <a href=\"https:&#x2F;&#x2F;meetkepler.com\" rel=\"nofollow\">https:&#x2F;&#x2F;meetkepler.com</a> \nWe\u2019re planning to open source the core. Waitlist is open for anyone who wants to try it out.<p>We\u2019re also exploring fine-tuning open-source reasoning models for infrastructure and SRE reasoning \u2014 with incident triage as our first focus.<p>It\u2019s very early and rough around the edges, but I\u2019d love your feedback:<p>What do you wish were automated during incidents?<p>What slows you down during triage?<p>Would you use a system like this during oncall?<p>If you want to jam or chat: <a href=\"https:&#x2F;&#x2F;calendly.com&#x2F;meetkepler&#x2F;30min\" rel=\"nofollow\">https:&#x2F;&#x2F;calendly.com&#x2F;meetkepler&#x2F;30min</a><p>Thanks!", "title": "Show HN: Kepler SRE agent that does rootcause analysis for Incidents", "updated_at": "2025-06-03T20:44:02Z", "url": "https://www.loom.com/share/78cdb41f6d504ef8b3457d4279ae170c"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "katiecastillo"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["loki", "production"], "value": "CrossnoKaye is looking for a Senior Backend Software Engineer to join our fast-paced, growing engineering department. This individual will work with a passionate team of software engineers and scientists to create highly efficient distributed systems with high-uptime. This developer will collaborate with the team to troubleshoot the software, write clean code, and have the opportunity to improve the features to the existing software. Your main responsibility will be to maintain and grow features in the backend code base and develop the workflows in Golang. The ideal candidate will be  flexible and adaptable enough to thrive in a fluid startup environment. Our current technology stack includes AWS, Golang, Vue.JS, Postgres, TimeseriesDB, Grafana/<em>Loki</em>, Kubernetes, and Docker.\nJob Responsibilities:\n\u25cf Evolve our distributed computing infrastructure using best-in-class engineering practices\n\u25cf Design and implement highly scalable and fault tolerant web-services and microservices to\nsupport our growing cloud infrastructure\n\u25cf Lead collaboration with other teams to define new features and improve existing software\n\u25cf Monitoring and improving application health and performance\n\u25cf Write unit tests as part of developing high quality and reliable software\n\u25cf Builds consensus and makes decisions in leading edge technologies\nQualifications\n\u25cf Bachelor's Degree in Computer Science (or related industry experience)\n\u25cf 5+ years\u2019 experience of developing server-side software\n \u25cf Expertise with Timeseries databases\n\u25cf Experience with RPC, network protocols, and horizontally scalable services\n\u25cf Knowledge of design patterns\n\u25cf Proficient in a structured language (C++, Java, Golang, Rust, etc)\n\u25cf Infrastructure and application observability\n\u25cf Experience with cloud-computing and distributed systems\n\u25cf Experience in instrumenting code for gathering <em>production</em> performance metrics\n\u25cf Strong experience in Service Architecture, API Design, and Database Schema Design"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Senior Back End Software Engineer (Remote)"}}, "_tags": ["story", "author_katiecastillo", "story_26057538", "ask_hn"], "author": "katiecastillo", "created_at": "2021-02-07T19:39:42Z", "created_at_i": 1612726782, "num_comments": 0, "objectID": "26057538", "points": 1, "story_id": 26057538, "story_text": "CrossnoKaye is looking for a Senior Backend Software Engineer to join our fast-paced, growing engineering department. This individual will work with a passionate team of software engineers and scientists to create highly efficient distributed systems with high-uptime. This developer will collaborate with the team to troubleshoot the software, write clean code, and have the opportunity to improve the features to the existing software. Your main responsibility will be to maintain and grow features in the backend code base and develop the workflows in Golang. The ideal candidate will be  flexible and adaptable enough to thrive in a fluid startup environment. Our current technology stack includes AWS, Golang, Vue.JS, Postgres, TimeseriesDB, Grafana&#x2F;Loki, Kubernetes, and Docker.\nJob Responsibilities:\n\u25cf Evolve our distributed computing infrastructure using best-in-class engineering practices\n\u25cf Design and implement highly scalable and fault tolerant web-services and microservices to\nsupport our growing cloud infrastructure\n\u25cf Lead collaboration with other teams to define new features and improve existing software\n\u25cf Monitoring and improving application health and performance\n\u25cf Write unit tests as part of developing high quality and reliable software\n\u25cf Builds consensus and makes decisions in leading edge technologies\nQualifications\n\u25cf Bachelor&#x27;s Degree in Computer Science (or related industry experience)\n\u25cf 5+ years\u2019 experience of developing server-side software\n \u25cf Expertise with Timeseries databases\n\u25cf Experience with RPC, network protocols, and horizontally scalable services\n\u25cf Knowledge of design patterns\n\u25cf Proficient in a structured language (C++, Java, Golang, Rust, etc)\n\u25cf Infrastructure and application observability\n\u25cf Experience with cloud-computing and distributed systems\n\u25cf Experience in instrumenting code for gathering production performance metrics\n\u25cf Strong experience in Service Architecture, API Design, and Database Schema Design", "title": "Senior Back End Software Engineer (Remote)", "updated_at": "2024-09-20T07:53:35Z"}], "hitsPerPage": 15, "nbHits": 10, "nbPages": 1, "page": 0, "params": "query=loki+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 8, "processingTimingsMS": {"_request": {"roundTrip": 15}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 6, "total": 7}, "total": 8}, "query": "loki production", "serverTimeMS": 10}}