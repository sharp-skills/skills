{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "muragekibicho"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "I enjoy making Instagram and Twitter bots. I also enjoy working with the <em>FFMPEG</em> C api - not the commandline. \nThis is my current Instagram bot-making course - https://zuck-cat.phoneworker.online/<p>I hope to make a second course and I'd love to know what people prefer:\n1. Reverse engineering the Twitter and Instagram APIs to make <em>production</em>-ready social media bots - we find and map all the HTTP endpoints for Twitter and Instagram. Capstone project is a bot that leaves the FIRST comment whenever a celebrity posts on IG or Twitter \n2. Learn the <em>FFMPEG</em> C api - we go through <em>FFMPEG</em> data structures, translate command line prompts into C code, navigate the large documentation and build a video decoder and encoder in C"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["ffmpeg"], "value": "Ask HN: Make social media bots or learn <em>FFmpeg</em> C API?"}}, "_tags": ["story", "author_muragekibicho", "story_34832384", "ask_hn"], "author": "muragekibicho", "created_at": "2023-02-17T08:49:15Z", "created_at_i": 1676623755, "num_comments": 0, "objectID": "34832384", "points": 1, "story_id": 34832384, "story_text": "I enjoy making Instagram and Twitter bots. I also enjoy working with the FFMPEG C api - not the commandline. \nThis is my current Instagram bot-making course - https:&#x2F;&#x2F;zuck-cat.phoneworker.online&#x2F;<p>I hope to make a second course and I&#x27;d love to know what people prefer:\n1. Reverse engineering the Twitter and Instagram APIs to make production-ready social media bots - we find and map all the HTTP endpoints for Twitter and Instagram. Capstone project is a bot that leaves the FIRST comment whenever a celebrity posts on IG or Twitter \n2. Learn the FFMPEG C api - we go through FFMPEG data structures, translate command line prompts into C code, navigate the large documentation and build a video decoder and encoder in C", "title": "Ask HN: Make social media bots or learn FFmpeg C API?", "updated_at": "2024-09-20T13:22:42Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "momciloo"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "I built this product back in 2018 as a small side project: a tool that turns short videos into physical flipbooks. After launching it, I didn't touch it for years. Life and work took over, and it sat idle. But it kept getting a few orders every month, which made it impossible to forget. So in December 2024, I decided to rebrand and revive it.<p>The initial version relied on various local printing offices. I kept switching from one to another, but the results were never quite right. Either the quality wasn't good enough, or the turnaround times were too long. Eventually, me and my wife bought all the necessary machines and moved <em>production</em> in-house.<p>Now, it's a family business. My wife and I handle everything: printing, binding, cutting, addressing, and shipping each flipbook. On the technical side, it\u2019s powered by Next.js, with <em>FFmpeg</em> extracting frames and handling overlays, and ImageMagick used for adding trim marks and creating the final PDFs.<p>After many years of working in IT, working on something tangible feels refreshing. It's satisfying to create something that brings people joy. And that is not hard to sell (like dev tools, for example haha). There are still challenges: we're experimenting with different cover papers, improving <em>production</em>, and testing new ideas without making things confusing. But that\u2019s part of what keeps us moving forward."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: I convert videos to printed flipbooks for living"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.videotoflip.com/"}}, "_tags": ["story", "author_momciloo", "story_42918902", "show_hn"], "author": "momciloo", "children": [42919246, 42919575, 42919601, 42919726, 42919816, 42919844, 42919869, 42919885, 42919900, 42920053, 42920063, 42920145, 42920162, 42920574, 42920602, 42920622, 42920876, 42920985, 42921063, 42921157, 42921158, 42921223, 42921522, 42921636, 42922165, 42922448, 42922532, 42922603, 42922874, 42922916, 42923674, 42923695, 42923891, 42923977, 42924098, 42924242, 42924817, 42924957, 42925129, 42925467, 42925728, 42925864, 42925937, 42926447, 42926922, 42927070, 42927405, 42928546, 42929461, 42931597, 42932171, 42933288, 42933646, 42935652, 42938166, 42958668], "created_at": "2025-02-03T15:09:55Z", "created_at_i": 1738595395, "num_comments": 123, "objectID": "42918902", "points": 513, "story_id": 42918902, "story_text": "I built this product back in 2018 as a small side project: a tool that turns short videos into physical flipbooks. After launching it, I didn&#x27;t touch it for years. Life and work took over, and it sat idle. But it kept getting a few orders every month, which made it impossible to forget. So in December 2024, I decided to rebrand and revive it.<p>The initial version relied on various local printing offices. I kept switching from one to another, but the results were never quite right. Either the quality wasn&#x27;t good enough, or the turnaround times were too long. Eventually, me and my wife bought all the necessary machines and moved production in-house.<p>Now, it&#x27;s a family business. My wife and I handle everything: printing, binding, cutting, addressing, and shipping each flipbook. On the technical side, it\u2019s powered by Next.js, with FFmpeg extracting frames and handling overlays, and ImageMagick used for adding trim marks and creating the final PDFs.<p>After many years of working in IT, working on something tangible feels refreshing. It&#x27;s satisfying to create something that brings people joy. And that is not hard to sell (like dev tools, for example haha). There are still challenges: we&#x27;re experimenting with different cover papers, improving production, and testing new ideas without making things confusing. But that\u2019s part of what keeps us moving forward.", "title": "Show HN: I convert videos to printed flipbooks for living", "updated_at": "2026-02-09T18:05:34Z", "url": "https://www.videotoflip.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "9ranty"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "Tella (<a href=\"https://www.tella.tv/\" rel=\"nofollow\">https://www.tella.tv/</a>) is a collaborative online video editor for screen and camera recordings. We're making video creation accessible to people who have no prior editing experience.<p>Sharing screen and camera recordings is a rapidly growing way for people to communicate at work, especially in technology where the subject matter is often on screens (new features, code, designs). But while people are creating more video for work, it's usually for the convenience of the creator and not the viewer. One-take screen recordings can be long, boring, and difficult to watch. We're trying to change this by letting people produce and edit their recordings so that it's a better experience for viewers.<p>Michiel and I used to work at a large remote company and this was where we saw the potential of edited video content in the workplace. One of the biggest challenges was keeping business teams up-to-date with product teams. The most effective solution was product teams sharing videos about their work over Slack, which the rest of the organisation watched in their own time. Product teams made videos about new projects, progress updates, launches, user research, and so on.<p>The most interesting aspect of the approach was that the videos weren't just screen recordings, they were edited and often well-produced videos. The better the <em>production</em>, the better the engagement was. Teams approached the <em>production</em> of these videos in the same way as preparing a slide deck for a presentation.<p>We loved the format and saw its potential, especially in a remote workplace, but it had some problems. Video editing is time consuming, and working on a video with a teammate takes even longer. Video editing also has a high barrier to entry. Purchasing Screenflow or Final Cut (or other long-format editors) and then learning how to use it prohibits people from trying video as a form of sharing information.<p>So we set out to build a video editor that focuses on screen and camera recording (where most of the subject matter comes from at work), allows for collaboration (many people work in teams and expect the tools they use to support this), and makes editing straightforward (putting together a video should be as simple as putting together a slide deck).<p>Our implementation takes a different approach to most editors. We wanted something that was fast, lightweight, and could run in a web browser\u2014appealing to people completely new to video editing. We also wanted to support real-time collaboration. Instead of transcoding all content to a video format, we created our own video player that controls the timing and display of HTML elements. Let's say your video consists of a couple recordings, some text, and some images. Tella plots these different bits of content on a timeline and then plays them back in sequence on a webpage.<p>The benefit of this is that we can use anything that you can do with HTML, CSS and JS to create a video. We're not bound to <em>ffmpeg</em> or other transcoders to generate our video for us. We take the document the user created and display that in the same way to the viewer (no converting step in between). This means we can stay lightweight and let you update the video whenever you like. There are no \u201csnapshots\u201d stored and the link always shows the source of truth.<p>The challenge with this is keeping all the content in sync. Using our earlier example: the first recording should play after the text and then the second recording exactly after the first ends. A more complex scenario would be where two videos need to play back at the same time: a screen recording and a camera recording\u2014these need to start and stop at the same moments. This is called \u201cMedia Synchronization\u201d, or MediaSync for short (<a href=\"https://www.springer.com/gp/book/9783319658391\" rel=\"nofollow\">https://www.springer.com/gp/book/9783319658391</a>). At the moment browsers don\u2019t have a lot of stable APIs that can help us, but they are in the works! One notable example is the Timing Object (<a href=\"https://webtiming.github.io/timingobject/\" rel=\"nofollow\">https://webtiming.github.io/timingobject/</a>) which outlines how you can sync multiple media elements to the same clock. Right now Tella mostly works by manually syncing all video elements on actions like \u201cplay\u201d or \u201cseek\u201d. Eventually we want to implement more of the techniques outlined in MediaSync, like slightly changing the speed of out of sync videos to let them catch up.<p>So far, people have been using Tella to create product demos, team updates, company announcements, sales pitches, investor pitches, and tutorial videos, as well as making video content for blogs and newsletters.<p>We'd love to hear what you think and answer any questions you might have. Thanks!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Tella (YC S20) \u2013 Collaborative video editing in the browser"}}, "_tags": ["story", "author_9ranty", "story_24158509", "launch_hn"], "author": "9ranty", "children": [24158569, 24158665, 24158668, 24158674, 24158696, 24158982, 24159136, 24159164, 24159220, 24159436, 24159479, 24160183, 24160343, 24160997, 24161057, 24161133, 24162299, 24162752, 24163981, 24164912, 24165400, 24165461, 24165906, 24165942, 24172633, 24187316], "created_at": "2020-08-14T14:40:47Z", "created_at_i": 1597416047, "num_comments": 81, "objectID": "24158509", "points": 221, "story_id": 24158509, "story_text": "Tella (<a href=\"https:&#x2F;&#x2F;www.tella.tv&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.tella.tv&#x2F;</a>) is a collaborative online video editor for screen and camera recordings. We&#x27;re making video creation accessible to people who have no prior editing experience.<p>Sharing screen and camera recordings is a rapidly growing way for people to communicate at work, especially in technology where the subject matter is often on screens (new features, code, designs). But while people are creating more video for work, it&#x27;s usually for the convenience of the creator and not the viewer. One-take screen recordings can be long, boring, and difficult to watch. We&#x27;re trying to change this by letting people produce and edit their recordings so that it&#x27;s a better experience for viewers.<p>Michiel and I used to work at a large remote company and this was where we saw the potential of edited video content in the workplace. One of the biggest challenges was keeping business teams up-to-date with product teams. The most effective solution was product teams sharing videos about their work over Slack, which the rest of the organisation watched in their own time. Product teams made videos about new projects, progress updates, launches, user research, and so on.<p>The most interesting aspect of the approach was that the videos weren&#x27;t just screen recordings, they were edited and often well-produced videos. The better the production, the better the engagement was. Teams approached the production of these videos in the same way as preparing a slide deck for a presentation.<p>We loved the format and saw its potential, especially in a remote workplace, but it had some problems. Video editing is time consuming, and working on a video with a teammate takes even longer. Video editing also has a high barrier to entry. Purchasing Screenflow or Final Cut (or other long-format editors) and then learning how to use it prohibits people from trying video as a form of sharing information.<p>So we set out to build a video editor that focuses on screen and camera recording (where most of the subject matter comes from at work), allows for collaboration (many people work in teams and expect the tools they use to support this), and makes editing straightforward (putting together a video should be as simple as putting together a slide deck).<p>Our implementation takes a different approach to most editors. We wanted something that was fast, lightweight, and could run in a web browser\u2014appealing to people completely new to video editing. We also wanted to support real-time collaboration. Instead of transcoding all content to a video format, we created our own video player that controls the timing and display of HTML elements. Let&#x27;s say your video consists of a couple recordings, some text, and some images. Tella plots these different bits of content on a timeline and then plays them back in sequence on a webpage.<p>The benefit of this is that we can use anything that you can do with HTML, CSS and JS to create a video. We&#x27;re not bound to ffmpeg or other transcoders to generate our video for us. We take the document the user created and display that in the same way to the viewer (no converting step in between). This means we can stay lightweight and let you update the video whenever you like. There are no \u201csnapshots\u201d stored and the link always shows the source of truth.<p>The challenge with this is keeping all the content in sync. Using our earlier example: the first recording should play after the text and then the second recording exactly after the first ends. A more complex scenario would be where two videos need to play back at the same time: a screen recording and a camera recording\u2014these need to start and stop at the same moments. This is called \u201cMedia Synchronization\u201d, or MediaSync for short (<a href=\"https:&#x2F;&#x2F;www.springer.com&#x2F;gp&#x2F;book&#x2F;9783319658391\" rel=\"nofollow\">https:&#x2F;&#x2F;www.springer.com&#x2F;gp&#x2F;book&#x2F;9783319658391</a>). At the moment browsers don\u2019t have a lot of stable APIs that can help us, but they are in the works! One notable example is the Timing Object (<a href=\"https:&#x2F;&#x2F;webtiming.github.io&#x2F;timingobject&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;webtiming.github.io&#x2F;timingobject&#x2F;</a>) which outlines how you can sync multiple media elements to the same clock. Right now Tella mostly works by manually syncing all video elements on actions like \u201cplay\u201d or \u201cseek\u201d. Eventually we want to implement more of the techniques outlined in MediaSync, like slightly changing the speed of out of sync videos to let them catch up.<p>So far, people have been using Tella to create product demos, team updates, company announcements, sales pitches, investor pitches, and tutorial videos, as well as making video content for blogs and newsletters.<p>We&#x27;d love to hear what you think and answer any questions you might have. Thanks!", "title": "Launch HN: Tella (YC S20) \u2013 Collaborative video editing in the browser", "updated_at": "2024-09-20T06:47:52Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fullstackchris"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "Hi everyone! I originally created CodeVideo as a little side project using <em>FFMPEG</em> WASM in the browser as an experiment, but it's since grown into my vision for a completely automated software educational course <em>production</em> system.<p>The idea is that you create the educational content once, then can export the course to multiple formats - as a video (of course!), but also as an interactive webpage, a blog post, or even a book, PDF, or PowerPoint! Basically a &quot;create once, ship everywhere&quot; concept.<p>Things will get more interesting as I incorporate stuff like spell check (for speech) and abstract syntax tree checking (for code), so you can quite literally check the validity of your software course in realtime as you build the course.<p>You can read more about the technical details and history on my Substack launch post:<p><a href=\"https://codevideo.substack.com/p/launching-codevideo-after-two-years\" rel=\"nofollow\">https://codevideo.substack.com/p/launching-codevideo-after-t...</a><p>And here's the intro video about how to use the studio:<p><a href=\"https://youtu.be/4nyuhWF6SS0\" rel=\"nofollow\">https://youtu.be/4nyuhWF6SS0</a><p>EDIT: added link to the mp4 created in the demo video:<p><a href=\"https://coffee-app.sfo2.cdn.digitaloceanspaces.com/codevideo/v3/a5edf4e4-c512-4b62-b7f9-11dbe689440e.mp4\" rel=\"nofollow\">https://coffee-app.sfo2.cdn.digitaloceanspaces.com/codevideo...</a><p>From an intellectual and software standpoint this product has been (and still is) an absolute blast to build - and as always, I've learned a TON along the way. Very excited to get feedback from the Hacker community - even (maybe especially?) the classic skeptical feedback ;)<p>As an engineer, I always suck at monetization and things like that - I already am wondering if the whole token system is too complex and perhaps a different model would be better. Again, waiting for feedback from everyone. Until then, enjoy the studio!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: CodeVideo \u2013 Two years in the making to build an event-sourced IDE"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://studio.codevideo.io"}}, "_tags": ["story", "author_fullstackchris", "story_43363276", "show_hn"], "author": "fullstackchris", "children": [43363456, 43363697, 43363784, 43363914, 43364050, 43364093, 43365747, 43367715, 43370635, 43390388], "created_at": "2025-03-14T15:01:01Z", "created_at_i": 1741964461, "num_comments": 20, "objectID": "43363276", "points": 69, "story_id": 43363276, "story_text": "Hi everyone! I originally created CodeVideo as a little side project using FFMPEG WASM in the browser as an experiment, but it&#x27;s since grown into my vision for a completely automated software educational course production system.<p>The idea is that you create the educational content once, then can export the course to multiple formats - as a video (of course!), but also as an interactive webpage, a blog post, or even a book, PDF, or PowerPoint! Basically a &quot;create once, ship everywhere&quot; concept.<p>Things will get more interesting as I incorporate stuff like spell check (for speech) and abstract syntax tree checking (for code), so you can quite literally check the validity of your software course in realtime as you build the course.<p>You can read more about the technical details and history on my Substack launch post:<p><a href=\"https:&#x2F;&#x2F;codevideo.substack.com&#x2F;p&#x2F;launching-codevideo-after-two-years\" rel=\"nofollow\">https:&#x2F;&#x2F;codevideo.substack.com&#x2F;p&#x2F;launching-codevideo-after-t...</a><p>And here&#x27;s the intro video about how to use the studio:<p><a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;4nyuhWF6SS0\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;4nyuhWF6SS0</a><p>EDIT: added link to the mp4 created in the demo video:<p><a href=\"https:&#x2F;&#x2F;coffee-app.sfo2.cdn.digitaloceanspaces.com&#x2F;codevideo&#x2F;v3&#x2F;a5edf4e4-c512-4b62-b7f9-11dbe689440e.mp4\" rel=\"nofollow\">https:&#x2F;&#x2F;coffee-app.sfo2.cdn.digitaloceanspaces.com&#x2F;codevideo...</a><p>From an intellectual and software standpoint this product has been (and still is) an absolute blast to build - and as always, I&#x27;ve learned a TON along the way. Very excited to get feedback from the Hacker community - even (maybe especially?) the classic skeptical feedback ;)<p>As an engineer, I always suck at monetization and things like that - I already am wondering if the whole token system is too complex and perhaps a different model would be better. Again, waiting for feedback from everyone. Until then, enjoy the studio!", "title": "Show HN: CodeVideo \u2013 Two years in the making to build an event-sourced IDE", "updated_at": "2025-10-06T10:18:30Z", "url": "https://studio.codevideo.io"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "hactually"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "Adobe recently announced the end-of-life for Adobe Animate, then walked it back after community backlash.<p>Regardless of what Adobe decides next, the message was clear: animators who depend on proprietary tools are one corporate decision away from losing their workflow.<p>2D animation deserves an open-source option that isn't a toy. We've been working with a professional animator to guide feature priorities and ensure we're building something that actually fits real <em>production</em> workflows - not just a tech demo.<p>Github Repo: <a href=\"https://github.com/17twenty/inamate\" rel=\"nofollow\">https://github.com/17twenty/inamate</a><p>We're at the stage where community feedback shapes the direction. If you're an animator, motion designer, or just someone who's been frustrated by the state of 2D animation tools \u2014 we'd love to hear:<p>- What features would make you switch from your current tool?<p>- What's the biggest pain point in your animation workflow?<p>- Is real-time collaboration actually useful for animation, or is it a gimmick?<p>Try it out, break it, and tell us what you think.<p>Built with Go, TS &amp; React, WebAssembly, PostgreSQL, WebSocket, <em>ffmpeg</em> (for video exports)."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Inamate \u2013 Open-source 2D animation tool (alternative to Adobe Animate)"}}, "_tags": ["story", "author_hactually", "story_46953567", "show_hn"], "author": "hactually", "children": [46953612, 46991084, 46991294, 46991317, 46991523, 46992688, 46992916, 47000988, 47007273], "created_at": "2026-02-10T00:15:54Z", "created_at_i": 1770682554, "num_comments": 13, "objectID": "46953567", "points": 15, "story_id": 46953567, "story_text": "Adobe recently announced the end-of-life for Adobe Animate, then walked it back after community backlash.<p>Regardless of what Adobe decides next, the message was clear: animators who depend on proprietary tools are one corporate decision away from losing their workflow.<p>2D animation deserves an open-source option that isn&#x27;t a toy. We&#x27;ve been working with a professional animator to guide feature priorities and ensure we&#x27;re building something that actually fits real production workflows - not just a tech demo.<p>Github Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;17twenty&#x2F;inamate\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;17twenty&#x2F;inamate</a><p>We&#x27;re at the stage where community feedback shapes the direction. If you&#x27;re an animator, motion designer, or just someone who&#x27;s been frustrated by the state of 2D animation tools \u2014 we&#x27;d love to hear:<p>- What features would make you switch from your current tool?<p>- What&#x27;s the biggest pain point in your animation workflow?<p>- Is real-time collaboration actually useful for animation, or is it a gimmick?<p>Try it out, break it, and tell us what you think.<p>Built with Go, TS &amp; React, WebAssembly, PostgreSQL, WebSocket, ffmpeg (for video exports).", "title": "Show HN: Inamate \u2013 Open-source 2D animation tool (alternative to Adobe Animate)", "updated_at": "2026-02-14T18:18:23Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "roberja90"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "Hey HN!!\nMany of us enjoyed the &quot;If YouTube had actual channels&quot; Show HN post last week. As developers of a similar project, we wanted to share our approach to building a simulated live experience. We\u2019ve spent around 18 months working on this and have managed to add some advanced features, including content-based interactivity. We are super excited to be releasing both the live stream and the code behind it.<p>Our idea was to combine the communal watching experience of television with the modern trappings of the interactive web. If you go to the stream, you will be watching along with everyone else. You can click around, get more information on what\u2019s in the shot, and even purchase what you see without leaving the page.<p>On the tech side, we learned a ton along the way. Today, we are sharing the code so others can hopefully learn something too. To be totally honest, the code embarrasses me. It's way too early to share this and far from a completed open-source project. It\u2019s an extreme example of \u201cdoing things that don\u2019t scale\u201d so please do not expect to just fire this up and run it in <em>production</em>. However, it does demonstrate solutions to a number of hard problems we faced\u2014for example, handling graceful updates to streams that are currently live and supporting reliable timed metadata.<p>We would love any feedback and will be hanging around here to answer any questions that may come up.<p>Github: <a href=\"https://github.com/james-a-rob/KodaStream\">https://github.com/james-a-rob/KodaStream</a><p>Live stream: <a href=\"https://www.sneakinpeace.com\" rel=\"nofollow\">https://www.sneakinpeace.com</a><p>Main tech: TypeScript | <em>FFmpeg</em> | HLS"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: We spent the last 18 months building an interactive live video stream"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/james-a-rob/KodaStream"}}, "_tags": ["story", "author_roberja90", "story_41292624", "show_hn"], "author": "roberja90", "children": [41299803, 41300010, 41300693], "created_at": "2024-08-19T16:51:28Z", "created_at_i": 1724086288, "num_comments": 2, "objectID": "41292624", "points": 7, "story_id": 41292624, "story_text": "Hey HN!!\nMany of us enjoyed the &quot;If YouTube had actual channels&quot; Show HN post last week. As developers of a similar project, we wanted to share our approach to building a simulated live experience. We\u2019ve spent around 18 months working on this and have managed to add some advanced features, including content-based interactivity. We are super excited to be releasing both the live stream and the code behind it.<p>Our idea was to combine the communal watching experience of television with the modern trappings of the interactive web. If you go to the stream, you will be watching along with everyone else. You can click around, get more information on what\u2019s in the shot, and even purchase what you see without leaving the page.<p>On the tech side, we learned a ton along the way. Today, we are sharing the code so others can hopefully learn something too. To be totally honest, the code embarrasses me. It&#x27;s way too early to share this and far from a completed open-source project. It\u2019s an extreme example of \u201cdoing things that don\u2019t scale\u201d so please do not expect to just fire this up and run it in production. However, it does demonstrate solutions to a number of hard problems we faced\u2014for example, handling graceful updates to streams that are currently live and supporting reliable timed metadata.<p>We would love any feedback and will be hanging around here to answer any questions that may come up.<p>Github: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;james-a-rob&#x2F;KodaStream\">https:&#x2F;&#x2F;github.com&#x2F;james-a-rob&#x2F;KodaStream</a><p>Live stream: <a href=\"https:&#x2F;&#x2F;www.sneakinpeace.com\" rel=\"nofollow\">https:&#x2F;&#x2F;www.sneakinpeace.com</a><p>Main tech: TypeScript | FFmpeg | HLS", "title": "Show HN: We spent the last 18 months building an interactive live video stream", "updated_at": "2024-09-20T17:43:02Z", "url": "https://github.com/james-a-rob/KodaStream"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "twwch"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "URL: <a href=\"https://github.com/twwch/next-chat-skills\" rel=\"nofollow\">https://github.com/twwch/next-chat-skills</a><p>---<p>Text (paste into the &quot;text&quot; field):<p>Hi HN,<p>I built an open-source AI assistant that can autonomously discover, install, and execute Skills to actually complete tasks for you.<p>The Problem:<p>Most AI chatbots today are stuck in &quot;read-only&quot; mode. They can tell you how to do something, but they can't do it. Want to convert a PPTX to PDF? The AI will explain how, but you still have to run the commands yourself.<p>The Solution:<p>Next-Chat-Skills is a self-hosted AI assistant with a plugin system called Skills. When you ask the AI to do something it can't handle natively, it:<p>1. Searches for a relevant Skill (like an app store for AI capabilities)\n2. Installs it automatically (npx skills add ...)\n3. Executes the Skill's scripts (Python, Node.js, Shell)\n4. Streams real-time output back to you in a terminal UI\n5. Recovers from errors by installing missing dependencies and retrying<p>For example:<p><pre><code>  User: &quot;Summarize this YouTube video for me&quot;\n  AI:   -&gt; Searches for a video-summarizer Skill\n        -&gt; Installs it (yt-dlp + Whisper)\n        -&gt; Downloads the video, transcribes audio\n        -&gt; Returns a structured summary\n</code></pre>\nNo manual setup. No copy-pasting commands. The AI handles the entire workflow.<p>What is a Skill?<p>A Skill is just a folder with a SKILL.md descriptor and some scripts:<p><pre><code>  ~/.agents/skills/video-summarizer/\n  \u251c\u2500\u2500 SKILL.md              # Metadata + description\n  \u251c\u2500\u2500 scripts/\n  \u2502   \u251c\u2500\u2500 download.py       # Download video\n  \u2502   \u251c\u2500\u2500 transcribe.py     # Whisper transcription\n  \u2502   \u2514\u2500\u2500 summarize.js      # Generate summary\n  \u2514\u2500\u2500 rules/                # Usage guidelines for the AI\n</code></pre>\nAnyone can create and share Skills. The AI reads the SKILL.md to understand when and how to invoke each script. It's composable \u2014 the more Skills you add, the more capable your assistant becomes.<p>Key Features:<p>- Autonomous Skill discovery &amp; installation \u2014 AI finds and installs what it needs\n- Real-time script execution \u2014 streams terminal output via SSE, supports Python/Node.js/Shell\n- File generation &amp; download \u2014 scripts can generate files (PPTX, PDF, images) that users can download directly from chat\n- Multi-file upload &amp; parsing \u2014 supports images, PDF, DOCX, XLSX, PPTX\n- Dual database \u2014 SQLite (zero-config) or PostgreSQL (<em>production</em>)\n- Optional auth \u2014 Google OAuth or fingerprint-based, works without login too\n- Docker-ready \u2014 pre-built image with Python, <em>FFmpeg</em>, LibreOffice, and popular Skills pre-installed\n- Works with any OpenAI-compatible API \u2014 GPT-4o, Claude (via proxy), local models, etc.<p>Tech Stack: Next.js 16, React 19, TypeScript, Vercel AI SDK, Tailwind CSS 4, shadcn/ui, Drizzle ORM (SQLite / PostgreSQL), Docker (Node.js 20 + Python 3)<p>Quick Start:<p><pre><code>  # Docker (fastest)\n  docker run -p 3000:3000 \\\n    -e OPENAI_API_KEY=sk-xxx \\\n    -e OPENAI_BASE_URL=https://api.openai.com/v1 \\\n    twwch/next-chat-skills:latest\n\n  # Or from source\n  git clone https://github.com/twwch/next-chat-skills\n  cd next-chat-skills\n  npm install &amp;&amp; npm run dev\n</code></pre>\nWhy I Built This:<p>I got tired of AI assistants that stop at &quot;here's a code snippet.&quot; I wanted an AI that could actually run the code, handle failures, install dependencies, and deliver the final result \u2014 like having a junior developer who can use any Skill you point them at.<p>The Skills system makes this extensible without modifying the core app. Anyone can package a workflow as a Skill and share it.<p>What's Next:<p>- Skill marketplace / registry for community sharing\n- Multi-step workflow chaining (Skill A output -&gt; Skill B input)\n- Better sandboxing for script execution\n- MCP (Model Context Protocol) integration<p>I'd love to hear your feedback. What Skills would you want to see? What's missing?<p>GitHub: <a href=\"https://github.com/twwch/next-chat-skills\" rel=\"nofollow\">https://github.com/twwch/next-chat-skills</a>\nLicense: Apache 2.0"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Give Your AI the Ability to Find, Install, and Use Skill Autonomously"}}, "_tags": ["story", "author_twwch", "story_46942091", "show_hn"], "author": "twwch", "created_at": "2026-02-09T06:13:22Z", "created_at_i": 1770617602, "num_comments": 0, "objectID": "46942091", "points": 2, "story_id": 46942091, "story_text": "URL: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;twwch&#x2F;next-chat-skills\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;twwch&#x2F;next-chat-skills</a><p>---<p>Text (paste into the &quot;text&quot; field):<p>Hi HN,<p>I built an open-source AI assistant that can autonomously discover, install, and execute Skills to actually complete tasks for you.<p>The Problem:<p>Most AI chatbots today are stuck in &quot;read-only&quot; mode. They can tell you how to do something, but they can&#x27;t do it. Want to convert a PPTX to PDF? The AI will explain how, but you still have to run the commands yourself.<p>The Solution:<p>Next-Chat-Skills is a self-hosted AI assistant with a plugin system called Skills. When you ask the AI to do something it can&#x27;t handle natively, it:<p>1. Searches for a relevant Skill (like an app store for AI capabilities)\n2. Installs it automatically (npx skills add ...)\n3. Executes the Skill&#x27;s scripts (Python, Node.js, Shell)\n4. Streams real-time output back to you in a terminal UI\n5. Recovers from errors by installing missing dependencies and retrying<p>For example:<p><pre><code>  User: &quot;Summarize this YouTube video for me&quot;\n  AI:   -&gt; Searches for a video-summarizer Skill\n        -&gt; Installs it (yt-dlp + Whisper)\n        -&gt; Downloads the video, transcribes audio\n        -&gt; Returns a structured summary\n</code></pre>\nNo manual setup. No copy-pasting commands. The AI handles the entire workflow.<p>What is a Skill?<p>A Skill is just a folder with a SKILL.md descriptor and some scripts:<p><pre><code>  ~&#x2F;.agents&#x2F;skills&#x2F;video-summarizer&#x2F;\n  \u251c\u2500\u2500 SKILL.md              # Metadata + description\n  \u251c\u2500\u2500 scripts&#x2F;\n  \u2502   \u251c\u2500\u2500 download.py       # Download video\n  \u2502   \u251c\u2500\u2500 transcribe.py     # Whisper transcription\n  \u2502   \u2514\u2500\u2500 summarize.js      # Generate summary\n  \u2514\u2500\u2500 rules&#x2F;                # Usage guidelines for the AI\n</code></pre>\nAnyone can create and share Skills. The AI reads the SKILL.md to understand when and how to invoke each script. It&#x27;s composable \u2014 the more Skills you add, the more capable your assistant becomes.<p>Key Features:<p>- Autonomous Skill discovery &amp; installation \u2014 AI finds and installs what it needs\n- Real-time script execution \u2014 streams terminal output via SSE, supports Python&#x2F;Node.js&#x2F;Shell\n- File generation &amp; download \u2014 scripts can generate files (PPTX, PDF, images) that users can download directly from chat\n- Multi-file upload &amp; parsing \u2014 supports images, PDF, DOCX, XLSX, PPTX\n- Dual database \u2014 SQLite (zero-config) or PostgreSQL (production)\n- Optional auth \u2014 Google OAuth or fingerprint-based, works without login too\n- Docker-ready \u2014 pre-built image with Python, FFmpeg, LibreOffice, and popular Skills pre-installed\n- Works with any OpenAI-compatible API \u2014 GPT-4o, Claude (via proxy), local models, etc.<p>Tech Stack: Next.js 16, React 19, TypeScript, Vercel AI SDK, Tailwind CSS 4, shadcn&#x2F;ui, Drizzle ORM (SQLite &#x2F; PostgreSQL), Docker (Node.js 20 + Python 3)<p>Quick Start:<p><pre><code>  # Docker (fastest)\n  docker run -p 3000:3000 \\\n    -e OPENAI_API_KEY=sk-xxx \\\n    -e OPENAI_BASE_URL=https:&#x2F;&#x2F;api.openai.com&#x2F;v1 \\\n    twwch&#x2F;next-chat-skills:latest\n\n  # Or from source\n  git clone https:&#x2F;&#x2F;github.com&#x2F;twwch&#x2F;next-chat-skills\n  cd next-chat-skills\n  npm install &amp;&amp; npm run dev\n</code></pre>\nWhy I Built This:<p>I got tired of AI assistants that stop at &quot;here&#x27;s a code snippet.&quot; I wanted an AI that could actually run the code, handle failures, install dependencies, and deliver the final result \u2014 like having a junior developer who can use any Skill you point them at.<p>The Skills system makes this extensible without modifying the core app. Anyone can package a workflow as a Skill and share it.<p>What&#x27;s Next:<p>- Skill marketplace &#x2F; registry for community sharing\n- Multi-step workflow chaining (Skill A output -&gt; Skill B input)\n- Better sandboxing for script execution\n- MCP (Model Context Protocol) integration<p>I&#x27;d love to hear your feedback. What Skills would you want to see? What&#x27;s missing?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;twwch&#x2F;next-chat-skills\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;twwch&#x2F;next-chat-skills</a>\nLicense: Apache 2.0", "title": "Show HN: Give Your AI the Ability to Find, Install, and Use Skill Autonomously", "updated_at": "2026-02-09T06:36:16Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "whario"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "&quot;Hi HN, I'm Mario, a full-stack developer. I built this 100% free, no-install web tool (using Python/FastAPI and <em>FFmpeg</em>) to simplify professional audio tasks for creators and small teams. I was frustrated by expensive, clunky desktop apps, so I focused on advanced features like a seamless multitrack mixer with crossfades, universal video-to-audio extraction (from URLs like YouTube), and robust conversion for challenging formats like WhatsApp OPUS/AMR. It's designed for efficiency in fast-paced <em>production</em> environments. Happy to answer any questions!&quot;"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Free Online Audio Utility with Multitrack Mixer and Opus/AMR Support"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://convertiraudioamp3.com/en"}}, "_tags": ["story", "author_whario", "story_45691653", "show_hn"], "author": "whario", "created_at": "2025-10-24T06:54:17Z", "created_at_i": 1761288857, "num_comments": 0, "objectID": "45691653", "points": 2, "story_id": 45691653, "story_text": "&quot;Hi HN, I&#x27;m Mario, a full-stack developer. I built this 100% free, no-install web tool (using Python&#x2F;FastAPI and FFmpeg) to simplify professional audio tasks for creators and small teams. I was frustrated by expensive, clunky desktop apps, so I focused on advanced features like a seamless multitrack mixer with crossfades, universal video-to-audio extraction (from URLs like YouTube), and robust conversion for challenging formats like WhatsApp OPUS&#x2F;AMR. It&#x27;s designed for efficiency in fast-paced production environments. Happy to answer any questions!&quot;", "title": "Show HN: Free Online Audio Utility with Multitrack Mixer and Opus/AMR Support", "updated_at": "2025-10-24T07:22:33Z", "url": "https://convertiraudioamp3.com/en"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "spupuz"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "Hi HN!<p>I've built VibeNVR, a self-hosted Network Video Recorder system focused on privacy, ease of deployment, and modern architecture.<p>*Why another NVR?*\nMost solutions are either enterprise-grade (complex, expensive) or hobbyist projects (lacking polish/security). I wanted something <em>production</em>-ready that deploys in minutes.<p>*Stack:*\n- Backend: Python (FastAPI + OpenCV + <em>FFmpeg</em>)\n- Frontend: React + Vite\n- Database: PostgreSQL\n- Deployment: Docker Compose<p>*Features:*\n- Motion detection with smart recording\n- Hardware acceleration (NVIDIA/Intel/AMD)\n- JWT-secured API &amp; media\n- Reverse proxy ready (Nginx/Traefik)\n- Event timeline with filters\n- Mobile-responsive UI<p>*Security approach:*\nAll internal services bind to localhost only. Media files require JWT auth. Designed to sit behind a reverse proxy for public access.<p>*Current state:*\nv1.17.1, MIT licensed, ~70 GitHub stars. Beta status but stable for <em>production</em> use.<p>Built it initially for my home lab, now using it daily with 6 cameras. Works great on Proxmox.<p>GitHub: <a href=\"https://github.com/spupuz/VibeNVR\" rel=\"nofollow\">https://github.com/spupuz/VibeNVR</a><p>Happy to answer questions about architecture, deployment, or future plans!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: VibeNVR \u2013 Modern, self-hosted NVR"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/spupuz/VibeNVR"}}, "_tags": ["story", "author_spupuz", "story_46991205", "show_hn"], "author": "spupuz", "created_at": "2026-02-12T16:53:59Z", "created_at_i": 1770915239, "num_comments": 0, "objectID": "46991205", "points": 1, "story_id": 46991205, "story_text": "Hi HN!<p>I&#x27;ve built VibeNVR, a self-hosted Network Video Recorder system focused on privacy, ease of deployment, and modern architecture.<p>*Why another NVR?*\nMost solutions are either enterprise-grade (complex, expensive) or hobbyist projects (lacking polish&#x2F;security). I wanted something production-ready that deploys in minutes.<p>*Stack:*\n- Backend: Python (FastAPI + OpenCV + FFmpeg)\n- Frontend: React + Vite\n- Database: PostgreSQL\n- Deployment: Docker Compose<p>*Features:*\n- Motion detection with smart recording\n- Hardware acceleration (NVIDIA&#x2F;Intel&#x2F;AMD)\n- JWT-secured API &amp; media\n- Reverse proxy ready (Nginx&#x2F;Traefik)\n- Event timeline with filters\n- Mobile-responsive UI<p>*Security approach:*\nAll internal services bind to localhost only. Media files require JWT auth. Designed to sit behind a reverse proxy for public access.<p>*Current state:*\nv1.17.1, MIT licensed, ~70 GitHub stars. Beta status but stable for production use.<p>Built it initially for my home lab, now using it daily with 6 cameras. Works great on Proxmox.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;spupuz&#x2F;VibeNVR\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;spupuz&#x2F;VibeNVR</a><p>Happy to answer questions about architecture, deployment, or future plans!", "title": "Show HN: VibeNVR \u2013 Modern, self-hosted NVR", "updated_at": "2026-02-12T16:59:22Z", "url": "https://github.com/spupuz/VibeNVR"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Tanziro"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["ffmpeg", "production"], "value": "Hi HN,<p>I built EchoEntry (<a href=\"https://echoentry.ai\" rel=\"nofollow\">https://echoentry.ai</a>) \u2013 a speech-to-text API optimized specifically for digits.<p>The problem: Generic STT APIs struggle with numbers. &quot;One oh five&quot; becomes &quot;105&quot; sometimes, &quot;15&quot; other times. For healthcare apps, warehouse systems, or IVR, this inconsistency breaks workflows.<p>My solution: Fine-tuned Whisper-small on 1-999 spoken numbers across 5 English accents. Gets 95% accuracy on 1-3 digit numbers.<p>Tech stack:\n- Custom Whisper model (1.7GB)\n- FastAPI backend\n- Deployed on 8GB Linode\n- <em>FFmpeg</em> for audio processing<p>Try it now (two commands, no signup):<p># Download test audio\ncurl -O <a href=\"https://echoentry.ai/test_audio.wav\" rel=\"nofollow\">https://echoentry.ai/test_audio.wav</a><p># Test the API\ncurl -X POST <a href=\"https://api.echoentry.ai/v1/transcribe\" rel=\"nofollow\">https://api.echoentry.ai/v1/transcribe</a> \\\n  -H &quot;X-Api-Key: demo_key_12345&quot; \\\n  -F &quot;file=@test_audio.wav;type=audio/wav&quot;<p>Currently free beta (1,000 calls/month per key). Looking for feedback on:\n1. What accuracy threshold makes this <em>production</em>-ready for you?\n2. Are there other number-heavy use cases I'm missing?\n3. Would you pay for this vs. using generic STT?<p>Docs: <a href=\"https://echoentry.ai/docs.html\" rel=\"nofollow\">https://echoentry.ai/docs.html</a><p>Happy to answer technical questions about the fine-tuning process or deployment!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Speech-to-Digits API \u2013 95% accuracy on spoken numbers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://echoentry.ai/"}}, "_tags": ["story", "author_Tanziro", "story_46778795", "show_hn"], "author": "Tanziro", "created_at": "2026-01-27T11:54:59Z", "created_at_i": 1769514899, "num_comments": 0, "objectID": "46778795", "points": 1, "story_id": 46778795, "story_text": "Hi HN,<p>I built EchoEntry (<a href=\"https:&#x2F;&#x2F;echoentry.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;echoentry.ai</a>) \u2013 a speech-to-text API optimized specifically for digits.<p>The problem: Generic STT APIs struggle with numbers. &quot;One oh five&quot; becomes &quot;105&quot; sometimes, &quot;15&quot; other times. For healthcare apps, warehouse systems, or IVR, this inconsistency breaks workflows.<p>My solution: Fine-tuned Whisper-small on 1-999 spoken numbers across 5 English accents. Gets 95% accuracy on 1-3 digit numbers.<p>Tech stack:\n- Custom Whisper model (1.7GB)\n- FastAPI backend\n- Deployed on 8GB Linode\n- FFmpeg for audio processing<p>Try it now (two commands, no signup):<p># Download test audio\ncurl -O <a href=\"https:&#x2F;&#x2F;echoentry.ai&#x2F;test_audio.wav\" rel=\"nofollow\">https:&#x2F;&#x2F;echoentry.ai&#x2F;test_audio.wav</a><p># Test the API\ncurl -X POST <a href=\"https:&#x2F;&#x2F;api.echoentry.ai&#x2F;v1&#x2F;transcribe\" rel=\"nofollow\">https:&#x2F;&#x2F;api.echoentry.ai&#x2F;v1&#x2F;transcribe</a> \\\n  -H &quot;X-Api-Key: demo_key_12345&quot; \\\n  -F &quot;file=@test_audio.wav;type=audio&#x2F;wav&quot;<p>Currently free beta (1,000 calls&#x2F;month per key). Looking for feedback on:\n1. What accuracy threshold makes this production-ready for you?\n2. Are there other number-heavy use cases I&#x27;m missing?\n3. Would you pay for this vs. using generic STT?<p>Docs: <a href=\"https:&#x2F;&#x2F;echoentry.ai&#x2F;docs.html\" rel=\"nofollow\">https:&#x2F;&#x2F;echoentry.ai&#x2F;docs.html</a><p>Happy to answer technical questions about the fine-tuning process or deployment!", "title": "Show HN: Speech-to-Digits API \u2013 95% accuracy on spoken numbers", "updated_at": "2026-01-27T11:56:27Z", "url": "https://echoentry.ai/"}], "hitsPerPage": 15, "nbHits": 10, "nbPages": 1, "page": 0, "params": "query=ffmpeg+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 8, "processingTimingsMS": {"_request": {"roundTrip": 26}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 4, "scanning": 2, "total": 7}, "total": 8}, "query": "ffmpeg production", "serverTimeMS": 10}}