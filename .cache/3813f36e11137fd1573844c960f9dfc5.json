{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "hddherman"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "How to take down <em>production</em> with a single <em>Helm</em> command"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["helm"], "value": "https://ounapuu.ee/posts/2024/04/04/<em>helm</em>-rollbljat/"}}, "_tags": ["story", "author_hddherman", "story_39926575"], "author": "hddherman", "children": [39927529, 39927631, 39927683, 39927749, 39927901, 39928300], "created_at": "2024-04-04T04:36:42Z", "created_at_i": 1712205402, "num_comments": 16, "objectID": "39926575", "points": 44, "story_id": 39926575, "title": "How to take down production with a single Helm command", "updated_at": "2024-09-20T16:47:18Z", "url": "https://ounapuu.ee/posts/2024/04/04/helm-rollbljat/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "luckydev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hey there. Anand, here from LocalOps.<p>We have built a deployment automation tool to provision <em>production</em> environments on AWS and other cloud providers, with just a few clicks. Especially to make Private-SaaS / Single tenant deployments easy.<p>We opened up signups for everyone, after being in closed beta for a while.<p>Checkout <a href=\"https://localops.co\" rel=\"nofollow\">https://localops.co</a><p>Why did we build LocalOps?<p>We found businesses in regulated industries such as Financial services and Healthcare have a lot of restrictions while signing up for B2B SaaS. They can\u2019t share their data as freely as others, need data hosting in their own region, need more privacy &amp; performance guarantees or at the very least need developers to maintain HIPAA, SOC2 and other compliances year on year.<p>We wanted to make it easier for small developer teams to easily deliver to these regulated businesses. So we built a cloud-native deployment tool to connect to any cloud provider and deploy applications with just a few clicks. Developers can deploy applications directly on customer\u2019s cloud environment, skip hosting these accounts themselves and side-step a lot of data security compliance obligations.<p>We have built LocalOps to automate such Private-SaaS deployments end to end on any public cloud.<p>Docker packages?<p>Alternatively, Devs can package their applications as Docker image and hand it off to customers IT team to self host, install and maintain. But I\u2019ve heard from CTOs that Day-2 operations are super hard in this way. Monitoring these deployments remotely, keeping them up-to-date and offering same experience as their public SaaS etc., lacks tools and automation.<p>Big developer teams build such tools in-house. We wanted to bring this to smaller developer teams as well.<p>How?<p>Each customer deployment is considered as an App environment. They work the way same no matter which cloud they run on.<p>One can signup for LocalOps, provide credentials to a target cloud account &amp; private <em>Helm</em> repo, and deploy a <em>production</em> grade app environment on that cloud in about 18-20mins and get a SSL enabled URL for the specific app instance. Each app environment is backed by a Kubernetes cluster to run Apps as <em>Helm</em> installations. Plus, we bundle each App environment with complementary tools to enable deployments, logs and metrics monitoring, encrypt data volumes and more to free up devs from making such choices.<p>Demo video: <a href=\"https://youtu.be/nWAu6NCvEU4?feature=shared\" rel=\"nofollow\">https://youtu.be/nWAu6NCvEU4?feature=shared</a><p>Anyone can signup now at <a href=\"https://console.localops.co/signup\" rel=\"nofollow\">https://console.localops.co/signup</a> and try out.<p>I would love to hear your feedback. Have you tried selling to these industries? Please add in your stories on making such private deployments, if you have one to share. It will be super helpful.<p>Cheers."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LocalOps \u2013 Deployment automation tool to make Private SaaS easy"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://localops.co"}}, "_tags": ["story", "author_luckydev", "story_41259062", "show_hn"], "author": "luckydev", "created_at": "2024-08-15T18:35:40Z", "created_at_i": 1723746940, "num_comments": 0, "objectID": "41259062", "points": 13, "story_id": 41259062, "story_text": "Hey there. Anand, here from LocalOps.<p>We have built a deployment automation tool to provision production environments on AWS and other cloud providers, with just a few clicks. Especially to make Private-SaaS &#x2F; Single tenant deployments easy.<p>We opened up signups for everyone, after being in closed beta for a while.<p>Checkout <a href=\"https:&#x2F;&#x2F;localops.co\" rel=\"nofollow\">https:&#x2F;&#x2F;localops.co</a><p>Why did we build LocalOps?<p>We found businesses in regulated industries such as Financial services and Healthcare have a lot of restrictions while signing up for B2B SaaS. They can\u2019t share their data as freely as others, need data hosting in their own region, need more privacy &amp; performance guarantees or at the very least need developers to maintain HIPAA, SOC2 and other compliances year on year.<p>We wanted to make it easier for small developer teams to easily deliver to these regulated businesses. So we built a cloud-native deployment tool to connect to any cloud provider and deploy applications with just a few clicks. Developers can deploy applications directly on customer\u2019s cloud environment, skip hosting these accounts themselves and side-step a lot of data security compliance obligations.<p>We have built LocalOps to automate such Private-SaaS deployments end to end on any public cloud.<p>Docker packages?<p>Alternatively, Devs can package their applications as Docker image and hand it off to customers IT team to self host, install and maintain. But I\u2019ve heard from CTOs that Day-2 operations are super hard in this way. Monitoring these deployments remotely, keeping them up-to-date and offering same experience as their public SaaS etc., lacks tools and automation.<p>Big developer teams build such tools in-house. We wanted to bring this to smaller developer teams as well.<p>How?<p>Each customer deployment is considered as an App environment. They work the way same no matter which cloud they run on.<p>One can signup for LocalOps, provide credentials to a target cloud account &amp; private Helm repo, and deploy a production grade app environment on that cloud in about 18-20mins and get a SSL enabled URL for the specific app instance. Each app environment is backed by a Kubernetes cluster to run Apps as Helm installations. Plus, we bundle each App environment with complementary tools to enable deployments, logs and metrics monitoring, encrypt data volumes and more to free up devs from making such choices.<p>Demo video: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;nWAu6NCvEU4?feature=shared\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;nWAu6NCvEU4?feature=shared</a><p>Anyone can signup now at <a href=\"https:&#x2F;&#x2F;console.localops.co&#x2F;signup\" rel=\"nofollow\">https:&#x2F;&#x2F;console.localops.co&#x2F;signup</a> and try out.<p>I would love to hear your feedback. Have you tried selling to these industries? Please add in your stories on making such private deployments, if you have one to share. It will be super helpful.<p>Cheers.", "title": "Show HN: LocalOps \u2013 Deployment automation tool to make Private SaaS easy", "updated_at": "2024-09-20T17:39:28Z", "url": "https://localops.co"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "moeidheidari"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hey HN We built CrossView because after managing dozens of Crossplane compositions across multiple clusters, kubectl get + mental graphing just wasn't cutting it anymore.v3.2.0 brings real-time updates (informers + websockets), multi-cluster switching, interactive composition \u2192 managed \u2192 claim graphs, dark mode, SSO, and stays strictly read-only for <em>production</em> safety.Deploys via <em>Helm</em> in ~2 minutes.Would love honest feedback especially on: UX when dealing with very large/messy compositions Missing Crossplane edge cases you run into Any perf issues at scale Repo + quickstart: <a href=\"https://github.com/corpobit/crossview\" rel=\"nofollow\">https://github.com/corpobit/crossview</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Crossview \u2013 visualize Crossplane resources and compositions"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/corpobit/crossview"}}, "_tags": ["story", "author_moeidheidari", "story_46845497", "show_hn"], "author": "moeidheidari", "created_at": "2026-02-01T11:43:19Z", "created_at_i": 1769946199, "num_comments": 0, "objectID": "46845497", "points": 2, "story_id": 46845497, "story_text": "Hey HN We built CrossView because after managing dozens of Crossplane compositions across multiple clusters, kubectl get + mental graphing just wasn&#x27;t cutting it anymore.v3.2.0 brings real-time updates (informers + websockets), multi-cluster switching, interactive composition \u2192 managed \u2192 claim graphs, dark mode, SSO, and stays strictly read-only for production safety.Deploys via Helm in ~2 minutes.Would love honest feedback especially on: UX when dealing with very large&#x2F;messy compositions Missing Crossplane edge cases you run into Any perf issues at scale Repo + quickstart: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;corpobit&#x2F;crossview\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;corpobit&#x2F;crossview</a>", "title": "Show HN: Crossview \u2013 visualize Crossplane resources and compositions", "updated_at": "2026-02-01T18:29:30Z", "url": "https://github.com/corpobit/crossview"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "moeidheidari"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hey HN\nWe built CrossView because after managing dozens of Crossplane compositions across multiple clusters, kubectl get + mental graphing just wasn't cutting it anymore.v3.2.0 brings real-time updates (informers + websockets), multi-cluster switching, interactive composition \u2192 managed \u2192 claim graphs, dark mode, SSO, and stays strictly read-only for <em>production</em> safety.Deploys via <em>Helm</em> in ~2 minutes.Would love honest feedback especially on:\nUX when dealing with very large/messy compositions\nMissing Crossplane edge cases you run into\nAny perf issues at scale\nRepo + quickstart: <a href=\"https://github.com/corpobit/crossview\" rel=\"nofollow\">https://github.com/corpobit/crossview</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Crossview \u2013 visualize Crossplane resources and compositions"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/corpobit/crossview"}}, "_tags": ["story", "author_moeidheidari", "story_46567163", "show_hn"], "author": "moeidheidari", "created_at": "2026-01-10T16:39:21Z", "created_at_i": 1768063161, "num_comments": 0, "objectID": "46567163", "points": 1, "story_id": 46567163, "story_text": "Hey HN\nWe built CrossView because after managing dozens of Crossplane compositions across multiple clusters, kubectl get + mental graphing just wasn&#x27;t cutting it anymore.v3.2.0 brings real-time updates (informers + websockets), multi-cluster switching, interactive composition \u2192 managed \u2192 claim graphs, dark mode, SSO, and stays strictly read-only for production safety.Deploys via Helm in ~2 minutes.Would love honest feedback especially on:\nUX when dealing with very large&#x2F;messy compositions\nMissing Crossplane edge cases you run into\nAny perf issues at scale\nRepo + quickstart: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;corpobit&#x2F;crossview\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;corpobit&#x2F;crossview</a>", "title": "Show HN: Crossview \u2013 visualize Crossplane resources and compositions", "updated_at": "2026-01-10T16:41:11Z", "url": "https://github.com/corpobit/crossview"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "shimont"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hi HN, this is Shimon and Eyar of Datree (<a href=\"https://www.datree.io/\" rel=\"nofollow\">https://www.datree.io/</a>).<p>When I was an Engineering Manager of Infrastructure at ironSource (NASDAQ:IS) for 400 developers, a developer made a mistake, causing a misconfiguration to reach <em>production</em>, which caused major problems for the company's infrastructure.<p>Mistakes happen all the time - you learn from them and hope to never make them again. But how can we prevent a <em>production</em> issue from recurring, or, how about a bigger challenge \u2014 how can you prevent the next one from the get-go?<p>In our case, we tried sending emails to our devs, writing Wikis, and hosting meetups and live sessions to educate our developers, but I felt that it just wasn\u2019t driving the message home. How can developers be expected to remember to configure a liveness probe or to put a memory limit in place for their Kubernetes workload when there are so many things that a dev must remember? Infra just isn\u2019t their primary focus.<p>Today, organizations want to delegate infra-as-code responsibilities to developers, but face a dilemma \u2014 even a small misconfiguration can cause major <em>production</em> issues. Some companies lock up infra changes and require ops teams to review all changes, which frustrates both sides. \nDevelopers want to ship features without waiting for infra. And infra teams don't want to \u201cbabysit\u201d developers by reviewing config files all day long, essentially acting as human debuggers for misconfigurations.<p>That\u2019s why I teamed up with Eyar to found Datree. Our mission is to help engineering teams prevent Kubernetes misconfigurations from reaching <em>production</em>. We believe that providing guardrails to developers protects their infra changes and frees up DevOps teams to focus on what matters most.<p>Datree provides a CLI tool (<a href=\"https://github.com/datreeio/datree\" rel=\"nofollow\">https://github.com/datreeio/datree</a>) that runs automated policy checks against your Kubernetes manifests and <em>Helm</em> charts, identifies any misconfigurations within, and suggests how to fix them. The tool comes with dozens of preset, best-practice rules covering the most common mistakes that could affect your <em>production</em>. In addition, you can write custom rules for your policy.<p>Our built-in rules are based on hundreds of Kubernetes post-mortems to ensure the prevention of issues such as resource limits/requests (MEM/CPU), liveness and readiness probes, labels on resources, Kubernetes schema validation, API version deprecation, and more.<p>Datree comes with a centralized policy dashboard enabling the infra team to dynamically configure rules that run on dev computers during the development phase, as well as within the CI/CD process. This central control point propagates policy checks automatically to all developers/machines in your company.<p>We initially launched Datree as a general purpose policy engine (see our YC Launch <a href=\"https://news.ycombinator.com/item?id=22536228\" rel=\"nofollow\">https://news.ycombinator.com/item?id=22536228</a>) in which you could configure all sorts of rules, but the market drove our focus toward infrastructure-as-code and, more specifically, Kubernetes, one of the most painful points of friction between developers and infrastructure teams.<p>When we adjusted to a Kubernetes-focused product, we pivoted our top-down sales-driven model to a bottom-up adoption-driven model focused on the user.<p>Our new dev tool is self-served and open-source. Hundreds of companies are using it to prevent Kubernetes misconfigurations and, in turn, are helping the tool improve by opening issues and submitting pull requests on GitHub.  Our product is well suited for self-evaluation and immediate value delivery. No demo calls \u2014 just 2 quick steps to try the product yourself!<p>TechWorld with Nana did a deep technical review of our product, which can be viewed at <a href=\"https://www.youtube.com/watch?v=hgUfH9Ab258\" rel=\"nofollow\">https://www.youtube.com/watch?v=hgUfH9Ab258</a>.<p>We look forward to hearing your feedback and answering any questions you may have. Thank you :)"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Datree (YC W20): Prevent K8s misconfigurations from reaching <em>production</em>"}}, "_tags": ["story", "author_shimont", "story_28918850", "show_hn"], "author": "shimont", "children": [28918892, 28918953, 28919001, 28919039, 28919123, 28919293, 28919343, 28919387, 28920372, 28920506, 28920534, 28920711, 28921587, 28921621, 28924571, 28927463], "created_at": "2021-10-19T15:04:08Z", "created_at_i": 1634655848, "num_comments": 26, "objectID": "28918850", "points": 144, "story_id": 28918850, "story_text": "Hi HN, this is Shimon and Eyar of Datree (<a href=\"https:&#x2F;&#x2F;www.datree.io&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.datree.io&#x2F;</a>).<p>When I was an Engineering Manager of Infrastructure at ironSource (NASDAQ:IS) for 400 developers, a developer made a mistake, causing a misconfiguration to reach production, which caused major problems for the company&#x27;s infrastructure.<p>Mistakes happen all the time - you learn from them and hope to never make them again. But how can we prevent a production issue from recurring, or, how about a bigger challenge \u2014 how can you prevent the next one from the get-go?<p>In our case, we tried sending emails to our devs, writing Wikis, and hosting meetups and live sessions to educate our developers, but I felt that it just wasn\u2019t driving the message home. How can developers be expected to remember to configure a liveness probe or to put a memory limit in place for their Kubernetes workload when there are so many things that a dev must remember? Infra just isn\u2019t their primary focus.<p>Today, organizations want to delegate infra-as-code responsibilities to developers, but face a dilemma \u2014 even a small misconfiguration can cause major production issues. Some companies lock up infra changes and require ops teams to review all changes, which frustrates both sides. \nDevelopers want to ship features without waiting for infra. And infra teams don&#x27;t want to \u201cbabysit\u201d developers by reviewing config files all day long, essentially acting as human debuggers for misconfigurations.<p>That\u2019s why I teamed up with Eyar to found Datree. Our mission is to help engineering teams prevent Kubernetes misconfigurations from reaching production. We believe that providing guardrails to developers protects their infra changes and frees up DevOps teams to focus on what matters most.<p>Datree provides a CLI tool (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;datreeio&#x2F;datree\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;datreeio&#x2F;datree</a>) that runs automated policy checks against your Kubernetes manifests and Helm charts, identifies any misconfigurations within, and suggests how to fix them. The tool comes with dozens of preset, best-practice rules covering the most common mistakes that could affect your production. In addition, you can write custom rules for your policy.<p>Our built-in rules are based on hundreds of Kubernetes post-mortems to ensure the prevention of issues such as resource limits&#x2F;requests (MEM&#x2F;CPU), liveness and readiness probes, labels on resources, Kubernetes schema validation, API version deprecation, and more.<p>Datree comes with a centralized policy dashboard enabling the infra team to dynamically configure rules that run on dev computers during the development phase, as well as within the CI&#x2F;CD process. This central control point propagates policy checks automatically to all developers&#x2F;machines in your company.<p>We initially launched Datree as a general purpose policy engine (see our YC Launch <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22536228\" rel=\"nofollow\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22536228</a>) in which you could configure all sorts of rules, but the market drove our focus toward infrastructure-as-code and, more specifically, Kubernetes, one of the most painful points of friction between developers and infrastructure teams.<p>When we adjusted to a Kubernetes-focused product, we pivoted our top-down sales-driven model to a bottom-up adoption-driven model focused on the user.<p>Our new dev tool is self-served and open-source. Hundreds of companies are using it to prevent Kubernetes misconfigurations and, in turn, are helping the tool improve by opening issues and submitting pull requests on GitHub.  Our product is well suited for self-evaluation and immediate value delivery. No demo calls \u2014 just 2 quick steps to try the product yourself!<p>TechWorld with Nana did a deep technical review of our product, which can be viewed at <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=hgUfH9Ab258\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=hgUfH9Ab258</a>.<p>We look forward to hearing your feedback and answering any questions you may have. Thank you :)", "title": "Show HN: Datree (YC W20): Prevent K8s misconfigurations from reaching production", "updated_at": "2024-09-20T09:37:16Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "maxthegeek1"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hey HN, I wanted to share the latest release of Dittofeed we\u2019ve been working on, v0.14.0. For those who aren't familiar, Dittofeed is an open-source, MIT-licensed alternative to customer engagement platforms like Customer.io and Klaviyo. This release brings significant improvements to Dittofeed:<p>- *<em>Helm</em> Chart*: We've added a <em>Helm</em> chart for easy installation on Kubernetes. Several teams are already using it in <em>production</em>.<p>- *Manual Segments*: Create segments based on a hardcoded list of users, great for targeting based on external data or analysis.<p>- *Email Attachments*: We've added support for email attachments, enhancing email campaign capabilities.<p>- *Performance Enhancements*: Significant improvements include an optimization to our segmentation engine, resulting in a 50% reduction in compute times!<p>- *Webhook and SMS Broadcasts*: Send one-off messages to external APIs and SMS providers.<p>- *Random Bucket Segment Type*: Useful for testing new journey paths or A/B testing.<p>- *Synchronize Properties in Webhook Nodes*: Safely retrieve and use data from your own APIs in your journeys.<p>- *Improved Admin APIs*: Extended APIs allow developers to programmatically read and update journeys, segments, and user properties.<p>- *HyperDX Integration*: Added support for correlating logs and traces with HyperDX via OpenTelemetry.<p>For a more detailed breakdown of what's included in v0.14.0, check out our full release notes:\nhttps://www.dittofeed.com/post/release-v0-14-0<p>If you're self-hosting Dittofeed, make sure to consult our latest upgrade guide:\nhttps://docs.dittofeed.com/deployment/self-hosted/upgrade-guide/v0-14-0<p>We'd really appreciate it if you visited and starred our repo to help support the project:\nhttps://github.com/dittofeed/dittofeed<p>We'd love to get your feedback and suggestions for new features and improvements.<p>Thank you!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["helm"], "value": "Dittofeed \u2013 <em>Helm</em> Chart, Email Attachments, Segmentation Improvements \u2013 v0.14.0"}}, "_tags": ["story", "author_maxthegeek1", "story_41258761", "ask_hn"], "author": "maxthegeek1", "children": [41258864], "created_at": "2024-08-15T18:03:59Z", "created_at_i": 1723745039, "num_comments": 1, "objectID": "41258761", "points": 2, "story_id": 41258761, "story_text": "Hey HN, I wanted to share the latest release of Dittofeed we\u2019ve been working on, v0.14.0. For those who aren&#x27;t familiar, Dittofeed is an open-source, MIT-licensed alternative to customer engagement platforms like Customer.io and Klaviyo. This release brings significant improvements to Dittofeed:<p>- *Helm Chart*: We&#x27;ve added a Helm chart for easy installation on Kubernetes. Several teams are already using it in production.<p>- *Manual Segments*: Create segments based on a hardcoded list of users, great for targeting based on external data or analysis.<p>- *Email Attachments*: We&#x27;ve added support for email attachments, enhancing email campaign capabilities.<p>- *Performance Enhancements*: Significant improvements include an optimization to our segmentation engine, resulting in a 50% reduction in compute times!<p>- *Webhook and SMS Broadcasts*: Send one-off messages to external APIs and SMS providers.<p>- *Random Bucket Segment Type*: Useful for testing new journey paths or A&#x2F;B testing.<p>- *Synchronize Properties in Webhook Nodes*: Safely retrieve and use data from your own APIs in your journeys.<p>- *Improved Admin APIs*: Extended APIs allow developers to programmatically read and update journeys, segments, and user properties.<p>- *HyperDX Integration*: Added support for correlating logs and traces with HyperDX via OpenTelemetry.<p>For a more detailed breakdown of what&#x27;s included in v0.14.0, check out our full release notes:\nhttps:&#x2F;&#x2F;www.dittofeed.com&#x2F;post&#x2F;release-v0-14-0<p>If you&#x27;re self-hosting Dittofeed, make sure to consult our latest upgrade guide:\nhttps:&#x2F;&#x2F;docs.dittofeed.com&#x2F;deployment&#x2F;self-hosted&#x2F;upgrade-guide&#x2F;v0-14-0<p>We&#x27;d really appreciate it if you visited and starred our repo to help support the project:\nhttps:&#x2F;&#x2F;github.com&#x2F;dittofeed&#x2F;dittofeed<p>We&#x27;d love to get your feedback and suggestions for new features and improvements.<p>Thank you!", "title": "Dittofeed \u2013 Helm Chart, Email Attachments, Segmentation Improvements \u2013 v0.14.0", "updated_at": "2024-11-15T14:17:06Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "notanaverageman"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hello HN, I am Yusuf from Ohayocorp (<a href=\"https://ohayocorp.com\" rel=\"nofollow\">https://ohayocorp.com</a>). I have been developing a package manager for Kubernetes and I am excited to share it with you all.<p>Currently, the go-to package manager for Kubernetes is <em>Helm</em>. <em>Helm</em> has many shortcomings and people have been looking for alternatives for a long time. There are actually several alternatives that have emerged, but none has gained significant traction to replace <em>Helm</em>. So, you might ask what makes Anemos different?<p>Anemos uses JavaScript/TypeScript to define and manage your Kubernetes manifests. It is a single-binary tool that is written in Go and uses the Goja runtime (its Sobek fork to be pedantic) to execute JavaScript/TypeScript code. It supports templating via JavaScript template literals. It also allows you to use an object-oriented approach for type safety and better IDE experience. As a third option, it provides APIs for direct YAML node manipulation. You can mix and match these approaches in any way you like.<p>Anemos allows you to define manifests for all your applications in a single project. You can also easily manage different environments like development, staging, and <em>production</em> in the same project. This brings centralized configuration management and makes it easier to maintain consistency across applications and environments.<p>Another key feature of Anemos is its ability to modify generated manifests whether it's generated by your own code or by third-party packages. No need to wait for maintainers to add a feature or fix a bug. It also allows you to modify and inspect your manifests in bulk, such as adding some labels to all your manifests or replacing your ingresses with OpenShift routes or giving an error if a workload misses a security context field.<p>Anemos also provides an easy way to use <em>Helm</em> charts in your projects, allowing you to leverage your existing charts while still benefiting from Anemos's features. You can migrate your <em>Helm</em> charts to Anemos at your own pace, without rewriting everything from scratch in one go.<p>What currently lacks in Anemos to make it a complete solution is applying the manifests to a Kubernetes cluster. I have this on my roadmap and plan to implement it soon.<p>I would appreciate any feedback, suggestions, or contributions from the community to help make Anemos better."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["helm"], "value": "Show HN: Anemos \u2013 Open source Kubernetes manifest manager, alternative to <em>Helm</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/ohayocorp/anemos"}}, "_tags": ["story", "author_notanaverageman", "story_44593605", "show_hn"], "author": "notanaverageman", "children": [44593722, 44601586], "created_at": "2025-07-17T14:06:11Z", "created_at_i": 1752761171, "num_comments": 2, "objectID": "44593605", "points": 1, "story_id": 44593605, "story_text": "Hello HN, I am Yusuf from Ohayocorp (<a href=\"https:&#x2F;&#x2F;ohayocorp.com\" rel=\"nofollow\">https:&#x2F;&#x2F;ohayocorp.com</a>). I have been developing a package manager for Kubernetes and I am excited to share it with you all.<p>Currently, the go-to package manager for Kubernetes is Helm. Helm has many shortcomings and people have been looking for alternatives for a long time. There are actually several alternatives that have emerged, but none has gained significant traction to replace Helm. So, you might ask what makes Anemos different?<p>Anemos uses JavaScript&#x2F;TypeScript to define and manage your Kubernetes manifests. It is a single-binary tool that is written in Go and uses the Goja runtime (its Sobek fork to be pedantic) to execute JavaScript&#x2F;TypeScript code. It supports templating via JavaScript template literals. It also allows you to use an object-oriented approach for type safety and better IDE experience. As a third option, it provides APIs for direct YAML node manipulation. You can mix and match these approaches in any way you like.<p>Anemos allows you to define manifests for all your applications in a single project. You can also easily manage different environments like development, staging, and production in the same project. This brings centralized configuration management and makes it easier to maintain consistency across applications and environments.<p>Another key feature of Anemos is its ability to modify generated manifests whether it&#x27;s generated by your own code or by third-party packages. No need to wait for maintainers to add a feature or fix a bug. It also allows you to modify and inspect your manifests in bulk, such as adding some labels to all your manifests or replacing your ingresses with OpenShift routes or giving an error if a workload misses a security context field.<p>Anemos also provides an easy way to use Helm charts in your projects, allowing you to leverage your existing charts while still benefiting from Anemos&#x27;s features. You can migrate your Helm charts to Anemos at your own pace, without rewriting everything from scratch in one go.<p>What currently lacks in Anemos to make it a complete solution is applying the manifests to a Kubernetes cluster. I have this on my roadmap and plan to implement it soon.<p>I would appreciate any feedback, suggestions, or contributions from the community to help make Anemos better.", "title": "Show HN: Anemos \u2013 Open source Kubernetes manifest manager, alternative to Helm", "updated_at": "2025-07-20T23:20:32Z", "url": "https://github.com/ohayocorp/anemos"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "KyleVlaros"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "<em>Helm</em> gives you powerful chart rendering, but it doesn't ship a safe, operator\u2011friendly plan/preview workflow for <em>production</em> changes. Helmer is a focused CLI that creates a <em>production</em>\u2011friendly plan workflow so teams can review changes before they hit the cluster."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["helm"], "value": "Show HN: Deploying <em>Helm</em> charts without fear"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/avkcode/helmer"}}, "_tags": ["story", "author_KyleVlaros", "story_46799348", "show_hn"], "author": "KyleVlaros", "created_at": "2026-01-28T18:14:27Z", "created_at_i": 1769624067, "num_comments": 0, "objectID": "46799348", "points": 1, "story_id": 46799348, "story_text": "Helm gives you powerful chart rendering, but it doesn&#x27;t ship a safe, operator\u2011friendly plan&#x2F;preview workflow for production changes. Helmer is a focused CLI that creates a production\u2011friendly plan workflow so teams can review changes before they hit the cluster.", "title": "Show HN: Deploying Helm charts without fear", "updated_at": "2026-01-28T18:15:49Z", "url": "https://github.com/avkcode/helmer"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ross_chanin"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hi HN,<p>I\u2019m Ross Chanin, co-founder and CEO of Artifact (<a href=\"https://www.heyartifact.com\" rel=\"nofollow\">https://www.heyartifact.com</a>), where we help people record the important stories in their lives, told in the voices of friends and family members. Think of us as an on-demand podcast studio you can focus on anything you want. We set you up with an interviewer in our marketplace, and we make the process easy, from scheduling to hosting interviews over the phone, and then delivering a polished edit that wouldn\u2019t sound out of place on the radio.<p>Our idea was born out of a personal sense of loss: My grandfather passed away and I found myself regretting that I hadn\u2019t captured him telling stories about his life. I thought others might feel the same about someone in their own lives.<p>The problem (for me, anyway) was that I didn\u2019t have an easy way to go deep with my grandfather. It was never the right time. Some of the questions I wanted to ask were quite personal. I don\u2019t have audio editing skills, so I was unsure what I would even do with the raw audio if I\u2019d recorded a conversation with him.<p>I mentioned what I wanted to do to my friend, George, who\u2019s a journalist. He stopped me and said, \u201cBut that\u2019s what journalists do. Why don\u2019t you hire a journalist?\u201d<p>We decided to try it as an experiment. My Aunt Cindy was about to turn 59, so George called up three of her oldest friends, interviewed them about their relationships with Cindy, and delivered an edit that, when Cindy heard it, had her laughing and crying in equal measure.<p>Cindy and her friends told us three things that we have since heard over and over: First, customers often tell our interviewers things they\u2019ve never told their loved ones, but would like to be able to say to them (example - <a href=\"https://www.instagram.com/p/CDlwIojFPmF/\" rel=\"nofollow\">https://www.instagram.com/p/CDlwIojFPmF/</a>). Second, we routinely hear from recipients that, as soon as they've heard an episode in which people talk about them, they want to call those people and thank them. And third, that the experience is helping people feel closer to each other.<p>But the truth is that we would have only ever been a cottage <em>production</em> studio with George and me at the <em>helm</em> (that\u2019s right, my friend George became a co-founder). This is where Moncef Biaz and Martin Gouy, our technical co-founders, enter the picture. Together, as a team, we think about Artifact as a marketplace, connecting the right interviewers with the right guests (e.g., if you need a bilingual Mandarin and English-speaking interviewer who is also great with 11-year-olds, we got you.) We\u2019ve also become obsessed with the state of audio recording and editing technology, effectively asking the question: &quot;How close can we get to studio-quality sound without the studio?&quot; The answer is: pretty darn close.<p>Far more important than the audio quality is the substance of what people are telling our interviewers. Our customers are telling us that these incredibly personal stories are becoming heirlooms for their friends and family. Customers are also teaching us new ways to use our service: wedding and anniversary gifts, family heritage, journaling, enterprise use-cases, etc.<p>We put lots of little snippets (with permission from our customers) on our Instagram. You can also hear a full Artifact\u2014commissioned by a couple who wanted to document the husband\u2019s cancer diagnosis and their shared journey (<a href=\"https://www.heyartifact.com/daryl\" rel=\"nofollow\">https://www.heyartifact.com/daryl</a>).<p>Ultimately, our goal is to make this service accessible in every language, geographic region, and culture. Because we all have a story to tell; what we didn\u2019t all have, until now, is someone to tell it to\u2014someone who knows how to ask the right questions, how to record it, and how to make sure it sounds great, so that we can easily share it with friends, family, and generations to come.<p>I\u2019d really love to hear the community\u2019s feedback and I\u2019m here to answer questions.<p>\u2014Ross<p>P.S. There\u2019s more detail on that first experiment, and how we expanded our scope from there, in this blog post (<a href=\"https://www.heyartifact.com/blog/hey-were-artifact/\" rel=\"nofollow\">https://www.heyartifact.com/blog/hey-were-artifact/</a>)."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Artifact (YC S20) \u2013 Personal podcasts with the people in your life"}}, "_tags": ["story", "author_ross_chanin", "story_24919931", "launch_hn"], "author": "ross_chanin", "children": [24920080, 24920748, 24921364, 24922439, 24922567, 24922721, 24922784, 24922841, 24923084, 24923340, 24923390, 24923720, 24923775, 24923932, 24924390, 24924698, 24925401, 24925690, 24925918, 24926665, 24927248, 24927676, 24927853, 24928274, 24928277, 24928424, 24928845, 24929272, 24929320, 24930882, 24931878, 24938541, 24939385, 24950844], "created_at": "2020-10-28T15:32:47Z", "created_at_i": 1603899167, "num_comments": 105, "objectID": "24919931", "points": 266, "story_id": 24919931, "story_text": "Hi HN,<p>I\u2019m Ross Chanin, co-founder and CEO of Artifact (<a href=\"https:&#x2F;&#x2F;www.heyartifact.com\" rel=\"nofollow\">https:&#x2F;&#x2F;www.heyartifact.com</a>), where we help people record the important stories in their lives, told in the voices of friends and family members. Think of us as an on-demand podcast studio you can focus on anything you want. We set you up with an interviewer in our marketplace, and we make the process easy, from scheduling to hosting interviews over the phone, and then delivering a polished edit that wouldn\u2019t sound out of place on the radio.<p>Our idea was born out of a personal sense of loss: My grandfather passed away and I found myself regretting that I hadn\u2019t captured him telling stories about his life. I thought others might feel the same about someone in their own lives.<p>The problem (for me, anyway) was that I didn\u2019t have an easy way to go deep with my grandfather. It was never the right time. Some of the questions I wanted to ask were quite personal. I don\u2019t have audio editing skills, so I was unsure what I would even do with the raw audio if I\u2019d recorded a conversation with him.<p>I mentioned what I wanted to do to my friend, George, who\u2019s a journalist. He stopped me and said, \u201cBut that\u2019s what journalists do. Why don\u2019t you hire a journalist?\u201d<p>We decided to try it as an experiment. My Aunt Cindy was about to turn 59, so George called up three of her oldest friends, interviewed them about their relationships with Cindy, and delivered an edit that, when Cindy heard it, had her laughing and crying in equal measure.<p>Cindy and her friends told us three things that we have since heard over and over: First, customers often tell our interviewers things they\u2019ve never told their loved ones, but would like to be able to say to them (example - <a href=\"https:&#x2F;&#x2F;www.instagram.com&#x2F;p&#x2F;CDlwIojFPmF&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.instagram.com&#x2F;p&#x2F;CDlwIojFPmF&#x2F;</a>). Second, we routinely hear from recipients that, as soon as they&#x27;ve heard an episode in which people talk about them, they want to call those people and thank them. And third, that the experience is helping people feel closer to each other.<p>But the truth is that we would have only ever been a cottage production studio with George and me at the helm (that\u2019s right, my friend George became a co-founder). This is where Moncef Biaz and Martin Gouy, our technical co-founders, enter the picture. Together, as a team, we think about Artifact as a marketplace, connecting the right interviewers with the right guests (e.g., if you need a bilingual Mandarin and English-speaking interviewer who is also great with 11-year-olds, we got you.) We\u2019ve also become obsessed with the state of audio recording and editing technology, effectively asking the question: &quot;How close can we get to studio-quality sound without the studio?&quot; The answer is: pretty darn close.<p>Far more important than the audio quality is the substance of what people are telling our interviewers. Our customers are telling us that these incredibly personal stories are becoming heirlooms for their friends and family. Customers are also teaching us new ways to use our service: wedding and anniversary gifts, family heritage, journaling, enterprise use-cases, etc.<p>We put lots of little snippets (with permission from our customers) on our Instagram. You can also hear a full Artifact\u2014commissioned by a couple who wanted to document the husband\u2019s cancer diagnosis and their shared journey (<a href=\"https:&#x2F;&#x2F;www.heyartifact.com&#x2F;daryl\" rel=\"nofollow\">https:&#x2F;&#x2F;www.heyartifact.com&#x2F;daryl</a>).<p>Ultimately, our goal is to make this service accessible in every language, geographic region, and culture. Because we all have a story to tell; what we didn\u2019t all have, until now, is someone to tell it to\u2014someone who knows how to ask the right questions, how to record it, and how to make sure it sounds great, so that we can easily share it with friends, family, and generations to come.<p>I\u2019d really love to hear the community\u2019s feedback and I\u2019m here to answer questions.<p>\u2014Ross<p>P.S. There\u2019s more detail on that first experiment, and how we expanded our scope from there, in this blog post (<a href=\"https:&#x2F;&#x2F;www.heyartifact.com&#x2F;blog&#x2F;hey-were-artifact&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.heyartifact.com&#x2F;blog&#x2F;hey-were-artifact&#x2F;</a>).", "title": "Launch HN: Artifact (YC S20) \u2013 Personal podcasts with the people in your life", "updated_at": "2024-09-20T07:14:05Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tylerflint"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hi HN, I'm Tyler Flint, one of the creators of qtap.<p>For a while now, my team and I at Qpoint.io have been grappling with the challenge of understanding what's actually happening inside the encrypted traffic leaving our <em>production</em> systems. Modern apps rely heavily on third-party APIs (think payment processors, data providers, etc.), but once TLS kicks in, figuring out exactly what data is being sent, identifying PII exposure, or debugging integration issues becomes incredibly difficult without resorting to complex and often brittle solutions.<p>Traditional approaches like forward proxies require terminating TLS (MITM), managing certificates, and often introduce performance bottlenecks or single points of failure. Network firewalls usually operate at L3/L4 and lack payload visibility. We felt there had to be a better way.<p>That's why we built qtap. It's a lightweight agent that uses eBPF to tap into network traffic at the kernel level. The key idea is to hook into common TLS libraries (like OpenSSL) before encryption and after decryption. This gives us deep visibility into the actual request/response payloads of HTTPS/TLS traffic without needing to terminate the connection or manage certs. Because it leverages eBPF, the performance impact is minimal compared to traditional methods.<p>With qtap, we can now see exactly which external services our apps are talking to, inspect the payloads for debugging or security auditing (e.g., spotting accidental PII leaks), monitor API performance/errors for third-party dependencies, and get a much clearer picture of our egress traffic patterns.<p>We've found this approach really powerful for improving reliability and security posture. We've packaged qtap as a Linux Binary, Docker container, and <em>Helm</em> chart for deployment.<p>This is still evolving, but we're excited about the potential of using eBPF for this kind of deep, yet non-intrusive, visibility.<p>We'd love to get the HN community's feedback:<p><pre><code>    Do you face similar challenges monitoring encrypted egress traffic?\n    What are your thoughts on using eBPF for this compared to other methods?\n    Any suggestions or potential use cases we haven't considered?\n</code></pre>\nHappy to answer any questions!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Using eBPF to see through encryption without a proxy"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/qpoint-io/qtap"}}, "_tags": ["story", "author_tylerflint", "story_43928118", "show_hn"], "author": "tylerflint", "children": [43928372, 43928451, 43928491, 43928616, 43928795, 43928843, 43928963, 43929384, 43929443, 43929883, 43929910, 43930265, 43930439, 43930474, 43930776, 43932015, 43932086, 43932260, 43932669, 43932948, 43933105, 43933316, 43933457, 43933509, 43939096], "created_at": "2025-05-08T16:49:11Z", "created_at_i": 1746722951, "num_comments": 78, "objectID": "43928118", "points": 259, "story_id": 43928118, "story_text": "Hi HN, I&#x27;m Tyler Flint, one of the creators of qtap.<p>For a while now, my team and I at Qpoint.io have been grappling with the challenge of understanding what&#x27;s actually happening inside the encrypted traffic leaving our production systems. Modern apps rely heavily on third-party APIs (think payment processors, data providers, etc.), but once TLS kicks in, figuring out exactly what data is being sent, identifying PII exposure, or debugging integration issues becomes incredibly difficult without resorting to complex and often brittle solutions.<p>Traditional approaches like forward proxies require terminating TLS (MITM), managing certificates, and often introduce performance bottlenecks or single points of failure. Network firewalls usually operate at L3&#x2F;L4 and lack payload visibility. We felt there had to be a better way.<p>That&#x27;s why we built qtap. It&#x27;s a lightweight agent that uses eBPF to tap into network traffic at the kernel level. The key idea is to hook into common TLS libraries (like OpenSSL) before encryption and after decryption. This gives us deep visibility into the actual request&#x2F;response payloads of HTTPS&#x2F;TLS traffic without needing to terminate the connection or manage certs. Because it leverages eBPF, the performance impact is minimal compared to traditional methods.<p>With qtap, we can now see exactly which external services our apps are talking to, inspect the payloads for debugging or security auditing (e.g., spotting accidental PII leaks), monitor API performance&#x2F;errors for third-party dependencies, and get a much clearer picture of our egress traffic patterns.<p>We&#x27;ve found this approach really powerful for improving reliability and security posture. We&#x27;ve packaged qtap as a Linux Binary, Docker container, and Helm chart for deployment.<p>This is still evolving, but we&#x27;re excited about the potential of using eBPF for this kind of deep, yet non-intrusive, visibility.<p>We&#x27;d love to get the HN community&#x27;s feedback:<p><pre><code>    Do you face similar challenges monitoring encrypted egress traffic?\n    What are your thoughts on using eBPF for this compared to other methods?\n    Any suggestions or potential use cases we haven&#x27;t considered?\n</code></pre>\nHappy to answer any questions!", "title": "Show HN: Using eBPF to see through encryption without a proxy", "updated_at": "2026-01-21T17:50:07Z", "url": "https://github.com/qpoint-io/qtap"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "edrenova"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hey HN, we're Evis and Nick and we're excited to be launching Neosync (<a href=\"https://www.github.com/nucleuscloud/neosync\">https://www.github.com/nucleuscloud/neosync</a>). Neosync is an open source platform that helps developers anonymize <em>production</em> data, generate synthetic data and sync it across their environments for better testing, debugging and developer experience.<p>Most developers and teams have some version of a database seed script that creates some mock data for their local and stage databases. The problem is that <em>production</em> data is messy and it\u2019s very difficult to replicate that with mock data. This causes two big problems for developers.<p>The first problem is that features seem to work locally/stage but have bugs and edge cases in <em>production</em> because the seed data you used to develop against was not representative of <em>production</em> data.<p>The second problem we saw was that debugging <em>production</em> errors would take a long time and would often resurface. When we see a bug in <em>production</em>, the first thing we want to do is reproduce it locally, but if we can\u2019t reproduce the state of the data locally, then we\u2019re kind of flying blind.<p>Working directly with <em>production</em> data would solve both of these problems but most teams can\u2019t because of: (1) privacy/security issues and (2) scale. So we set out to solve these two problems with Neosync.<p>We solve the privacy and security problem using anonymization and synthetic data. We have 40+ pre-built transformers (or you can write your own in code) that can anonymize PII or sensitive data so that it\u2019s safe to use locally. Additionally, you can generate synthetic data from scratch that fits your existing schema across your database.<p>The second problem is scale. Some <em>production</em> databases are too big to fit locally or just have more data than you need. Also, in some cases, you may want to debug a certain customer\u2019s data and you only want their data. We solve this with subsetting. You can pass in a SQL query to filter your table(s) and Neosync will handle all of the heavy lifting including referential integrity.<p>At the core of Neosync does three things:  (1) It streams data from a source to one or multiple destination databases. We never store your sensitive data. (2) While that data is being streamed, we transform it. You define which schemas and tables you want to sync and at the column level, select a transformer that defines how you want to anonymize the data or generate synthetic data. (3) We subset your data based on your filters.<p>We do all of this while handling referential integrity. Whether you have primary keys, foreign keys, unique constraints, circular dependencies (within a table and across tables), sequences and more, Neosync preserves those references.<p>We also ship with APIs, a Terraform provider, a CLI and Github action that you can use to hydrate a CI database.<p>Neosync is an open source project written in Go and Typescript and can be run on Docker Compose, Bare Metal, or Kubernetes via <em>Helm</em>. You can also use our hosted platform or managed platform that you can deploy in your VPC. We also have a hosted platform with a generous free tier - <a href=\"https://neosync.dev\">https://neosync.dev</a><p>Here's a brief loom demo:  <a href=\"https://www.loom.com/share/ac21378d01cd4d848cf723e4960e8338?sid=2faf613c-92be-44fa-9278-c8087e777356\" rel=\"nofollow\">https://www.loom.com/share/ac21378d01cd4d848cf723e4960e8338?...</a><p>We'd love any feedback you have!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Neosync \u2013 Open-Source Data Anonymization for Postgres and MySQL"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/nucleuscloud/neosync"}}, "_tags": ["story", "author_edrenova", "story_40443927", "show_hn"], "author": "edrenova", "children": [40444727, 40444808, 40445047, 40445439, 40445958, 40446590, 40446843, 40447452, 40448743, 40449655, 40451468], "created_at": "2024-05-22T17:53:45Z", "created_at_i": 1716400425, "num_comments": 44, "objectID": "40443927", "points": 246, "story_id": 40443927, "story_text": "Hey HN, we&#x27;re Evis and Nick and we&#x27;re excited to be launching Neosync (<a href=\"https:&#x2F;&#x2F;www.github.com&#x2F;nucleuscloud&#x2F;neosync\">https:&#x2F;&#x2F;www.github.com&#x2F;nucleuscloud&#x2F;neosync</a>). Neosync is an open source platform that helps developers anonymize production data, generate synthetic data and sync it across their environments for better testing, debugging and developer experience.<p>Most developers and teams have some version of a database seed script that creates some mock data for their local and stage databases. The problem is that production data is messy and it\u2019s very difficult to replicate that with mock data. This causes two big problems for developers.<p>The first problem is that features seem to work locally&#x2F;stage but have bugs and edge cases in production because the seed data you used to develop against was not representative of production data.<p>The second problem we saw was that debugging production errors would take a long time and would often resurface. When we see a bug in production, the first thing we want to do is reproduce it locally, but if we can\u2019t reproduce the state of the data locally, then we\u2019re kind of flying blind.<p>Working directly with production data would solve both of these problems but most teams can\u2019t because of: (1) privacy&#x2F;security issues and (2) scale. So we set out to solve these two problems with Neosync.<p>We solve the privacy and security problem using anonymization and synthetic data. We have 40+ pre-built transformers (or you can write your own in code) that can anonymize PII or sensitive data so that it\u2019s safe to use locally. Additionally, you can generate synthetic data from scratch that fits your existing schema across your database.<p>The second problem is scale. Some production databases are too big to fit locally or just have more data than you need. Also, in some cases, you may want to debug a certain customer\u2019s data and you only want their data. We solve this with subsetting. You can pass in a SQL query to filter your table(s) and Neosync will handle all of the heavy lifting including referential integrity.<p>At the core of Neosync does three things:  (1) It streams data from a source to one or multiple destination databases. We never store your sensitive data. (2) While that data is being streamed, we transform it. You define which schemas and tables you want to sync and at the column level, select a transformer that defines how you want to anonymize the data or generate synthetic data. (3) We subset your data based on your filters.<p>We do all of this while handling referential integrity. Whether you have primary keys, foreign keys, unique constraints, circular dependencies (within a table and across tables), sequences and more, Neosync preserves those references.<p>We also ship with APIs, a Terraform provider, a CLI and Github action that you can use to hydrate a CI database.<p>Neosync is an open source project written in Go and Typescript and can be run on Docker Compose, Bare Metal, or Kubernetes via Helm. You can also use our hosted platform or managed platform that you can deploy in your VPC. We also have a hosted platform with a generous free tier - <a href=\"https:&#x2F;&#x2F;neosync.dev\">https:&#x2F;&#x2F;neosync.dev</a><p>Here&#x27;s a brief loom demo:  <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;ac21378d01cd4d848cf723e4960e8338?sid=2faf613c-92be-44fa-9278-c8087e777356\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;ac21378d01cd4d848cf723e4960e8338?...</a><p>We&#x27;d love any feedback you have!", "title": "Show HN: Neosync \u2013 Open-Source Data Anonymization for Postgres and MySQL", "updated_at": "2025-08-14T22:38:55Z", "url": "https://github.com/nucleuscloud/neosync"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "suryao"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hello HN, I'm Surya, the founder of Argonaut (<a href=\"https://www.argonaut.dev/\">https://www.argonaut.dev/</a>), a unified platform that tries to make software ops painless, so teams can focus on building features instead of building and managing infrastructure. Argonaut combines Kubernetes PaaS, a CI pipeline builder, and a Terraform state manager with prebuilt AWS and GCP modules. Here\u2019s a demo: <a href=\"https://www.youtube.com/watch?v=8DZsYXxA2tQ\">https://www.youtube.com/watch?v=8DZsYXxA2tQ</a>.<p>I\u2019ve helped build infrastructure tooling from scratch at multiple companies and realized two things: that the shape of the solution with the advent of containerization, Kubernetes, and hyperscalers is quite similar across orgs, and that highly knowledgeable engineers are needed to build and manage this system.<p>Internal infrastructure teams juggle a multitude of tasks\u2014provisioning cloud infrastructure, configuring runtimes, building code, securing artifacts, running tests, and deploying at scale. Post-deployment, they're also tasked with monitoring app performance, errors, uptime, and cost visibility. It's a lot of work, and having to build this tooling in-house is a deep inefficiency in engineering teams. The root of the problem is that AWS and GCP provide a lower level of abstraction than the entities, such as environments and applications, that developers have to deal with, and a ton of work is getting duplicated, often by underfunded teams, across many orgs. Argonaut\u2019s objective is to be the developer platform and control center that you would otherwise have to build internally.<p>Argonaut provides an intuitive developer experience that simplifies working with Kubernetes and enables developer self-service, reducing the burden on devops and platform teams. We've productized this workflow orchestration, incorporating best practices to provide a push-to-deploy experience with flexible pipelines and scalable infrastructure, all within minutes.<p>Our users are startups across various domains like healthcare, IoT, fintech, AI, and SaaS products. Over the last two years, we\u2019ve enabled customers to scale their engineering teams 10x and manage 10+ environments in parallel without needing a dedicated infra/DevOps team, saving them precious time and resources.<p>Argonaut lets you set up <em>production</em>-ready infrastructure and customize as you scale. We then let you set up automated deployments of your application in minutes. We offer configurable build-and-deploy pipelines, powered by Dagger and ArgoCD, and deep integration with GitHub Actions and GitLab CI.  In addition, we support container registries, multiple cloud accounts, observability stacks, cost visibility providers, CDNs, and the entire <em>helm</em> chart ecosystem of Kubernetes, with more integrations on the way.<p>Key features include: (1) easily create environments encapsulating cloud infrastructure, applications, and deployment pipelines (2) autoscaling deployments for apps and cronjobs to GCP and AWS with a progression across environments (3) compose deployments across multiple environments with our visual pipeline builder (4) get cost estimates before making infra changes, giving you a clear understanding of your expenses; (5) managed Terraform state and pre-built modules that just work, fostering team collaboration on infrastructure.<p>Argonaut is self-serve, so you can sign up and start using the product right away: <a href=\"https://ship.argonaut.dev\">https://ship.argonaut.dev</a>. There is a free tier that doesn't require a credit card to get started. We'd be delighted to have you try it, and are happy to help with onboarding.<p>If your teams work with AWS, GCP, or Kubernetes, I\u2019d love to hear about your experiences, pain points, and what you think a product like Argonaut should be able to do. Looking forward to your comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Argonaut (YC S21) \u2013 Easily Deploy Apps and Infra to AWS and GCP"}}, "_tags": ["story", "author_suryao", "story_36480230", "launch_hn"], "author": "suryao", "children": [36480563, 36481562, 36481779, 36481788, 36481867, 36481946, 36481958, 36482015, 36482017, 36482292, 36482375, 36482570, 36483009, 36483364, 36483568, 36484649, 36484751, 36484791, 36485216, 36485392, 36485534, 36485547, 36486525, 36486646, 36487852, 36488032, 36489557], "created_at": "2023-06-26T14:41:47Z", "created_at_i": 1687790507, "num_comments": 76, "objectID": "36480230", "points": 152, "story_id": 36480230, "story_text": "Hello HN, I&#x27;m Surya, the founder of Argonaut (<a href=\"https:&#x2F;&#x2F;www.argonaut.dev&#x2F;\">https:&#x2F;&#x2F;www.argonaut.dev&#x2F;</a>), a unified platform that tries to make software ops painless, so teams can focus on building features instead of building and managing infrastructure. Argonaut combines Kubernetes PaaS, a CI pipeline builder, and a Terraform state manager with prebuilt AWS and GCP modules. Here\u2019s a demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8DZsYXxA2tQ\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8DZsYXxA2tQ</a>.<p>I\u2019ve helped build infrastructure tooling from scratch at multiple companies and realized two things: that the shape of the solution with the advent of containerization, Kubernetes, and hyperscalers is quite similar across orgs, and that highly knowledgeable engineers are needed to build and manage this system.<p>Internal infrastructure teams juggle a multitude of tasks\u2014provisioning cloud infrastructure, configuring runtimes, building code, securing artifacts, running tests, and deploying at scale. Post-deployment, they&#x27;re also tasked with monitoring app performance, errors, uptime, and cost visibility. It&#x27;s a lot of work, and having to build this tooling in-house is a deep inefficiency in engineering teams. The root of the problem is that AWS and GCP provide a lower level of abstraction than the entities, such as environments and applications, that developers have to deal with, and a ton of work is getting duplicated, often by underfunded teams, across many orgs. Argonaut\u2019s objective is to be the developer platform and control center that you would otherwise have to build internally.<p>Argonaut provides an intuitive developer experience that simplifies working with Kubernetes and enables developer self-service, reducing the burden on devops and platform teams. We&#x27;ve productized this workflow orchestration, incorporating best practices to provide a push-to-deploy experience with flexible pipelines and scalable infrastructure, all within minutes.<p>Our users are startups across various domains like healthcare, IoT, fintech, AI, and SaaS products. Over the last two years, we\u2019ve enabled customers to scale their engineering teams 10x and manage 10+ environments in parallel without needing a dedicated infra&#x2F;DevOps team, saving them precious time and resources.<p>Argonaut lets you set up production-ready infrastructure and customize as you scale. We then let you set up automated deployments of your application in minutes. We offer configurable build-and-deploy pipelines, powered by Dagger and ArgoCD, and deep integration with GitHub Actions and GitLab CI.  In addition, we support container registries, multiple cloud accounts, observability stacks, cost visibility providers, CDNs, and the entire helm chart ecosystem of Kubernetes, with more integrations on the way.<p>Key features include: (1) easily create environments encapsulating cloud infrastructure, applications, and deployment pipelines (2) autoscaling deployments for apps and cronjobs to GCP and AWS with a progression across environments (3) compose deployments across multiple environments with our visual pipeline builder (4) get cost estimates before making infra changes, giving you a clear understanding of your expenses; (5) managed Terraform state and pre-built modules that just work, fostering team collaboration on infrastructure.<p>Argonaut is self-serve, so you can sign up and start using the product right away: <a href=\"https:&#x2F;&#x2F;ship.argonaut.dev\">https:&#x2F;&#x2F;ship.argonaut.dev</a>. There is a free tier that doesn&#x27;t require a credit card to get started. We&#x27;d be delighted to have you try it, and are happy to help with onboarding.<p>If your teams work with AWS, GCP, or Kubernetes, I\u2019d love to hear about your experiences, pain points, and what you think a product like Argonaut should be able to do. Looking forward to your comments!", "title": "Launch HN: Argonaut (YC S21) \u2013 Easily Deploy Apps and Infra to AWS and GCP", "updated_at": "2025-04-29T16:07:26Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "BigRedEye"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hey HN! We are happy to share Perforator \u2013 our internal cluster-wide profiler with great support for native languages and a built-in AutoFDO pipeline to simplify sPGO builds. Perforator allows you to profile most binaries without having to recompile or adjust the build process. We use it at Yandex to profile each pod inside a large cluster at modest speed (99Hz), collecting petabytes of profiles every day.<p>There's a blog post about it at <a href=\"https://medium.com/yandex/yandexs-high-performance-profiler-is-now-open-source-95e291df9d18\" rel=\"nofollow\">https://medium.com/yandex/yandexs-high-performance-profiler-...</a>.<p>Inspired by Google-Wide Profiling, we started continuous profiling years ago with simple tools like poormansprofiler.org. With the rise of eBPF, we came up with a simple and elegant solution providing detailed profiles without noticeable overhead. Pretty wild when you can see the guts of your <em>production</em> binaries in a flamegraph without them even noticing.<p>Some technical details:<p>- Our main contribution is infrastructure for continuous PGO using AutoFDO. Google and Meta have done tremendous work on building PGO infrastructure, and we made the last missing piece of the puzzle to make this work well and scalable.<p>- Native binaries are profiled through eh_frame analysis, interpreted/JIT-compiled languages are profiled through perf-pid.map or hardcoded structure offsets.<p>- We render profiles in multiple ways, the most common one is a fast implementation of FlameGraphs, rendering 1M frames in 100ms.<p>- We provide <em>Helm</em> charts to easily deploy Perforator on your k8s cluster.<p>- You can use Perforator in standalone mode as a replacement for perf record.<p>I'd love to answer your questions about the tool!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Perforator \u2013 cluster-wide profiling tool for large data centers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/yandex/perforator"}}, "_tags": ["story", "author_BigRedEye", "story_42896716", "show_hn"], "author": "BigRedEye", "children": [42888712, 42888979, 42897393, 42900026, 42914005, 42920585], "created_at": "2025-02-01T08:00:34Z", "created_at_i": 1738396834, "num_comments": 13, "objectID": "42896716", "points": 78, "story_id": 42896716, "story_text": "Hey HN! We are happy to share Perforator \u2013 our internal cluster-wide profiler with great support for native languages and a built-in AutoFDO pipeline to simplify sPGO builds. Perforator allows you to profile most binaries without having to recompile or adjust the build process. We use it at Yandex to profile each pod inside a large cluster at modest speed (99Hz), collecting petabytes of profiles every day.<p>There&#x27;s a blog post about it at <a href=\"https:&#x2F;&#x2F;medium.com&#x2F;yandex&#x2F;yandexs-high-performance-profiler-is-now-open-source-95e291df9d18\" rel=\"nofollow\">https:&#x2F;&#x2F;medium.com&#x2F;yandex&#x2F;yandexs-high-performance-profiler-...</a>.<p>Inspired by Google-Wide Profiling, we started continuous profiling years ago with simple tools like poormansprofiler.org. With the rise of eBPF, we came up with a simple and elegant solution providing detailed profiles without noticeable overhead. Pretty wild when you can see the guts of your production binaries in a flamegraph without them even noticing.<p>Some technical details:<p>- Our main contribution is infrastructure for continuous PGO using AutoFDO. Google and Meta have done tremendous work on building PGO infrastructure, and we made the last missing piece of the puzzle to make this work well and scalable.<p>- Native binaries are profiled through eh_frame analysis, interpreted&#x2F;JIT-compiled languages are profiled through perf-pid.map or hardcoded structure offsets.<p>- We render profiles in multiple ways, the most common one is a fast implementation of FlameGraphs, rendering 1M frames in 100ms.<p>- We provide Helm charts to easily deploy Perforator on your k8s cluster.<p>- You can use Perforator in standalone mode as a replacement for perf record.<p>I&#x27;d love to answer your questions about the tool!", "title": "Show HN: Perforator \u2013 cluster-wide profiling tool for large data centers", "updated_at": "2026-01-25T05:05:03Z", "url": "https://github.com/yandex/perforator"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "empath-nirvana"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "I built this kubernetes operator as a proof of concept this weekend..  It only has a single required item in the spec, a freeform description field.  The operator will use chatgpt to generate a spec, then immediately apply it to the cluster.  It makes some attempt to correct errors if there's a problem with the syntax.  It will leave additional comments, questions or instructions in the status field of the object.  I built this in a weekend and it's still quite unrefined.  It's in no way <em>production</em> ready, please don't use it for anything real, but it works better than you would think, considering how simple it is.  If you're going to use it, run it on a local cluster like 'kind'.<p>Some descriptions to try:<p>* install a redis namespace with a redis cluster and a service in it\n* create an argocd application in the argocd namespace to install velero.\n* write a python script that lists all ec2 instances in us-east-1, and run it as a k8s job with the aws credentials already saved in the default namespace..<p>a somewhat longer description that also worked:\ngiven the following spec:\n    ---\n    kind: MagicHappens\n    apiVersion: gptmagic.io/v1\n    metadata:\n      name: foo\n    spec:\n      description: this is a freeform description field that will be sent to chatgpt to generate kubernetes resources\n      dryRun: false\n    ---\n    Can you create more magic happens resources, each of which describes an argocd application that needs to be created to install a <em>helm</em> chart for one of the standard cluster addons that need to be installed on a cluster for it to be <em>production</em> ready.  The description should be be freeform text like the following: &quot;Create an argocd application in the argocd namespace to install istio from the <em>helm</em> chart with all the defaults&quot; or &quot;Create an argocd application in the argocd namespace to install prometheus and grafana, with an ingress enabled for grafana&quot;. Be very thorough and included as many apps that might be needed for a prod ready cluster using industry standard CNCF projects if possible.<p>(this produces a list of additional resources for the operator, which the operator then goes on to create argocd applications for -- it also left comments with instructions on one of the resources for how configure it to work with your cloud provider<p>something to note is that since you can run arbitrary containers with arbitrary commands, and chatgpt can write arbitrary code, you don't have to limit yourself to k8s stuff.. if you've got saas credentials on the cluster, you can just tell it to run a python script as a job to do whatever you want.<p>Since most people are cowards, there's a dryRun field that defaults to true, so it only attaches the spec to the object.<p>It is <i>scary</i> how well this works."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Magic Happens \u2013 let ChatGPT manage your Kubernetes cluster"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/empath-nirvana/magic-happens"}}, "_tags": ["story", "author_empath-nirvana", "story_35604715", "show_hn"], "author": "empath-nirvana", "children": [35605883, 35605948, 35606084, 35606114, 35606151, 35606176, 35606190, 35606233, 35606750], "created_at": "2023-04-17T18:39:22Z", "created_at_i": 1681756762, "num_comments": 32, "objectID": "35604715", "points": 48, "story_id": 35604715, "story_text": "I built this kubernetes operator as a proof of concept this weekend..  It only has a single required item in the spec, a freeform description field.  The operator will use chatgpt to generate a spec, then immediately apply it to the cluster.  It makes some attempt to correct errors if there&#x27;s a problem with the syntax.  It will leave additional comments, questions or instructions in the status field of the object.  I built this in a weekend and it&#x27;s still quite unrefined.  It&#x27;s in no way production ready, please don&#x27;t use it for anything real, but it works better than you would think, considering how simple it is.  If you&#x27;re going to use it, run it on a local cluster like &#x27;kind&#x27;.<p>Some descriptions to try:<p>* install a redis namespace with a redis cluster and a service in it\n* create an argocd application in the argocd namespace to install velero.\n* write a python script that lists all ec2 instances in us-east-1, and run it as a k8s job with the aws credentials already saved in the default namespace..<p>a somewhat longer description that also worked:\ngiven the following spec:\n    ---\n    kind: MagicHappens\n    apiVersion: gptmagic.io&#x2F;v1\n    metadata:\n      name: foo\n    spec:\n      description: this is a freeform description field that will be sent to chatgpt to generate kubernetes resources\n      dryRun: false\n    ---\n    Can you create more magic happens resources, each of which describes an argocd application that needs to be created to install a helm chart for one of the standard cluster addons that need to be installed on a cluster for it to be production ready.  The description should be be freeform text like the following: &quot;Create an argocd application in the argocd namespace to install istio from the helm chart with all the defaults&quot; or &quot;Create an argocd application in the argocd namespace to install prometheus and grafana, with an ingress enabled for grafana&quot;. Be very thorough and included as many apps that might be needed for a prod ready cluster using industry standard CNCF projects if possible.<p>(this produces a list of additional resources for the operator, which the operator then goes on to create argocd applications for -- it also left comments with instructions on one of the resources for how configure it to work with your cloud provider<p>something to note is that since you can run arbitrary containers with arbitrary commands, and chatgpt can write arbitrary code, you don&#x27;t have to limit yourself to k8s stuff.. if you&#x27;ve got saas credentials on the cluster, you can just tell it to run a python script as a job to do whatever you want.<p>Since most people are cowards, there&#x27;s a dryRun field that defaults to true, so it only attaches the spec to the object.<p>It is <i>scary</i> how well this works.", "title": "Show HN: Magic Happens \u2013 let ChatGPT manage your Kubernetes cluster", "updated_at": "2024-09-20T13:45:32Z", "url": "https://github.com/empath-nirvana/magic-happens"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justintorre75"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["helm", "production"], "value": "Hey HN, we're Justin and Cole, the founders of Helicone (<a href=\"https://helicone.ai\">https://helicone.ai</a>). Helicone is an open-source platform that helps teams build better LLM applications through a complete development lifecycle of logging, evaluation, experimentation, and release.<p>You can try our free demo by signing up (<a href=\"https://helicone.ai/signup\">https://helicone.ai/signup</a>) or self-deploy with our new fully open-source <em>helm</em> chart (<a href=\"https://helicone.ai/selfhost\">https://helicone.ai/selfhost</a>).<p>When we first launched 22 months ago, we focused on providing visibility into LLM applications. With just a single line of code, teams could trace requests and responses, track token usage, and debug <em>production</em> issues. That simple integration has since processed over 2.1B requests and 2.6T tokens, working with teams ranging from startups to Fortune 500 companies.<p>However, as we scaled and our customers matured, it became clear that logging alone wasn\u2019t enough to manage <em>production</em>-grade applications.<p>Teams like Cursor and V0 have shown what peak AI application performance looks like and it's our goal to help teams achieve that quality. From speaking with users, we realized our platform was missing the necessary tools to create an iterative improvement loop - prompt management, evaluations, and experimentation.<p>Helicone V1: Log \u2192 Review \u2192 Release (Hope it works)<p>From talking with our users, we noticed a pattern: while many successfully launch their MVP quickly, the teams that achieve peak performance take a systematic approach to improvement. They identify inconsistent behaviors through evaluation, experiment methodically with prompts, and measure the impact of each change. This observation shaped our new workflow:<p>Helicone V2: Log \u2192 Evaluate \u2192 Experiment \u2192 Review \u2192 Release<p>It begins with comprehensive logging, capturing the entire context of an LLM application. Not just prompts and responses, but variables, chain steps, embeddings, tool calls, and vector DB interactions (<a href=\"https://docs.helicone.ai/features/sessions\">https://docs.helicone.ai/features/sessions</a>).<p>Yet even with detailed traces, probabilistic systems are notoriously hard to debug at scale. So, we released evaluators (either via LLM-as-judge or custom Python evaluators leveraging the CodeSandbox SDK - <a href=\"https://codesandbox.io/docs/sdk/sandboxes\" rel=\"nofollow\">https://codesandbox.io/docs/sdk/sandboxes</a>).<p>From there, our users were able to more easily monitor performance and investigate what went wrong. Did the embedding search return poor results? Did a tool call fail? Did the prompt mishandle an edge case?<p>But teams would still edit prompts in a playground, run a few test cases, and deploy based on intuition. This lacked the systematic testing we\u2019re used to in traditional software development. That\u2019s why we built experiments (similar to Anthropic's workbench but model-agnostic) (<a href=\"https://docs.helicone.ai/features/experiments\">https://docs.helicone.ai/features/experiments</a>).<p>For instance, when a prompt generates occasional rude support responses, you can test prompt variations against historical conversations. Each variant runs through your <em>production</em> evaluators, measuring real improvement before deployment.<p>Once deployed, the cycle begins again.<p>We recognize that Helicone can\u2019t solve all of the problems you might face when building an LLM application, but we hope that we can help you bring a better product to your customers through our new workflow.<p>If you're curious how our infrastructure handled our growth:<p>Our initial architecture struggled - synchronous log processing overwhelmed our database and query times went from milliseconds to minutes. We've completely rebuilt our infrastructure with two key changes: 1) using Kafka to decouple log ingestion from processing, and 2) splitting storage by access pattern across S3, Kafka, and ClickHouse. This was a long journey but resulted in zero data loss and fast query times even at billions of records. You can read about that here: <a href=\"https://upstash.com/blog/implementing-upstash-kafka-with-cloudflare-workers\" rel=\"nofollow\">https://upstash.com/blog/implementing-upstash-kafka-with-clo...</a><p>We'd love your feedback and questions - join us in this HN thread or on Discord (<a href=\"https://discord.gg/2TkeWdXNPQ\" rel=\"nofollow\">https://discord.gg/2TkeWdXNPQ</a>). If you're interested in contributing to what we build next, check out our GitHub."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Helicone (YC W23) \u2013 OSS LLM Observability and Development Platform"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Helicone/helicone"}}, "_tags": ["story", "author_justintorre75", "story_42806254", "show_hn"], "author": "justintorre75", "children": [42806552, 42807032, 42807448, 42808975, 42815368, 42815985, 42821779, 42854752], "created_at": "2025-01-23T17:58:41Z", "created_at_i": 1737655121, "num_comments": 7, "objectID": "42806254", "points": 29, "story_id": 42806254, "story_text": "Hey HN, we&#x27;re Justin and Cole, the founders of Helicone (<a href=\"https:&#x2F;&#x2F;helicone.ai\">https:&#x2F;&#x2F;helicone.ai</a>). Helicone is an open-source platform that helps teams build better LLM applications through a complete development lifecycle of logging, evaluation, experimentation, and release.<p>You can try our free demo by signing up (<a href=\"https:&#x2F;&#x2F;helicone.ai&#x2F;signup\">https:&#x2F;&#x2F;helicone.ai&#x2F;signup</a>) or self-deploy with our new fully open-source helm chart (<a href=\"https:&#x2F;&#x2F;helicone.ai&#x2F;selfhost\">https:&#x2F;&#x2F;helicone.ai&#x2F;selfhost</a>).<p>When we first launched 22 months ago, we focused on providing visibility into LLM applications. With just a single line of code, teams could trace requests and responses, track token usage, and debug production issues. That simple integration has since processed over 2.1B requests and 2.6T tokens, working with teams ranging from startups to Fortune 500 companies.<p>However, as we scaled and our customers matured, it became clear that logging alone wasn\u2019t enough to manage production-grade applications.<p>Teams like Cursor and V0 have shown what peak AI application performance looks like and it&#x27;s our goal to help teams achieve that quality. From speaking with users, we realized our platform was missing the necessary tools to create an iterative improvement loop - prompt management, evaluations, and experimentation.<p>Helicone V1: Log \u2192 Review \u2192 Release (Hope it works)<p>From talking with our users, we noticed a pattern: while many successfully launch their MVP quickly, the teams that achieve peak performance take a systematic approach to improvement. They identify inconsistent behaviors through evaluation, experiment methodically with prompts, and measure the impact of each change. This observation shaped our new workflow:<p>Helicone V2: Log \u2192 Evaluate \u2192 Experiment \u2192 Review \u2192 Release<p>It begins with comprehensive logging, capturing the entire context of an LLM application. Not just prompts and responses, but variables, chain steps, embeddings, tool calls, and vector DB interactions (<a href=\"https:&#x2F;&#x2F;docs.helicone.ai&#x2F;features&#x2F;sessions\">https:&#x2F;&#x2F;docs.helicone.ai&#x2F;features&#x2F;sessions</a>).<p>Yet even with detailed traces, probabilistic systems are notoriously hard to debug at scale. So, we released evaluators (either via LLM-as-judge or custom Python evaluators leveraging the CodeSandbox SDK - <a href=\"https:&#x2F;&#x2F;codesandbox.io&#x2F;docs&#x2F;sdk&#x2F;sandboxes\" rel=\"nofollow\">https:&#x2F;&#x2F;codesandbox.io&#x2F;docs&#x2F;sdk&#x2F;sandboxes</a>).<p>From there, our users were able to more easily monitor performance and investigate what went wrong. Did the embedding search return poor results? Did a tool call fail? Did the prompt mishandle an edge case?<p>But teams would still edit prompts in a playground, run a few test cases, and deploy based on intuition. This lacked the systematic testing we\u2019re used to in traditional software development. That\u2019s why we built experiments (similar to Anthropic&#x27;s workbench but model-agnostic) (<a href=\"https:&#x2F;&#x2F;docs.helicone.ai&#x2F;features&#x2F;experiments\">https:&#x2F;&#x2F;docs.helicone.ai&#x2F;features&#x2F;experiments</a>).<p>For instance, when a prompt generates occasional rude support responses, you can test prompt variations against historical conversations. Each variant runs through your production evaluators, measuring real improvement before deployment.<p>Once deployed, the cycle begins again.<p>We recognize that Helicone can\u2019t solve all of the problems you might face when building an LLM application, but we hope that we can help you bring a better product to your customers through our new workflow.<p>If you&#x27;re curious how our infrastructure handled our growth:<p>Our initial architecture struggled - synchronous log processing overwhelmed our database and query times went from milliseconds to minutes. We&#x27;ve completely rebuilt our infrastructure with two key changes: 1) using Kafka to decouple log ingestion from processing, and 2) splitting storage by access pattern across S3, Kafka, and ClickHouse. This was a long journey but resulted in zero data loss and fast query times even at billions of records. You can read about that here: <a href=\"https:&#x2F;&#x2F;upstash.com&#x2F;blog&#x2F;implementing-upstash-kafka-with-cloudflare-workers\" rel=\"nofollow\">https:&#x2F;&#x2F;upstash.com&#x2F;blog&#x2F;implementing-upstash-kafka-with-clo...</a><p>We&#x27;d love your feedback and questions - join us in this HN thread or on Discord (<a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;2TkeWdXNPQ\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.gg&#x2F;2TkeWdXNPQ</a>). If you&#x27;re interested in contributing to what we build next, check out our GitHub.", "title": "Show HN: Helicone (YC W23) \u2013 OSS LLM Observability and Development Platform", "updated_at": "2025-02-21T17:27:49Z", "url": "https://github.com/Helicone/helicone"}], "hitsPerPage": 15, "nbHits": 775, "nbPages": 52, "page": 0, "params": "query=helm+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 11, "processingTimingsMS": {"_request": {"roundTrip": 14}, "afterFetch": {"format": {"highlighting": 2, "total": 2}}, "fetch": {"query": 6, "scanning": 3, "total": 10}, "total": 11}, "query": "helm production", "serverTimeMS": 14}}