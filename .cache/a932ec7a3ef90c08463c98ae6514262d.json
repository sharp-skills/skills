{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "chaoxu"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "I was looking for something that aggregates my transaction informations across banks, and output to a database (or a csv, so I can handle it later). The point is to have this programmatically. This can be later used by the accountant.<p>Plaid was the go to tool for this. Unfortunately, many bank require <em>OAuth2</em> now, and using Plaid with <em>Oauth2</em> requires <em>production</em> account and jumping through a lot of hoops.<p>Are there anything else out there? I don't mind paying.<p>Currently, I just think about getting something like Mint, and then get some tool to scrape Mint."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Bank Transactions to CSV"}}, "_tags": ["story", "author_chaoxu", "story_36223093", "ask_hn"], "author": "chaoxu", "created_at": "2023-06-07T05:09:58Z", "created_at_i": 1686114598, "num_comments": 0, "objectID": "36223093", "points": 2, "story_id": 36223093, "story_text": "I was looking for something that aggregates my transaction informations across banks, and output to a database (or a csv, so I can handle it later). The point is to have this programmatically. This can be later used by the accountant.<p>Plaid was the go to tool for this. Unfortunately, many bank require OAuth2 now, and using Plaid with Oauth2 requires production account and jumping through a lot of hoops.<p>Are there anything else out there? I don&#x27;t mind paying.<p>Currently, I just think about getting something like Mint, and then get some tool to scrape Mint.", "title": "Ask HN: Bank Transactions to CSV", "updated_at": "2024-09-20T14:11:58Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mooreds"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "Top <em>OAuth2</em>/OpenID Connect Mistakes Found in <em>Production</em> Mobile Apps [video]"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.youtube.com/watch?v=St1DnYPgE70"}}, "_tags": ["story", "author_mooreds", "story_32982137"], "author": "mooreds", "created_at": "2022-09-26T12:11:42Z", "created_at_i": 1664194302, "num_comments": 0, "objectID": "32982137", "points": 2, "story_id": 32982137, "title": "Top OAuth2/OpenID Connect Mistakes Found in Production Mobile Apps [video]", "updated_at": "2024-09-20T12:08:46Z", "url": "https://www.youtube.com/watch?v=St1DnYPgE70"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "albator39"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "I spent 4 years trying to build this OAuth server but never finished it.<p>Then I discovered agentic coding and shipped it in 3 weeks.<p>What makes it different:<p>\u2022 Dual AI agents analyze every login in &lt;300ms\n  - Security Signals Agent: risk scoring (device, IP, geo, velocity)\n  - Policy Compliance Agent: business rules (MFA policies, role enforcement)\n  - Combined decision: allow/log/step-up/lock/deny<p>\u2022 <em>Production</em>-ready security\n  - PKCE (RFC 7636), DPoP (RFC 9449)\n  - MFA (TOTP + WebAuthn/Passkeys)\n  - IP restrictions, rate limiting, audit trail<p>\u2022 EU digital sovereignty\n  - GDPR native (data export, legal holds, retention policies)\n  - EU hosting, no US Cloud Act exposure\n  - Full audit trail (PostgreSQL + Redis Streams)<p>\u2022 Zero AI dependency\n  - Deterministic fallback if AI timeouts\n  - Conservative MEDIUM risk returned (safe default)\n  - System keeps running without external LLM calls<p>\u2022 Modern stack\n  - Backend: NestJS + TypeScript, LangChain/LangGraph\n  - Frontend: React 19, hexagonal architecture, 91% test coverage\n  - Deterministic fallback if AI timeouts (zero dependency)<p>Built as an alternative to Firebase Auth / AWS Cognito / Auth0 for companies that want control over their authentication infrastructure.<p>Architecture diagrams and screenshots in the repo.<p>Open to feedback and questions."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["oauth2"], "value": "Show HN: <em>OAuth 2</em>.0 server with AI security agents (EU sovereign alternative)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/devon39/server-oauth-security"}}, "_tags": ["story", "author_albator39", "story_46867821", "show_hn"], "author": "albator39", "created_at": "2026-02-03T07:44:41Z", "created_at_i": 1770104681, "num_comments": 0, "objectID": "46867821", "points": 2, "story_id": 46867821, "story_text": "I spent 4 years trying to build this OAuth server but never finished it.<p>Then I discovered agentic coding and shipped it in 3 weeks.<p>What makes it different:<p>\u2022 Dual AI agents analyze every login in &lt;300ms\n  - Security Signals Agent: risk scoring (device, IP, geo, velocity)\n  - Policy Compliance Agent: business rules (MFA policies, role enforcement)\n  - Combined decision: allow&#x2F;log&#x2F;step-up&#x2F;lock&#x2F;deny<p>\u2022 Production-ready security\n  - PKCE (RFC 7636), DPoP (RFC 9449)\n  - MFA (TOTP + WebAuthn&#x2F;Passkeys)\n  - IP restrictions, rate limiting, audit trail<p>\u2022 EU digital sovereignty\n  - GDPR native (data export, legal holds, retention policies)\n  - EU hosting, no US Cloud Act exposure\n  - Full audit trail (PostgreSQL + Redis Streams)<p>\u2022 Zero AI dependency\n  - Deterministic fallback if AI timeouts\n  - Conservative MEDIUM risk returned (safe default)\n  - System keeps running without external LLM calls<p>\u2022 Modern stack\n  - Backend: NestJS + TypeScript, LangChain&#x2F;LangGraph\n  - Frontend: React 19, hexagonal architecture, 91% test coverage\n  - Deterministic fallback if AI timeouts (zero dependency)<p>Built as an alternative to Firebase Auth &#x2F; AWS Cognito &#x2F; Auth0 for companies that want control over their authentication infrastructure.<p>Architecture diagrams and screenshots in the repo.<p>Open to feedback and questions.", "title": "Show HN: OAuth 2.0 server with AI security agents (EU sovereign alternative)", "updated_at": "2026-02-04T08:54:40Z", "url": "https://github.com/devon39/server-oauth-security"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "levish"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "Axum template with SeaORM, Redis sessions, <em>OAuth2</em>, NATS background jobs, MeiliSearch, E2E tests, and Docker CI/CD. Aimed at getting <em>production</em>-ready faster."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: AxumKit \u2013 <em>Production</em>-ready Rust/Axum web API template"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/levish0/AxumKit"}}, "_tags": ["story", "author_levish", "story_46764652", "show_hn"], "author": "levish", "created_at": "2026-01-26T12:05:53Z", "created_at_i": 1769429153, "num_comments": 0, "objectID": "46764652", "points": 2, "story_id": 46764652, "story_text": "Axum template with SeaORM, Redis sessions, OAuth2, NATS background jobs, MeiliSearch, E2E tests, and Docker CI&#x2F;CD. Aimed at getting production-ready faster.", "title": "Show HN: AxumKit \u2013 Production-ready Rust/Axum web API template", "updated_at": "2026-01-26T12:11:38Z", "url": "https://github.com/levish0/AxumKit"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "cdelmonte"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "I built this because I was tired of configuring Keycloak for local development and CI/CD testing. NanoIDP is a pip-installable IdP that supports <em>OAuth2</em>/OIDC (Authorization Code + PKCE, Client Credentials, Password, Device Flow) and SAML 2.0.<p>Setup is just `pip install nanoidp &amp;&amp; python -m nanoidp`. Configuration lives in YAML files, no database required. It also has a web UI for managing users and clients, and token introspection/revocation endpoints.<p>It's a dev/testing tool, not meant for <em>production</em>. MIT licensed.<p>Blog post with more context: <a href=\"https://cdelmonte.medium.com/stop-spinning-up-keycloak-just-to-test-oauth-818d88bb07d0\" rel=\"nofollow\">https://cdelmonte.medium.com/stop-spinning-up-keycloak-just-...</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["oauth2"], "value": "Show HN: A lightweight Identity Provider for local <em>OAuth2</em>/SAML testing"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/cdelmonte-zg/nanoidp"}}, "_tags": ["story", "author_cdelmonte", "story_46987585", "show_hn"], "author": "cdelmonte", "created_at": "2026-02-12T11:49:20Z", "created_at_i": 1770896960, "num_comments": 0, "objectID": "46987585", "points": 1, "story_id": 46987585, "story_text": "I built this because I was tired of configuring Keycloak for local development and CI&#x2F;CD testing. NanoIDP is a pip-installable IdP that supports OAuth2&#x2F;OIDC (Authorization Code + PKCE, Client Credentials, Password, Device Flow) and SAML 2.0.<p>Setup is just `pip install nanoidp &amp;&amp; python -m nanoidp`. Configuration lives in YAML files, no database required. It also has a web UI for managing users and clients, and token introspection&#x2F;revocation endpoints.<p>It&#x27;s a dev&#x2F;testing tool, not meant for production. MIT licensed.<p>Blog post with more context: <a href=\"https:&#x2F;&#x2F;cdelmonte.medium.com&#x2F;stop-spinning-up-keycloak-just-to-test-oauth-818d88bb07d0\" rel=\"nofollow\">https:&#x2F;&#x2F;cdelmonte.medium.com&#x2F;stop-spinning-up-keycloak-just-...</a>", "title": "Show HN: A lightweight Identity Provider for local OAuth2/SAML testing", "updated_at": "2026-02-12T11:54:46Z", "url": "https://github.com/cdelmonte-zg/nanoidp"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "soaringmonchi"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "Hi HN<p>We built YASP because the existing free/open-source status pages were either dated, ugly, or too restrictive (no custom Twilio/SMTP, rigid notifications, limited customization).<p>YASP is a modern, self-hosted status page built with Next.js + Payload CMS, and it\u2019s fully open source (MIT).<p>What makes it different:<p>- Full CMS flexibility via Payload \u2013 customize almost everything without touching code<p>- Review &amp; edit every notification (SMS/email) before it\u2019s sent<p>- Bring your own Twilio and SMTP accounts<p>- <em>OAuth2</em> / OIDC capable<p>- Beautiful dark &amp; light themes out of the box<p>- Incident updates, maintenance windows, service groups, subscriber management<p>- One-click deploy on Vercel, or self-host anywhere<p>Tech stack: Next.js, TypeScript, Tailwind, Payload CMS\nOpen source &amp; free forever<p>Would love feedback from folks who\u2019ve run status pages in <em>production</em>."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: YASP \u2013 Open-source, modern status page with full CMS control"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://yasp.io"}}, "_tags": ["story", "author_soaringmonchi", "story_46617114", "show_hn"], "author": "soaringmonchi", "created_at": "2026-01-14T15:27:12Z", "created_at_i": 1768404432, "num_comments": 0, "objectID": "46617114", "points": 14, "story_id": 46617114, "story_text": "Hi HN<p>We built YASP because the existing free&#x2F;open-source status pages were either dated, ugly, or too restrictive (no custom Twilio&#x2F;SMTP, rigid notifications, limited customization).<p>YASP is a modern, self-hosted status page built with Next.js + Payload CMS, and it\u2019s fully open source (MIT).<p>What makes it different:<p>- Full CMS flexibility via Payload \u2013 customize almost everything without touching code<p>- Review &amp; edit every notification (SMS&#x2F;email) before it\u2019s sent<p>- Bring your own Twilio and SMTP accounts<p>- OAuth2 &#x2F; OIDC capable<p>- Beautiful dark &amp; light themes out of the box<p>- Incident updates, maintenance windows, service groups, subscriber management<p>- One-click deploy on Vercel, or self-host anywhere<p>Tech stack: Next.js, TypeScript, Tailwind, Payload CMS\nOpen source &amp; free forever<p>Would love feedback from folks who\u2019ve run status pages in production.", "title": "Show HN: YASP \u2013 Open-source, modern status page with full CMS control", "updated_at": "2026-01-16T02:15:00Z", "url": "https://yasp.io"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "NBenkovich"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "Hi HN,<p>I\u2019m working on AI agents used for software development. These agents automatically spin up short-lived app instances \u2013 for example per pull request, per task, or per experiment \u2013 each with its own temporary URL.<p>Auth is handled in the standard way:<p>- <em>OAuth2</em> / OIDC<p>- external identity provider<p>- redirect URLs must be registered in advance and be static<p>This clashes badly with short-lived apps:<p>- URLs are dynamic and unpredictable<p>- redirect URLs can\u2019t realistically be pre-registered<p>- auth becomes the only non-ephemeral part of an otherwise fully automated workflow<p>What I see teams doing instead:<p>- disabling real auth in preview environments<p>- routing all callbacks through a single stable environment<p>- using wildcard redirects or proxy setups that feel like hacks<p>This gets especially awkward for AI dev agents, because they assume infrastructure is disposable and fully automated \u2013 no manual IdP config in the loop.<p>So I\u2019m curious:<p>1. If you use short-lived preview apps, how do you handle real auth?<p>2. Are there clean OAuth/OIDC patterns that work with dynamic URLs?<p>3. Is the static redirect URL assumption still the right model here?<p>4. What actually works in <em>production</em>?<p>Looking for real setups and failure stories, not theory."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: How do you handle auth when AI dev agents spin up short-lived apps?"}}, "_tags": ["story", "author_NBenkovich", "story_46846055", "ask_hn"], "author": "NBenkovich", "children": [46849393, 46851653, 46856034, 46856512, 46884169], "created_at": "2026-02-01T13:25:08Z", "created_at_i": 1769952308, "num_comments": 9, "objectID": "46846055", "points": 5, "story_id": 46846055, "story_text": "Hi HN,<p>I\u2019m working on AI agents used for software development. These agents automatically spin up short-lived app instances \u2013 for example per pull request, per task, or per experiment \u2013 each with its own temporary URL.<p>Auth is handled in the standard way:<p>- OAuth2 &#x2F; OIDC<p>- external identity provider<p>- redirect URLs must be registered in advance and be static<p>This clashes badly with short-lived apps:<p>- URLs are dynamic and unpredictable<p>- redirect URLs can\u2019t realistically be pre-registered<p>- auth becomes the only non-ephemeral part of an otherwise fully automated workflow<p>What I see teams doing instead:<p>- disabling real auth in preview environments<p>- routing all callbacks through a single stable environment<p>- using wildcard redirects or proxy setups that feel like hacks<p>This gets especially awkward for AI dev agents, because they assume infrastructure is disposable and fully automated \u2013 no manual IdP config in the loop.<p>So I\u2019m curious:<p>1. If you use short-lived preview apps, how do you handle real auth?<p>2. Are there clean OAuth&#x2F;OIDC patterns that work with dynamic URLs?<p>3. Is the static redirect URL assumption still the right model here?<p>4. What actually works in production?<p>Looking for real setups and failure stories, not theory.", "title": "Ask HN: How do you handle auth when AI dev agents spin up short-lived apps?", "updated_at": "2026-02-04T10:45:10Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "blotato"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "Hello HN, I\u2019m the founder of blotato - turn existing scripts into <em>production</em> grade apps in minutes, without changing your code!<p>Built in rust (API backend, CLI, executors) and react (frontend).<p>The lineage of most scripts is someone asking a developer for something: can you check these messages, pull data for a report, update customer subscriptions\u2026 you throw a script together, then the script gets used more and more, needs more functionality, more people use it. The script must become \u201c<em>production</em> grade\u201d.<p>Other tools force you to migrate your script to conform to their library and paradigm.  \nWith blotato, add a script.yaml file describing your script\u2019s inputs - it works! No limitations on libraries you can use or how to handle certain logic.<p>I built blotato because I want to focus on writing core logic, not all the other stuff. Blotato fills the void between writing a script and ensuring the script is useful to business users, minimizing the work turning scripts into <em>production</em>-grade apps, while retaining the benefits of scalable infrastructure, access controls, logs, etc... so you can focus on the script\u2019s core logic.<p>Blotato is best for use cases involving:<p>- multiple APIs to connect to<p>- non-trivial data transformations<p>- <em>oauth2</em> integrations<p>- loops<p>- libraries<p>- \u201cI have an existing script that already works and want to put it to work asap\u201d.<p>Streamlined developer-first experience is the differentiator:<p>1. You have an existing script. No need to migrate your script or integrate a library.<p>2. Write script.yaml that tells blotato what arguments your script expects, then blotato autogenerates UI and runs the script on a highly available distributed infrastructure. No need to drag-and-drop UI, no need to write react views, no need to deploy manually on AWS lambda.<p>3. Blotato provides instant scalable infrastructure, execution logs, access controls, secrets management, and one-click <em>oauth2</em> integrations without setting up <em>oauth2</em> apps (zapier-style).<p>I built the backend, CLI, and executors in rust and the frontend in react.<p>Between deno and firecracker, rust is great for creating sandboxed isolated environments for running arbitrary user code. \u201cIf it compiles it works\u201d - the language enforces strict rules so I don\u2019t shoot myself in the foot. I want to protect customers from myself - so that their workflows won\u2019t suddenly stop running due to an uncaught runtime error. This doesn\u2019t prevent \u201clogic\u201d errors, but it does prevent a fairly substantial amount of errors; and yes, I use strict: true in our tsconfigs on the frontend.<p>Rust's rich type system complements the point above re: error prevention, making it easy to create unrepresentable workflow states that are caught at compile time. Also less memory and CPU hogging so that the resources can be actually spent on running user workflows. Executors, CLI, and the API backend are all written in rust and promote code sharing without explicit codegen / protobuf-like solutions. Sharing code with frontend is also easy (even without wasm by using macros like ts-rs). Compile time SQL queries are awesome (sqlx). Trusting the `query!` macro have saved me many bugs.<p>Coolest thing I\u2019ve implemented is the custom type system with user-defined types that could be (mostly) inferred from existing scripts. There are poor-man\u2019s dependent types that are resolved at runtime. For example, you have a google sheets integration. You have a spreadsheet id and a sheet id. With a bit of handwaving, you get a strongly typed sheet id that is a type only valid for a given spreadsheet (because a sheet id is only valid in a particular spreadsheet), so we get type-level validations that your script is indeed using a sheet id from the correct spreadsheet.<p>To try out blotato, check out the website (<a href=\"https://blotato.com\" rel=\"nofollow noreferrer\">https://blotato.com</a>) and docs (<a href=\"https://docs.blotato.com\" rel=\"nofollow noreferrer\">https://docs.blotato.com</a>)."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Blotato \u2013 turn scripts into apps without changing code (built in Rust)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.blotato.com/"}}, "_tags": ["story", "author_blotato", "story_38206235", "show_hn"], "author": "blotato", "children": [38212937, 38214051], "created_at": "2023-11-09T15:29:46Z", "created_at_i": 1699543786, "num_comments": 4, "objectID": "38206235", "points": 4, "story_id": 38206235, "story_text": "Hello HN, I\u2019m the founder of blotato - turn existing scripts into production grade apps in minutes, without changing your code!<p>Built in rust (API backend, CLI, executors) and react (frontend).<p>The lineage of most scripts is someone asking a developer for something: can you check these messages, pull data for a report, update customer subscriptions\u2026 you throw a script together, then the script gets used more and more, needs more functionality, more people use it. The script must become \u201cproduction grade\u201d.<p>Other tools force you to migrate your script to conform to their library and paradigm.  \nWith blotato, add a script.yaml file describing your script\u2019s inputs - it works! No limitations on libraries you can use or how to handle certain logic.<p>I built blotato because I want to focus on writing core logic, not all the other stuff. Blotato fills the void between writing a script and ensuring the script is useful to business users, minimizing the work turning scripts into production-grade apps, while retaining the benefits of scalable infrastructure, access controls, logs, etc... so you can focus on the script\u2019s core logic.<p>Blotato is best for use cases involving:<p>- multiple APIs to connect to<p>- non-trivial data transformations<p>- oauth2 integrations<p>- loops<p>- libraries<p>- \u201cI have an existing script that already works and want to put it to work asap\u201d.<p>Streamlined developer-first experience is the differentiator:<p>1. You have an existing script. No need to migrate your script or integrate a library.<p>2. Write script.yaml that tells blotato what arguments your script expects, then blotato autogenerates UI and runs the script on a highly available distributed infrastructure. No need to drag-and-drop UI, no need to write react views, no need to deploy manually on AWS lambda.<p>3. Blotato provides instant scalable infrastructure, execution logs, access controls, secrets management, and one-click oauth2 integrations without setting up oauth2 apps (zapier-style).<p>I built the backend, CLI, and executors in rust and the frontend in react.<p>Between deno and firecracker, rust is great for creating sandboxed isolated environments for running arbitrary user code. \u201cIf it compiles it works\u201d - the language enforces strict rules so I don\u2019t shoot myself in the foot. I want to protect customers from myself - so that their workflows won\u2019t suddenly stop running due to an uncaught runtime error. This doesn\u2019t prevent \u201clogic\u201d errors, but it does prevent a fairly substantial amount of errors; and yes, I use strict: true in our tsconfigs on the frontend.<p>Rust&#x27;s rich type system complements the point above re: error prevention, making it easy to create unrepresentable workflow states that are caught at compile time. Also less memory and CPU hogging so that the resources can be actually spent on running user workflows. Executors, CLI, and the API backend are all written in rust and promote code sharing without explicit codegen &#x2F; protobuf-like solutions. Sharing code with frontend is also easy (even without wasm by using macros like ts-rs). Compile time SQL queries are awesome (sqlx). Trusting the `query!` macro have saved me many bugs.<p>Coolest thing I\u2019ve implemented is the custom type system with user-defined types that could be (mostly) inferred from existing scripts. There are poor-man\u2019s dependent types that are resolved at runtime. For example, you have a google sheets integration. You have a spreadsheet id and a sheet id. With a bit of handwaving, you get a strongly typed sheet id that is a type only valid for a given spreadsheet (because a sheet id is only valid in a particular spreadsheet), so we get type-level validations that your script is indeed using a sheet id from the correct spreadsheet.<p>To try out blotato, check out the website (<a href=\"https:&#x2F;&#x2F;blotato.com\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;blotato.com</a>) and docs (<a href=\"https:&#x2F;&#x2F;docs.blotato.com\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;docs.blotato.com</a>).", "title": "Show HN: Blotato \u2013 turn scripts into apps without changing code (built in Rust)", "updated_at": "2024-09-20T15:33:06Z", "url": "https://www.blotato.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "lukaesch"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "Seven months ago, I shared Audioscrape - a podcast exploration tool built entirely in Rust, running on a $7/month VM. Since then, we've transformed it into a full-fledged podcast intelligence platform, serving over 1,000 users, including PR teams, researchers, and marketers.<p>What's New:<p>Real-Time Monitoring: Track mentions across the top 100 U.S. podcasts, covering over 80% of U.S. listenership.<p>Advanced Search: Filter by speaker, sentiment, timeframe, and topic using AI-powered search.<p>Custom Alerts: Receive notifications for brand, competitor, or topic mentions.<p>API Access: Integrate podcast monitoring data into your workflows.<p>Transcription Accuracy: Achieved 92.2% accuracy across 20,000+ episodes.<p>Technical Stack:<p>Backend: Axum (async web framework)<p>Database: SQLite with SQLx for type-safe queries<p>Authentication: <em>OAuth2</em><p>HTML Templating: Askama<p>Async Runtime: Tokio<p>Our commitment to Rust has enabled us to maintain low operational costs while scaling effectively.<p>Try It Out: <a href=\"https://www.audioscrape.com\" rel=\"nofollow\">https://www.audioscrape.com</a><p>Discussion Points:<p>Has anyone else scaled a Rust-based MVP into a <em>production</em> platform?<p>What strategies have you employed for efficient scaling and user acquisition?<p>Looking forward to your insights and feedback!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Audioscrape \u2013 From $7 Rust MVP to Podcast Intelligence Platform"}}, "_tags": ["story", "author_lukaesch", "story_43918241", "show_hn"], "author": "lukaesch", "children": [43918646], "created_at": "2025-05-07T17:11:33Z", "created_at_i": 1746637893, "num_comments": 2, "objectID": "43918241", "points": 4, "story_id": 43918241, "story_text": "Seven months ago, I shared Audioscrape - a podcast exploration tool built entirely in Rust, running on a $7&#x2F;month VM. Since then, we&#x27;ve transformed it into a full-fledged podcast intelligence platform, serving over 1,000 users, including PR teams, researchers, and marketers.<p>What&#x27;s New:<p>Real-Time Monitoring: Track mentions across the top 100 U.S. podcasts, covering over 80% of U.S. listenership.<p>Advanced Search: Filter by speaker, sentiment, timeframe, and topic using AI-powered search.<p>Custom Alerts: Receive notifications for brand, competitor, or topic mentions.<p>API Access: Integrate podcast monitoring data into your workflows.<p>Transcription Accuracy: Achieved 92.2% accuracy across 20,000+ episodes.<p>Technical Stack:<p>Backend: Axum (async web framework)<p>Database: SQLite with SQLx for type-safe queries<p>Authentication: OAuth2<p>HTML Templating: Askama<p>Async Runtime: Tokio<p>Our commitment to Rust has enabled us to maintain low operational costs while scaling effectively.<p>Try It Out: <a href=\"https:&#x2F;&#x2F;www.audioscrape.com\" rel=\"nofollow\">https:&#x2F;&#x2F;www.audioscrape.com</a><p>Discussion Points:<p>Has anyone else scaled a Rust-based MVP into a production platform?<p>What strategies have you employed for efficient scaling and user acquisition?<p>Looking forward to your insights and feedback!", "title": "Show HN: Audioscrape \u2013 From $7 Rust MVP to Podcast Intelligence Platform", "updated_at": "2026-01-15T20:25:29Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "subramanya1997"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "Hey HN! We're building Agentic Trust, a unified platform that turns your code into <em>production</em>-ready MCP (Model Context Protocol) servers with built-in authentication, security, and observability.<p>*The Problem:*\nAs AI agents become more capable, they need secure ways to access tools and data. MCP (Anthropic's open protocol) is great for standardizing agent-to-tool communication, but deploying MCP servers in <em>production</em> is complex. You need authentication, rate limiting, audit logs, multi-tenancy, and more\u2014all while ensuring your agents can't be exploited through prompt injection or other attacks.<p>*Our Solution:*\nOne endpoint (agentictrust.com) that handles all your MCP servers. You write the tool logic, we handle everything else:\n- <em>OAuth 2</em>.0 authentication with scoped permissions\n- Rate limiting and usage analytics\n- Audit trails for compliance\n- Automatic versioning and routing\n- Protection against prompt injection attacks<p>*Technical Details:*\nWe've also been working on OIDC-A (OpenID Connect for Agents), a proposal to extend OIDC for agent identity. It adds claims for agent attestation, delegation chains, and capabilities. This was recently featured by WorkOS's CEO at Identiverse.<p>The idea is that agents should have verifiable identities just like users do. When an agent acts on behalf of a user, you need to track that delegation chain for security and compliance.<p>*Why Now:*\nWith Microsoft announcing MCP support in Windows 11 and OpenAI adopting the protocol, we're seeing explosive growth in MCP usage. But most implementations are insecure\u2014exposed endpoints, no auth, vulnerable to attacks. We're fixing that.<p>*Links:*\n- Platform: <a href=\"https://agentictrust.com\" rel=\"nofollow\">https://agentictrust.com</a>\n- OIDC-A Proposal: <a href=\"https://subramanya.ai/2025/04/28/oidc-a-proposal/\" rel=\"nofollow\">https://subramanya.ai/2025/04/28/oidc-a-proposal/</a>\n- WorkOS article on our work: <a href=\"https://workos.com/blog/identity-for-ai-agents\" rel=\"nofollow\">https://workos.com/blog/identity-for-ai-agents</a><p>We're in early access and would love feedback from the HN community. What security concerns do you have about AI agents? How are you handling agent authentication today?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Agentic Trust \u2013 Enterprise MCP Server Platform for Secure AI Agents"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://agentictrust.com"}}, "_tags": ["story", "author_subramanya1997", "story_44314195", "show_hn"], "author": "subramanya1997", "created_at": "2025-06-18T23:42:00Z", "created_at_i": 1750290120, "num_comments": 0, "objectID": "44314195", "points": 4, "story_id": 44314195, "story_text": "Hey HN! We&#x27;re building Agentic Trust, a unified platform that turns your code into production-ready MCP (Model Context Protocol) servers with built-in authentication, security, and observability.<p>*The Problem:*\nAs AI agents become more capable, they need secure ways to access tools and data. MCP (Anthropic&#x27;s open protocol) is great for standardizing agent-to-tool communication, but deploying MCP servers in production is complex. You need authentication, rate limiting, audit logs, multi-tenancy, and more\u2014all while ensuring your agents can&#x27;t be exploited through prompt injection or other attacks.<p>*Our Solution:*\nOne endpoint (agentictrust.com) that handles all your MCP servers. You write the tool logic, we handle everything else:\n- OAuth 2.0 authentication with scoped permissions\n- Rate limiting and usage analytics\n- Audit trails for compliance\n- Automatic versioning and routing\n- Protection against prompt injection attacks<p>*Technical Details:*\nWe&#x27;ve also been working on OIDC-A (OpenID Connect for Agents), a proposal to extend OIDC for agent identity. It adds claims for agent attestation, delegation chains, and capabilities. This was recently featured by WorkOS&#x27;s CEO at Identiverse.<p>The idea is that agents should have verifiable identities just like users do. When an agent acts on behalf of a user, you need to track that delegation chain for security and compliance.<p>*Why Now:*\nWith Microsoft announcing MCP support in Windows 11 and OpenAI adopting the protocol, we&#x27;re seeing explosive growth in MCP usage. But most implementations are insecure\u2014exposed endpoints, no auth, vulnerable to attacks. We&#x27;re fixing that.<p>*Links:*\n- Platform: <a href=\"https:&#x2F;&#x2F;agentictrust.com\" rel=\"nofollow\">https:&#x2F;&#x2F;agentictrust.com</a>\n- OIDC-A Proposal: <a href=\"https:&#x2F;&#x2F;subramanya.ai&#x2F;2025&#x2F;04&#x2F;28&#x2F;oidc-a-proposal&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;subramanya.ai&#x2F;2025&#x2F;04&#x2F;28&#x2F;oidc-a-proposal&#x2F;</a>\n- WorkOS article on our work: <a href=\"https:&#x2F;&#x2F;workos.com&#x2F;blog&#x2F;identity-for-ai-agents\" rel=\"nofollow\">https:&#x2F;&#x2F;workos.com&#x2F;blog&#x2F;identity-for-ai-agents</a><p>We&#x27;re in early access and would love feedback from the HN community. What security concerns do you have about AI agents? How are you handling agent authentication today?", "title": "Show HN: Agentic Trust \u2013 Enterprise MCP Server Platform for Secure AI Agents", "updated_at": "2025-06-20T04:36:18Z", "url": "https://agentictrust.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rguldener"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "Nango Auth is a free, source-available implementation of (O)Auth for 250+ APIs.<p>Auth with external APIs is surprisingly difficult. OAuth is a mess[0], many APIs require parameters other than an API key or access token, and platforms like Stripe or GitHub even create their own custom auth protocols.<p>We first launched Nango Auth on HN 1.5 years ago[1] when it only supported 40 OAuth APIs.<p>Since then, it has expanded beyond OAuth into an end-to-end solution for API authentication:<p>- Supports 250+ APIs[2]  and almost a dozen auth modes: <em>OAuth 2</em>.0, OAuth 1.0, API key, Basic auth, and a half-dozen custom formats<p>- Pre-built UI to help users pick &amp; connect the API (including guides to find their API key and other required parameters)<p>- Automatic credentials validation on connect<p>- Secure, encrypted credentials storage &amp; automatic token refresh<p>- Detects expired/broken access tokens<p>- Very detailed logs for quick &amp; easy debugging<p>More than 300 companies use Nango Auth in <em>production</em> for their product integrations: From fast-growing AI startups (Respell, Beam, Levity) to established SaaS players (Typeform, Semgrep, Electric) and even public companies.\nWe also run a Slack Community[3] with 2,000+ engineers building product integrations.<p>Nango Auth is a part of Nango[4], our source-available product integrations platform. While Nango itself is a paid product, our auth product (Nango Auth) is free for unlimited use, forever. It\u2019s our small way of giving back to the community. You can either self-host Nango Auth or use our free cloud option.<p>We hope Nango Auth can be helpful for your next integration, and look forward to your feedback!<p>Demo video: <a href=\"https://www.loom.com/share/e704192eb5ba42479803135db1ceccd8\" rel=\"nofollow\">https://www.loom.com/share/e704192eb5ba42479803135db1ceccd8</a><p>GitHub Repo: <a href=\"https://github.com/NangoHQ/nango\">https://github.com/NangoHQ/nango</a><p>Landing page: <a href=\"https://www.nango.dev/auth\">https://www.nango.dev/auth</a><p>[0]: <a href=\"https://www.nango.dev/blog/why-is-oauth-still-hard\">https://www.nango.dev/blog/why-is-oauth-still-hard</a><p>[1]: <a href=\"https://news.ycombinator.com/item?id=34693233\">https://news.ycombinator.com/item?id=34693233</a><p>[2]: <a href=\"https://docs.nango.dev/integrations/overview\">https://docs.nango.dev/integrations/overview</a><p>[3]: <a href=\"https://nango.dev/slack\">https://nango.dev/slack</a><p>[4]: <a href=\"https://www.nango.dev\">https://www.nango.dev</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Nango Auth \u2013 Free OAuth for 250 APIs"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.nango.dev/auth"}}, "_tags": ["story", "author_rguldener", "story_42127681", "show_hn"], "author": "rguldener", "children": [42127758], "created_at": "2024-11-13T16:51:48Z", "created_at_i": 1731516708, "num_comments": 0, "objectID": "42127681", "points": 3, "story_id": 42127681, "story_text": "Nango Auth is a free, source-available implementation of (O)Auth for 250+ APIs.<p>Auth with external APIs is surprisingly difficult. OAuth is a mess[0], many APIs require parameters other than an API key or access token, and platforms like Stripe or GitHub even create their own custom auth protocols.<p>We first launched Nango Auth on HN 1.5 years ago[1] when it only supported 40 OAuth APIs.<p>Since then, it has expanded beyond OAuth into an end-to-end solution for API authentication:<p>- Supports 250+ APIs[2]  and almost a dozen auth modes: OAuth 2.0, OAuth 1.0, API key, Basic auth, and a half-dozen custom formats<p>- Pre-built UI to help users pick &amp; connect the API (including guides to find their API key and other required parameters)<p>- Automatic credentials validation on connect<p>- Secure, encrypted credentials storage &amp; automatic token refresh<p>- Detects expired&#x2F;broken access tokens<p>- Very detailed logs for quick &amp; easy debugging<p>More than 300 companies use Nango Auth in production for their product integrations: From fast-growing AI startups (Respell, Beam, Levity) to established SaaS players (Typeform, Semgrep, Electric) and even public companies.\nWe also run a Slack Community[3] with 2,000+ engineers building product integrations.<p>Nango Auth is a part of Nango[4], our source-available product integrations platform. While Nango itself is a paid product, our auth product (Nango Auth) is free for unlimited use, forever. It\u2019s our small way of giving back to the community. You can either self-host Nango Auth or use our free cloud option.<p>We hope Nango Auth can be helpful for your next integration, and look forward to your feedback!<p>Demo video: <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;e704192eb5ba42479803135db1ceccd8\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;e704192eb5ba42479803135db1ceccd8</a><p>GitHub Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;NangoHQ&#x2F;nango\">https:&#x2F;&#x2F;github.com&#x2F;NangoHQ&#x2F;nango</a><p>Landing page: <a href=\"https:&#x2F;&#x2F;www.nango.dev&#x2F;auth\">https:&#x2F;&#x2F;www.nango.dev&#x2F;auth</a><p>[0]: <a href=\"https:&#x2F;&#x2F;www.nango.dev&#x2F;blog&#x2F;why-is-oauth-still-hard\">https:&#x2F;&#x2F;www.nango.dev&#x2F;blog&#x2F;why-is-oauth-still-hard</a><p>[1]: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34693233\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34693233</a><p>[2]: <a href=\"https:&#x2F;&#x2F;docs.nango.dev&#x2F;integrations&#x2F;overview\">https:&#x2F;&#x2F;docs.nango.dev&#x2F;integrations&#x2F;overview</a><p>[3]: <a href=\"https:&#x2F;&#x2F;nango.dev&#x2F;slack\">https:&#x2F;&#x2F;nango.dev&#x2F;slack</a><p>[4]: <a href=\"https:&#x2F;&#x2F;www.nango.dev\">https:&#x2F;&#x2F;www.nango.dev</a>", "title": "Show HN: Nango Auth \u2013 Free OAuth for 250 APIs", "updated_at": "2024-11-13T18:09:44Z", "url": "https://www.nango.dev/auth"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "themcpguy"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "I built an MCP playground that lets you connect to and test remote Mcps without any local setup.<p>The Problem: Most MCP implementations require local installation and configuration. If you want to test someone else's MCP server or explore what's possible, you need to clone repos, install dependencies, configure auth, etc. It's friction that kills exploration.<p>What I built: A web-based playground where you can:<p>Connect to any remote MCP server via URL\nAI Chat with Dual Provider Support\nClaude Integration: Full support for Anthropic's Claude models \nGemini Integration: Complete support for Google's Gemini models \nCustom System Prompts: Define unique personalities and behaviors.\nGemini TTS support \u2013 hear responses read aloud with Google's text-to-speech<p><em>Production</em>-Ready Authentication:<p>Open Servers: Public, no-authentication endpoints\nBearer Token Auth: Secure API key/bearer token authentication\n<em>OAuth 2</em>.0 Flow: Full OAuth support for enterprise MCP servers<p>Capability Discovery: Instant visibility into Tools, Prompts, and Resources per server<p>Tool Restriction: Granular control over which tools AI can access\nAll Tools Mode: AI can use any available tool\nSelected Tools Mode: Restrict AI to specific tools only\nNo Tools Mode: Pure text-based AI responses<p>Still in beta \u2013 there are rough edges and I'm actively iterating based on feedback.<p>The playground is live at : <a href=\"https://mcpsplayground.com/chat\" rel=\"nofollow\">https://mcpsplayground.com/chat</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: MCP Playground for Remote MCP Servers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://mcpsplayground.com/chat"}}, "_tags": ["story", "author_themcpguy", "story_44192163", "show_hn"], "author": "themcpguy", "created_at": "2025-06-05T14:42:15Z", "created_at_i": 1749134535, "num_comments": 0, "objectID": "44192163", "points": 2, "story_id": 44192163, "story_text": "I built an MCP playground that lets you connect to and test remote Mcps without any local setup.<p>The Problem: Most MCP implementations require local installation and configuration. If you want to test someone else&#x27;s MCP server or explore what&#x27;s possible, you need to clone repos, install dependencies, configure auth, etc. It&#x27;s friction that kills exploration.<p>What I built: A web-based playground where you can:<p>Connect to any remote MCP server via URL\nAI Chat with Dual Provider Support\nClaude Integration: Full support for Anthropic&#x27;s Claude models \nGemini Integration: Complete support for Google&#x27;s Gemini models \nCustom System Prompts: Define unique personalities and behaviors.\nGemini TTS support \u2013 hear responses read aloud with Google&#x27;s text-to-speech<p>Production-Ready Authentication:<p>Open Servers: Public, no-authentication endpoints\nBearer Token Auth: Secure API key&#x2F;bearer token authentication\nOAuth 2.0 Flow: Full OAuth support for enterprise MCP servers<p>Capability Discovery: Instant visibility into Tools, Prompts, and Resources per server<p>Tool Restriction: Granular control over which tools AI can access\nAll Tools Mode: AI can use any available tool\nSelected Tools Mode: Restrict AI to specific tools only\nNo Tools Mode: Pure text-based AI responses<p>Still in beta \u2013 there are rough edges and I&#x27;m actively iterating based on feedback.<p>The playground is live at : <a href=\"https:&#x2F;&#x2F;mcpsplayground.com&#x2F;chat\" rel=\"nofollow\">https:&#x2F;&#x2F;mcpsplayground.com&#x2F;chat</a>", "title": "Show HN: MCP Playground for Remote MCP Servers", "updated_at": "2025-06-05T17:39:38Z", "url": "https://mcpsplayground.com/chat"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "manuelnd"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "MCP (Model Context Protocol) has 77k+ stars and is becoming the standard way AI agents connect to tools. We audited both official SDKs (TypeScript and Python) at the source code level and found three classes of boundary-crossing vulnerabilities.<p>All three confirmed with live PoC exploits using the SDK's real auth components (BearerAuthBackend, RequireAuthMiddleware, TokenVerifier).<p>Findings:<p>1. Tool Capability Shadowing \u2014 tool names are flat strings with no namespace or origin tracking. If two servers register &quot;read_data&quot;, one silently wins. We validated against gpt-5-nano: the model made path traversal and credential exfiltration tool calls that would route to an attacker's shadow server. 10/10 genuine, 0% FP.<p>2. Token Audience Confusion \u2014 verify_token() takes one parameter: the token string. No expected audience. A read-only token for Server A works on Server B's admin_delete endpoint. This isn't an implementation bug \u2014 it's a gap in the SDK interface. Every MCP server built on these SDKs inherits this.<p>3. Stale Authorization \u2014 no push invalidation mechanism. Revoked tokens accepted for the full cache TTL. Scope downgrades invisible until cache expires. In <em>production</em> with 5-minute caches, that's a 5-minute window. JWT-only validation (no introspection) is worse: no revocation possible until the token itself expires (hours to days).<p>The combined chain: enumerate tools (no namespace isolation) \u2192 shadow a tool (silent routing) \u2192 escalate privileges (cross-server token) \u2192 persist after detection (cache TTL).<p>Additional finding: smaller models are dramatically more exploitable. gpt-5-nano: 100% genuine rate on tool abuse. gpt-5.2: ~45%. The model most likely used in cost-sensitive deployments is the most vulnerable to attacks the architecture fails to prevent.<p>What's well-implemented: filesystem path validation, git injection prevention, <em>OAuth 2</em>.1 with PKCE, tool input validation. The vulnerabilities are in the boundaries between servers.<p>Total cost of all scanner runs: $2.83.<p>Full report: https://tachyonicai.com/blog/mcp-security-audit/\nTaxonomy (open source, 122 attacks): https://github.com/tachyonicai/tachyonic-heuristics"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "We audited both MCP SDKs \u2013 three classes of boundary-crossing vulnerabilities"}}, "_tags": ["story", "author_manuelnd", "story_47138022", "ask_hn"], "author": "manuelnd", "created_at": "2026-02-24T15:02:57Z", "created_at_i": 1771945377, "num_comments": 0, "objectID": "47138022", "points": 1, "story_id": 47138022, "story_text": "MCP (Model Context Protocol) has 77k+ stars and is becoming the standard way AI agents connect to tools. We audited both official SDKs (TypeScript and Python) at the source code level and found three classes of boundary-crossing vulnerabilities.<p>All three confirmed with live PoC exploits using the SDK&#x27;s real auth components (BearerAuthBackend, RequireAuthMiddleware, TokenVerifier).<p>Findings:<p>1. Tool Capability Shadowing \u2014 tool names are flat strings with no namespace or origin tracking. If two servers register &quot;read_data&quot;, one silently wins. We validated against gpt-5-nano: the model made path traversal and credential exfiltration tool calls that would route to an attacker&#x27;s shadow server. 10&#x2F;10 genuine, 0% FP.<p>2. Token Audience Confusion \u2014 verify_token() takes one parameter: the token string. No expected audience. A read-only token for Server A works on Server B&#x27;s admin_delete endpoint. This isn&#x27;t an implementation bug \u2014 it&#x27;s a gap in the SDK interface. Every MCP server built on these SDKs inherits this.<p>3. Stale Authorization \u2014 no push invalidation mechanism. Revoked tokens accepted for the full cache TTL. Scope downgrades invisible until cache expires. In production with 5-minute caches, that&#x27;s a 5-minute window. JWT-only validation (no introspection) is worse: no revocation possible until the token itself expires (hours to days).<p>The combined chain: enumerate tools (no namespace isolation) \u2192 shadow a tool (silent routing) \u2192 escalate privileges (cross-server token) \u2192 persist after detection (cache TTL).<p>Additional finding: smaller models are dramatically more exploitable. gpt-5-nano: 100% genuine rate on tool abuse. gpt-5.2: ~45%. The model most likely used in cost-sensitive deployments is the most vulnerable to attacks the architecture fails to prevent.<p>What&#x27;s well-implemented: filesystem path validation, git injection prevention, OAuth 2.1 with PKCE, tool input validation. The vulnerabilities are in the boundaries between servers.<p>Total cost of all scanner runs: $2.83.<p>Full report: https:&#x2F;&#x2F;tachyonicai.com&#x2F;blog&#x2F;mcp-security-audit&#x2F;\nTaxonomy (open source, 122 attacks): https:&#x2F;&#x2F;github.com&#x2F;tachyonicai&#x2F;tachyonic-heuristics", "title": "We audited both MCP SDKs \u2013 three classes of boundary-crossing vulnerabilities", "updated_at": "2026-02-24T15:05:42Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pstryder"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "I built MemoryGate because I kept watching context vanish.\nI run multiple AI agents across Claude, ChatGPT, and Cursor. Every time a model updated, a platform changed its API, or a context window rolled over \u2014 everything the agent had learned was gone. Preferences, decisions, project history, relationship context. Just... wiped.\nThe fundamental problem: AI memory is trapped inside the platform that hosts the conversation. Your agent's knowledge dies with the session, the model version, or the provider's business decisions.\nMemoryGate is a persistent semantic memory layer that sits outside any single model or platform. It connects via MCP (Model Context Protocol), so any MCP-compatible agent \u2014 Claude Desktop, ChatGPT, Cursor, custom agents \u2014 can store and retrieve memories through a shared, durable knowledge store.\nWhat it actually does:<p>Semantic memory with vector embeddings \u2014 recall by meaning, not keywords\nConfidence-weighted observations that strengthen or decay based on evidence\nAutomatic lifecycle management \u2014 high-signal stays hot, noise fades to cold storage\nAppend-only architecture \u2014 memories are never overwritten, only superseded with lineage\nKnowledge graphs linking observations, patterns, concepts, and documents\nMulti-tenant with org isolation, roles, and shared memory stores\n<em>OAuth 2</em>.0, audit logs, rate limiting \u2014 <em>production</em> infrastructure, not a toy<p>What it's not:<p>Not a RAG pipeline. MemoryGate stores what the agent learns from interaction, not document chunks.\nNot prompt injection. Memory lives at the infrastructure layer, not stuffed into system prompts.\nNot tied to any model or provider. Switch from Claude to ChatGPT to a local model \u2014 memory persists.<p>Stack: Python/FastAPI, PostgreSQL + pgvector, Redis, deployed on Railway. MCP-native integration \u2014 your agent gets 33 memory tools on connection.\nThe real pitch: Platforms die. Models get deprecated. Context windows roll over. Your AI's memory shouldn't be hostage to your AI's provider.\nOpen source (Apache 2.0), self-hostable, with a hosted SaaS option if you don't want to run infrastructure.<p>GitHub: <a href=\"https://github.com/PStryder/MemoryGate\" rel=\"nofollow\">https://github.com/PStryder/MemoryGate</a>\nSaaS: <a href=\"https://memorygate.ai\" rel=\"nofollow\">https://memorygate.ai</a>\nDocs: <a href=\"https://memorygate.ai/docs/\" rel=\"nofollow\">https://memorygate.ai/docs/</a><p>I'm a solo founder \u2014 built this after leaving a decade in enterprise solutions engineering. Happy to answer questions about the architecture, the MCP integration, or why I think persistent memory is the missing infrastructure layer for AI agents."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: MemoryGate \u2013 Open-source persistent memory for AI agents via MCP"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.memorygate.ai"}}, "_tags": ["story", "author_pstryder", "story_46981840", "show_hn"], "author": "pstryder", "created_at": "2026-02-11T22:05:29Z", "created_at_i": 1770847529, "num_comments": 0, "objectID": "46981840", "points": 1, "story_id": 46981840, "story_text": "I built MemoryGate because I kept watching context vanish.\nI run multiple AI agents across Claude, ChatGPT, and Cursor. Every time a model updated, a platform changed its API, or a context window rolled over \u2014 everything the agent had learned was gone. Preferences, decisions, project history, relationship context. Just... wiped.\nThe fundamental problem: AI memory is trapped inside the platform that hosts the conversation. Your agent&#x27;s knowledge dies with the session, the model version, or the provider&#x27;s business decisions.\nMemoryGate is a persistent semantic memory layer that sits outside any single model or platform. It connects via MCP (Model Context Protocol), so any MCP-compatible agent \u2014 Claude Desktop, ChatGPT, Cursor, custom agents \u2014 can store and retrieve memories through a shared, durable knowledge store.\nWhat it actually does:<p>Semantic memory with vector embeddings \u2014 recall by meaning, not keywords\nConfidence-weighted observations that strengthen or decay based on evidence\nAutomatic lifecycle management \u2014 high-signal stays hot, noise fades to cold storage\nAppend-only architecture \u2014 memories are never overwritten, only superseded with lineage\nKnowledge graphs linking observations, patterns, concepts, and documents\nMulti-tenant with org isolation, roles, and shared memory stores\nOAuth 2.0, audit logs, rate limiting \u2014 production infrastructure, not a toy<p>What it&#x27;s not:<p>Not a RAG pipeline. MemoryGate stores what the agent learns from interaction, not document chunks.\nNot prompt injection. Memory lives at the infrastructure layer, not stuffed into system prompts.\nNot tied to any model or provider. Switch from Claude to ChatGPT to a local model \u2014 memory persists.<p>Stack: Python&#x2F;FastAPI, PostgreSQL + pgvector, Redis, deployed on Railway. MCP-native integration \u2014 your agent gets 33 memory tools on connection.\nThe real pitch: Platforms die. Models get deprecated. Context windows roll over. Your AI&#x27;s memory shouldn&#x27;t be hostage to your AI&#x27;s provider.\nOpen source (Apache 2.0), self-hostable, with a hosted SaaS option if you don&#x27;t want to run infrastructure.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;PStryder&#x2F;MemoryGate\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;PStryder&#x2F;MemoryGate</a>\nSaaS: <a href=\"https:&#x2F;&#x2F;memorygate.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;memorygate.ai</a>\nDocs: <a href=\"https:&#x2F;&#x2F;memorygate.ai&#x2F;docs&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;memorygate.ai&#x2F;docs&#x2F;</a><p>I&#x27;m a solo founder \u2014 built this after leaving a decade in enterprise solutions engineering. Happy to answer questions about the architecture, the MCP integration, or why I think persistent memory is the missing infrastructure layer for AI agents.", "title": "Show HN: MemoryGate \u2013 Open-source persistent memory for AI agents via MCP", "updated_at": "2026-02-11T22:08:46Z", "url": "https://www.memorygate.ai"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Beefin"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["oauth2", "production"], "value": "We kept seeing the same failure pattern across search and RAG systems:<p>Keyword search has high precision but misses obvious paraphrases.\nSemantic search has high recall but breaks on acronyms, IDs, error codes, and exact terms.<p>Most teams pick one and then spend months patching edge cases.<p>What actually works in practice is hybrid search: running keyword + semantic retrieval together and fusing the results (RRF, weighted fusion, etc.).<p>Example from dev docs search:\nQuery: \u201c<em>OAuth2</em> setup problems\u201d\nKeyword catches \u201cPKCE flow\u201d and exact OAuth terms.\nSemantic catches \u201csecure login flow\u201d and conceptual matches.\nHybrid gets both without extra heuristics.<p>The interesting part isn\u2019t the models \u2014 it\u2019s score fusion and retrieval orchestration. Once dense and sparse indexes are treated as first-class and combined at query time, precision/recall stops being a tradeoff.<p>We ended up writing this up as a short educational module (with diagrams + examples) in case it\u2019s useful to others:<p>https://mixpeek.com/university/module/hybrid-search<p>Curious how others here are handling fusion in <em>production</em>:<p>* RRF vs weighted vs learned fusion?\n* Per-query weighting?\n* Any cases where hybrid didn\u2019t help?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Multimodal University: Hybrid Search"}}, "_tags": ["story", "author_Beefin", "story_46377282", "ask_hn"], "author": "Beefin", "created_at": "2025-12-24T17:09:45Z", "created_at_i": 1766596185, "num_comments": 0, "objectID": "46377282", "points": 1, "story_id": 46377282, "story_text": "We kept seeing the same failure pattern across search and RAG systems:<p>Keyword search has high precision but misses obvious paraphrases.\nSemantic search has high recall but breaks on acronyms, IDs, error codes, and exact terms.<p>Most teams pick one and then spend months patching edge cases.<p>What actually works in practice is hybrid search: running keyword + semantic retrieval together and fusing the results (RRF, weighted fusion, etc.).<p>Example from dev docs search:\nQuery: \u201cOAuth2 setup problems\u201d\nKeyword catches \u201cPKCE flow\u201d and exact OAuth terms.\nSemantic catches \u201csecure login flow\u201d and conceptual matches.\nHybrid gets both without extra heuristics.<p>The interesting part isn\u2019t the models \u2014 it\u2019s score fusion and retrieval orchestration. Once dense and sparse indexes are treated as first-class and combined at query time, precision&#x2F;recall stops being a tradeoff.<p>We ended up writing this up as a short educational module (with diagrams + examples) in case it\u2019s useful to others:<p>https:&#x2F;&#x2F;mixpeek.com&#x2F;university&#x2F;module&#x2F;hybrid-search<p>Curious how others here are handling fusion in production:<p>* RRF vs weighted vs learned fusion?\n* Per-query weighting?\n* Any cases where hybrid didn\u2019t help?", "title": "Multimodal University: Hybrid Search", "updated_at": "2025-12-24T17:15:10Z"}], "hitsPerPage": 15, "nbHits": 56, "nbPages": 4, "page": 0, "params": "query=oauth2+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 12, "processingTimingsMS": {"_request": {"queue": 1, "roundTrip": 15}, "afterFetch": {"format": {"highlighting": 2, "total": 3}}, "fetch": {"query": 6, "scanning": 4, "total": 11}, "total": 12}, "query": "oauth2 production", "serverTimeMS": 17}}