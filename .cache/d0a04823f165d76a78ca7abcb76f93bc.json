{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "simzor"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Access to Multiple Snapchat <em>Grafana</em> <em>Production</em> Dashboards"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://hackerone.com/reports/663628"}}, "_tags": ["story", "author_simzor", "story_25014956"], "author": "simzor", "created_at": "2020-11-07T13:30:55Z", "created_at_i": 1604755855, "num_comments": 0, "objectID": "25014956", "points": 2, "story_id": 25014956, "title": "Access to Multiple Snapchat Grafana Production Dashboards", "updated_at": "2024-09-20T07:15:03Z", "url": "https://hackerone.com/reports/663628"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "hacker_dedsec"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Access to multiple <em>production</em> <em>Grafana</em> dashboards"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://hackerone.com/reports/663628"}}, "_tags": ["story", "author_hacker_dedsec", "story_24999959"], "author": "hacker_dedsec", "created_at": "2020-11-05T17:17:33Z", "created_at_i": 1604596653, "num_comments": 0, "objectID": "24999959", "points": 1, "story_id": 24999959, "title": "Access to multiple production Grafana dashboards", "updated_at": "2024-09-20T07:22:18Z", "url": "https://hackerone.com/reports/663628"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "devsecopsify"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Load Testing in <em>Production</em> with <em>Grafana</em> Loki, Kubernetes and Go"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "https://tech.loveholidays.com/load-testing-in-<em>production</em>-with-<em>grafana</em>-loki-kubernetes-and-golang-1699554d2aa3?gi=dce6cd88d60c"}}, "_tags": ["story", "author_devsecopsify", "story_30307904"], "author": "devsecopsify", "created_at": "2022-02-11T22:45:18Z", "created_at_i": 1644619518, "num_comments": 0, "objectID": "30307904", "points": 6, "story_id": 30307904, "title": "Load Testing in Production with Grafana Loki, Kubernetes and Go", "updated_at": "2024-09-20T10:26:39Z", "url": "https://tech.loveholidays.com/load-testing-in-production-with-grafana-loki-kubernetes-and-golang-1699554d2aa3?gi=dce6cd88d60c"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "defilan"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Hi HN! I built LLMKube, a Kubernetes operator for deploying GPU-accelerated LLMs in <em>production</em>. One command gets you from zero to inference with full observability.<p>Why this exists: Regulated industries (healthcare, defense, finance) need air-gapped LLM\ndeployments, but existing tools are either single-node only (Ollama) or lack GPU optimization and\nSLO enforcement. LLMKube bridges the gap.<p>What's working:<p>- 17x speedup with NVIDIA GPUs (64 tok/s on Llama 3.2 3B vs 4.6 tok/s CPU)<p>- One command: llmkube deploy llama-3b --gpu (auto CUDA setup, scheduling, layer offloading)<p>- <em>Production</em> observability: Prometheus + <em>Grafana</em> + DCGM GPU metrics out of the box<p>- OpenAI-compatible API endpoints<p>- Terraform configs for GKE GPU clusters with auto-scale to zero<p>Tech: Kubernetes CRDs, llama.cpp with CUDA, NVIDIA GPU Operator, cost-optimized spot instances\n(~$50-150/mo dev workloads).<p>Status: v0.2.0 <em>production</em>-ready for single-GPU deployments on standard K8s clusters. Multi-GPU and\nmulti-node model sharding on the roadmap.<p>Apache 2.0 licensed. Would love feedback from anyone running LLMs in <em>production</em>!<p>Website: <a href=\"https://llmkube.com\" rel=\"nofollow\">https://llmkube.com</a><p>GitHub: <a href=\"https://github.com/Defilan/LLMKube\" rel=\"nofollow\">https://github.com/Defilan/LLMKube</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LLMKube \u2013 Kubernetes for Local LLMs with GPU Acceleration"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/defilantech/LLMKube"}}, "_tags": ["story", "author_defilan", "story_45968719", "show_hn"], "author": "defilan", "created_at": "2025-11-18T16:47:49Z", "created_at_i": 1763484469, "num_comments": 0, "objectID": "45968719", "points": 5, "story_id": 45968719, "story_text": "Hi HN! I built LLMKube, a Kubernetes operator for deploying GPU-accelerated LLMs in production. One command gets you from zero to inference with full observability.<p>Why this exists: Regulated industries (healthcare, defense, finance) need air-gapped LLM\ndeployments, but existing tools are either single-node only (Ollama) or lack GPU optimization and\nSLO enforcement. LLMKube bridges the gap.<p>What&#x27;s working:<p>- 17x speedup with NVIDIA GPUs (64 tok&#x2F;s on Llama 3.2 3B vs 4.6 tok&#x2F;s CPU)<p>- One command: llmkube deploy llama-3b --gpu (auto CUDA setup, scheduling, layer offloading)<p>- Production observability: Prometheus + Grafana + DCGM GPU metrics out of the box<p>- OpenAI-compatible API endpoints<p>- Terraform configs for GKE GPU clusters with auto-scale to zero<p>Tech: Kubernetes CRDs, llama.cpp with CUDA, NVIDIA GPU Operator, cost-optimized spot instances\n(~$50-150&#x2F;mo dev workloads).<p>Status: v0.2.0 production-ready for single-GPU deployments on standard K8s clusters. Multi-GPU and\nmulti-node model sharding on the roadmap.<p>Apache 2.0 licensed. Would love feedback from anyone running LLMs in production!<p>Website: <a href=\"https:&#x2F;&#x2F;llmkube.com\" rel=\"nofollow\">https:&#x2F;&#x2F;llmkube.com</a><p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Defilan&#x2F;LLMKube\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Defilan&#x2F;LLMKube</a>", "title": "Show HN: LLMKube \u2013 Kubernetes for Local LLMs with GPU Acceleration", "updated_at": "2025-11-18T17:12:51Z", "url": "https://github.com/defilantech/LLMKube"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "codelev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "I still use Docker Swarm. It's simple, fast to set up, and easy to teach. But I got tired of managing routing to microservices. Nginx and HAProxy need config redeploys, Traefik is powerful, but its configuration via labels quickly becomes unmaintainable.<p>So I built Millau \u2014 a free ingress proxy and load balancer for Docker Swarm microservices. Add a few labels to the service, and traffic gets routed. No proxy restarts, no config files. Example:\n```\n# service\ndeploy:\n  labels:\n    - &quot;millau.enabled=true&quot;\n    - &quot;millau.port=9000&quot;\n```<p>Millau listens to Docker events, discovers labeled services, and routes traffic using host and path matching. Unlike Traefik, Millau supports load balancing across services. I use it to deploy different versions e.g. Blue, Green, Red of the same microservice. If Red crashes, Millau routes to Blue or Green. If Blue slows down, Millau marks it inactive for 60s and routes to Green.<p>Millau supports TLS termination and mTLS. It exposes Prometheus metrics and ships with a prebuilt <em>Grafana</em> dashboard. It's in <em>production</em>, serving its own site and several side projects.<p>Let me know what you think, especially if you're still using Docker ecosystem in prod. Thanks."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Millau \u2013 self-configuring ingress proxy for Docker Swarm"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/codelev/millau"}}, "_tags": ["story", "author_codelev", "story_44101748", "show_hn"], "author": "codelev", "children": [44119231], "created_at": "2025-05-26T21:15:03Z", "created_at_i": 1748294103, "num_comments": 2, "objectID": "44101748", "points": 4, "story_id": 44101748, "story_text": "I still use Docker Swarm. It&#x27;s simple, fast to set up, and easy to teach. But I got tired of managing routing to microservices. Nginx and HAProxy need config redeploys, Traefik is powerful, but its configuration via labels quickly becomes unmaintainable.<p>So I built Millau \u2014 a free ingress proxy and load balancer for Docker Swarm microservices. Add a few labels to the service, and traffic gets routed. No proxy restarts, no config files. Example:\n```\n# service\ndeploy:\n  labels:\n    - &quot;millau.enabled=true&quot;\n    - &quot;millau.port=9000&quot;\n```<p>Millau listens to Docker events, discovers labeled services, and routes traffic using host and path matching. Unlike Traefik, Millau supports load balancing across services. I use it to deploy different versions e.g. Blue, Green, Red of the same microservice. If Red crashes, Millau routes to Blue or Green. If Blue slows down, Millau marks it inactive for 60s and routes to Green.<p>Millau supports TLS termination and mTLS. It exposes Prometheus metrics and ships with a prebuilt Grafana dashboard. It&#x27;s in production, serving its own site and several side projects.<p>Let me know what you think, especially if you&#x27;re still using Docker ecosystem in prod. Thanks.", "title": "Show HN: Millau \u2013 self-configuring ingress proxy for Docker Swarm", "updated_at": "2025-05-31T06:39:17Z", "url": "https://github.com/codelev/millau"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kiyanwang"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "We\u2019re building a <em>production</em> readiness review process at <em>Grafana</em> Labs"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "https://<em>grafana</em>.com/blog/2021/10/13/how-were-building-a-<em>production</em>-readiness-review-process-at-<em>grafana</em>-labs/"}}, "_tags": ["story", "author_kiyanwang", "story_28976961"], "author": "kiyanwang", "created_at": "2021-10-24T11:35:49Z", "created_at_i": 1635075349, "num_comments": 0, "objectID": "28976961", "points": 2, "story_id": 28976961, "title": "We\u2019re building a production readiness review process at Grafana Labs", "updated_at": "2024-09-20T09:43:10Z", "url": "https://grafana.com/blog/2021/10/13/how-were-building-a-production-readiness-review-process-at-grafana-labs/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "cbos"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Do you need <em>production</em> metrics for local development, use <em>Grafana</em> as proxy"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grafana"], "value": "https://ceesbos.nl/posts/20241108-<em>grafana</em>-as-proxy/"}}, "_tags": ["story", "author_cbos", "story_42147206"], "author": "cbos", "children": [42147207], "created_at": "2024-11-15T14:23:56Z", "created_at_i": 1731680636, "num_comments": 1, "objectID": "42147206", "points": 1, "story_id": 42147206, "title": "Do you need production metrics for local development, use Grafana as proxy", "updated_at": "2024-11-15T14:25:57Z", "url": "https://ceesbos.nl/posts/20241108-grafana-as-proxy/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "elenasamuylova"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Hi HN, we are Elena and Emeli, co-founders of Evidently AI <a href=\"http://evidentlyai.com\" rel=\"nofollow\">http://evidentlyai.com</a>. We're building monitoring for machine learning models in <em>production</em>. The tool is open source and available on GitHub: <a href=\"https://github.com/evidentlyai/evidently\" rel=\"nofollow\">https://github.com/evidentlyai/evidently</a>. You can use it locally in a Jupyter notebook or in a Bash shell. There\u2019s a video showing how it works in Jupyter here: <a href=\"https://www.youtube.com/watch?v=NPtTKYxm524\" rel=\"nofollow\">https://www.youtube.com/watch?v=NPtTKYxm524</a>.<p>Machine learning models can stop working as expected, often for non-obvious reasons. If this happens to a marketing personalization model, you might spam your customers by mistake. If this happens to credit scoring models, you might face legal and reputational risks. And so on. To catch issues with the model, it is not enough to just look at service metrics like latency. You have to track data quality, data drift (did the inputs change too much?), underperforming segments (does the model fail only for users in a certain region?), model metrics (accuracy, ROC AUC, mean error, etc.), and so on.<p>Emeli and I have been friends for many years. We first met when we both worked at Yandex (the company behind CatBoost and ClickHouse). We worked on creating ML systems for large enterprises. We then co-founded a startup focused on ML for manufacturing. Overall we've worked on more than 50 real-world ML projects, from e-commerce recommendations to steel <em>production</em> optimization. We faced the monitoring problem on our own when we put models in <em>production</em> and had to create and build custom dashboards. Emeli is also an ML instructor on Coursera (co-author of the most popular ML course in Russian) and a number of offline courses. She knows first-hand how many data scientists try to repeatedly implement the same things over and over. There is no reason why everyone should have to build their own version of something like drift detection.<p>We spent a couple of months talking to ML teams from different industries. We learned that there are no good, standard solutions for model monitoring. Some quoted us horror stories about broken models left unnoticed which led to $100K+ in losses. Others showed us home-grown dashboards and complained they are hard to maintain. Some said they simply have a recurring task to look at the logs once per month, and often catch the issues late. It is surprising how often models are not monitored until the first failure. We spoke to many teams who said that only after the first breakdown they started to think about monitoring. Some never do, and failures go undetected.<p>If you want to calculate a couple of performance metrics on top of your data, it is easy to do ad hoc. But if you want to have stable visibility into different models, you need to consider edge cases, choose the right statistical tests and implement them, design visuals, define thresholds for alerts etc. That is a harder problem that combines statistics and engineering. Beyond that, monitoring often involves sharing the results with different teams: from domain experts to developers. In practice, data scientists often end up sharing screenshots of their plots and sending files here and there. Building a maintainable software system that supports these workflows is a project in itself, and machine learning teams usually do not have time or resources for it.<p>Since there is no standard open-source solution, we decided to build one. We want to automate as much as possible to help people focus on the modeling work that matters, not boilerplate code.<p>Our main tool is an open-source Python library that generates interactive reports on ML model performance. To get it, you need to provide the model logs (input features, prediction, and ground truth if available) and reference data (usually from training). Then you choose the report type and we generate a set of dashboards. We have pre-built several reports to detect things like data drift, prediction drift, visualize performance metrics, and help understand where the model makes errors. We can display these in a Jupyter notebook or HTML. We can also generate a JSON profile instead of a report. You can then integrate this output with any external tool (like <em>Grafana</em>) and build a workflow you want to trigger retraining or alerts.<p>Under the hood, we perform the needed calculations (e.g. Kolmogorov Smirnov or Chi-Squared test to detect drift) and generate multiple interactive tables and plots (using Plotly on the backend). Right now it works with tabular data only. In the future, we plan to add more data types, reports and make it easier to customize metrics. Our goal is to make it dead easy to understand all aspects of model performance and monitor them.<p>We differ from other approaches in a couple of ways. There are end-to-end ML platforms on the market that include monitoring features. These work for teams who are ready to trade flexibility in order to have an all-in-one tool. But most teams we spoke to have custom needs and prefer to build their own platform from open components. We want to create a tool that does one thing well and is easy to integrate with whatever stack you use. There are also some proprietary ML monitoring solutions on the market, but we believe that tools like these should be open, transparent, and available for self-hosting. That is why we are building it as open source.<p>We launched under Apache 2.0 license so that everyone can use the tool. For now, our focus is to get adoption for the open-source project. We don\u2019t plan to charge individual users or small teams. We believe that the open-source project should remain open and be highly valuable. Later on, we plan to make money by providing a hosted cloud version for teams that do not want to run it themselves. We're also considering an open-core business model where we charge for features that large companies care about like single sign-on, security and audits.<p>If you work in tech companies, you might think that many ML infra problems are already solved. But in more traditional industries like manufacturing, retail, finance, etc., ML is just hitting adoption. Their ML needs and environment are often very different due to legacy IT systems, regulations, and types of use cases they work with. Now that many move from ML proof-of-concept projects to <em>production</em>, they will need the tools to help run the models reliably.<p>We are super excited to share this early release, and we\u2019d love if you could give it a try: <a href=\"https://github.com/evidentlyai/evidently\" rel=\"nofollow\">https://github.com/evidentlyai/evidently</a>. If you run models in <em>production</em> - let us know how you monitor them and if anything is missing. If you need some help to test the tool - happy to chat! We want to build this open-source project together with the community, so let us know if you have any thoughts or feedback."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Launch HN: Evidently AI (YC S21) \u2013 Track and Debug ML Models in <em>Production</em>"}}, "_tags": ["story", "author_elenasamuylova", "story_27760627", "launch_hn"], "author": "elenasamuylova", "children": [27760694, 27761194, 27762103, 27764697, 27765167, 27765758, 27768352, 27769787, 27778496], "created_at": "2021-07-07T13:02:26Z", "created_at_i": 1625662946, "num_comments": 17, "objectID": "27760627", "points": 111, "story_id": 27760627, "story_text": "Hi HN, we are Elena and Emeli, co-founders of Evidently AI <a href=\"http:&#x2F;&#x2F;evidentlyai.com\" rel=\"nofollow\">http:&#x2F;&#x2F;evidentlyai.com</a>. We&#x27;re building monitoring for machine learning models in production. The tool is open source and available on GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;evidentlyai&#x2F;evidently\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;evidentlyai&#x2F;evidently</a>. You can use it locally in a Jupyter notebook or in a Bash shell. There\u2019s a video showing how it works in Jupyter here: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=NPtTKYxm524\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=NPtTKYxm524</a>.<p>Machine learning models can stop working as expected, often for non-obvious reasons. If this happens to a marketing personalization model, you might spam your customers by mistake. If this happens to credit scoring models, you might face legal and reputational risks. And so on. To catch issues with the model, it is not enough to just look at service metrics like latency. You have to track data quality, data drift (did the inputs change too much?), underperforming segments (does the model fail only for users in a certain region?), model metrics (accuracy, ROC AUC, mean error, etc.), and so on.<p>Emeli and I have been friends for many years. We first met when we both worked at Yandex (the company behind CatBoost and ClickHouse). We worked on creating ML systems for large enterprises. We then co-founded a startup focused on ML for manufacturing. Overall we&#x27;ve worked on more than 50 real-world ML projects, from e-commerce recommendations to steel production optimization. We faced the monitoring problem on our own when we put models in production and had to create and build custom dashboards. Emeli is also an ML instructor on Coursera (co-author of the most popular ML course in Russian) and a number of offline courses. She knows first-hand how many data scientists try to repeatedly implement the same things over and over. There is no reason why everyone should have to build their own version of something like drift detection.<p>We spent a couple of months talking to ML teams from different industries. We learned that there are no good, standard solutions for model monitoring. Some quoted us horror stories about broken models left unnoticed which led to $100K+ in losses. Others showed us home-grown dashboards and complained they are hard to maintain. Some said they simply have a recurring task to look at the logs once per month, and often catch the issues late. It is surprising how often models are not monitored until the first failure. We spoke to many teams who said that only after the first breakdown they started to think about monitoring. Some never do, and failures go undetected.<p>If you want to calculate a couple of performance metrics on top of your data, it is easy to do ad hoc. But if you want to have stable visibility into different models, you need to consider edge cases, choose the right statistical tests and implement them, design visuals, define thresholds for alerts etc. That is a harder problem that combines statistics and engineering. Beyond that, monitoring often involves sharing the results with different teams: from domain experts to developers. In practice, data scientists often end up sharing screenshots of their plots and sending files here and there. Building a maintainable software system that supports these workflows is a project in itself, and machine learning teams usually do not have time or resources for it.<p>Since there is no standard open-source solution, we decided to build one. We want to automate as much as possible to help people focus on the modeling work that matters, not boilerplate code.<p>Our main tool is an open-source Python library that generates interactive reports on ML model performance. To get it, you need to provide the model logs (input features, prediction, and ground truth if available) and reference data (usually from training). Then you choose the report type and we generate a set of dashboards. We have pre-built several reports to detect things like data drift, prediction drift, visualize performance metrics, and help understand where the model makes errors. We can display these in a Jupyter notebook or HTML. We can also generate a JSON profile instead of a report. You can then integrate this output with any external tool (like Grafana) and build a workflow you want to trigger retraining or alerts.<p>Under the hood, we perform the needed calculations (e.g. Kolmogorov Smirnov or Chi-Squared test to detect drift) and generate multiple interactive tables and plots (using Plotly on the backend). Right now it works with tabular data only. In the future, we plan to add more data types, reports and make it easier to customize metrics. Our goal is to make it dead easy to understand all aspects of model performance and monitor them.<p>We differ from other approaches in a couple of ways. There are end-to-end ML platforms on the market that include monitoring features. These work for teams who are ready to trade flexibility in order to have an all-in-one tool. But most teams we spoke to have custom needs and prefer to build their own platform from open components. We want to create a tool that does one thing well and is easy to integrate with whatever stack you use. There are also some proprietary ML monitoring solutions on the market, but we believe that tools like these should be open, transparent, and available for self-hosting. That is why we are building it as open source.<p>We launched under Apache 2.0 license so that everyone can use the tool. For now, our focus is to get adoption for the open-source project. We don\u2019t plan to charge individual users or small teams. We believe that the open-source project should remain open and be highly valuable. Later on, we plan to make money by providing a hosted cloud version for teams that do not want to run it themselves. We&#x27;re also considering an open-core business model where we charge for features that large companies care about like single sign-on, security and audits.<p>If you work in tech companies, you might think that many ML infra problems are already solved. But in more traditional industries like manufacturing, retail, finance, etc., ML is just hitting adoption. Their ML needs and environment are often very different due to legacy IT systems, regulations, and types of use cases they work with. Now that many move from ML proof-of-concept projects to production, they will need the tools to help run the models reliably.<p>We are super excited to share this early release, and we\u2019d love if you could give it a try: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;evidentlyai&#x2F;evidently\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;evidentlyai&#x2F;evidently</a>. If you run models in production - let us know how you monitor them and if anything is missing. If you need some help to test the tool - happy to chat! We want to build this open-source project together with the community, so let us know if you have any thoughts or feedback.", "title": "Launch HN: Evidently AI (YC S21) \u2013 Track and Debug ML Models in Production", "updated_at": "2024-09-20T08:55:53Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "talboren"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Hi Hacker News! Shahar and Tal from Keep Here.<p>We were tired of creating alerts for our applications, so we've built an open-source GitHub Bot that lets you write application alerts using plain English. The code is open-sourced: <a href=\"https://github.com/keephq/keep\">https://github.com/keephq/keep</a> so you can review it yourself.<p>Every developer and DevOps professional is familiar with the fact that in order to ensure your application works in <em>production</em>, you need to access your observability tool's user interface (such as <em>Grafana</em>, Datadog, New Relic, etc.) and carefully determine how to create alerts that effectively monitor your application.<p>Instead, by installing Keep, every time you open a PR, the bot combines the alert description (alerts under the .keep directory) with the tool context (mostly the configuration of the alerts you already have) to generate (GPT) new alerts that keep you monitored.<p>So, for example, if you create a .keep/db-timeout.yaml and open a PR, the bot will comment on the PR with the actual alert you can deploy to your tool.<p># The alert text in plain English\nalert: |\n   Alert when the connections to the database are slower than 5 seconds for more than 5 minutes\nprovider: <em>grafana</em><p>You can Install the bot and connect your providers via <a href=\"https://platform.keephq.dev\">https://platform.keephq.dev</a> (after login, you'll start the installation flow) or just clone the repository and use docker-compose to start the web app and the installation flow.<p>Demo Video - <a href=\"https://www.loom.com/share/23541a03944c4dca99b0504a1753d1b4\" rel=\"nofollow\">https://www.loom.com/share/23541a03944c4dca99b0504a1753d1b4</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Keep \u2013 Create <em>production</em> alerts from plain English"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/keephq/keep"}}, "_tags": ["story", "author_talboren", "story_36223543", "show_hn"], "author": "talboren", "children": [36223682, 36224517, 36224539, 36224729, 36226454, 36236962, 36238023, 36238796, 36298001], "created_at": "2023-06-07T06:25:48Z", "created_at_i": 1686119148, "num_comments": 13, "objectID": "36223543", "points": 46, "story_id": 36223543, "story_text": "Hi Hacker News! Shahar and Tal from Keep Here.<p>We were tired of creating alerts for our applications, so we&#x27;ve built an open-source GitHub Bot that lets you write application alerts using plain English. The code is open-sourced: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;keephq&#x2F;keep\">https:&#x2F;&#x2F;github.com&#x2F;keephq&#x2F;keep</a> so you can review it yourself.<p>Every developer and DevOps professional is familiar with the fact that in order to ensure your application works in production, you need to access your observability tool&#x27;s user interface (such as Grafana, Datadog, New Relic, etc.) and carefully determine how to create alerts that effectively monitor your application.<p>Instead, by installing Keep, every time you open a PR, the bot combines the alert description (alerts under the .keep directory) with the tool context (mostly the configuration of the alerts you already have) to generate (GPT) new alerts that keep you monitored.<p>So, for example, if you create a .keep&#x2F;db-timeout.yaml and open a PR, the bot will comment on the PR with the actual alert you can deploy to your tool.<p># The alert text in plain English\nalert: |\n   Alert when the connections to the database are slower than 5 seconds for more than 5 minutes\nprovider: grafana<p>You can Install the bot and connect your providers via <a href=\"https:&#x2F;&#x2F;platform.keephq.dev\">https:&#x2F;&#x2F;platform.keephq.dev</a> (after login, you&#x27;ll start the installation flow) or just clone the repository and use docker-compose to start the web app and the installation flow.<p>Demo Video - <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;23541a03944c4dca99b0504a1753d1b4\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;23541a03944c4dca99b0504a1753d1b4</a>", "title": "Show HN: Keep \u2013 Create production alerts from plain English", "updated_at": "2024-09-20T14:11:58Z", "url": "https://github.com/keephq/keep"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "el_duderino"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "A <em>Production</em> Outage Was Caused Using Kubernetes Pod Priorities"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "https://<em>grafana</em>.com/blog/2019/07/24/how-a-<em>production</em>-outage-was-caused-using-kubernetes-pod-priorities/"}}, "_tags": ["story", "author_el_duderino", "story_20515606"], "author": "el_duderino", "children": [20518102], "created_at": "2019-07-24T14:27:08Z", "created_at_i": 1563978428, "num_comments": 3, "objectID": "20515606", "points": 35, "story_id": 20515606, "title": "A Production Outage Was Caused Using Kubernetes Pod Priorities", "updated_at": "2024-09-20T04:34:42Z", "url": "https://grafana.com/blog/2019/07/24/how-a-production-outage-was-caused-using-kubernetes-pod-priorities/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Dimittri"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https://sonarly.com\">https://sonarly.com</a>), an AI engineer for <em>production</em>. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here's a demo: <a href=\"https://www.youtube.com/watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https://www.youtube.com/watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from <em>production</em> alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it's a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in <em>production</em>, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can't catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don't need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don't want to add a new tracker or change their monitoring stack, as these platforms do the job they're supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog/<em>Grafana</em>). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the <em>production</em> system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts/day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don't look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180/day to 50/day (by grouping issues) and giving a severity based on the impact on the user/infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https://sonarly.com\">https://sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I'll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything's constructive!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your <em>production</em> alerts"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://sonarly.com/"}}, "_tags": ["story", "author_Dimittri", "story_47049776", "launch_hn"], "author": "Dimittri", "children": [47052409, 47054509, 47055028, 47055342, 47061697], "created_at": "2026-02-17T17:03:09Z", "created_at_i": 1771347789, "num_comments": 17, "objectID": "47049776", "points": 30, "story_id": 47049776, "story_text": "Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>), an AI engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here&#x27;s a demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it&#x27;s a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in production, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can&#x27;t catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don&#x27;t need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don&#x27;t want to add a new tracker or change their monitoring stack, as these platforms do the job they&#x27;re supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog&#x2F;Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts&#x2F;day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don&#x27;t look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180&#x2F;day to 50&#x2F;day (by grouping issues) and giving a severity based on the impact on the user&#x2F;infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I&#x27;ll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything&#x27;s constructive!", "title": "Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your production alerts", "updated_at": "2026-02-24T03:55:09Z", "url": "https://sonarly.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kirankgollu"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Hey HN -<p>Kiran and Vijay here.<p>We built Oodle after seeing teams forced to choose between low cost, great experience, zero ops, and no lock-in. Even with premium tools, debugging still meant switching between <em>Grafana</em> for metrics, OpenSearch for logs, and Jaeger or Tempo for traces - copying timestamps, losing context, and burning time during incidents.<p>So we decided to rethink observability from first principles - in both architecture and experience.<p>Architecturally, we borrowed ideas from Snowflake: separated storage and compute so each can scale independently. All telemetry - metrics, logs, and traces - is stored on S3 in a custom columnar format, while serverless compute scales on demand. The result is 3\u20135\u00d7 lower cost, massive scale, and zero operational overhead, with full compatibility for <em>Grafana</em> dashboards, PromQL, and OpenSearch queries.<p>On the experience side, Oodle unifies everything you already use. It works with your existing <em>Grafana</em> and OpenSearch setup, but when an alert fires, Oodle automatically correlates metrics, logs, and traces in one view - showing the latency spike, the related logs, and the exact service that caused it.<p>It\u2019s already in <em>production</em> across SaaS, fintech, and healthcare companies processing 10 TB+ logs/day and 50 M+ time-series/day.<p>We\u2019ve both spent years building large-scale data systems. Vijay worked on Rubrik\u2019s petabyte-scale file system on object storage, and I helped build AWS S3 and DynamoDB before leading Rubrik\u2019s cloud platform. Oodle applies the same design principles to observability.<p>You can try a live OpenTelemetry demo in &lt; 5 minutes (no signup needed): \n<a href=\"https://play.oodle.ai/settings?isUnifiedExperienceTourModalOpen=true\" rel=\"nofollow\">https://play.oodle.ai/settings?isUnifiedExperienceTourModalO...</a><p>or watch a short product walkthrough here: \n<a href=\"https://www.youtube.com/watch?v=wdYWDG3dRkU\" rel=\"nofollow\">https://www.youtube.com/watch?v=wdYWDG3dRkU</a><p>Would love feedback - what\u2019s your biggest observability pain today: cost, debuggability, or lock-in?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grafana"], "value": "Show HN: Oodle \u2013 Unified Debugging with OpenSearch and <em>Grafana</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://blog.oodle.ai/meet-oodle-unified-and-ai-native-observability/"}}, "_tags": ["story", "author_kirankgollu", "story_45812312", "show_hn"], "author": "kirankgollu", "children": [45816488], "created_at": "2025-11-04T15:49:08Z", "created_at_i": 1762271348, "num_comments": 3, "objectID": "45812312", "points": 11, "story_id": 45812312, "story_text": "Hey HN -<p>Kiran and Vijay here.<p>We built Oodle after seeing teams forced to choose between low cost, great experience, zero ops, and no lock-in. Even with premium tools, debugging still meant switching between Grafana for metrics, OpenSearch for logs, and Jaeger or Tempo for traces - copying timestamps, losing context, and burning time during incidents.<p>So we decided to rethink observability from first principles - in both architecture and experience.<p>Architecturally, we borrowed ideas from Snowflake: separated storage and compute so each can scale independently. All telemetry - metrics, logs, and traces - is stored on S3 in a custom columnar format, while serverless compute scales on demand. The result is 3\u20135\u00d7 lower cost, massive scale, and zero operational overhead, with full compatibility for Grafana dashboards, PromQL, and OpenSearch queries.<p>On the experience side, Oodle unifies everything you already use. It works with your existing Grafana and OpenSearch setup, but when an alert fires, Oodle automatically correlates metrics, logs, and traces in one view - showing the latency spike, the related logs, and the exact service that caused it.<p>It\u2019s already in production across SaaS, fintech, and healthcare companies processing 10 TB+ logs&#x2F;day and 50 M+ time-series&#x2F;day.<p>We\u2019ve both spent years building large-scale data systems. Vijay worked on Rubrik\u2019s petabyte-scale file system on object storage, and I helped build AWS S3 and DynamoDB before leading Rubrik\u2019s cloud platform. Oodle applies the same design principles to observability.<p>You can try a live OpenTelemetry demo in &lt; 5 minutes (no signup needed): \n<a href=\"https:&#x2F;&#x2F;play.oodle.ai&#x2F;settings?isUnifiedExperienceTourModalOpen=true\" rel=\"nofollow\">https:&#x2F;&#x2F;play.oodle.ai&#x2F;settings?isUnifiedExperienceTourModalO...</a><p>or watch a short product walkthrough here: \n<a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wdYWDG3dRkU\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wdYWDG3dRkU</a><p>Would love feedback - what\u2019s your biggest observability pain today: cost, debuggability, or lock-in?", "title": "Show HN: Oodle \u2013 Unified Debugging with OpenSearch and Grafana", "updated_at": "2025-11-05T02:42:45Z", "url": "https://blog.oodle.ai/meet-oodle-unified-and-ai-native-observability/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tschuehly"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "I am currently developing a project that I want to start a business with.<p>I've read a lot about Observability, Logging, Testing etc.  But what techniques and tools are important when moving to a <em>production</em> setting?<p>Is it necessary to setup a <em>Grafana</em> + Prometheus Stack to observe my application?<p>Is it necessary to create full end to end test on a staging environment?<p>Is it necessary to move from my current docker-compose setup to kubernetes and implement automatic health checks and so on?<p>Is it necessary to figure out how to load test my application with multiple servers to circumvent my slow internet connection?<p>If you have resources or tipps to share, it would help me very much."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: What is important when moving a project to <em>production</em> as a solo dev?"}}, "_tags": ["story", "author_tschuehly", "story_32859079", "ask_hn"], "author": "tschuehly", "children": [32866327], "created_at": "2022-09-15T22:08:30Z", "created_at_i": 1663279710, "num_comments": 1, "objectID": "32859079", "points": 1, "story_id": 32859079, "story_text": "I am currently developing a project that I want to start a business with.<p>I&#x27;ve read a lot about Observability, Logging, Testing etc.  But what techniques and tools are important when moving to a production setting?<p>Is it necessary to setup a Grafana + Prometheus Stack to observe my application?<p>Is it necessary to create full end to end test on a staging environment?<p>Is it necessary to move from my current docker-compose setup to kubernetes and implement automatic health checks and so on?<p>Is it necessary to figure out how to load test my application with multiple servers to circumvent my slow internet connection?<p>If you have resources or tipps to share, it would help me very much.", "title": "Ask HN: What is important when moving a project to production as a solo dev?", "updated_at": "2024-09-20T12:03:53Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pm3310"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Let's say that I have developed a restful service using FAST API. Is there any tool or library that can provide an estimate of log costs for Datadog (or any other observability tool like <em>Grafana</em> or New Relic) for this service before pushing it to <em>production</em>? I assume I need to provide some input on expected traffic and where the logs are saved"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Get log cost estimate before pushing to <em>production</em>"}}, "_tags": ["story", "author_pm3310", "story_37118558", "ask_hn"], "author": "pm3310", "children": [37118695, 37118770], "created_at": "2023-08-14T07:59:50Z", "created_at_i": 1691999990, "num_comments": 0, "objectID": "37118558", "points": 1, "story_id": 37118558, "story_text": "Let&#x27;s say that I have developed a restful service using FAST API. Is there any tool or library that can provide an estimate of log costs for Datadog (or any other observability tool like Grafana or New Relic) for this service before pushing it to production? I assume I need to provide some input on expected traffic and where the logs are saved", "title": "Get log cost estimate before pushing to production", "updated_at": "2024-12-22T04:08:15Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "9dev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grafana", "production"], "value": "Our Hosting provider, Hetzner, has recently started charging for public IPv4 addresses - as they should! Those numbers started getting expensive. This prompted me to try and set up a new server cluster using IPv6 exclusively, and see how far I could get before having to give in and purchase an additional v4 address.<p>The experiment ended much sooner than I had anticipated. Some of the road blocks I hit along the way:<p><pre><code>  - The GitHub API and its code load endpoints are not reachable via IPv6, making it impossible to download release artefacts from many projects, lots of which distribute their software via GitHub exclusively (Prometheus for instance).\n  - The default Ubuntu key servers aren't reachable via IPv6, making it difficult to install packages from third-party registries, such as Docker or <em>Grafana</em>. While debugging, I noticed huge swaths of the GPG infrastructure are defunct: There aren't many key servers left at all, and the only one I found actually working via IPv6 was pgpkeys.eu.\n  - BitBucket cannot deploy to IPv6 hosts, as pipelines don't support IPv6 at all. You can self-host a pipeline runner and connect to it via v6, BUT it needs to have a dual stack - otherwise the runner won't start.\n  - Hetzner itself doesn't even provide their own API via IPv6 (which we talk to for in-cluster service discovery. Oh, the irony.\n</code></pre>\nIt seems IPv6 is still not viable, more than a decade after launch. Do you use it in <em>production</em>? If so, how? What issues did you hit?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Tell HN: IPv6-only still pretty much unusable"}}, "_tags": ["story", "author_9dev", "story_33894933", "ask_hn"], "author": "9dev", "children": [33895147, 33895173, 33895592, 33895714, 33895772, 33895791, 33895825, 33895853, 33895858, 33895922, 33895983, 33896027, 33896038, 33896092, 33896218, 33896219, 33896222, 33896235, 33896242, 33896283, 33896538, 33896554, 33896642, 33896700, 33896925, 33896928, 33897053, 33897071, 33897091, 33897093, 33897130, 33897251, 33897398, 33897488, 33897667, 33897674, 33897688, 33897818, 33898030, 33898111, 33898161, 33898323, 33898416, 33898466, 33899101, 33899102, 33899360, 33899400, 33899516, 33899797, 33899814, 33899831, 33899846, 33899851, 33900251, 33900330, 33900978, 33900998, 33900999, 33901060, 33901116, 33901137, 33901271, 33901341, 33901388, 33902441, 33902779, 33902916, 33903019, 33903037, 33903210, 33903706, 33903946, 33904433, 33906472, 33906804, 33907732, 33908705, 33916008, 33930651], "created_at": "2022-12-07T14:51:09Z", "created_at_i": 1670424669, "num_comments": 631, "objectID": "33894933", "points": 686, "story_id": 33894933, "story_text": "Our Hosting provider, Hetzner, has recently started charging for public IPv4 addresses - as they should! Those numbers started getting expensive. This prompted me to try and set up a new server cluster using IPv6 exclusively, and see how far I could get before having to give in and purchase an additional v4 address.<p>The experiment ended much sooner than I had anticipated. Some of the road blocks I hit along the way:<p><pre><code>  - The GitHub API and its code load endpoints are not reachable via IPv6, making it impossible to download release artefacts from many projects, lots of which distribute their software via GitHub exclusively (Prometheus for instance).\n  - The default Ubuntu key servers aren&#x27;t reachable via IPv6, making it difficult to install packages from third-party registries, such as Docker or Grafana. While debugging, I noticed huge swaths of the GPG infrastructure are defunct: There aren&#x27;t many key servers left at all, and the only one I found actually working via IPv6 was pgpkeys.eu.\n  - BitBucket cannot deploy to IPv6 hosts, as pipelines don&#x27;t support IPv6 at all. You can self-host a pipeline runner and connect to it via v6, BUT it needs to have a dual stack - otherwise the runner won&#x27;t start.\n  - Hetzner itself doesn&#x27;t even provide their own API via IPv6 (which we talk to for in-cluster service discovery. Oh, the irony.\n</code></pre>\nIt seems IPv6 is still not viable, more than a decade after launch. Do you use it in production? If so, how? What issues did you hit?", "title": "Tell HN: IPv6-only still pretty much unusable", "updated_at": "2026-02-14T18:56:38Z"}], "hitsPerPage": 15, "nbHits": 40, "nbPages": 3, "page": 0, "params": "query=grafana+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 26, "processingTimingsMS": {"_request": {"roundTrip": 20}, "afterFetch": {"format": {"highlighting": 2, "total": 2}}, "fetch": {"query": 19, "scanning": 5, "total": 25}, "total": 26}, "query": "grafana production", "serverTimeMS": 29}}