{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "cjavelona"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "Hi everyone - we're Charles and Shikhar from Cosmic AI. We help people prototype, iterate and deploy AI workflows. Cosmic offers a streamlined process from deploying an AI notebook prototype to an API with a click of a button. We shrink months of deployment to a few minutes.<p>Here's a demo video: <a href=\"https://youtu.be/vstTKo_ChUQ\" rel=\"nofollow\">https://youtu.be/vstTKo_ChUQ</a><p>Cosmic AI was designed to to help AI engineers showcase AI prototypes to stakeholders quickly without involving a cloud or a data engineer.<p>We were building POCs to showcase AI agents and we noticed that 80% of our work was working on the infrastructure - the AI part was easy. Shikhar used to work at Discord to ship ML models also experienced similar pains of deploying ML to <em>production</em>. Several other teams we spoke to shared similar stories of ML engineers migrating code from a notebook to fit the boilerplate standard of deployment. It took them months to deploy. We questioned why there wasn't an easy to deploy to <em>production</em> like <em>Vercel</em>/Heroku.<p>Cosmic lets AI engineers ship their AI agent workflow code to <em>production</em>. We handle everything -- data pipelines, containerizing, code compliance, secret key management, and deployment -- so the AI engineer can focus on the AI logic.<p>Cosmic AI is on a invite only basis - we'd love feedback on our approach to streamline workflows for AI engineers. If we missed any problems you're encountering at work as an AI engineer - please let us know in the comment sections."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Cosmic AI \u2013 Prototype. Iterate. Deploy AI"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.trycosmic.ai/"}}, "_tags": ["story", "author_cjavelona", "story_43246630", "show_hn"], "author": "cjavelona", "created_at": "2025-03-03T21:00:01Z", "created_at_i": 1741035601, "num_comments": 0, "objectID": "43246630", "points": 1, "story_id": 43246630, "story_text": "Hi everyone - we&#x27;re Charles and Shikhar from Cosmic AI. We help people prototype, iterate and deploy AI workflows. Cosmic offers a streamlined process from deploying an AI notebook prototype to an API with a click of a button. We shrink months of deployment to a few minutes.<p>Here&#x27;s a demo video: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;vstTKo_ChUQ\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;vstTKo_ChUQ</a><p>Cosmic AI was designed to to help AI engineers showcase AI prototypes to stakeholders quickly without involving a cloud or a data engineer.<p>We were building POCs to showcase AI agents and we noticed that 80% of our work was working on the infrastructure - the AI part was easy. Shikhar used to work at Discord to ship ML models also experienced similar pains of deploying ML to production. Several other teams we spoke to shared similar stories of ML engineers migrating code from a notebook to fit the boilerplate standard of deployment. It took them months to deploy. We questioned why there wasn&#x27;t an easy to deploy to production like Vercel&#x2F;Heroku.<p>Cosmic lets AI engineers ship their AI agent workflow code to production. We handle everything -- data pipelines, containerizing, code compliance, secret key management, and deployment -- so the AI engineer can focus on the AI logic.<p>Cosmic AI is on a invite only basis - we&#x27;d love feedback on our approach to streamline workflows for AI engineers. If we missed any problems you&#x27;re encountering at work as an AI engineer - please let us know in the comment sections.", "title": "Show HN: Cosmic AI \u2013 Prototype. Iterate. Deploy AI", "updated_at": "2025-03-03T21:03:15Z", "url": "https://www.trycosmic.ai/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "waterdolphin"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "We have a site in a niche space that is going to be completely ready to launch by end of October. In the meantime we want to launch a side product to gauge interest and to build some basic brand recognition. The way we are planning to do this is by having a subdomain off of our regular domain, hosting this product there and have a &quot;Coming Soon&quot; page as the homepage of our regular site. I have 2 questions regarding this:<p>1. From a marketing/SEO perspective is this the correct way to go about it? What are some SEO considerations we should keep in mind with something like this? We don't want a situation in the future where our subdomain ends up being more popular than our regular domain.\n2. How can we best do this through <em>Vercel</em>? As I understand it, projects on <em>Vercel</em> can only have one <em>production</em> domain. I can create another project and then link the same repository to it but that is causing its own set of problems. Is there a way to add a second <em>production</em> domain on <em>Vercel</em> or some other method that <em>Vercel</em>/Nextjs provides that we can use?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["vercel"], "value": "How to manage launch a side/simplified product for a site hosted on <em>Vercel</em>"}}, "_tags": ["story", "author_waterdolphin", "story_41387037", "ask_hn"], "author": "waterdolphin", "created_at": "2024-08-29T03:39:37Z", "created_at_i": 1724902777, "num_comments": 0, "objectID": "41387037", "points": 2, "story_id": 41387037, "story_text": "We have a site in a niche space that is going to be completely ready to launch by end of October. In the meantime we want to launch a side product to gauge interest and to build some basic brand recognition. The way we are planning to do this is by having a subdomain off of our regular domain, hosting this product there and have a &quot;Coming Soon&quot; page as the homepage of our regular site. I have 2 questions regarding this:<p>1. From a marketing&#x2F;SEO perspective is this the correct way to go about it? What are some SEO considerations we should keep in mind with something like this? We don&#x27;t want a situation in the future where our subdomain ends up being more popular than our regular domain.\n2. How can we best do this through Vercel? As I understand it, projects on Vercel can only have one production domain. I can create another project and then link the same repository to it but that is causing its own set of problems. Is there a way to add a second production domain on Vercel or some other method that Vercel&#x2F;Nextjs provides that we can use?", "title": "How to manage launch a side/simplified product for a site hosted on Vercel", "updated_at": "2024-09-20T17:43:43Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "waterdolphin"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "We have a site in a niche space that is going to be completely ready to launch by end of October. In the meantime we want to launch a side product to gauge interest and to build some basic brand recognition. The way we are planning to do this is by having a subdomain off of our regular domain, hosting this product there and have a &quot;Coming Soon&quot; page as the homepage of our regular site. I have 2 questions regarding this:<p>1. From a marketing/SEO perspective is this the correct way to go about it? What are some SEO considerations we should keep in mind with something like this? We don't want a situation in the future where our subdomain ends up being more popular than our regular domain. \n2. How can we best do this through <em>Vercel</em>? As I understand it, projects on <em>Vercel</em> can only have one <em>production</em> domain. I can create another project and then link the same repository to it but that is causing its own set of problems. Is there a way to add a second <em>production</em> domain on <em>Vercel</em> or some other method that <em>Vercel</em>/Nextjs provides that we can use?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["vercel"], "value": "Ask HN: How to launch a side product for a site hosted on <em>Vercel</em>"}}, "_tags": ["story", "author_waterdolphin", "story_41391218", "ask_hn"], "author": "waterdolphin", "children": [41392707], "created_at": "2024-08-29T14:32:55Z", "created_at_i": 1724941975, "num_comments": 1, "objectID": "41391218", "points": 1, "story_id": 41391218, "story_text": "We have a site in a niche space that is going to be completely ready to launch by end of October. In the meantime we want to launch a side product to gauge interest and to build some basic brand recognition. The way we are planning to do this is by having a subdomain off of our regular domain, hosting this product there and have a &quot;Coming Soon&quot; page as the homepage of our regular site. I have 2 questions regarding this:<p>1. From a marketing&#x2F;SEO perspective is this the correct way to go about it? What are some SEO considerations we should keep in mind with something like this? We don&#x27;t want a situation in the future where our subdomain ends up being more popular than our regular domain. \n2. How can we best do this through Vercel? As I understand it, projects on Vercel can only have one production domain. I can create another project and then link the same repository to it but that is causing its own set of problems. Is there a way to add a second production domain on Vercel or some other method that Vercel&#x2F;Nextjs provides that we can use?", "title": "Ask HN: How to launch a side product for a site hosted on Vercel", "updated_at": "2024-09-20T17:44:06Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "guyyug"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "We are thinking to migrate to <em>vercel</em> with a Django app and wondered if it is a <em>production</em> ready solution. Does anyone has an experience with python + <em>vercel</em>? Thanks"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "Is anyone uses <em>vercel</em> for Python apps in <em>production</em>"}}, "_tags": ["story", "author_guyyug", "story_37098381", "ask_hn"], "author": "guyyug", "children": [37123541], "created_at": "2023-08-12T09:09:33Z", "created_at_i": 1691831373, "num_comments": 1, "objectID": "37098381", "points": 1, "story_id": 37098381, "story_text": "We are thinking to migrate to vercel with a Django app and wondered if it is a production ready solution. Does anyone has an experience with python + vercel? Thanks", "title": "Is anyone uses vercel for Python apps in production", "updated_at": "2024-09-20T14:55:06Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "haniehz"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "Hey HN!<p>We built a cloud platform that lets you deploy MCP servers (agents, tools, ChatGPT apps) to <em>production</em> instantly. Think <em>Vercel</em>, but for agents using the Model Context Protocol (MCP).<p>The problem: MCP servers are great locally, but running them in <em>production</em> is a pain. You need hosting, handle auth, manage secrets, deal with long-running processes, and connect everything securely.<p>With mcp-c, it's: uvx mcp-agent deploy<p>You get a <em>production</em> URL that works with Claude Desktop, ChatGPT, Cursor, or any MCP client.<p>What makes this different:\n- Durable execution (agents can run for hours without timing via Temporal)\n- Built-in secrets management\n- Full MCP spec support (sampling, notifications, logging)\n- Free during beta<p>Try this hosted ChatGPT Pizza example:<p>Pizza ordering agent: <a href=\"https://pizzaz.demos.mcp-agent.com/sse\" rel=\"nofollow\">https://pizzaz.demos.mcp-agent.com/sse</a><p>We're the team behind mcp-agent (8K GitHub stars), and this is our answer to &quot;how do I actually ship this?&quot;<p>Open beta, free to use. Would love your feedback on what would make this more useful.<p>Link to docs: <a href=\"https://docs.mcp-agent.com/get-started/cloud\" rel=\"nofollow\">https://docs.mcp-agent.com/get-started/cloud</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Durable cloud hosting for MCP servers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Haniehz1/mcp-agent"}}, "_tags": ["story", "author_haniehz", "story_45889042", "show_hn"], "author": "haniehz", "children": [45889750, 45890652], "created_at": "2025-11-11T16:15:42Z", "created_at_i": 1762877742, "num_comments": 3, "objectID": "45889042", "points": 2, "story_id": 45889042, "story_text": "Hey HN!<p>We built a cloud platform that lets you deploy MCP servers (agents, tools, ChatGPT apps) to production instantly. Think Vercel, but for agents using the Model Context Protocol (MCP).<p>The problem: MCP servers are great locally, but running them in production is a pain. You need hosting, handle auth, manage secrets, deal with long-running processes, and connect everything securely.<p>With mcp-c, it&#x27;s: uvx mcp-agent deploy<p>You get a production URL that works with Claude Desktop, ChatGPT, Cursor, or any MCP client.<p>What makes this different:\n- Durable execution (agents can run for hours without timing via Temporal)\n- Built-in secrets management\n- Full MCP spec support (sampling, notifications, logging)\n- Free during beta<p>Try this hosted ChatGPT Pizza example:<p>Pizza ordering agent: <a href=\"https:&#x2F;&#x2F;pizzaz.demos.mcp-agent.com&#x2F;sse\" rel=\"nofollow\">https:&#x2F;&#x2F;pizzaz.demos.mcp-agent.com&#x2F;sse</a><p>We&#x27;re the team behind mcp-agent (8K GitHub stars), and this is our answer to &quot;how do I actually ship this?&quot;<p>Open beta, free to use. Would love your feedback on what would make this more useful.<p>Link to docs: <a href=\"https:&#x2F;&#x2F;docs.mcp-agent.com&#x2F;get-started&#x2F;cloud\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.mcp-agent.com&#x2F;get-started&#x2F;cloud</a>", "title": "Show HN: Durable cloud hosting for MCP servers", "updated_at": "2025-11-12T17:23:59Z", "url": "https://github.com/Haniehz1/mcp-agent"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "nikhonit"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "I built this because running yt-dlp in <em>production</em> (especially on serverless/<em>Vercel</em>) is a nightmare of IP blocks, cold starts, and binary dependencies.<p>TranscriptAPI is a lightweight wrapper that handles the extraction, formatting, and proxy rotation. It prioritizes manual captions over auto-generated ones and returns clean JSON with timestamps, ready for RAG pipelines.<p>The MCP (Model Context Protocol) Integration: I recently added native MCP support. If you use Claude Desktop or other MCP-compliant agents, you can add this API as a tool to 'watch' videos directly in your chat context without manually copying transcripts.<p>Technical Stack:<p>Backend: Python (FastAPI) on AWS Lambda (for burst scaling)<p>Caching: Redis (to prevent hitting YouTube for the same video twice)<p>Challenge: Handling 'drifting' timestamps in long livestreams where the auto-generated captions lose sync with the video frame.<p>It has a free tier for hobbyists. I\u2019m curious to hear how you\u2019re handling the context-window limits when feeding full 3-hour transcripts to LLMs"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: A JSON API for YouTube Transcript with MCP Support"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://transcriptapi.com/"}}, "_tags": ["story", "author_nikhonit", "story_46368162", "show_hn"], "author": "nikhonit", "children": [46486817], "created_at": "2025-12-23T18:59:05Z", "created_at_i": 1766516345, "num_comments": 0, "objectID": "46368162", "points": 2, "story_id": 46368162, "story_text": "I built this because running yt-dlp in production (especially on serverless&#x2F;Vercel) is a nightmare of IP blocks, cold starts, and binary dependencies.<p>TranscriptAPI is a lightweight wrapper that handles the extraction, formatting, and proxy rotation. It prioritizes manual captions over auto-generated ones and returns clean JSON with timestamps, ready for RAG pipelines.<p>The MCP (Model Context Protocol) Integration: I recently added native MCP support. If you use Claude Desktop or other MCP-compliant agents, you can add this API as a tool to &#x27;watch&#x27; videos directly in your chat context without manually copying transcripts.<p>Technical Stack:<p>Backend: Python (FastAPI) on AWS Lambda (for burst scaling)<p>Caching: Redis (to prevent hitting YouTube for the same video twice)<p>Challenge: Handling &#x27;drifting&#x27; timestamps in long livestreams where the auto-generated captions lose sync with the video frame.<p>It has a free tier for hobbyists. I\u2019m curious to hear how you\u2019re handling the context-window limits when feeding full 3-hour transcripts to LLMs", "title": "Show HN: A JSON API for YouTube Transcript with MCP Support", "updated_at": "2026-01-04T10:51:18Z", "url": "https://transcriptapi.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "0xrelogic"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "I architected a GitHub streak notification service that scales to 10,000+ users while staying on free tiers. Here's the infrastructure design and cost optimization strategy.<p><em>Production</em>: <a href=\"https://streakyy.vercel.app\" rel=\"nofollow\">https://streakyy.<em>vercel</em>.app</a>\nGitHub: <a href=\"https://github.com/0xReLogic/Streaky\" rel=\"nofollow\">https://github.com/0xReLogic/Streaky</a>\nArchitecture: Open source, MIT license<p>=== Architecture Overview ===<p>Challenge: Build a daily cron system that processes 10k+ users in parallel, sends Discord/Telegram notifications, stays within Cloudflare free tier limits, maintains 99.9% uptime, and costs $0/month.<p>Solution: Distributed queue system with Service Bindings + Rust proxy for IP isolation<p>=== Infrastructure Design ===<p>1. Distributed Processing (Cloudflare Workers)<p>Problem: Sequential processing doesn't scale. Single Worker has 30s CPU limit.<p>Solution: Fan-out architecture with Service Bindings\n- Main scheduler initializes batch queue\n- Dispatches N workers via env.SELF.fetch()\n- Each worker = isolated execution context with fresh CPU budget\n- Automatic load balancing by Cloudflare<p>Capacity: 100k requests/day (10k users = 10% utilization)\nPerformance: 10 users in ~10 seconds, linear scalability<p>2. Queue Management (Cloudflare D1)<p>D1 SQLite with atomic operations for distributed queue without external dependencies.<p>Benefits: Atomic claim, no race conditions, built-in idempotency\nCapacity: 50k writes/day (30k used = 60% utilization)<p>3. IP Isolation Layer (Rust VPS)<p>Problem: Cloudflare Workers share IP pools \u2192 rate limiting (429 errors)<p>Solution: Lightweight Rust proxy on Koyeb\n- Axum framework\n- AES-256-GCM encryption\n- Stateless, zero-trust architecture\n- Cold start ~10s, warm ~3.6s\n- 100% success rate<p>Cost: Koyeb free tier (512MB RAM) - $0/month<p>4. Data Layer (Cloudflare D1)<p>Schema: users, notifications, cron_queue\nCapacity: 5GB storage (10MB used = 0.2%)<p>=== Cost Analysis ===<p>Current (10k users): $0/month\n- Cloudflare Workers: 10k/100k req/day\n- D1: 30k/50k writes/day\n- Koyeb: ~1GB/100GB bandwidth\n- <em>Vercel</em>: ~5GB/100GB bandwidth<p>50k users: $10/month ($0.0002/user)\n- D1: $5/month, Koyeb: $5/month<p>100k users: $25/month ($0.00025/user)\n- Workers: $5, D1: $10, Koyeb: $10<p>Total Cost: $0/month for 10k users<p>Happy to discuss infrastructure decisions, scaling strategies, or cost optimization techniques."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Architecting for 10k users on Cloudflare's free tier"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/0xReLogic/Streaky"}}, "_tags": ["story", "author_0xrelogic", "story_45679174", "show_hn"], "author": "0xrelogic", "created_at": "2025-10-23T07:28:08Z", "created_at_i": 1761204488, "num_comments": 0, "objectID": "45679174", "points": 2, "story_id": 45679174, "story_text": "I architected a GitHub streak notification service that scales to 10,000+ users while staying on free tiers. Here&#x27;s the infrastructure design and cost optimization strategy.<p>Production: <a href=\"https:&#x2F;&#x2F;streakyy.vercel.app\" rel=\"nofollow\">https:&#x2F;&#x2F;streakyy.vercel.app</a>\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;0xReLogic&#x2F;Streaky\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;0xReLogic&#x2F;Streaky</a>\nArchitecture: Open source, MIT license<p>=== Architecture Overview ===<p>Challenge: Build a daily cron system that processes 10k+ users in parallel, sends Discord&#x2F;Telegram notifications, stays within Cloudflare free tier limits, maintains 99.9% uptime, and costs $0&#x2F;month.<p>Solution: Distributed queue system with Service Bindings + Rust proxy for IP isolation<p>=== Infrastructure Design ===<p>1. Distributed Processing (Cloudflare Workers)<p>Problem: Sequential processing doesn&#x27;t scale. Single Worker has 30s CPU limit.<p>Solution: Fan-out architecture with Service Bindings\n- Main scheduler initializes batch queue\n- Dispatches N workers via env.SELF.fetch()\n- Each worker = isolated execution context with fresh CPU budget\n- Automatic load balancing by Cloudflare<p>Capacity: 100k requests&#x2F;day (10k users = 10% utilization)\nPerformance: 10 users in ~10 seconds, linear scalability<p>2. Queue Management (Cloudflare D1)<p>D1 SQLite with atomic operations for distributed queue without external dependencies.<p>Benefits: Atomic claim, no race conditions, built-in idempotency\nCapacity: 50k writes&#x2F;day (30k used = 60% utilization)<p>3. IP Isolation Layer (Rust VPS)<p>Problem: Cloudflare Workers share IP pools \u2192 rate limiting (429 errors)<p>Solution: Lightweight Rust proxy on Koyeb\n- Axum framework\n- AES-256-GCM encryption\n- Stateless, zero-trust architecture\n- Cold start ~10s, warm ~3.6s\n- 100% success rate<p>Cost: Koyeb free tier (512MB RAM) - $0&#x2F;month<p>4. Data Layer (Cloudflare D1)<p>Schema: users, notifications, cron_queue\nCapacity: 5GB storage (10MB used = 0.2%)<p>=== Cost Analysis ===<p>Current (10k users): $0&#x2F;month\n- Cloudflare Workers: 10k&#x2F;100k req&#x2F;day\n- D1: 30k&#x2F;50k writes&#x2F;day\n- Koyeb: ~1GB&#x2F;100GB bandwidth\n- Vercel: ~5GB&#x2F;100GB bandwidth<p>50k users: $10&#x2F;month ($0.0002&#x2F;user)\n- D1: $5&#x2F;month, Koyeb: $5&#x2F;month<p>100k users: $25&#x2F;month ($0.00025&#x2F;user)\n- Workers: $5, D1: $10, Koyeb: $10<p>Total Cost: $0&#x2F;month for 10k users<p>Happy to discuss infrastructure decisions, scaling strategies, or cost optimization techniques.", "title": "Show HN: Architecting for 10k users on Cloudflare's free tier", "updated_at": "2025-10-23T07:55:30Z", "url": "https://github.com/0xReLogic/Streaky"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "todsacerdoti"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Prioritize <em>Production</em> deployments to build before any queued Preview"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "https://<em>vercel</em>.com/changelog/prioritize-<em>production</em>-deployments-to-build-before-any-queued-preview"}}, "_tags": ["story", "author_todsacerdoti", "story_37124525"], "author": "todsacerdoti", "created_at": "2023-08-14T18:17:56Z", "created_at_i": 1692037076, "num_comments": 0, "objectID": "37124525", "points": 1, "story_id": 37124525, "title": "Prioritize Production deployments to build before any queued Preview", "updated_at": "2024-09-20T14:47:41Z", "url": "https://vercel.com/changelog/prioritize-production-deployments-to-build-before-any-queued-preview"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "deltadarkly"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "Show HN: <em>Vercel</em> for AI agents, one-click <em>production</em> deployment for any framework"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://cloud.ai-dank.xyz/"}}, "_tags": ["story", "author_deltadarkly", "story_46900726", "show_hn"], "author": "deltadarkly", "children": [46900797, 46901275], "created_at": "2026-02-05T15:28:12Z", "created_at_i": 1770305292, "num_comments": 3, "objectID": "46900726", "points": 4, "story_id": 46900726, "title": "Show HN: Vercel for AI agents, one-click production deployment for any framework", "updated_at": "2026-02-06T03:27:02Z", "url": "https://cloud.ai-dank.xyz/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "vmatsiiako"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "Hi HN, we\u2019re the co-founders of Infisical (<a href=\"https://infisical.com\">https://infisical.com</a>), an open-source platform to sync application secrets and configs across your engineering team and infrastructure. We enable teams to store their secrets in a centralized location and distribute them anywhere from local development processes to staging/<em>production</em> environments.<p>Our Github is at <a href=\"https://github.com/infisical/infisical\">https://github.com/infisical/infisical</a>.<p>We previously worked at AWS, Figma, and another startup, where we frequently ran into problems dealing with secret management. For example, many companies used .env files to maintain their development secrets and struggled to keep secrets in sync amongst their teams (this routinely posed security and efficiency issues \u2014 secrets can get leaked or go missing). Some companies (especially bigger ones) used solutions like Vault which can be difficult to set up, maintain, and afford.<p>While secret managers exist, they\u2019re imperfect for many reasons: open-source solutions are either too complicated, not comprehensive, not user-friendly, or a mix of all three; there are nicer closed-source solutions but with no self-hosted options available. The gap we see is to make something that\u2019s simple, open-source, and powerful.<p>On the open-source front, our goal is to provide full transparency of our codebase and enable anyone in the community to build anything they want in an optimal secret management solution. If you need any feature or integration that we don\u2019t yet support, you can post an issue about it or directly send in a PR to be reviewed immediately.<p>You can inject the right set of secrets for any environment into your application by using the Infisical CLI together with your application start command (e.g. infisical run -- npm run dev). This removes the need to use a .env file. Everything stays encrypted with encryption/decryption operations occurring on the client-side \u2014 under the hood, secrets are encrypted by vault keys for which there are multiple copies of vault keys encrypted under the public key of each member of a vault (ensuring only members of vaults can decrypt secrets pertaining to that vault locally). An alternative way is to use our Open API - though it\u2019s a little complicated, and we\u2019re working on adding SDKs to abstract away the cryptography.<p>Infisical integrates with staging and <em>production</em> cloud services like AWS, <em>Vercel</em>, GitHub Actions, and Circle CI. We also added support for integrations with Docker, Kubernetes, and Terraform. Infisical is now a central source of truth for secrets across the entire development cycle from development to <em>production</em> with new integration releases every week.\nOne interesting thing is that, by default, our platform is end-to-end encrypted but users can opt out of that if they need to integrate with cloud platforms that require secrets to be sent in decrypted format (e.g. GitHub Actions, <em>Vercel</em>, Render). We\u2019re the only solution that we know of that offers this E2EE-with opt-out ability.<p>Since our last Show HN (<a href=\"https://news.ycombinator.com/item?id=34510516\" rel=\"nofollow\">https://news.ycombinator.com/item?id=34510516</a>), we\u2019ve layered authentication with 2FA (more MFA options coming soon) and upgraded all private key encryption/decryption steps to involve a 256-bit protected key decrypted by another key generated via Argon2id KDF from the user\u2019s password. We are starting the process of obtaining SOC2 and other security and compliance certifications. You can read more about our security here: <a href=\"https://infisical.com/docs/security/overview\">https://infisical.com/docs/security/overview</a><p>Beyond this, we\u2019ve added integrations with PM2, AWS Secrets Manager, AWS Parameter Store, Circle CI, Travis CI, GitLab CI/CD, Terraform and more. We\u2019ve also redesigned the main dashboard and added more advanced organizational structure for secrets. Lastly, we have added role-based access control, and improved our Kubernetes operator: your clusters are now auto-redeployed when secrets in Infisical change. In the coming weeks and months, we plan to add features like secret rotation, improved audit logs, SDKs and alerts; as well as increase the range of our integrations; and continue fortifying platform security and stability.<p>We\u2019ve launched this repo under the MIT license so any developer can use the platform. We don\u2019t charge individual developers or small teams\u2014all the integrations are fully available to everyone. We make money by charging a license fee for enterprise features as well as providing a hosted version and support.<p>If you found it interesting, you can see a demo video here: <a href=\"https://www.loom.com/share/9a8904c6ecc84d0899d53ee1f7a36385\" rel=\"nofollow\">https://www.loom.com/share/9a8904c6ecc84d0899d53ee1f7a36385</a><p>We\u2019d love for you to give Infisical a try (<a href=\"https://infisical.com\">https://infisical.com</a>) and provide any feedback. If you're interested, our code is available here: <a href=\"https://github.com/infisical/infisical\">https://github.com/infisical/infisical</a>. If we don\u2019t have something, let us know and we\u2019d be happy to build it for you. We look forward to your comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Infisical (YC W23) \u2013 Open-source secrets manager for developers"}}, "_tags": ["story", "author_vmatsiiako", "story_34955699", "launch_hn"], "author": "vmatsiiako", "children": [34955766, 34956280, 34956303, 34956309, 34956386, 34956484, 34956666, 34956694, 34956746, 34956759, 34956928, 34957054, 34957380, 34957529, 34957673, 34957748, 34958009, 34958209, 34958403, 34959016, 34959650, 34961850, 34962065, 34963142, 34963683, 34964525, 34964797, 34967332, 34967354], "created_at": "2023-02-27T12:39:29Z", "created_at_i": 1677501569, "num_comments": 121, "objectID": "34955699", "points": 231, "story_id": 34955699, "story_text": "Hi HN, we\u2019re the co-founders of Infisical (<a href=\"https:&#x2F;&#x2F;infisical.com\">https:&#x2F;&#x2F;infisical.com</a>), an open-source platform to sync application secrets and configs across your engineering team and infrastructure. We enable teams to store their secrets in a centralized location and distribute them anywhere from local development processes to staging&#x2F;production environments.<p>Our Github is at <a href=\"https:&#x2F;&#x2F;github.com&#x2F;infisical&#x2F;infisical\">https:&#x2F;&#x2F;github.com&#x2F;infisical&#x2F;infisical</a>.<p>We previously worked at AWS, Figma, and another startup, where we frequently ran into problems dealing with secret management. For example, many companies used .env files to maintain their development secrets and struggled to keep secrets in sync amongst their teams (this routinely posed security and efficiency issues \u2014 secrets can get leaked or go missing). Some companies (especially bigger ones) used solutions like Vault which can be difficult to set up, maintain, and afford.<p>While secret managers exist, they\u2019re imperfect for many reasons: open-source solutions are either too complicated, not comprehensive, not user-friendly, or a mix of all three; there are nicer closed-source solutions but with no self-hosted options available. The gap we see is to make something that\u2019s simple, open-source, and powerful.<p>On the open-source front, our goal is to provide full transparency of our codebase and enable anyone in the community to build anything they want in an optimal secret management solution. If you need any feature or integration that we don\u2019t yet support, you can post an issue about it or directly send in a PR to be reviewed immediately.<p>You can inject the right set of secrets for any environment into your application by using the Infisical CLI together with your application start command (e.g. infisical run -- npm run dev). This removes the need to use a .env file. Everything stays encrypted with encryption&#x2F;decryption operations occurring on the client-side \u2014 under the hood, secrets are encrypted by vault keys for which there are multiple copies of vault keys encrypted under the public key of each member of a vault (ensuring only members of vaults can decrypt secrets pertaining to that vault locally). An alternative way is to use our Open API - though it\u2019s a little complicated, and we\u2019re working on adding SDKs to abstract away the cryptography.<p>Infisical integrates with staging and production cloud services like AWS, Vercel, GitHub Actions, and Circle CI. We also added support for integrations with Docker, Kubernetes, and Terraform. Infisical is now a central source of truth for secrets across the entire development cycle from development to production with new integration releases every week.\nOne interesting thing is that, by default, our platform is end-to-end encrypted but users can opt out of that if they need to integrate with cloud platforms that require secrets to be sent in decrypted format (e.g. GitHub Actions, Vercel, Render). We\u2019re the only solution that we know of that offers this E2EE-with opt-out ability.<p>Since our last Show HN (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34510516\" rel=\"nofollow\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34510516</a>), we\u2019ve layered authentication with 2FA (more MFA options coming soon) and upgraded all private key encryption&#x2F;decryption steps to involve a 256-bit protected key decrypted by another key generated via Argon2id KDF from the user\u2019s password. We are starting the process of obtaining SOC2 and other security and compliance certifications. You can read more about our security here: <a href=\"https:&#x2F;&#x2F;infisical.com&#x2F;docs&#x2F;security&#x2F;overview\">https:&#x2F;&#x2F;infisical.com&#x2F;docs&#x2F;security&#x2F;overview</a><p>Beyond this, we\u2019ve added integrations with PM2, AWS Secrets Manager, AWS Parameter Store, Circle CI, Travis CI, GitLab CI&#x2F;CD, Terraform and more. We\u2019ve also redesigned the main dashboard and added more advanced organizational structure for secrets. Lastly, we have added role-based access control, and improved our Kubernetes operator: your clusters are now auto-redeployed when secrets in Infisical change. In the coming weeks and months, we plan to add features like secret rotation, improved audit logs, SDKs and alerts; as well as increase the range of our integrations; and continue fortifying platform security and stability.<p>We\u2019ve launched this repo under the MIT license so any developer can use the platform. We don\u2019t charge individual developers or small teams\u2014all the integrations are fully available to everyone. We make money by charging a license fee for enterprise features as well as providing a hosted version and support.<p>If you found it interesting, you can see a demo video here: <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;9a8904c6ecc84d0899d53ee1f7a36385\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;9a8904c6ecc84d0899d53ee1f7a36385</a><p>We\u2019d love for you to give Infisical a try (<a href=\"https:&#x2F;&#x2F;infisical.com\">https:&#x2F;&#x2F;infisical.com</a>) and provide any feedback. If you&#x27;re interested, our code is available here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;infisical&#x2F;infisical\">https:&#x2F;&#x2F;github.com&#x2F;infisical&#x2F;infisical</a>. If we don\u2019t have something, let us know and we\u2019d be happy to build it for you. We look forward to your comments!", "title": "Launch HN: Infisical (YC W23) \u2013 Open-source secrets manager for developers", "updated_at": "2025-07-30T03:53:08Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "the_plug"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "Hi HN ,<p>I got tired of writing the same boilerplate over and over \u2014 DB setup, auth, routes, security \u2014 every time I built a backend.<p>So I built Pipo360 \u2014 an AI-powered tool that generates <em>production</em>-ready backends in under 60 seconds, from just a plain-text description.<p>How it works:\nType what you need<p>\u201cCreate a task management API with user auth and MongoDB\u201d<p>Hit Generate<p>Get real, exportable code<p>Auth (JWT)<p>Database schema<p>CRUD routes<p>Deployable to <em>Vercel</em>, AWS, etc.<p>No templates. No lock-in. Just code that works.<p>Why it\u2019s different:\nBuilt with Gemini AI + human supervision (to ensure real prod-quality output)<p>Exports to MongoDB, PostgreSQL, MySQL, SQLite<p>Secure by default (JWT, RBAC, etc.)<p>Supports no-login backend previews<p>Try it live (No signup needed):\n <a href=\"https://pipo360.xyz\" rel=\"nofollow\">https://pipo360.xyz</a><p>Would love feedback:\nWhat backend would you try first?<p>What would make it better for your workflow?<p>Would open sourcing part of it be useful?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Pipo360 \u2013 Generate <em>production</em>-ready back end APIs in 60 seconds with AI"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://pipo360.xyz"}}, "_tags": ["story", "author_the_plug", "story_44284544", "show_hn"], "author": "the_plug", "children": [44284960, 44284994, 44285161, 44285220, 44285364, 44289918], "created_at": "2025-06-15T19:46:16Z", "created_at_i": 1750016776, "num_comments": 23, "objectID": "44284544", "points": 18, "story_id": 44284544, "story_text": "Hi HN ,<p>I got tired of writing the same boilerplate over and over \u2014 DB setup, auth, routes, security \u2014 every time I built a backend.<p>So I built Pipo360 \u2014 an AI-powered tool that generates production-ready backends in under 60 seconds, from just a plain-text description.<p>How it works:\nType what you need<p>\u201cCreate a task management API with user auth and MongoDB\u201d<p>Hit Generate<p>Get real, exportable code<p>Auth (JWT)<p>Database schema<p>CRUD routes<p>Deployable to Vercel, AWS, etc.<p>No templates. No lock-in. Just code that works.<p>Why it\u2019s different:\nBuilt with Gemini AI + human supervision (to ensure real prod-quality output)<p>Exports to MongoDB, PostgreSQL, MySQL, SQLite<p>Secure by default (JWT, RBAC, etc.)<p>Supports no-login backend previews<p>Try it live (No signup needed):\n <a href=\"https:&#x2F;&#x2F;pipo360.xyz\" rel=\"nofollow\">https:&#x2F;&#x2F;pipo360.xyz</a><p>Would love feedback:\nWhat backend would you try first?<p>What would make it better for your workflow?<p>Would open sourcing part of it be useful?", "title": "Show HN: Pipo360 \u2013 Generate production-ready back end APIs in 60 seconds with AI", "updated_at": "2025-09-12T21:09:20Z", "url": "https://pipo360.xyz"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "buildinext"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "Hey HN! I've been building Quell, an AI QA testing agent designed to run tests directly triggered from issue trackers (Jira, Linear) or designs (Figma) or CI/CD platforms (<em>Vercel</em>, Netlify, GitHub), ensuring rapid and accurate acceptance testing without manual effort.<p>What problem does it solve?\nAs a product manager and founder myself, I constantly faced issues releasing a new feature only to discover critical bugs or that the build doesn't fully meet acceptance criteria. Bottlenecks in our QA and release process\u2014tickets and issues stuck in manual testing, slow deployments due to delayed verification, and missed acceptance criteria leading to bugs slipping into <em>production</em>. I built Quell to automate these tedious steps, freeing up teams to focus on actual feature development and faster iterations.<p>How is it different?<p>Integrates with existing dev workflows\u2014triggers tests via Jira/Linear issue status automatically testing <em>Vercel</em>, or Netlify or other URL deployment builds.<p>Tests against explicit acceptance criteria pulled directly from your issue tracker.<p>Current Capabilities (free to test):<p>Automatically trigger QA runs from Linear/Jira issue state transitions or <em>Vercel</em>/Netlify deploy previews.<p>Generate immediate test reports and tickets for issues spotted.<p>Quell is ready to test right now\u2014email only required to try out demo functionality directly:<p>Try out [Quellit.ai](<a href=\"http://quellit.ai/\" rel=\"nofollow\">http://quellit.ai/</a>) for free<p>I'm actively iterating based on user feedback\u2014would love to hear your thoughts, suggestions, or even criticisms on the idea, implementation, integrations, or anything else.<p>Thanks for checking it out!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["vercel"], "value": "Show HN: Quell \u2013 AI QA Agent Working Across Linear, <em>Vercel</em>, Jira, Netlify, Figma"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.quellit.ai/"}}, "_tags": ["story", "author_buildinext", "story_44083596", "show_hn"], "author": "buildinext", "children": [44187584, 44193749], "created_at": "2025-05-24T20:25:34Z", "created_at_i": 1748118334, "num_comments": 2, "objectID": "44083596", "points": 7, "story_id": 44083596, "story_text": "Hey HN! I&#x27;ve been building Quell, an AI QA testing agent designed to run tests directly triggered from issue trackers (Jira, Linear) or designs (Figma) or CI&#x2F;CD platforms (Vercel, Netlify, GitHub), ensuring rapid and accurate acceptance testing without manual effort.<p>What problem does it solve?\nAs a product manager and founder myself, I constantly faced issues releasing a new feature only to discover critical bugs or that the build doesn&#x27;t fully meet acceptance criteria. Bottlenecks in our QA and release process\u2014tickets and issues stuck in manual testing, slow deployments due to delayed verification, and missed acceptance criteria leading to bugs slipping into production. I built Quell to automate these tedious steps, freeing up teams to focus on actual feature development and faster iterations.<p>How is it different?<p>Integrates with existing dev workflows\u2014triggers tests via Jira&#x2F;Linear issue status automatically testing Vercel, or Netlify or other URL deployment builds.<p>Tests against explicit acceptance criteria pulled directly from your issue tracker.<p>Current Capabilities (free to test):<p>Automatically trigger QA runs from Linear&#x2F;Jira issue state transitions or Vercel&#x2F;Netlify deploy previews.<p>Generate immediate test reports and tickets for issues spotted.<p>Quell is ready to test right now\u2014email only required to try out demo functionality directly:<p>Try out [Quellit.ai](<a href=\"http:&#x2F;&#x2F;quellit.ai&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;quellit.ai&#x2F;</a>) for free<p>I&#x27;m actively iterating based on user feedback\u2014would love to hear your thoughts, suggestions, or even criticisms on the idea, implementation, integrations, or anything else.<p>Thanks for checking it out!", "title": "Show HN: Quell \u2013 AI QA Agent Working Across Linear, Vercel, Jira, Netlify, Figma", "updated_at": "2025-06-06T04:40:09Z", "url": "https://www.quellit.ai/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fazlerocks"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "I've been building AI applications using Next.js, GPT, and Langchain. As I'm approaching <em>production</em> scale, I'm curious how others are handling deployment infrastructure.<p>Current stack:\n- Next.js on <em>Vercel</em>\n- Serverless functions for AI/LLM endpoints\n- Pinecone for vector storage<p>Questions for those running AI in <em>production</em>:<p>1. What's your serverless infrastructure choice? (<em>Vercel</em>/Cloud Run/Lambda)<p>2. How are you handling state management for long-running agent tasks?<p>3. What's your approach to cost optimization with LLM API calls?<p>4. Are you self-hosting any components?<p>5. How are you handling vector store scaling?<p>Particularly interested in hearing from teams who've scaled beyond prototype stage. Have you hit any unexpected limitations with serverless for AI workloads?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: What's your serverless stack for AI/LLM apps in <em>production</em>?"}}, "_tags": ["story", "author_fazlerocks", "story_42659704", "ask_hn"], "author": "fazlerocks", "children": [42660081], "created_at": "2025-01-10T20:22:57Z", "created_at_i": 1736540577, "num_comments": 3, "objectID": "42659704", "points": 3, "story_id": 42659704, "story_text": "I&#x27;ve been building AI applications using Next.js, GPT, and Langchain. As I&#x27;m approaching production scale, I&#x27;m curious how others are handling deployment infrastructure.<p>Current stack:\n- Next.js on Vercel\n- Serverless functions for AI&#x2F;LLM endpoints\n- Pinecone for vector storage<p>Questions for those running AI in production:<p>1. What&#x27;s your serverless infrastructure choice? (Vercel&#x2F;Cloud Run&#x2F;Lambda)<p>2. How are you handling state management for long-running agent tasks?<p>3. What&#x27;s your approach to cost optimization with LLM API calls?<p>4. Are you self-hosting any components?<p>5. How are you handling vector store scaling?<p>Particularly interested in hearing from teams who&#x27;ve scaled beyond prototype stage. Have you hit any unexpected limitations with serverless for AI workloads?", "title": "Ask HN: What's your serverless stack for AI/LLM apps in production?", "updated_at": "2025-01-12T21:52:01Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "cyw"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "tl;dr: Like <em>Vercel</em>, but for stateful AI agents. Deploy your container and instantly get an agent with persistent memory, auto-recovery, and a live API endpoint\u2014zero infrastructure work required.<p>Hey HN, I\u2019m Cyw, the founder of Agentainer (<a href=\"https://agentainer.io/\" rel=\"nofollow\">https://agentainer.io/</a>), a platform designed to deploy and manage long-running AI agents with zero DevOps. We just launched the first open source version of Agentainer: Agentainer Lab (<a href=\"https://github.com/oso95/Agentainer-lab\">https://github.com/oso95/Agentainer-lab</a>) on GitHub.<p>Little bit of background: most infrastructure today is built for short-lived, stateless workloads\u2014Lambda, Cloud Run, or even Kubernetes pods. But AI agents aren\u2019t like that. They\u2019re long-running processes with memory, history, and evolving state. Running them reliably in <em>production</em> usually means gluing together a bunch of services (volume mounts, retry queues, crash recovery, gateways, etc.) just to approximate what a simple web app gets out of the box.<p>To make my life easier when deploying agents for projects (both personal and work-related), I started designing an infrastructure layer that could treat agents as durable services from day one. No YAML. No juggling services. Just give it a Docker image or Dockerfile, and Agentainer handles the rest. Basically, a <em>Vercel</em>-like solution.<p>Each agent runs in its own isolated container, with persistent volume mounts, crash recovery, and queued request replay. If an agent crashes mid-task, it restarts and picks up where it left off. Agentainer gives every agent a clean proxy endpoint by default, so you don\u2019t have to worry about port management or network config. Oh, if you\u2019ve ever built long-running agents, you know how important checkpoints are\u2014I got it taken care of already. (Check out: <a href=\"https://github.com/oso95/Agentainer-lab/blob/main/docs/RESILIENT_AGENTS.md\">https://github.com/oso95/Agentainer-lab/blob/main/docs/RESIL...</a>)<p>Everything is CLI-first and API-accessible. In fact, I originally built this so my own coding agent could manage infrastructure without burning tokens repeating shell commands lol. You can deploy, restart, or remove agents programmatically\u2014and the same flow works in dev and prod.<p>I did some math, and for the right workloads like agentic backends with frequent requests or persistent state, this architecture could reduce cloud costs significantly, even by 30~40%, by replacing per-request billing and minimizing infra sprawl. We\u2019re still early, but excited to see what others build on top of it.<p>Anyway, right now Agentainer Lab is focused on local dev and self-hosting. The bigger Agentainer.io roadmap includes observability, audit logs, backup/restore, and full auto-scaling to unlock the full experience. If you\u2019re interested, you can sign up for early access on our website, we\u2019ll only send you one email when the <em>production</em> version launches, and then your email will be deleted from our database.<p>GitHub: <a href=\"https://github.com/oso95/Agentainer-lab\">https://github.com/oso95/Agentainer-lab</a>\nPlatform: <a href=\"https://agentainer.io\" rel=\"nofollow\">https://agentainer.io</a><p>Would love to hear feedback from others working on LLM agents or trying to run stateful workloads in <em>production</em>. What\u2019s your current setup? Do you think this can help you?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["vercel"], "value": "Show HN: I Built \"<em>Vercel</em> for Stateful AI Agents\" \u2013 open-source, cost-efficient"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/oso95/Agentainer-lab"}}, "_tags": ["story", "author_cyw", "story_44716929", "show_hn"], "author": "cyw", "children": [44728333, 44728440], "created_at": "2025-07-28T23:15:36Z", "created_at_i": 1753744536, "num_comments": 1, "objectID": "44716929", "points": 2, "story_id": 44716929, "story_text": "tl;dr: Like Vercel, but for stateful AI agents. Deploy your container and instantly get an agent with persistent memory, auto-recovery, and a live API endpoint\u2014zero infrastructure work required.<p>Hey HN, I\u2019m Cyw, the founder of Agentainer (<a href=\"https:&#x2F;&#x2F;agentainer.io&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;agentainer.io&#x2F;</a>), a platform designed to deploy and manage long-running AI agents with zero DevOps. We just launched the first open source version of Agentainer: Agentainer Lab (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;oso95&#x2F;Agentainer-lab\">https:&#x2F;&#x2F;github.com&#x2F;oso95&#x2F;Agentainer-lab</a>) on GitHub.<p>Little bit of background: most infrastructure today is built for short-lived, stateless workloads\u2014Lambda, Cloud Run, or even Kubernetes pods. But AI agents aren\u2019t like that. They\u2019re long-running processes with memory, history, and evolving state. Running them reliably in production usually means gluing together a bunch of services (volume mounts, retry queues, crash recovery, gateways, etc.) just to approximate what a simple web app gets out of the box.<p>To make my life easier when deploying agents for projects (both personal and work-related), I started designing an infrastructure layer that could treat agents as durable services from day one. No YAML. No juggling services. Just give it a Docker image or Dockerfile, and Agentainer handles the rest. Basically, a Vercel-like solution.<p>Each agent runs in its own isolated container, with persistent volume mounts, crash recovery, and queued request replay. If an agent crashes mid-task, it restarts and picks up where it left off. Agentainer gives every agent a clean proxy endpoint by default, so you don\u2019t have to worry about port management or network config. Oh, if you\u2019ve ever built long-running agents, you know how important checkpoints are\u2014I got it taken care of already. (Check out: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;oso95&#x2F;Agentainer-lab&#x2F;blob&#x2F;main&#x2F;docs&#x2F;RESILIENT_AGENTS.md\">https:&#x2F;&#x2F;github.com&#x2F;oso95&#x2F;Agentainer-lab&#x2F;blob&#x2F;main&#x2F;docs&#x2F;RESIL...</a>)<p>Everything is CLI-first and API-accessible. In fact, I originally built this so my own coding agent could manage infrastructure without burning tokens repeating shell commands lol. You can deploy, restart, or remove agents programmatically\u2014and the same flow works in dev and prod.<p>I did some math, and for the right workloads like agentic backends with frequent requests or persistent state, this architecture could reduce cloud costs significantly, even by 30~40%, by replacing per-request billing and minimizing infra sprawl. We\u2019re still early, but excited to see what others build on top of it.<p>Anyway, right now Agentainer Lab is focused on local dev and self-hosting. The bigger Agentainer.io roadmap includes observability, audit logs, backup&#x2F;restore, and full auto-scaling to unlock the full experience. If you\u2019re interested, you can sign up for early access on our website, we\u2019ll only send you one email when the production version launches, and then your email will be deleted from our database.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;oso95&#x2F;Agentainer-lab\">https:&#x2F;&#x2F;github.com&#x2F;oso95&#x2F;Agentainer-lab</a>\nPlatform: <a href=\"https:&#x2F;&#x2F;agentainer.io\" rel=\"nofollow\">https:&#x2F;&#x2F;agentainer.io</a><p>Would love to hear feedback from others working on LLM agents or trying to run stateful workloads in production. What\u2019s your current setup? Do you think this can help you?", "title": "Show HN: I Built \"Vercel for Stateful AI Agents\" \u2013 open-source, cost-efficient", "updated_at": "2025-07-29T21:27:37Z", "url": "https://github.com/oso95/Agentainer-lab"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Ohans_favour"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["vercel", "production"], "value": "Hey HN,<p>We built Bluebag, a runtime that lets you import Agent Skills from skills.sh and use them in your <em>production</em> AI agents.<p>Demo: <a href=\"https://www.bluebag.ai/playground\" rel=\"nofollow\">https://www.bluebag.ai/playground</a>\nBlog: <a href=\"https://www.bluebag.ai/blog/import-skills-sh-into-bluebag\" rel=\"nofollow\">https://www.bluebag.ai/blog/import-skills-sh-into-bluebag</a>\nDocs: <a href=\"https://bluebag.ai/docs\" rel=\"nofollow\">https://bluebag.ai/docs</a><p>The problem: skills.sh has 100+ open-source Agent Skills (PDF processing, code review, data extraction, etc.). They work great in Claude and Cursor. But if you want to use them in your own agent with <em>Vercel</em> AI SDK or LangChain, you need to build sandboxing, dependency management, file storage, etc.<p>What we built: Import a skill, get tools back. We handle the infrastructure.<p><pre><code>    import { Bluebag } from &quot;@bluebag/ai-sdk&quot;;\n    \n    const bluebag = new Bluebag({ apiKey: process.env.BLUEBAG_API_KEY });\n    const config = await bluebag.enhance({ model, messages });\n    const result = streamText(config);\n</code></pre>\nTo import a skill, just swap the URL:<p><pre><code>    skills.sh/owner/skill \u2192 bluebag.ai/owner/skill\n</code></pre>\nThat's it. The skill runs in an isolated sandbox with deps pre-installed.<p>Stack: Isolated VMs per tenant, skills loaded at /skills/{name}, tools for bash/code execution/file access injected automatically.<p>Works with: <em>Vercel</em> AI SDK, LangChain, any model (Claude, GPT-4, Gemini, Llama).<p>Would love feedback, especially on what skills you'd want to run in <em>production</em>."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Import any skills.sh skill and run it in <em>production</em> (2 lines of code)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.bluebag.ai/blog/import-skills-sh-into-bluebag"}}, "_tags": ["story", "author_Ohans_favour", "story_46827560", "show_hn"], "author": "Ohans_favour", "created_at": "2026-01-30T17:55:15Z", "created_at_i": 1769795715, "num_comments": 0, "objectID": "46827560", "points": 2, "story_id": 46827560, "story_text": "Hey HN,<p>We built Bluebag, a runtime that lets you import Agent Skills from skills.sh and use them in your production AI agents.<p>Demo: <a href=\"https:&#x2F;&#x2F;www.bluebag.ai&#x2F;playground\" rel=\"nofollow\">https:&#x2F;&#x2F;www.bluebag.ai&#x2F;playground</a>\nBlog: <a href=\"https:&#x2F;&#x2F;www.bluebag.ai&#x2F;blog&#x2F;import-skills-sh-into-bluebag\" rel=\"nofollow\">https:&#x2F;&#x2F;www.bluebag.ai&#x2F;blog&#x2F;import-skills-sh-into-bluebag</a>\nDocs: <a href=\"https:&#x2F;&#x2F;bluebag.ai&#x2F;docs\" rel=\"nofollow\">https:&#x2F;&#x2F;bluebag.ai&#x2F;docs</a><p>The problem: skills.sh has 100+ open-source Agent Skills (PDF processing, code review, data extraction, etc.). They work great in Claude and Cursor. But if you want to use them in your own agent with Vercel AI SDK or LangChain, you need to build sandboxing, dependency management, file storage, etc.<p>What we built: Import a skill, get tools back. We handle the infrastructure.<p><pre><code>    import { Bluebag } from &quot;@bluebag&#x2F;ai-sdk&quot;;\n    \n    const bluebag = new Bluebag({ apiKey: process.env.BLUEBAG_API_KEY });\n    const config = await bluebag.enhance({ model, messages });\n    const result = streamText(config);\n</code></pre>\nTo import a skill, just swap the URL:<p><pre><code>    skills.sh&#x2F;owner&#x2F;skill \u2192 bluebag.ai&#x2F;owner&#x2F;skill\n</code></pre>\nThat&#x27;s it. The skill runs in an isolated sandbox with deps pre-installed.<p>Stack: Isolated VMs per tenant, skills loaded at &#x2F;skills&#x2F;{name}, tools for bash&#x2F;code execution&#x2F;file access injected automatically.<p>Works with: Vercel AI SDK, LangChain, any model (Claude, GPT-4, Gemini, Llama).<p>Would love feedback, especially on what skills you&#x27;d want to run in production.", "title": "Show HN: Import any skills.sh skill and run it in production (2 lines of code)", "updated_at": "2026-01-31T07:25:40Z", "url": "https://www.bluebag.ai/blog/import-skills-sh-into-bluebag"}], "hitsPerPage": 15, "nbHits": 128, "nbPages": 9, "page": 0, "params": "query=vercel+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 11, "processingTimingsMS": {"_request": {"roundTrip": 19}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 7, "scanning": 2, "total": 10}, "total": 11}, "query": "vercel production", "serverTimeMS": 13}}