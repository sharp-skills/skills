{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pigon1002"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "I'm building an open-source LMS with a content studio for instructors \u2014 exams, quizzes, assignments, courses.<p>Hit a wall with preview.<p>Instructors don't want a fake preview.<p>They want to actually take the exam they just built \u2014 real timer, real submission, real grading, state persisted across requests \u2014 then either publish it or throw everything away.<p>Looked at three options.<p>PostgreSQL schema separation is conceptually the cleanest but Django migrations get painful fast.<p>is_draft flags end up as conditionals in every layer.<p>Snapshot tables can't run real workflows.<p>What I actually want is <em>pytest</em>-style DB isolation in <em>production</em>.<p>Persistable, discardable.<p>Does this exist? How do systems like this usually handle it?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: How do you implement <em>production</em>-grade draft isolation in Django?"}}, "_tags": ["story", "author_pigon1002", "story_47135209", "ask_hn"], "author": "pigon1002", "children": [47136088], "created_at": "2026-02-24T10:13:39Z", "created_at_i": 1771928019, "num_comments": 1, "objectID": "47135209", "points": 1, "story_id": 47135209, "story_text": "I&#x27;m building an open-source LMS with a content studio for instructors \u2014 exams, quizzes, assignments, courses.<p>Hit a wall with preview.<p>Instructors don&#x27;t want a fake preview.<p>They want to actually take the exam they just built \u2014 real timer, real submission, real grading, state persisted across requests \u2014 then either publish it or throw everything away.<p>Looked at three options.<p>PostgreSQL schema separation is conceptually the cleanest but Django migrations get painful fast.<p>is_draft flags end up as conditionals in every layer.<p>Snapshot tables can&#x27;t run real workflows.<p>What I actually want is pytest-style DB isolation in production.<p>Persistable, discardable.<p>Does this exist? How do systems like this usually handle it?", "title": "Ask HN: How do you implement production-grade draft isolation in Django?", "updated_at": "2026-02-24T12:06:41Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mdifrancesco"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "Hi HN,<p>Today I open-sourced SpokedPy \u2014 a visual-first programming platform that treats source code as a fully translatable, executable, and auditable data structure.<p>I built the entire core 81k+ lines of modular, <em>production</em>-oriented Python, 633+ <em>pytest</em> cases including property-based) in just 7 intense winter days (mostly with Claude Opus 4.6 which was released 3 days after the initial start of development).<p>It was a \u201cfor fun\u201d side project to see how dealing with multiple languages in a single visual way could be accomplished, and in doing so I was able to chase some features that I didn't see coming during the specification creation and am pleasantly surprised in the result.<p>Core architecture highlights:<p>\u2022 Universal Intermediate Representation (UIR) \u2013 lossless bidirectional round-tripping and editing between visual nodes and real source code across Python, JavaScript/TypeScript, Rust, Go, Java, C#, Kotlin, Swift, Scala, C, SQL, Bash, Lua, PHP, Ruby, R and more<p>\u2022 Four visual paradigms in one canvas: block-based, flow, graph, and hybrid<p>\u2022 Live execution engine with a register-file style Node Registry that supports true hot-swapping of implementations across 15+ language backends<p>\u2022 Parallax SVG rendering \u2013 the actual generated source code floats inside every visual node as you edit (very satisfying)<p>\u2022 Full Session Ledger (Kafka-style event sourcing) for complete time-travel debugging and auditability<p>\u2022 AST-Grep-powered cross-language refactoring<p>Full 40+-page technical specification (reads like a paper):<p>https://github.com/madnguvu/spokedpy/blob/main/docs/TECHNICAL_SPECIFICATION.md<p>Repo + one-minute local demo (MIT license, no sign-up, runs on localhost:5002):<p>https://github.com/madnguvu/spokedpy<p>Quick start:<p>1. `git clone https://github.com/madnguvu/spokedpy.git &amp;&amp; cd spokedpy`\n2. `pip install -r requirements.txt`\n3. `cd web_interface &amp;&amp; python app.py`\n4. Open http://localhost:5002<p>This was a fun and rewarding project that gave me some excellent insights into Claude Opus 4.6 in particular \u2014 felt like the right time to let one go.<p>This is v0.1 released literally hours ago. I\u2019d genuinely love candid technical feedback \u2014 especially on the UIR design, Node Registry hot-swapping, ledger semantics, or visual canvas ergonomics. Suggestions for additional language backends or edge-case demos are also very welcome.<p>Happy to answer any questions in the thread.<p>Enjoy,\nMatthew DiFrancesco\nReach out:  difran@gmail.com"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "SpokedPy \u2013 Polyglot visual IDE with Universal IR, live execution (17 languages)"}}, "_tags": ["story", "author_mdifrancesco", "story_47154642", "ask_hn"], "author": "mdifrancesco", "created_at": "2026-02-25T17:28:49Z", "created_at_i": 1772040529, "num_comments": 0, "objectID": "47154642", "points": 1, "story_id": 47154642, "story_text": "Hi HN,<p>Today I open-sourced SpokedPy \u2014 a visual-first programming platform that treats source code as a fully translatable, executable, and auditable data structure.<p>I built the entire core 81k+ lines of modular, production-oriented Python, 633+ pytest cases including property-based) in just 7 intense winter days (mostly with Claude Opus 4.6 which was released 3 days after the initial start of development).<p>It was a \u201cfor fun\u201d side project to see how dealing with multiple languages in a single visual way could be accomplished, and in doing so I was able to chase some features that I didn&#x27;t see coming during the specification creation and am pleasantly surprised in the result.<p>Core architecture highlights:<p>\u2022 Universal Intermediate Representation (UIR) \u2013 lossless bidirectional round-tripping and editing between visual nodes and real source code across Python, JavaScript&#x2F;TypeScript, Rust, Go, Java, C#, Kotlin, Swift, Scala, C, SQL, Bash, Lua, PHP, Ruby, R and more<p>\u2022 Four visual paradigms in one canvas: block-based, flow, graph, and hybrid<p>\u2022 Live execution engine with a register-file style Node Registry that supports true hot-swapping of implementations across 15+ language backends<p>\u2022 Parallax SVG rendering \u2013 the actual generated source code floats inside every visual node as you edit (very satisfying)<p>\u2022 Full Session Ledger (Kafka-style event sourcing) for complete time-travel debugging and auditability<p>\u2022 AST-Grep-powered cross-language refactoring<p>Full 40+-page technical specification (reads like a paper):<p>https:&#x2F;&#x2F;github.com&#x2F;madnguvu&#x2F;spokedpy&#x2F;blob&#x2F;main&#x2F;docs&#x2F;TECHNICAL_SPECIFICATION.md<p>Repo + one-minute local demo (MIT license, no sign-up, runs on localhost:5002):<p>https:&#x2F;&#x2F;github.com&#x2F;madnguvu&#x2F;spokedpy<p>Quick start:<p>1. `git clone https:&#x2F;&#x2F;github.com&#x2F;madnguvu&#x2F;spokedpy.git &amp;&amp; cd spokedpy`\n2. `pip install -r requirements.txt`\n3. `cd web_interface &amp;&amp; python app.py`\n4. Open http:&#x2F;&#x2F;localhost:5002<p>This was a fun and rewarding project that gave me some excellent insights into Claude Opus 4.6 in particular \u2014 felt like the right time to let one go.<p>This is v0.1 released literally hours ago. I\u2019d genuinely love candid technical feedback \u2014 especially on the UIR design, Node Registry hot-swapping, ledger semantics, or visual canvas ergonomics. Suggestions for additional language backends or edge-case demos are also very welcome.<p>Happy to answer any questions in the thread.<p>Enjoy,\nMatthew DiFrancesco\nReach out:  difran@gmail.com", "title": "SpokedPy \u2013 Polyglot visual IDE with Universal IR, live execution (17 languages)", "updated_at": "2026-02-25T17:30:03Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "samj1912"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "Show HN: CloudCoil \u2013 <em>Production</em>-ready Python client for the cloud-native ecosystem<p>I built CloudCoil (<a href=\"https://github.com/cloudcoil/cloudcoil\">https://github.com/cloudcoil/cloudcoil</a>) to make cloud-native development in Python feel first-class, starting with a modern async Kubernetes client. Frustrated with existing tools that felt like awkward ports from Go/Java, I focused on creating an API that Python developers would actually enjoy using.<p>Installation is as simple as:\n    uv add cloudcoil[kubernetes]  # Using uv (recommended)\n    pip install cloudcoil[kubernetes]  # Using pip<p>Key features:\n-  Elegant, truly Pythonic API that follows Python idioms\n-  Async-first with native async/await (but sync works too!)\n-  Full type safety with MyPy + Pydantic\n-  Zero-config <em>pytest</em> fixtures for K8s integration testing<p>Quick taste of the API:<p><pre><code>    # It's this simple to work with resources\n    service = k8s.core.v1.Service.get(&quot;kubernetes&quot;)\n\n    # Async iteration feels natural\n    async for pod in await k8s.core.v1.Pod.async_list():\n        print(f&quot;Found pod: {pod.metadata.name}&quot;)\n\n    # Create resources with pure Python syntax\n    deployment = k8s.apps.v1.Deployment(\n        metadata=dict(name=&quot;web&quot;),\n        spec=dict(replicas=3)\n    ).create()\n</code></pre>\nThe ecosystem is growing! We already have first-class integrations for:<p>- cert-manager (cloudcoil.models.cert_manager)\n- FluxCD (cloudcoil.models.fluxcd)\n- Kyverno (cloudcoil.models.kyverno)<p>Missing your favorite operator? I've made it super easy to add new integrations using our cookiecutter template and codegen tools.<p>I'd especially love feedback on:\n1. The API design - does it feel natural to Python devs?\n2. Testing features - what else would make k8s testing easier?\n3. Which operators/CRDs you'd most like to see integrated next<p>Check out <a href=\"https://github.com/cloudcoil/cloudcoil\">https://github.com/cloudcoil/cloudcoil</a> or try it out with PyPI: cloudcoil"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: CloudCoil \u2013 <em>Production</em>-ready Python client for cloud-native ecosystem"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/cloudcoil/cloudcoil"}}, "_tags": ["story", "author_samj1912", "story_42785596", "show_hn"], "author": "samj1912", "created_at": "2025-01-21T21:56:04Z", "created_at_i": 1737496564, "num_comments": 0, "objectID": "42785596", "points": 5, "story_id": 42785596, "story_text": "Show HN: CloudCoil \u2013 Production-ready Python client for the cloud-native ecosystem<p>I built CloudCoil (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;cloudcoil&#x2F;cloudcoil\">https:&#x2F;&#x2F;github.com&#x2F;cloudcoil&#x2F;cloudcoil</a>) to make cloud-native development in Python feel first-class, starting with a modern async Kubernetes client. Frustrated with existing tools that felt like awkward ports from Go&#x2F;Java, I focused on creating an API that Python developers would actually enjoy using.<p>Installation is as simple as:\n    uv add cloudcoil[kubernetes]  # Using uv (recommended)\n    pip install cloudcoil[kubernetes]  # Using pip<p>Key features:\n-  Elegant, truly Pythonic API that follows Python idioms\n-  Async-first with native async&#x2F;await (but sync works too!)\n-  Full type safety with MyPy + Pydantic\n-  Zero-config pytest fixtures for K8s integration testing<p>Quick taste of the API:<p><pre><code>    # It&#x27;s this simple to work with resources\n    service = k8s.core.v1.Service.get(&quot;kubernetes&quot;)\n\n    # Async iteration feels natural\n    async for pod in await k8s.core.v1.Pod.async_list():\n        print(f&quot;Found pod: {pod.metadata.name}&quot;)\n\n    # Create resources with pure Python syntax\n    deployment = k8s.apps.v1.Deployment(\n        metadata=dict(name=&quot;web&quot;),\n        spec=dict(replicas=3)\n    ).create()\n</code></pre>\nThe ecosystem is growing! We already have first-class integrations for:<p>- cert-manager (cloudcoil.models.cert_manager)\n- FluxCD (cloudcoil.models.fluxcd)\n- Kyverno (cloudcoil.models.kyverno)<p>Missing your favorite operator? I&#x27;ve made it super easy to add new integrations using our cookiecutter template and codegen tools.<p>I&#x27;d especially love feedback on:\n1. The API design - does it feel natural to Python devs?\n2. Testing features - what else would make k8s testing easier?\n3. Which operators&#x2F;CRDs you&#x27;d most like to see integrated next<p>Check out <a href=\"https:&#x2F;&#x2F;github.com&#x2F;cloudcoil&#x2F;cloudcoil\">https:&#x2F;&#x2F;github.com&#x2F;cloudcoil&#x2F;cloudcoil</a> or try it out with PyPI: cloudcoil", "title": "Show HN: CloudCoil \u2013 Production-ready Python client for cloud-native ecosystem", "updated_at": "2025-01-24T23:11:31Z", "url": "https://github.com/cloudcoil/cloudcoil"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "niklasdev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "Hello HN<p>After building numerous FastAPI backends, I consistently found myself repeating the same configuration, including auth, email, payments, migrations, etc.<p>In order to handle all the boilerplate, I developed FastLaunchAPI, a <em>production</em>-ready startup package.<p>Included are:<p><pre><code>    JWT authentication with social login and email\n\n    Webhooks and Stripe billing together\n\n    Alembic, PostgreSQL, and SQLAlchemy\n\n    Background work for celebrities\n\n    SMTP email that includes templates\n\n    Integration of LangChain and OpenAI\n\n    Setup for <em>Pytest</em> + API documents\n</code></pre>\nIt is designed to bring you to <em>production</em> in less than half an hour and is modular.<p>There is a comprehensive documentation, which can be viewed before pruchase.<p>I would be happy to hear any comments or inquiries!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: FastLaunchAPI \u2013 A <em>production</em>-ready FastAPI template batteries included"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://fastlaunchapi.dev/"}}, "_tags": ["story", "author_niklasdev", "story_44716785", "show_hn"], "author": "niklasdev", "children": [44719492, 44721484], "created_at": "2025-07-28T22:59:25Z", "created_at_i": 1753743565, "num_comments": 4, "objectID": "44716785", "points": 2, "story_id": 44716785, "story_text": "Hello HN<p>After building numerous FastAPI backends, I consistently found myself repeating the same configuration, including auth, email, payments, migrations, etc.<p>In order to handle all the boilerplate, I developed FastLaunchAPI, a production-ready startup package.<p>Included are:<p><pre><code>    JWT authentication with social login and email\n\n    Webhooks and Stripe billing together\n\n    Alembic, PostgreSQL, and SQLAlchemy\n\n    Background work for celebrities\n\n    SMTP email that includes templates\n\n    Integration of LangChain and OpenAI\n\n    Setup for Pytest + API documents\n</code></pre>\nIt is designed to bring you to production in less than half an hour and is modular.<p>There is a comprehensive documentation, which can be viewed before pruchase.<p>I would be happy to hear any comments or inquiries!", "title": "Show HN: FastLaunchAPI \u2013 A production-ready FastAPI template batteries included", "updated_at": "2026-02-01T13:26:44Z", "url": "https://fastlaunchapi.dev/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Winipedia"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "pyrig \u2013 <em>Production</em>-ready Python project infrastructure in three commands<p>I built pyrig to stop spending hours setting up the same project infrastructure repeatedly.<p>uv init\nuv add pyrig\nuv run pyrig init<p>You get: source structure with a Typer CLI, <em>pytest</em> with 90% coverage enforcement, GitHub Actions (CI, release, deploy), MkDocs site, git hooks, Containerfile, and all the config files \u2014 pyproject.toml, .gitignore, branch protection, issue templates, everything for a full Python project.<p>Ships with all of Astral's tools (uv, ruff with all rules enabled, ty), plus <em>pytest</em>-cov, bandit, pip-audit, rumdl, prek, MkDocs Material, and Podman. Everything is pre-configured and wired into CI/CD and git hooks from the start.<p>The interesting part is what happens after scaffolding.<p>pyrig isn't a one-shot template generator. Every config is a Python class. Running &quot;pyrig mkroot&quot; regenerates and validates all configs \u2014 merging missing values without removing your customizations. Change your project description in pyproject.toml, rerun, and it propagates to your README and docs. Fully idempotent.<p><em>pytest</em> enforces project correctness. 11 autouse session fixtures run before your tests: they verify every source module has a corresponding test file (auto-generating skeletons if missing), that no unittest usage exists, that src/ doesn't import from dev/, that there are no namespace packages, and that configs are up to date. You can't get a green test suite with a broken project structure.<p>Zero-boilerplate CLIs. Any public function in subcommands.py becomes a CLI command automatically \u2014 no decorators, no registration:<p>my_project/dev/cli/subcommands.py\ndef greet(name: str) -&gt; None:\n&quot;&quot;&quot;Say hello.&quot;&quot;&quot;\nprint(f&quot;Hello, {name}!&quot;)<p>$ uv run my-project greet --name World\nHello, World!<p>Automatic test generation. Add a new file my_project/src/utils.py, run <em>pytest</em>, and tests/test_my_project/test_src/test_utils.py appears with a NotImplementedError stub so you know what still needs writing. Customizable via subclassing.<p>Config subclassing. Want a custom git hook? Subclass PrekConfigFile, call super(), append your hook. pyrig discovers it automatically \u2014 the leaf class in the dependency chain always wins.<p>Multi-package inheritance. Build a base package on top of pyrig with shared configs, fixtures, and CLI commands. Every downstream project inherits everything:<p>pyrig -&gt; service-base -&gt; auth-service\n-&gt; payment-service\n-&gt; notification-service<p>All three services get the same standards, hooks, and CI/CD \u2014 defined once in service-base.<p>Everything is adjustable. Every tool and config can be customized or replaced through subclassing. Tools like ruff, ty, and <em>pytest</em> are wrapped in Tool classes \u2014 subclass one and pyrig uses yours instead. Want black instead of ruff? Subclass it. Config files work the same way. Standard Python inheritance, no patching.<p>Source: <a href=\"https://github.com/Winipedia/pyrig\" rel=\"nofollow\">https://github.com/Winipedia/pyrig</a>\nDocs: <a href=\"https://winipedia.github.io/pyrig/\" rel=\"nofollow\">https://winipedia.github.io/pyrig/</a>\nPyPI: <a href=\"https://pypi.org/project/pyrig/\" rel=\"nofollow\">https://pypi.org/project/pyrig/</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Pyrig \u2013 One command to set up a <em>production</em>-ready Python project"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Winipedia/pyrig"}}, "_tags": ["story", "author_Winipedia", "story_46947598", "show_hn"], "author": "Winipedia", "created_at": "2026-02-09T16:55:25Z", "created_at_i": 1770656125, "num_comments": 0, "objectID": "46947598", "points": 2, "story_id": 46947598, "story_text": "pyrig \u2013 Production-ready Python project infrastructure in three commands<p>I built pyrig to stop spending hours setting up the same project infrastructure repeatedly.<p>uv init\nuv add pyrig\nuv run pyrig init<p>You get: source structure with a Typer CLI, pytest with 90% coverage enforcement, GitHub Actions (CI, release, deploy), MkDocs site, git hooks, Containerfile, and all the config files \u2014 pyproject.toml, .gitignore, branch protection, issue templates, everything for a full Python project.<p>Ships with all of Astral&#x27;s tools (uv, ruff with all rules enabled, ty), plus pytest-cov, bandit, pip-audit, rumdl, prek, MkDocs Material, and Podman. Everything is pre-configured and wired into CI&#x2F;CD and git hooks from the start.<p>The interesting part is what happens after scaffolding.<p>pyrig isn&#x27;t a one-shot template generator. Every config is a Python class. Running &quot;pyrig mkroot&quot; regenerates and validates all configs \u2014 merging missing values without removing your customizations. Change your project description in pyproject.toml, rerun, and it propagates to your README and docs. Fully idempotent.<p>pytest enforces project correctness. 11 autouse session fixtures run before your tests: they verify every source module has a corresponding test file (auto-generating skeletons if missing), that no unittest usage exists, that src&#x2F; doesn&#x27;t import from dev&#x2F;, that there are no namespace packages, and that configs are up to date. You can&#x27;t get a green test suite with a broken project structure.<p>Zero-boilerplate CLIs. Any public function in subcommands.py becomes a CLI command automatically \u2014 no decorators, no registration:<p>my_project&#x2F;dev&#x2F;cli&#x2F;subcommands.py\ndef greet(name: str) -&gt; None:\n&quot;&quot;&quot;Say hello.&quot;&quot;&quot;\nprint(f&quot;Hello, {name}!&quot;)<p>$ uv run my-project greet --name World\nHello, World!<p>Automatic test generation. Add a new file my_project&#x2F;src&#x2F;utils.py, run pytest, and tests&#x2F;test_my_project&#x2F;test_src&#x2F;test_utils.py appears with a NotImplementedError stub so you know what still needs writing. Customizable via subclassing.<p>Config subclassing. Want a custom git hook? Subclass PrekConfigFile, call super(), append your hook. pyrig discovers it automatically \u2014 the leaf class in the dependency chain always wins.<p>Multi-package inheritance. Build a base package on top of pyrig with shared configs, fixtures, and CLI commands. Every downstream project inherits everything:<p>pyrig -&gt; service-base -&gt; auth-service\n-&gt; payment-service\n-&gt; notification-service<p>All three services get the same standards, hooks, and CI&#x2F;CD \u2014 defined once in service-base.<p>Everything is adjustable. Every tool and config can be customized or replaced through subclassing. Tools like ruff, ty, and pytest are wrapped in Tool classes \u2014 subclass one and pyrig uses yours instead. Want black instead of ruff? Subclass it. Config files work the same way. Standard Python inheritance, no patching.<p>Source: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Winipedia&#x2F;pyrig\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Winipedia&#x2F;pyrig</a>\nDocs: <a href=\"https:&#x2F;&#x2F;winipedia.github.io&#x2F;pyrig&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;winipedia.github.io&#x2F;pyrig&#x2F;</a>\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;pyrig&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;pyrig&#x2F;</a>", "title": "Show HN: Pyrig \u2013 One command to set up a production-ready Python project", "updated_at": "2026-02-09T17:29:49Z", "url": "https://github.com/Winipedia/pyrig"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "krawczstef"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "Hey HN! We\u2019re Stefan and Elijah, co-founders of DAGWorks (<a href=\"https:///www.dagworks.io\" rel=\"nofollow\">https:///www.dagworks.io</a>). We\u2019re on a mission to eliminate the insane inefficiency of building and maintaining ML pipelines in <em>production</em>.<p>DAGWorks is based on Hamilton, an open-source project that we created and recently forked (<a href=\"https://github.com/dagworks-inc/hamilton\">https://github.com/dagworks-inc/hamilton</a>). Hamilton is a set of high-level conventions for Python functions that can be automatically converted into working ETL pipelines. To that, we're adding a closed-source offering that goes a step further, plugging these functions into a wide array of <em>production</em> ML stacks.<p>ML pipelines consist of computational steps (code + data) that produce a working statistical model that a business can use. A typical pipeline might be (1) pull raw data (Extract), (2) transform that data into inputs for the model (Transform), (3) define a statistical model (Transform), (4) use that statistical model to predict on another data set (Transform) and (5) push that data for downstream use (Load). Instead of \u201cpipeline\u201d you might hear people call this \u201cworkflow\u201d, \u201cETL\u201d (Extract-Transform-Load), and so on.<p>Maintaining these in <em>production</em> is insanely inefficient because you need both data scientists and software engineers to do it. Data scientists know the models and data, but most can't write the code needed to get things working in <em>production</em> infrastructure\u2014for example, a lot of mid-size companies out there use Snowflake to store data, Pandas/Spark to transform it, and something like databrick's MLFlow to handle model serving. Engineers can handle the latter, but mostly aren't experts in the ML stuff. It's a classic impedance mismatch, with all the horror stories you'd expect\u2014e.g. when data scientists make a change, engineers (or data scientists who aren\u2019t engineers) have to manually propagate the change in <em>production</em>. We've talked to teams who are spending as much as 50% of their time doing this. That's not just expensive, it's gruntwork\u2014those engineers should be working on something else! Basically, maintaining ML pipelines over time sucks for most teams.<p>One way out is to hire people who combine both skills, i.e. data scientists who can also write <em>production</em> code. But these are rare and expensive, and in our experience they usually are only expert at one side of the equation and not as good at the other.<p>The other way is to build your own platform to automatically integrate models + data into your <em>production</em> stack. That way the data scientists can maintain their own work without needing to hand things off to engineers. However, most companies can't afford to make this investment, and even for the ones that can, such in-house layers tend to end up in spaghetti code and tech debt hell, because they're not the company's core product.<p>Elijah and I have been building data and ML tooling for the last 7 years, most recently at Stitch Fix, where we built a ML platform that served over 100 data scientists from various modeling disciplines (some of our blog posts, like [1], hit the front page of HN - thanks!). We saw first hand the issues teams encountered with ML pipelines.<p>Most companies running ML in <em>production</em> need a ratio of 1:1 or 2:1 data scientists to engineers. At bigger companies like Stitch Fix, the ratio is more like 10:1\u2014way more efficient\u2014because they can afford to build the kind of platform described above. With DAGWorks, we want to bring the power of an intuitive ML Pipeline platform to all data science teams, so a ratio of 1:1 is no longer required. A junior data scientist should be able to easily and safely write <em>production</em> code without deep knowledge of underlying infrastructure.<p>We decided to build our startup around Hamilton, in large part due to the reception that it got here [2] - thanks HN! We came up with Hamilton while we were at Stitch Fix (note: if you start an open-source project at an employer, we recommend forking it right away when you start a company. We only just did that and left behind ~900 stars...). We are betting on it being our abstraction layer to enable our vision of how to go about building and maintaining ML pipelines, given what we learned at Stitch Fix. We believe a solution has to have an open source component to be successful (we invite you to check out the code). In terms of why the name DAGWorks? We named the company after Directed Acyclic Graphs because we think the DAG representation, which Hamilton also provides, is key.<p>A quick primer on Hamilton. With Hamilton we use a new paradigm in Python (well not quite \u201cnew\u201d as <em>pytest</em> fixtures use this approach) for defining model pipelines. Users write declarative functions instead of writing procedural code. For example, rather than writing the following pandas code:<p><pre><code>  df['col_c'] = df['col_a'] + df['col_b']\n</code></pre>\nYou would write:<p><pre><code>  def col_c(col_a: pd.Series, col_b: pd.Series) -&gt; pd.Series:\n       &quot;&quot;&quot;Creating column c from summing column a and column b.&quot;&quot;&quot;\n       return col_a + col_b\n</code></pre>\nThen if you wanted to create a new column that used `col_c` you would write:<p><pre><code>  def col_d(col_c: pd.Series) -&gt; pd.Series:\n       # logic\n</code></pre>\nThese functions then define a &quot;dataflow&quot; or a directed acyclic graph (DAG), i.e. we can create a \u201cgraph\u201d with nodes: col_a, col_b, col_c, and col_d, and connect them with edges to know the order in which to call the functions to compute any result. Since you\u2019re forced to write functions, everything becomes unit testable and documentation friendly, with the ability to display lineage. You can kind of think of Hamilton as &quot;DBT for python functions&quot;, if you know what DBT is. Have we piqued your interest? Want to go play with Hamilton? We created <a href=\"https://www.tryhamilton.dev/\" rel=\"nofollow\">https://www.tryhamilton.dev/</a> leveraging pyodide (note it can take a while to load) so you can play around with the basics without leaving your browser - it even works on mobile!<p>What we think is cool about Hamilton is that you don\u2019t need to specify an \u201cexplicit pipeline declaration step\u201d, because it\u2019s all encoded in the function and parameter names! Moreover, everything is encapsulated in functions. So from a framework perspective, if we wanted to (for example) log timing information, or introspect inputs/outputs, delegate the function to Dask or Ray, we can inject that at a framework level, without having to pollute user code. Additionally, we can expose &quot;decorators&quot; (e.g. @tag(...)) that can specify extra metadata to annotate the DAG with, or for use at run time. This is where our DAGWorks Platform fits in, providing off-the-shelf closed source extras in this way.<p>Now, for those of you thinking there\u2019s a lot of competition in this space, or what we\u2019re proposing sounds very similar to existing solutions, here\u2019s some thoughts to help distinguish Hamilton from other approaches/technology: (1) Hamilton's core design principle is helping people write more maintainable code; at a nuts and bolts level, what Hamilton replaces is procedural code that one would write. (2) Hamilton runs anywhere that python runs: notebook, a python script, within airflow, within your python web service, pyspark, etc. E.g. People use Hamilton for executing code in batch tasks and online web services. (3) Hamilton doesn't replace a macro orchestration system like airflow, prefect, dagster, metaflow, zenML, etc. It runs within/uses them. Hamilton helps you not only model the micro - e.g. feature engineering - but can also help you model the macro - e.g. model pipelines. That said, given how big machines are these days, model pipelines can commonly run on a single machine - Hamilton is perfect for this. (4) Hamilton doesn't replace things like Dask, Ray, Spark -- it can run on them, or delegate to them. (5) Hamilton isn't just for building dataframes, though it\u2019s quite good for that, you can model any python object creation with it. Hamilton is data type agnostic.<p>Our closed source offering is currently in private beta, but we'd love to include you in it (see next paragraph). Hamilton is free to use (BSD-3 license) and we\u2019re investing in it heavily. We\u2019re still working through pricing options for the closed source platform; we think we\u2019ll follow the leads of others in the space like Weights &amp; Biases, and Hex.tech here in how they price. For those interested, here\u2019s a video walkthrough of Hamilton, which includes a teaser of what we\u2019re building on the closed source side - <a href=\"https://www.loom.com/share/5d30a96b3261490d91713a18ab27d3b7\" rel=\"nofollow\">https://www.loom.com/share/5d30a96b3261490d91713a18ab27d3b7</a>.<p>Lastly, (1) we\u2019d love feedback on Hamilton (<a href=\"https://github.com/dagworks-inc/hamilton\">https://github.com/dagworks-inc/hamilton</a>) and on any of the above, and what we could do better. To stress the importance of your feedback, we\u2019re going all-in on Hamilton. If Hamilton fails, DAGWorks fails. Given that Hamilton is a bit of a \u201cswiss army knife\u201d of what you could do with it, we need help prioritizing features. E.g. we just released experimental PySpark UDF map support, is that useful? Or perhaps you have streaming feature engineering needs where we could add better support? Or you want a feature to auto generate unit test stubs? Or maybe you are doing a lot of time-series forecasting and want more power features in Hamilton to help you manage inputs to your model? We\u2019d love to hear from you! (2) For those interested in the closed source DAGWorks Platform, you can sign up for early access via www.dagworks.io (leave your email, or schedule a call with me) \u2013 we apologize for not having a self-serve way to onboard just yet. (3) If there\u2019s something this post hasn\u2019t answered, do ask, we\u2019ll try to give you an answer! We look forward to any and all of your comments!<p>[1] <a href=\"https://news.ycombinator.com/item?id=29417998\" rel=\"nofollow\">https://news.ycombinator.com/item?id=29417998</a><p>[2] <a href=\"https://news.ycombinator.com/item?id=29158021\" rel=\"nofollow\">https://news.ycombinator.com/item?id=29158021</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: DAGWorks \u2013 ML platform for data science teams"}}, "_tags": ["story", "author_krawczstef", "story_35056903", "launch_hn"], "author": "krawczstef", "children": [35056929, 35057391, 35057433, 35057781, 35057932, 35058129, 35058183, 35058211, 35059430, 35060613, 35060752, 35060915, 35061668, 35062734, 35062962, 35065862], "created_at": "2023-03-07T16:04:36Z", "created_at_i": 1678205076, "num_comments": 65, "objectID": "35056903", "points": 182, "story_id": 35056903, "story_text": "Hey HN! We\u2019re Stefan and Elijah, co-founders of DAGWorks (<a href=\"https:&#x2F;&#x2F;&#x2F;www.dagworks.io\" rel=\"nofollow\">https:&#x2F;&#x2F;&#x2F;www.dagworks.io</a>). We\u2019re on a mission to eliminate the insane inefficiency of building and maintaining ML pipelines in production.<p>DAGWorks is based on Hamilton, an open-source project that we created and recently forked (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;dagworks-inc&#x2F;hamilton\">https:&#x2F;&#x2F;github.com&#x2F;dagworks-inc&#x2F;hamilton</a>). Hamilton is a set of high-level conventions for Python functions that can be automatically converted into working ETL pipelines. To that, we&#x27;re adding a closed-source offering that goes a step further, plugging these functions into a wide array of production ML stacks.<p>ML pipelines consist of computational steps (code + data) that produce a working statistical model that a business can use. A typical pipeline might be (1) pull raw data (Extract), (2) transform that data into inputs for the model (Transform), (3) define a statistical model (Transform), (4) use that statistical model to predict on another data set (Transform) and (5) push that data for downstream use (Load). Instead of \u201cpipeline\u201d you might hear people call this \u201cworkflow\u201d, \u201cETL\u201d (Extract-Transform-Load), and so on.<p>Maintaining these in production is insanely inefficient because you need both data scientists and software engineers to do it. Data scientists know the models and data, but most can&#x27;t write the code needed to get things working in production infrastructure\u2014for example, a lot of mid-size companies out there use Snowflake to store data, Pandas&#x2F;Spark to transform it, and something like databrick&#x27;s MLFlow to handle model serving. Engineers can handle the latter, but mostly aren&#x27;t experts in the ML stuff. It&#x27;s a classic impedance mismatch, with all the horror stories you&#x27;d expect\u2014e.g. when data scientists make a change, engineers (or data scientists who aren\u2019t engineers) have to manually propagate the change in production. We&#x27;ve talked to teams who are spending as much as 50% of their time doing this. That&#x27;s not just expensive, it&#x27;s gruntwork\u2014those engineers should be working on something else! Basically, maintaining ML pipelines over time sucks for most teams.<p>One way out is to hire people who combine both skills, i.e. data scientists who can also write production code. But these are rare and expensive, and in our experience they usually are only expert at one side of the equation and not as good at the other.<p>The other way is to build your own platform to automatically integrate models + data into your production stack. That way the data scientists can maintain their own work without needing to hand things off to engineers. However, most companies can&#x27;t afford to make this investment, and even for the ones that can, such in-house layers tend to end up in spaghetti code and tech debt hell, because they&#x27;re not the company&#x27;s core product.<p>Elijah and I have been building data and ML tooling for the last 7 years, most recently at Stitch Fix, where we built a ML platform that served over 100 data scientists from various modeling disciplines (some of our blog posts, like [1], hit the front page of HN - thanks!). We saw first hand the issues teams encountered with ML pipelines.<p>Most companies running ML in production need a ratio of 1:1 or 2:1 data scientists to engineers. At bigger companies like Stitch Fix, the ratio is more like 10:1\u2014way more efficient\u2014because they can afford to build the kind of platform described above. With DAGWorks, we want to bring the power of an intuitive ML Pipeline platform to all data science teams, so a ratio of 1:1 is no longer required. A junior data scientist should be able to easily and safely write production code without deep knowledge of underlying infrastructure.<p>We decided to build our startup around Hamilton, in large part due to the reception that it got here [2] - thanks HN! We came up with Hamilton while we were at Stitch Fix (note: if you start an open-source project at an employer, we recommend forking it right away when you start a company. We only just did that and left behind ~900 stars...). We are betting on it being our abstraction layer to enable our vision of how to go about building and maintaining ML pipelines, given what we learned at Stitch Fix. We believe a solution has to have an open source component to be successful (we invite you to check out the code). In terms of why the name DAGWorks? We named the company after Directed Acyclic Graphs because we think the DAG representation, which Hamilton also provides, is key.<p>A quick primer on Hamilton. With Hamilton we use a new paradigm in Python (well not quite \u201cnew\u201d as pytest fixtures use this approach) for defining model pipelines. Users write declarative functions instead of writing procedural code. For example, rather than writing the following pandas code:<p><pre><code>  df[&#x27;col_c&#x27;] = df[&#x27;col_a&#x27;] + df[&#x27;col_b&#x27;]\n</code></pre>\nYou would write:<p><pre><code>  def col_c(col_a: pd.Series, col_b: pd.Series) -&gt; pd.Series:\n       &quot;&quot;&quot;Creating column c from summing column a and column b.&quot;&quot;&quot;\n       return col_a + col_b\n</code></pre>\nThen if you wanted to create a new column that used `col_c` you would write:<p><pre><code>  def col_d(col_c: pd.Series) -&gt; pd.Series:\n       # logic\n</code></pre>\nThese functions then define a &quot;dataflow&quot; or a directed acyclic graph (DAG), i.e. we can create a \u201cgraph\u201d with nodes: col_a, col_b, col_c, and col_d, and connect them with edges to know the order in which to call the functions to compute any result. Since you\u2019re forced to write functions, everything becomes unit testable and documentation friendly, with the ability to display lineage. You can kind of think of Hamilton as &quot;DBT for python functions&quot;, if you know what DBT is. Have we piqued your interest? Want to go play with Hamilton? We created <a href=\"https:&#x2F;&#x2F;www.tryhamilton.dev&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.tryhamilton.dev&#x2F;</a> leveraging pyodide (note it can take a while to load) so you can play around with the basics without leaving your browser - it even works on mobile!<p>What we think is cool about Hamilton is that you don\u2019t need to specify an \u201cexplicit pipeline declaration step\u201d, because it\u2019s all encoded in the function and parameter names! Moreover, everything is encapsulated in functions. So from a framework perspective, if we wanted to (for example) log timing information, or introspect inputs&#x2F;outputs, delegate the function to Dask or Ray, we can inject that at a framework level, without having to pollute user code. Additionally, we can expose &quot;decorators&quot; (e.g. @tag(...)) that can specify extra metadata to annotate the DAG with, or for use at run time. This is where our DAGWorks Platform fits in, providing off-the-shelf closed source extras in this way.<p>Now, for those of you thinking there\u2019s a lot of competition in this space, or what we\u2019re proposing sounds very similar to existing solutions, here\u2019s some thoughts to help distinguish Hamilton from other approaches&#x2F;technology: (1) Hamilton&#x27;s core design principle is helping people write more maintainable code; at a nuts and bolts level, what Hamilton replaces is procedural code that one would write. (2) Hamilton runs anywhere that python runs: notebook, a python script, within airflow, within your python web service, pyspark, etc. E.g. People use Hamilton for executing code in batch tasks and online web services. (3) Hamilton doesn&#x27;t replace a macro orchestration system like airflow, prefect, dagster, metaflow, zenML, etc. It runs within&#x2F;uses them. Hamilton helps you not only model the micro - e.g. feature engineering - but can also help you model the macro - e.g. model pipelines. That said, given how big machines are these days, model pipelines can commonly run on a single machine - Hamilton is perfect for this. (4) Hamilton doesn&#x27;t replace things like Dask, Ray, Spark -- it can run on them, or delegate to them. (5) Hamilton isn&#x27;t just for building dataframes, though it\u2019s quite good for that, you can model any python object creation with it. Hamilton is data type agnostic.<p>Our closed source offering is currently in private beta, but we&#x27;d love to include you in it (see next paragraph). Hamilton is free to use (BSD-3 license) and we\u2019re investing in it heavily. We\u2019re still working through pricing options for the closed source platform; we think we\u2019ll follow the leads of others in the space like Weights &amp; Biases, and Hex.tech here in how they price. For those interested, here\u2019s a video walkthrough of Hamilton, which includes a teaser of what we\u2019re building on the closed source side - <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;5d30a96b3261490d91713a18ab27d3b7\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;5d30a96b3261490d91713a18ab27d3b7</a>.<p>Lastly, (1) we\u2019d love feedback on Hamilton (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;dagworks-inc&#x2F;hamilton\">https:&#x2F;&#x2F;github.com&#x2F;dagworks-inc&#x2F;hamilton</a>) and on any of the above, and what we could do better. To stress the importance of your feedback, we\u2019re going all-in on Hamilton. If Hamilton fails, DAGWorks fails. Given that Hamilton is a bit of a \u201cswiss army knife\u201d of what you could do with it, we need help prioritizing features. E.g. we just released experimental PySpark UDF map support, is that useful? Or perhaps you have streaming feature engineering needs where we could add better support? Or you want a feature to auto generate unit test stubs? Or maybe you are doing a lot of time-series forecasting and want more power features in Hamilton to help you manage inputs to your model? We\u2019d love to hear from you! (2) For those interested in the closed source DAGWorks Platform, you can sign up for early access via www.dagworks.io (leave your email, or schedule a call with me) \u2013 we apologize for not having a self-serve way to onboard just yet. (3) If there\u2019s something this post hasn\u2019t answered, do ask, we\u2019ll try to give you an answer! We look forward to any and all of your comments!<p>[1] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29417998\" rel=\"nofollow\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29417998</a><p>[2] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29158021\" rel=\"nofollow\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29158021</a>", "title": "Launch HN: DAGWorks \u2013 ML platform for data science teams", "updated_at": "2024-09-20T13:27:32Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kvptkr"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "Hi HN! We\u2019re Adrien and Kanav. We met at our previous job, where we spent about a third of our lives combating a constant firehose of bugs. In the hope of reducing this pain for others in the future, we\u2019re working on automating debugging.<p>We\u2019re currently working on a platform that ingests logs and then automatically reproduces, root causes and ultimately fixes <em>production</em> bugs as they happen. You can see some of our work on this here - <a href=\"https://news.ycombinator.com/item?id=39528087\">https://news.ycombinator.com/item?id=39528087</a><p>As we were building the root-cause phase of our automated debugger, we realized that we developed something that resembled an omniscient debugger. Like an omniscient debugger, it also keeps track of variable assignments over time but, you can interact with it at a higher level than a debugger using natural language. We ended up sticking it in a <em>pytest</em> plugin and have been using it ourselves for development over the past few weeks.<p>Using this <em>pytest</em> plugin, you\u2019re able to reason at a much higher level than conventional debuggers and can ask questions like:<p>- Why did function x get hit?<p>- Why was variable y set to this value?<p>- What changes can I make to this code to make this test pass?<p>Here\u2019s a brief demo of this in action: <a href=\"https://www.loom.com/share/94ebe34097a343c39876d7109f2a1428\" rel=\"nofollow\">https://www.loom.com/share/94ebe34097a343c39876d7109f2a1428</a><p>To achieve this, we first instrument the test using sys.settrace (or, on versions of python &gt;3.12, the far better sys.monitoring!) to keep a history of all the functions that were called, along with the calling line numbers. We then re-run the test and use AST parsing to find all the variable assignments and keep track of those changes over time. We also use AST parsing to obtain the source code for these functions. We then neatly format and pass all this context to GPT.<p>We\u2019d love it if you checked the <em>pytest</em> plugin out and we welcome all feedback :) . If you want to chat bugs, our emails are also always open - kanav@leaping.io"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Leaping \u2013 Debug Python tests instantly with an LLM debugger"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/leapingio/leaping"}}, "_tags": ["story", "author_kvptkr", "story_39791301", "show_hn"], "author": "kvptkr", "children": [39792211, 39793207, 39793213, 39793239, 39795083], "created_at": "2024-03-22T14:52:06Z", "created_at_i": 1711119126, "num_comments": 20, "objectID": "39791301", "points": 120, "story_id": 39791301, "story_text": "Hi HN! We\u2019re Adrien and Kanav. We met at our previous job, where we spent about a third of our lives combating a constant firehose of bugs. In the hope of reducing this pain for others in the future, we\u2019re working on automating debugging.<p>We\u2019re currently working on a platform that ingests logs and then automatically reproduces, root causes and ultimately fixes production bugs as they happen. You can see some of our work on this here - <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39528087\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39528087</a><p>As we were building the root-cause phase of our automated debugger, we realized that we developed something that resembled an omniscient debugger. Like an omniscient debugger, it also keeps track of variable assignments over time but, you can interact with it at a higher level than a debugger using natural language. We ended up sticking it in a pytest plugin and have been using it ourselves for development over the past few weeks.<p>Using this pytest plugin, you\u2019re able to reason at a much higher level than conventional debuggers and can ask questions like:<p>- Why did function x get hit?<p>- Why was variable y set to this value?<p>- What changes can I make to this code to make this test pass?<p>Here\u2019s a brief demo of this in action: <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;94ebe34097a343c39876d7109f2a1428\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;94ebe34097a343c39876d7109f2a1428</a><p>To achieve this, we first instrument the test using sys.settrace (or, on versions of python &gt;3.12, the far better sys.monitoring!) to keep a history of all the functions that were called, along with the calling line numbers. We then re-run the test and use AST parsing to find all the variable assignments and keep track of those changes over time. We also use AST parsing to obtain the source code for these functions. We then neatly format and pass all this context to GPT.<p>We\u2019d love it if you checked the pytest plugin out and we welcome all feedback :) . If you want to chat bugs, our emails are also always open - kanav@leaping.io", "title": "Show HN: Leaping \u2013 Debug Python tests instantly with an LLM debugger", "updated_at": "2024-09-20T16:42:32Z", "url": "https://github.com/leapingio/leaping"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "calebkaiser"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "Hey HN! I'm Caleb, one of the contributors to Opik, a new open source framework for LLM evaluations.<p>Over the last few months, my colleagues and I have been working on a project to solve what we see as the most painful parts of writing evals for an LLM application. For this initial release, we've focused on a few core features that we think are the most essential:<p>- Simplifying the implementation of more complex LLM-based evaluation metrics, like Hallucination and Moderation.<p>- Enabling step-by-step tracking, such that you can test and debug each individual component of your LLM application, even in more complex multi-agent architectures.<p>- Exposing an API for &quot;model unit tests&quot; (built on <em>Pytest</em>), to allow you to run evals as part of your CI/CD pipelines<p>- Providing an easy UI for scoring, annotating, and versioning your logged LLM data, for further evaluation or training.<p>It's often hard to feel like you can trust an LLM application in <em>production</em>, not just because of the stochastic nature of the model, but because of the opaqueness of the application itself. Our belief is that with better tooling for evaluations, we can meaningfully improve this situation, and unlock a new wave of LLM applications.<p>You can run Opik locally, or with a free API key via our cloud platform. You can use it with any model server or hosted model, but we currently have a built-in integration with the OpenAI Python library, which means it automatically works not just with OpenAI models, but with any model served via a compatible model server (ollama, vLLM, etc). Opik also currently has out-of-the-box integrations with LangChain, LlamaIndex, Ragas, and a few other popular tools.<p>This is our initial release of Opik, so if you have any feedback or questions, I'd love to hear them!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Opik, an open source LLM evaluation framework"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/comet-ml/opik"}}, "_tags": ["story", "author_calebkaiser", "story_41567192", "show_hn"], "author": "calebkaiser", "children": [41567374, 41568516, 41571313, 41575655, 41579948, 41597605], "created_at": "2024-09-17T13:01:44Z", "created_at_i": 1726578104, "num_comments": 15, "objectID": "41567192", "points": 86, "story_id": 41567192, "story_text": "Hey HN! I&#x27;m Caleb, one of the contributors to Opik, a new open source framework for LLM evaluations.<p>Over the last few months, my colleagues and I have been working on a project to solve what we see as the most painful parts of writing evals for an LLM application. For this initial release, we&#x27;ve focused on a few core features that we think are the most essential:<p>- Simplifying the implementation of more complex LLM-based evaluation metrics, like Hallucination and Moderation.<p>- Enabling step-by-step tracking, such that you can test and debug each individual component of your LLM application, even in more complex multi-agent architectures.<p>- Exposing an API for &quot;model unit tests&quot; (built on Pytest), to allow you to run evals as part of your CI&#x2F;CD pipelines<p>- Providing an easy UI for scoring, annotating, and versioning your logged LLM data, for further evaluation or training.<p>It&#x27;s often hard to feel like you can trust an LLM application in production, not just because of the stochastic nature of the model, but because of the opaqueness of the application itself. Our belief is that with better tooling for evaluations, we can meaningfully improve this situation, and unlock a new wave of LLM applications.<p>You can run Opik locally, or with a free API key via our cloud platform. You can use it with any model server or hosted model, but we currently have a built-in integration with the OpenAI Python library, which means it automatically works not just with OpenAI models, but with any model served via a compatible model server (ollama, vLLM, etc). Opik also currently has out-of-the-box integrations with LangChain, LlamaIndex, Ragas, and a few other popular tools.<p>This is our initial release of Opik, so if you have any feedback or questions, I&#x27;d love to hear them!", "title": "Show HN: Opik, an open source LLM evaluation framework", "updated_at": "2025-07-24T12:54:05Z", "url": "https://github.com/comet-ml/opik"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bulba4aur"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "I built this because Cursor, Claude Code and other agentic AI tools kept giving me tests that looked fine but failed when I ran them. Or worse - I'd ask the agent to run them and it would start looping: fix tests, those fail, then it starts &quot;fixing&quot; my code so tests pass, or just deletes assertions so they &quot;pass&quot;.<p>Out of that frustration I built KeelTest - a VS Code extension that generates <em>pytest</em> tests and executes them, got hooked and decided to push this project forward... When tests fail, it tries to figure out why:<p>- Generation error: Attemps to fix it automatically, then tries again<p>- Bug in your source code: flags it and explains what's wrong<p>How it works:<p>- Static analysis to map dependencies, patterns, services to mock.<p>- Generate a plan for each function and what edge cases to cover<p>- Generate those tests<p>- Execute in &quot;sandbox&quot;<p>- Self-heal failures or flag source bugs<p>Python + <em>pytest</em> only for now. Alpha stage - not all codebases work reliably. But testing on personal projects and a few <em>production</em> apps at work, it's been consistently decent. Works best on simpler applications, sometimes glitches on monorepos setups. Supports Poetry/UV/plain pip setups.<p>Install from VS Code marketplace: <a href=\"https://marketplace.visualstudio.com/items?itemName=KeelCode.keeltest\" rel=\"nofollow\">https://marketplace.visualstudio.com/items?itemName=KeelCode...</a><p>More detailed writeup how it works: <a href=\"https://keelcode.dev/blog/introducing-keeltest\" rel=\"nofollow\">https://keelcode.dev/blog/introducing-keeltest</a><p>Free tier is 7 tests files/month (current limit is &lt;=300 source LOC). To make it easier to try without signing up, giving away a few API keys (they have shared ~30 test files generation quota):<p>KEY-1: tgai_jHOEgOfpMJ_mrtNgSQ6iKKKXFm1RQ7FJOkI0a7LJiWg<p>KEY-2: tgai_NlSZN-4yRYZ15g5SAbDb0V0DRMfVw-bcEIOuzbycip0<p>KEY-3: tgai_kiiSIikrBZothZYqQ76V6zNbb2Qv-o6qiZjYZjeaczc<p>KEY-4: tgai_JBfSV_4w-87bZHpJYX0zLQ8kJfFrzas4dzj0vu31K5E<p>Would love your honest feedback where this could go next, and on which setups it failed, how it failed, it has quite verbose debug output at this stage!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: KeelTest \u2013 AI-driven VS Code unit test generator with bug discovery"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://keelcode.dev/keeltest"}}, "_tags": ["story", "author_bulba4aur", "story_46526088", "show_hn"], "author": "bulba4aur", "children": [46526724, 46526942, 46526999, 46529148, 46585131], "created_at": "2026-01-07T13:22:35Z", "created_at_i": 1767792155, "num_comments": 15, "objectID": "46526088", "points": 30, "story_id": 46526088, "story_text": "I built this because Cursor, Claude Code and other agentic AI tools kept giving me tests that looked fine but failed when I ran them. Or worse - I&#x27;d ask the agent to run them and it would start looping: fix tests, those fail, then it starts &quot;fixing&quot; my code so tests pass, or just deletes assertions so they &quot;pass&quot;.<p>Out of that frustration I built KeelTest - a VS Code extension that generates pytest tests and executes them, got hooked and decided to push this project forward... When tests fail, it tries to figure out why:<p>- Generation error: Attemps to fix it automatically, then tries again<p>- Bug in your source code: flags it and explains what&#x27;s wrong<p>How it works:<p>- Static analysis to map dependencies, patterns, services to mock.<p>- Generate a plan for each function and what edge cases to cover<p>- Generate those tests<p>- Execute in &quot;sandbox&quot;<p>- Self-heal failures or flag source bugs<p>Python + pytest only for now. Alpha stage - not all codebases work reliably. But testing on personal projects and a few production apps at work, it&#x27;s been consistently decent. Works best on simpler applications, sometimes glitches on monorepos setups. Supports Poetry&#x2F;UV&#x2F;plain pip setups.<p>Install from VS Code marketplace: <a href=\"https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=KeelCode.keeltest\" rel=\"nofollow\">https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=KeelCode...</a><p>More detailed writeup how it works: <a href=\"https:&#x2F;&#x2F;keelcode.dev&#x2F;blog&#x2F;introducing-keeltest\" rel=\"nofollow\">https:&#x2F;&#x2F;keelcode.dev&#x2F;blog&#x2F;introducing-keeltest</a><p>Free tier is 7 tests files&#x2F;month (current limit is &lt;=300 source LOC). To make it easier to try without signing up, giving away a few API keys (they have shared ~30 test files generation quota):<p>KEY-1: tgai_jHOEgOfpMJ_mrtNgSQ6iKKKXFm1RQ7FJOkI0a7LJiWg<p>KEY-2: tgai_NlSZN-4yRYZ15g5SAbDb0V0DRMfVw-bcEIOuzbycip0<p>KEY-3: tgai_kiiSIikrBZothZYqQ76V6zNbb2Qv-o6qiZjYZjeaczc<p>KEY-4: tgai_JBfSV_4w-87bZHpJYX0zLQ8kJfFrzas4dzj0vu31K5E<p>Would love your honest feedback where this could go next, and on which setups it failed, how it failed, it has quite verbose debug output at this stage!", "title": "Show HN: KeelTest \u2013 AI-driven VS Code unit test generator with bug discovery", "updated_at": "2026-01-12T07:19:45Z", "url": "https://keelcode.dev/keeltest"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jaraganittah"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "I'm Nagaraj, the founder of Tesmon. Having been both a developer and an engineering leader, from big tech companies to startups, I've personally encountered the challenges of integration testing. It's often so cumbersome that we resort to manual testing.<p>The world of testing is fragmented across multiple frameworks and platforms like Postman, RestAssured, <em>pytest</em>, Selenium, Cypress, Testim, and various custom in-house code using open source frameworks. Building and maintaining this testing infrastructure is not only time-consuming but can be a real pain.<p>Our mission is to revolutionize testing into an assertion-free, near zero effort process by leveraging AI/ML.<p>Current challenges?<p>- Environment Differences: Tests often fail across different environments\u2014local, staging, <em>production</em>\u2014due to setup discrepancies and versioning issues between environments.<p>- Complex Negative Testing: Difficult to manage sequences like receiving an HTTP 200 followed by a 407 error for the same resource.<p>- Frequent Updates: Tests require extensive updates with each application change, including writing assertions for all critical fields in large response bodies.<p>- Unreliable Self-Healing: Self-healing tests frequently fail to correct issues automatically, necessitating manual intervention.<p>- Scalability Issues: No-code/low-code UI tests struggle to scale, becoming a significant time drain as the test suite grows.<p>- Trade-offs Between Automation and Deadlines: Testing automation is often deprioritized due to urgent deadlines, leading to increased manual testing and the risk of regressions.<p>- Resource Limitations: Restricted access to essential databases and services due to compliance requirements, including the need to set up VPN access and manage Jenkins hosted in public clouds because of private subnet restrictions.<p>- Isolation Challenges: Testing everything from only the frontend or backend is often impractical, highlighting the need for integrated testing solutions.<p>- Effort in Framework Construction: Building a testing framework requires significant effort, especially when dealing with modularization and managing different testing environments.<p>How does testing code differ from product code?<p>Testing usually involves sending requests and checking responses, a process that doesn't need to run continuously like microservices or batch jobs. This makes it perfect for AI/ML automation, as the predictable nature of testing tasks allows for efficient learning and optimization without the complexity of continuous monitoring. Traditional programming adds unnecessary complexity to testing, which Tesmon simplifies.<p>Unit tests vs integration tests?<p>We cannot ship code without thorough integration testing, whether automated or manual. This step is critical for identifying significant issues that might not be caught by unit testing alone. Unit tests are primarily used for testing common code and libraries, ensuring that each individual component functions correctly in isolation.<p>Tesmon's Approach: We've completely rethought testing with the Tesmon Platform:<p>- Interactive Local Testing: Directly interact with your APIs, databases, caches, Kafka, and more through Tesmon Desktop, which autonomously creates tests.<p>- Single-Click Test Updates: Tesmon adapts to changes in your system and can integrate updates with just a click.<p>- Extended Lifecycle with Tesmon Cloud: From local development through staging to <em>production</em>, Tesmon covers it all.<p>- Unified Frontend and Backend Testing: Execute comprehensive testing across both frontend and backend within a single test.<p>- Zero Assertions: Utilize AI/ML models to ensure testing requires no manual assertions.<p>Get Started Now. Download Tesmon Desktop today on Mac/Windows and transform your testing.<p><a href=\"https://tesmon.io/desktop\" rel=\"nofollow\">https://tesmon.io/desktop</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Tesmon Platform == Postman + Cypress + RestAssured + Database + More"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://tesmon.io/"}}, "_tags": ["story", "author_jaraganittah", "story_40497644", "show_hn"], "author": "jaraganittah", "children": [40497735, 40507803], "created_at": "2024-05-28T05:36:14Z", "created_at_i": 1716874574, "num_comments": 2, "objectID": "40497644", "points": 14, "story_id": 40497644, "story_text": "I&#x27;m Nagaraj, the founder of Tesmon. Having been both a developer and an engineering leader, from big tech companies to startups, I&#x27;ve personally encountered the challenges of integration testing. It&#x27;s often so cumbersome that we resort to manual testing.<p>The world of testing is fragmented across multiple frameworks and platforms like Postman, RestAssured, pytest, Selenium, Cypress, Testim, and various custom in-house code using open source frameworks. Building and maintaining this testing infrastructure is not only time-consuming but can be a real pain.<p>Our mission is to revolutionize testing into an assertion-free, near zero effort process by leveraging AI&#x2F;ML.<p>Current challenges?<p>- Environment Differences: Tests often fail across different environments\u2014local, staging, production\u2014due to setup discrepancies and versioning issues between environments.<p>- Complex Negative Testing: Difficult to manage sequences like receiving an HTTP 200 followed by a 407 error for the same resource.<p>- Frequent Updates: Tests require extensive updates with each application change, including writing assertions for all critical fields in large response bodies.<p>- Unreliable Self-Healing: Self-healing tests frequently fail to correct issues automatically, necessitating manual intervention.<p>- Scalability Issues: No-code&#x2F;low-code UI tests struggle to scale, becoming a significant time drain as the test suite grows.<p>- Trade-offs Between Automation and Deadlines: Testing automation is often deprioritized due to urgent deadlines, leading to increased manual testing and the risk of regressions.<p>- Resource Limitations: Restricted access to essential databases and services due to compliance requirements, including the need to set up VPN access and manage Jenkins hosted in public clouds because of private subnet restrictions.<p>- Isolation Challenges: Testing everything from only the frontend or backend is often impractical, highlighting the need for integrated testing solutions.<p>- Effort in Framework Construction: Building a testing framework requires significant effort, especially when dealing with modularization and managing different testing environments.<p>How does testing code differ from product code?<p>Testing usually involves sending requests and checking responses, a process that doesn&#x27;t need to run continuously like microservices or batch jobs. This makes it perfect for AI&#x2F;ML automation, as the predictable nature of testing tasks allows for efficient learning and optimization without the complexity of continuous monitoring. Traditional programming adds unnecessary complexity to testing, which Tesmon simplifies.<p>Unit tests vs integration tests?<p>We cannot ship code without thorough integration testing, whether automated or manual. This step is critical for identifying significant issues that might not be caught by unit testing alone. Unit tests are primarily used for testing common code and libraries, ensuring that each individual component functions correctly in isolation.<p>Tesmon&#x27;s Approach: We&#x27;ve completely rethought testing with the Tesmon Platform:<p>- Interactive Local Testing: Directly interact with your APIs, databases, caches, Kafka, and more through Tesmon Desktop, which autonomously creates tests.<p>- Single-Click Test Updates: Tesmon adapts to changes in your system and can integrate updates with just a click.<p>- Extended Lifecycle with Tesmon Cloud: From local development through staging to production, Tesmon covers it all.<p>- Unified Frontend and Backend Testing: Execute comprehensive testing across both frontend and backend within a single test.<p>- Zero Assertions: Utilize AI&#x2F;ML models to ensure testing requires no manual assertions.<p>Get Started Now. Download Tesmon Desktop today on Mac&#x2F;Windows and transform your testing.<p><a href=\"https:&#x2F;&#x2F;tesmon.io&#x2F;desktop\" rel=\"nofollow\">https:&#x2F;&#x2F;tesmon.io&#x2F;desktop</a>", "title": "Show HN: Tesmon Platform == Postman + Cypress + RestAssured + Database + More", "updated_at": "2025-03-30T20:17:12Z", "url": "https://tesmon.io/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "_cfl0"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "Hi, we are working on a tool for speeding up test runs, by skipping tests unaffected by code changes.<p>Effectivly, Saving 80-95% of the time, by skipping 80-95% of tests.<p>We started a few months ago, and have managed to get into a few <em>production</em> CI systems.\nAll our prospects and users are on holiday right now.\nSo we decided to repackage and open-source for local test running.\navailable here (<a href=\"https://github.com/nabaz-io/nabaz\">https://github.com/nabaz-io/nabaz</a>) under MIT license.<p>One line change:\n<em>pytest</em> -v -&gt; nabaz test --cmdline &quot;<em>pytest</em> -v&quot;<p>Stalk us on GitHub, or just Star us.\nAsk questions, we'll answer in under 30 seconds. we have auto refresh on."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: We built a tool for fast-forwarding 95% of tests (MIT)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/nabaz-io/nabaz"}}, "_tags": ["story", "author__cfl0", "story_33183512", "show_hn"], "author": "_cfl0", "children": [33184746, 33184786, 33184839, 33188448, 33188732, 33203361, 33206606], "created_at": "2022-10-12T21:23:32Z", "created_at_i": 1665609812, "num_comments": 17, "objectID": "33183512", "points": 8, "story_id": 33183512, "story_text": "Hi, we are working on a tool for speeding up test runs, by skipping tests unaffected by code changes.<p>Effectivly, Saving 80-95% of the time, by skipping 80-95% of tests.<p>We started a few months ago, and have managed to get into a few production CI systems.\nAll our prospects and users are on holiday right now.\nSo we decided to repackage and open-source for local test running.\navailable here (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;nabaz-io&#x2F;nabaz\">https:&#x2F;&#x2F;github.com&#x2F;nabaz-io&#x2F;nabaz</a>) under MIT license.<p>One line change:\npytest -v -&gt; nabaz test --cmdline &quot;pytest -v&quot;<p>Stalk us on GitHub, or just Star us.\nAsk questions, we&#x27;ll answer in under 30 seconds. we have auto refresh on.", "title": "Show HN: We built a tool for fast-forwarding 95% of tests (MIT)", "updated_at": "2024-09-20T12:17:42Z", "url": "https://github.com/nabaz-io/nabaz"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "johnnycash926"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "We built an open-source CLI that generates code, runs tests, fixes failures, and gets an independent AI review \u2014 all before you see the output.\nWe started with a multi-model pipeline where different AI models handled different stages (architect, implement, refactor, verify). We assumed more models meant better code. Then we benchmarked it: 39% average quality score at $4.85 per run. A single model scored 94% at $0.36. Our pipeline was actively making things worse.\nSo we killed it and rebuilt around what developers actually do when they get AI-generated code: run it, test it, fix what breaks. The Loop generates code, runs <em>pytest</em> automatically, feeds failures back for targeted fixes, and repeats until all tests pass. Then an independent Arbiter (always a different model than the generator) reviews the final output.\nLatest benchmark across three tasks (simple CLI, REST API, async multi-agent system):\nSingle Sonnet: 94% avg, 10 min dev time, $0.36\nSingle o3:     81% avg,  4 min dev time, $0.44\nMulti-model:   88% avg,  9 min dev time, $5.59\nCRTX Loop:     99% avg,  2 min dev time, $1.80\n&quot;Dev time&quot; estimates how long a developer would spend debugging the output before it's <em>production</em>-ready. The Loop's hardest prompt produced 127 passing tests with zero failures.\nWhen the Loop hits a test it can't fix, it has a three-tier escalation: diagnose the root cause before patching, strip context to just the failing test and source file, then bring in a different model for a second opinion. The goal is zero dev time on every run.\nModel-agnostic \u2014 works with Claude, GPT, o3, Gemini, Grok, DeepSeek. Bring your own API keys. Apache 2.0.\npip install crtx\n<a href=\"https://github.com/CRTXAI/crtx\" rel=\"nofollow\">https://github.com/CRTXAI/crtx</a>\nWe published the benchmark tool too \u2014 run crtx benchmark --quick to reproduce our results with your own keys. Curious what scores people get on different providers and tasks."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: CRTX \u2013 AI code gen that tests and fixes its own output (OSS)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/CRTXAI/CRTX"}}, "_tags": ["story", "author_johnnycash926", "story_47096764", "show_hn"], "author": "johnnycash926", "children": [47097239], "created_at": "2026-02-21T02:10:03Z", "created_at_i": 1771639803, "num_comments": 1, "objectID": "47096764", "points": 2, "story_id": 47096764, "story_text": "We built an open-source CLI that generates code, runs tests, fixes failures, and gets an independent AI review \u2014 all before you see the output.\nWe started with a multi-model pipeline where different AI models handled different stages (architect, implement, refactor, verify). We assumed more models meant better code. Then we benchmarked it: 39% average quality score at $4.85 per run. A single model scored 94% at $0.36. Our pipeline was actively making things worse.\nSo we killed it and rebuilt around what developers actually do when they get AI-generated code: run it, test it, fix what breaks. The Loop generates code, runs pytest automatically, feeds failures back for targeted fixes, and repeats until all tests pass. Then an independent Arbiter (always a different model than the generator) reviews the final output.\nLatest benchmark across three tasks (simple CLI, REST API, async multi-agent system):\nSingle Sonnet: 94% avg, 10 min dev time, $0.36\nSingle o3:     81% avg,  4 min dev time, $0.44\nMulti-model:   88% avg,  9 min dev time, $5.59\nCRTX Loop:     99% avg,  2 min dev time, $1.80\n&quot;Dev time&quot; estimates how long a developer would spend debugging the output before it&#x27;s production-ready. The Loop&#x27;s hardest prompt produced 127 passing tests with zero failures.\nWhen the Loop hits a test it can&#x27;t fix, it has a three-tier escalation: diagnose the root cause before patching, strip context to just the failing test and source file, then bring in a different model for a second opinion. The goal is zero dev time on every run.\nModel-agnostic \u2014 works with Claude, GPT, o3, Gemini, Grok, DeepSeek. Bring your own API keys. Apache 2.0.\npip install crtx\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;CRTXAI&#x2F;crtx\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;CRTXAI&#x2F;crtx</a>\nWe published the benchmark tool too \u2014 run crtx benchmark --quick to reproduce our results with your own keys. Curious what scores people get on different providers and tasks.", "title": "Show HN: CRTX \u2013 AI code gen that tests and fixes its own output (OSS)", "updated_at": "2026-02-21T11:44:43Z", "url": "https://github.com/CRTXAI/CRTX"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "alepot55"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "I tested Claude 3 Haiku on &quot;What is 247 * 18?&quot; across 100 trials.\nPass rate: 70%. 95% CI: 48%-85%. A task any calculator solves 100%\nof the time.<p>This is the core problem with agent evals today: one run tells you\nnothing. The same prompt, same model, same tools \u2014 different result\nevery time.<p>I built agentrial to fix this. It's a <em>pytest</em>-style CLI that runs\nyour agent N times and gives you:<p>- Wilson confidence intervals on pass rate\n- Step-level failure attribution (Fisher exact test pinpoints which\n  tool call or reasoning step diverges between pass/fail runs)\n- Real API cost from response metadata\n- A GitHub Action that blocks PRs when reliability drops<p>Usage is minimal \u2014 write a YAML config, run &quot;agentrial run&quot;:<p><pre><code>  pip install agentrial\n</code></pre>\nTested extensively with LangGraph agents. 100 trials cost $0.06.\nMIT licensed, no telemetry, runs locally.<p>Looking for feedback on what metrics matter most when you're\nshipping agents to <em>production</em>."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Ran an AI agent 100x \u2013 pass rate 70%, not 100%"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/alepot55/agentrial"}}, "_tags": ["story", "author_alepot55", "story_46943395", "show_hn"], "author": "alepot55", "created_at": "2026-02-09T09:35:43Z", "created_at_i": 1770629743, "num_comments": 0, "objectID": "46943395", "points": 2, "story_id": 46943395, "story_text": "I tested Claude 3 Haiku on &quot;What is 247 * 18?&quot; across 100 trials.\nPass rate: 70%. 95% CI: 48%-85%. A task any calculator solves 100%\nof the time.<p>This is the core problem with agent evals today: one run tells you\nnothing. The same prompt, same model, same tools \u2014 different result\nevery time.<p>I built agentrial to fix this. It&#x27;s a pytest-style CLI that runs\nyour agent N times and gives you:<p>- Wilson confidence intervals on pass rate\n- Step-level failure attribution (Fisher exact test pinpoints which\n  tool call or reasoning step diverges between pass&#x2F;fail runs)\n- Real API cost from response metadata\n- A GitHub Action that blocks PRs when reliability drops<p>Usage is minimal \u2014 write a YAML config, run &quot;agentrial run&quot;:<p><pre><code>  pip install agentrial\n</code></pre>\nTested extensively with LangGraph agents. 100 trials cost $0.06.\nMIT licensed, no telemetry, runs locally.<p>Looking for feedback on what metrics matter most when you&#x27;re\nshipping agents to production.", "title": "Show HN: Ran an AI agent 100x \u2013 pass rate 70%, not 100%", "updated_at": "2026-02-09T15:03:34Z", "url": "https://github.com/alepot55/agentrial"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "chunktort"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pytest", "production"], "value": "I built AgentForge, a minimal multi-LLM orchestrator. Total size: ~15KB of Python code.<p>Why? LangChain added 250ms overhead per request. I needed something simpler.<p>Performance vs LangChain (1,000 requests):\n- Avg latency: 420ms -&gt; 65ms\n- Memory/request: 12MB -&gt; 3MB\n- Cold start: 2.5s -&gt; 0.3s\n- Test time: 45s -&gt; 3s<p>Size: 15KB + 2 dependencies (httpx, <em>pytest</em>) vs LangChain's 15MB+ and 47 packages.<p>In <em>production</em>: 89% LLM cost reduction via 3-tier Redis caching (88% cache hit rate, verified benchmarks). 4.3M tool dispatches/sec in the core engine.<p>What it does:\n1. Multi-agent orchestration -- route tasks to specialized agents with automatic fallbacks\n2. Testing built-in -- MockLLMClient lets you assert agent behavior without API keys\n3. <em>Production</em> patterns -- circuit breakers, rate limiting, caching included<p>Install: pip install agentforge<p>Live demo: <a href=\"https://ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/\" rel=\"nofollow\">https://ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/</a><p>Packaged version with docs and deployment guide: <a href=\"https://chunkmaster1.gumroad.com\" rel=\"nofollow\">https://chunkmaster1.gumroad.com</a><p>When to use it: <em>production</em> reliability matters, latency is a concern, you want full test coverage.\nWhen not to: prototyping, internal tools, team already on LangChain ecosystem.<p>Questions welcome."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: AgentForge \u2013 Multi-LLM Orchestrator in 15KB"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/ChunkyTortoise/ai-orchestrator"}}, "_tags": ["story", "author_chunktort", "story_47056310", "show_hn"], "author": "chunktort", "created_at": "2026-02-18T02:18:12Z", "created_at_i": 1771381092, "num_comments": 0, "objectID": "47056310", "points": 1, "story_id": 47056310, "story_text": "I built AgentForge, a minimal multi-LLM orchestrator. Total size: ~15KB of Python code.<p>Why? LangChain added 250ms overhead per request. I needed something simpler.<p>Performance vs LangChain (1,000 requests):\n- Avg latency: 420ms -&gt; 65ms\n- Memory&#x2F;request: 12MB -&gt; 3MB\n- Cold start: 2.5s -&gt; 0.3s\n- Test time: 45s -&gt; 3s<p>Size: 15KB + 2 dependencies (httpx, pytest) vs LangChain&#x27;s 15MB+ and 47 packages.<p>In production: 89% LLM cost reduction via 3-tier Redis caching (88% cache hit rate, verified benchmarks). 4.3M tool dispatches&#x2F;sec in the core engine.<p>What it does:\n1. Multi-agent orchestration -- route tasks to specialized agents with automatic fallbacks\n2. Testing built-in -- MockLLMClient lets you assert agent behavior without API keys\n3. Production patterns -- circuit breakers, rate limiting, caching included<p>Install: pip install agentforge<p>Live demo: <a href=\"https:&#x2F;&#x2F;ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app&#x2F;</a><p>Packaged version with docs and deployment guide: <a href=\"https:&#x2F;&#x2F;chunkmaster1.gumroad.com\" rel=\"nofollow\">https:&#x2F;&#x2F;chunkmaster1.gumroad.com</a><p>When to use it: production reliability matters, latency is a concern, you want full test coverage.\nWhen not to: prototyping, internal tools, team already on LangChain ecosystem.<p>Questions welcome.", "title": "Show HN: AgentForge \u2013 Multi-LLM Orchestrator in 15KB", "updated_at": "2026-02-18T02:22:20Z", "url": "https://github.com/ChunkyTortoise/ai-orchestrator"}], "hitsPerPage": 15, "nbHits": 14, "nbPages": 1, "page": 0, "params": "query=pytest+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 11, "processingTimingsMS": {"_request": {"roundTrip": 15}, "afterFetch": {"format": {"highlighting": 2, "total": 3}}, "fetch": {"query": 7, "scanning": 2, "total": 10}, "total": 11}, "query": "pytest production", "serverTimeMS": 15}}