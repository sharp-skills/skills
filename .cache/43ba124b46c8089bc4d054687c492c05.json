{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tylerreed"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "RunOS lets you run your application stack on any infrastructure - cloud VMs, bare metal, or on-premises - without vendor lock-in.<p>What it does:\n- One-click deployment of 20+ <em>production</em>-ready services (PostgreSQL, Kafka, <em>MinIO</em>, etc.) with security hardening, backups, and HA by default.<p>- Git-based app deployment with automatic Docker builds, SSL certificates, and Kubernetes orchestration.<p>- Infrastructure portability - switch between providers without code changes.<p>- Automatic service discovery and integration between your apps and services.<p>Built on Kubernetes but hides the complexity. You get container orchestration benefits without the configuration overhead.<p>Looking for feedback on:\n- Platform UX and onboarding experience\n- Which VPS providers to prioritize\n- Which services to add next<p>Try it: <a href=\"https://runos.com\" rel=\"nofollow\">https://runos.com</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: RunOS \u2013 Run Your Own Cloud on Your Own Infra"}}, "_tags": ["story", "author_tylerreed", "story_45661012", "show_hn"], "author": "tylerreed", "created_at": "2025-10-21T20:08:22Z", "created_at_i": 1761077302, "num_comments": 0, "objectID": "45661012", "points": 4, "story_id": 45661012, "story_text": "RunOS lets you run your application stack on any infrastructure - cloud VMs, bare metal, or on-premises - without vendor lock-in.<p>What it does:\n- One-click deployment of 20+ production-ready services (PostgreSQL, Kafka, MinIO, etc.) with security hardening, backups, and HA by default.<p>- Git-based app deployment with automatic Docker builds, SSL certificates, and Kubernetes orchestration.<p>- Infrastructure portability - switch between providers without code changes.<p>- Automatic service discovery and integration between your apps and services.<p>Built on Kubernetes but hides the complexity. You get container orchestration benefits without the configuration overhead.<p>Looking for feedback on:\n- Platform UX and onboarding experience\n- Which VPS providers to prioritize\n- Which services to add next<p>Try it: <a href=\"https:&#x2F;&#x2F;runos.com\" rel=\"nofollow\">https:&#x2F;&#x2F;runos.com</a>", "title": "Show HN: RunOS \u2013 Run Your Own Cloud on Your Own Infra", "updated_at": "2025-10-23T09:02:00Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "gunapologist99"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Seems like a few of the non-AWS S3 object storage services (even big ones like R2) have had some <em>production</em> issues.<p>Anyone care to describe their experiences?<p>I'd like to store billions of smallish files (photos etc) in <em>production</em>. Price (incl bandwidth) is important, but uptime and latency (where S3 does suffer) are more important."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["minio"], "value": "Ask HN: S3/Cloudflare R2/GCS/Wasabi/B2/Bunny or <em>Minio</em>/Seaweed/Garage/Ceph?"}}, "_tags": ["story", "author_gunapologist99", "story_39895205", "ask_hn"], "author": "gunapologist99", "children": [39895286, 39895988, 39900965, 39906821], "created_at": "2024-04-01T15:31:01Z", "created_at_i": 1711985461, "num_comments": 7, "objectID": "39895205", "points": 4, "story_id": 39895205, "story_text": "Seems like a few of the non-AWS S3 object storage services (even big ones like R2) have had some production issues.<p>Anyone care to describe their experiences?<p>I&#x27;d like to store billions of smallish files (photos etc) in production. Price (incl bandwidth) is important, but uptime and latency (where S3 does suffer) are more important.", "title": "Ask HN: S3/Cloudflare R2/GCS/Wasabi/B2/Bunny or Minio/Seaweed/Garage/Ceph?", "updated_at": "2024-09-20T16:46:50Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "furkansahin"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "Hello,\nAt Ubicloud, we are trying to develop an object storage system service. It will be initially used for our own needs such as backups of varying services. In the longer term, we will also <em>production</em>ize it for our customers.<p>While looking into <em>MinIO</em> and alternatives, I realized there are different viable options.<p>Is there anyone here with experience on running any of these services professionally and how do you feel about it? Technically, would you trust to anyone of these projects more than the others? There is already a comparison document in SeaweedFS repository but I want to hear about the operational side of the things.<p>Thank you so much!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["minio"], "value": "Ask HN: <em>MinIO</em> vs. SeaweedFS vs. any other suggestion?"}}, "_tags": ["story", "author_furkansahin", "story_37659848", "ask_hn"], "author": "furkansahin", "children": [37660934, 37707556, 37748321], "created_at": "2023-09-26T14:25:52Z", "created_at_i": 1695738352, "num_comments": 4, "objectID": "37659848", "points": 1, "story_id": 37659848, "story_text": "Hello,\nAt Ubicloud, we are trying to develop an object storage system service. It will be initially used for our own needs such as backups of varying services. In the longer term, we will also productionize it for our customers.<p>While looking into MinIO and alternatives, I realized there are different viable options.<p>Is there anyone here with experience on running any of these services professionally and how do you feel about it? Technically, would you trust to anyone of these projects more than the others? There is already a comparison document in SeaweedFS repository but I want to hear about the operational side of the things.<p>Thank you so much!", "title": "Ask HN: MinIO vs. SeaweedFS vs. any other suggestion?", "updated_at": "2025-12-18T16:09:46Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "deofoo"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "My wife was planning to open a micro-bakery and we started looking at software to manage recipes, inventory, orders, and <em>production</em>. Everything was either expensive, too generic, or both. The workflows for a small-batch manufacturer aren\u2019t that complex, but the pricing acts like they are.<p>So I built Craftplan. All the features were tailored to what she actually needed, and I figured other small-scale manufacturers (soap makers, breweries, candle makers, etc.) probably need the same things. So I\u2019m putting it out there for free.<p><pre><code>  - Live demo: https://craftplan.fly.dev (test@test.com / Aa123123123123)\n  - GitHub: https://github.com/puemos/craftplan\n  - Docs: https://puemos.github.io/craftplan\n  - Self-hosting guide: https://puemos.github.io/craftplan/docs/self-hosting/\n</code></pre>\nWhat it does:<p><pre><code>  - Product catalog with versioned recipes (BOMs) and automatic cost rollups across materials, labor, and overhead\n  - Inventory tracking with lot traceability, expiry dates, allergen/nutrition flags, and demand forecasting\n  - Order processing with calendar-based scheduling and allocation to <em>production</em> batches\n  - <em>Production</em> planner with make sheets, material consumption from specific lots, and cost snapshots\n  - Purchase orders with receiving workflow that auto-creates inventory lots\n  - Basic CRM for customers and suppliers\n  - CSV import/export, iCal calendar feed, JSON:API and GraphQL endpoints\n</code></pre>\nExperience building with Elixir, Ash and Liveview:<p><pre><code>  - Speed: you get to test and improve things sooo fast. The DSL makes it simple to translate your thinking into live product\n  - Extensibility: With Ash + LiveView you can add more features so easily. Adding JSON:API + Grapghql was a few minutes.\n  - UX: I believe LiveView makes it simple to deliver great UX since it forcing you to keep things simple with no so much interaction overhead which most of the time means better and simple experience\n</code></pre>\nSelf-hosting:<p><pre><code>  - Docker image: `ghcr.io/puemos/craftplan` (amd64 + arm64)\n  - Docker Compose bundles PostgreSQL 16 + <em>MinIO</em>.\n\n</code></pre>\nOther details:<p><pre><code>  - Email config from UI (SMTP, SendGrid, Mailgun, Postmark, Brevo, Amazon SES)\n  - API keys encrypted at rest (AES-256-GCM)\n  - Role-based access (admin/staff)\n  - Tech stack: Elixir, Ash Framework, Phoenix LiveView, PostgreSQL\n  - License: AGPLv3\n\n</code></pre>\nFeedback welcome (and needed!)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Craftplan \u2013 Elixir-based micro-ERP for small-scale manufacturers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://puemos.github.io/craftplan/"}}, "_tags": ["story", "author_deofoo", "story_46869383", "show_hn"], "author": "deofoo", "children": [46930597, 46930885, 46930925], "created_at": "2026-02-03T11:01:48Z", "created_at_i": 1770116508, "num_comments": 4, "objectID": "46869383", "points": 22, "story_id": 46869383, "story_text": "My wife was planning to open a micro-bakery and we started looking at software to manage recipes, inventory, orders, and production. Everything was either expensive, too generic, or both. The workflows for a small-batch manufacturer aren\u2019t that complex, but the pricing acts like they are.<p>So I built Craftplan. All the features were tailored to what she actually needed, and I figured other small-scale manufacturers (soap makers, breweries, candle makers, etc.) probably need the same things. So I\u2019m putting it out there for free.<p><pre><code>  - Live demo: https:&#x2F;&#x2F;craftplan.fly.dev (test@test.com &#x2F; Aa123123123123)\n  - GitHub: https:&#x2F;&#x2F;github.com&#x2F;puemos&#x2F;craftplan\n  - Docs: https:&#x2F;&#x2F;puemos.github.io&#x2F;craftplan\n  - Self-hosting guide: https:&#x2F;&#x2F;puemos.github.io&#x2F;craftplan&#x2F;docs&#x2F;self-hosting&#x2F;\n</code></pre>\nWhat it does:<p><pre><code>  - Product catalog with versioned recipes (BOMs) and automatic cost rollups across materials, labor, and overhead\n  - Inventory tracking with lot traceability, expiry dates, allergen&#x2F;nutrition flags, and demand forecasting\n  - Order processing with calendar-based scheduling and allocation to production batches\n  - Production planner with make sheets, material consumption from specific lots, and cost snapshots\n  - Purchase orders with receiving workflow that auto-creates inventory lots\n  - Basic CRM for customers and suppliers\n  - CSV import&#x2F;export, iCal calendar feed, JSON:API and GraphQL endpoints\n</code></pre>\nExperience building with Elixir, Ash and Liveview:<p><pre><code>  - Speed: you get to test and improve things sooo fast. The DSL makes it simple to translate your thinking into live product\n  - Extensibility: With Ash + LiveView you can add more features so easily. Adding JSON:API + Grapghql was a few minutes.\n  - UX: I believe LiveView makes it simple to deliver great UX since it forcing you to keep things simple with no so much interaction overhead which most of the time means better and simple experience\n</code></pre>\nSelf-hosting:<p><pre><code>  - Docker image: `ghcr.io&#x2F;puemos&#x2F;craftplan` (amd64 + arm64)\n  - Docker Compose bundles PostgreSQL 16 + MinIO.\n\n</code></pre>\nOther details:<p><pre><code>  - Email config from UI (SMTP, SendGrid, Mailgun, Postmark, Brevo, Amazon SES)\n  - API keys encrypted at rest (AES-256-GCM)\n  - Role-based access (admin&#x2F;staff)\n  - Tech stack: Elixir, Ash Framework, Phoenix LiveView, PostgreSQL\n  - License: AGPLv3\n\n</code></pre>\nFeedback welcome (and needed!)", "title": "Show HN: Craftplan \u2013 Elixir-based micro-ERP for small-scale manufacturers", "updated_at": "2026-02-12T07:14:30Z", "url": "https://puemos.github.io/craftplan/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "soumyadeb"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "RudderStack (https://github.com/rudderlabs/rudder-server) is an open-source customer data infrastructure (a.k.a open-source alternative to Segment).<p>We firstly want to thank the HN community for showing us love and support in our previous HN post (https://news.ycombinator.com/item?id=21081756). At that point, we had just open-sourced the repo and were not fully prepared for a Show HN. We wanted to share updates since then and also do our official Show HN.<p>Updates since Sept 2019<p>=================<p>1. Changed the name from Rudder to RudderStack :)<p>2. API compatibility with Segment<p>3. Open-source control plane so no dependency on the hosted control plane for open-source users. (https://github.com/rudderlabs/rudder-server/blob/config-gen/utils/config-gen/README.md)<p>4. Multiple hosting options: Docker, Kubernetes, Native, Terraform.<p>5. 30 integrations (https://rudderstack.com/) including cloud mode and device mode<p>6. Support all the popular data-warehouses &amp; lakes - RedShift, Snowflake, BigQuery, S3, Google Cloud Storage, Azure Blob Storage (via <em>MINIO</em>)<p>7. Detailed documentation - https://docs.rudderstack.com/<p>8. Multiple <em>production</em> deployments including few really large ones (our largest deployment is sending a peak of ~40K events/sec, ~100M events/day)<p>9. Switched license from SSPL to AGPLv3 (after long discussions internally as well as on HN)<p>10. Built some interesting Analytics &amp; ML use cases<p>11. Launched our \u201cpaid plans\u201d  (primarily around managed hosting)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: RudderStack, open-source CDI (open-source Segment alternative)"}}, "_tags": ["story", "author_soumyadeb", "story_22567153", "show_hn"], "author": "soumyadeb", "children": [22568167], "created_at": "2020-03-13T13:41:04Z", "created_at_i": 1584106864, "num_comments": 1, "objectID": "22567153", "points": 12, "story_id": 22567153, "story_text": "RudderStack (https:&#x2F;&#x2F;github.com&#x2F;rudderlabs&#x2F;rudder-server) is an open-source customer data infrastructure (a.k.a open-source alternative to Segment).<p>We firstly want to thank the HN community for showing us love and support in our previous HN post (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21081756). At that point, we had just open-sourced the repo and were not fully prepared for a Show HN. We wanted to share updates since then and also do our official Show HN.<p>Updates since Sept 2019<p>=================<p>1. Changed the name from Rudder to RudderStack :)<p>2. API compatibility with Segment<p>3. Open-source control plane so no dependency on the hosted control plane for open-source users. (https:&#x2F;&#x2F;github.com&#x2F;rudderlabs&#x2F;rudder-server&#x2F;blob&#x2F;config-gen&#x2F;utils&#x2F;config-gen&#x2F;README.md)<p>4. Multiple hosting options: Docker, Kubernetes, Native, Terraform.<p>5. 30 integrations (https:&#x2F;&#x2F;rudderstack.com&#x2F;) including cloud mode and device mode<p>6. Support all the popular data-warehouses &amp; lakes - RedShift, Snowflake, BigQuery, S3, Google Cloud Storage, Azure Blob Storage (via MINIO)<p>7. Detailed documentation - https:&#x2F;&#x2F;docs.rudderstack.com&#x2F;<p>8. Multiple production deployments including few really large ones (our largest deployment is sending a peak of ~40K events&#x2F;sec, ~100M events&#x2F;day)<p>9. Switched license from SSPL to AGPLv3 (after long discussions internally as well as on HN)<p>10. Built some interesting Analytics &amp; ML use cases<p>11. Launched our \u201cpaid plans\u201d  (primarily around managed hosting)", "title": "Show HN: RudderStack, open-source CDI (open-source Segment alternative)", "updated_at": "2024-09-20T05:53:23Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Vonng"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "Yo, Pigsty is a local-first open-source alternative for RDS PostgreSQL, with monitoring, HA, PITR, IaC, and lots of stuff to run a <em>production</em>-grade PostgreSQL Service on your nodes for free.  Demo: http://demo.pigsty.cc.<p>* Battery-Included PostgreSQL Distribution, with PostGIS, TimescaleDB, Citus united as one.\n* Incredible observability powered by Prometheus &amp; Grafana stack.\n* Self-healing HA PGSQL cluster, powered by patroni, haproxy, etcd\u2026\n* Auto-Configured PITR, powered by pgbackrest and optional <em>MinIO</em> cluster\n* Declarative API, Database-as-Code implemented with Ansible playbooks.\n* Versatile UseCases, Run Docker Apps, Run demos, Visualize data with ECharts panels.\n* Install in one command, provision IaaS with Terraform, and test with local Vagrant sandbox.<p>Features: https://github.com/Vonng/pigsty/blob/master/docs/FEATURE.md<p>I'm Vonng, the author of Pigsty and a PostgreSQL DBA of a large deployment (25K cores, 1PB TP data). Pigsty alleviates my pain of managing such a massive elephant in <em>production</em>.<p>It works well for us, and I'm glad to make it open-source. Since cloud RDS PostgreSQL are so expensive, sometimes it charges 10x compared to underlying EC2 and storage. An open-source alternative could help.<p>I'm happy to answer any questions about Pigsty ;)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "How about an open-source RDS alternative for PostgreSQL?"}}, "_tags": ["story", "author_Vonng", "story_34966165", "ask_hn"], "author": "Vonng", "created_at": "2023-02-28T05:59:00Z", "created_at_i": 1677563940, "num_comments": 0, "objectID": "34966165", "points": 4, "story_id": 34966165, "story_text": "Yo, Pigsty is a local-first open-source alternative for RDS PostgreSQL, with monitoring, HA, PITR, IaC, and lots of stuff to run a production-grade PostgreSQL Service on your nodes for free.  Demo: http:&#x2F;&#x2F;demo.pigsty.cc.<p>* Battery-Included PostgreSQL Distribution, with PostGIS, TimescaleDB, Citus united as one.\n* Incredible observability powered by Prometheus &amp; Grafana stack.\n* Self-healing HA PGSQL cluster, powered by patroni, haproxy, etcd\u2026\n* Auto-Configured PITR, powered by pgbackrest and optional MinIO cluster\n* Declarative API, Database-as-Code implemented with Ansible playbooks.\n* Versatile UseCases, Run Docker Apps, Run demos, Visualize data with ECharts panels.\n* Install in one command, provision IaaS with Terraform, and test with local Vagrant sandbox.<p>Features: https:&#x2F;&#x2F;github.com&#x2F;Vonng&#x2F;pigsty&#x2F;blob&#x2F;master&#x2F;docs&#x2F;FEATURE.md<p>I&#x27;m Vonng, the author of Pigsty and a PostgreSQL DBA of a large deployment (25K cores, 1PB TP data). Pigsty alleviates my pain of managing such a massive elephant in production.<p>It works well for us, and I&#x27;m glad to make it open-source. Since cloud RDS PostgreSQL are so expensive, sometimes it charges 10x compared to underlying EC2 and storage. An open-source alternative could help.<p>I&#x27;m happy to answer any questions about Pigsty ;)", "title": "How about an open-source RDS alternative for PostgreSQL?", "updated_at": "2026-01-12T18:44:33Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "movebx101"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "We\u2019re considering using an AGPL-licensed database tool as part of our saas.\nAGPL has always felt a bit scary. It\u2019s got a reputation for being \u201cviral\u201d ( in a way that even companies like Google explicitly avoid it in their projects. There\u2019s a lot of FUD and ambiguity around what it means to be compliant \u2014 especially for server-side use. I have read about how companies like iText or <em>MinIO</em> that are kind of abusing the licence to get you to pay for a subscription to their services.<p>Our plan to be compliant is:<p>Fork the AGPL repo and modify it slightly for our workload<p>Publish our changes in a public GitHub repo under the same AGPL license (linking the original repo)<p>Publish that version as an npm package<p>Use that package in our Node.js server<p>Add a clear link to the source on our website (in docs or footer)<p>There\u2019s no private fork \u2014 just a clean public repo that exactly matches what we run in <em>production</em>.<p>Would this be enough to comply with AGPL\u2019s network-use clause? Or do we need to go further, e.g. display the AGPL license explicitly in our app UI, or provide the source in a different way?<p>Has anyone here dealt with this in <em>production</em>? We want to do the right thing, even if it means to not use the project at all."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: How to be compliant with the AGPL licence?"}}, "_tags": ["story", "author_movebx101", "story_44270609", "ask_hn"], "author": "movebx101", "children": [44273970], "created_at": "2025-06-13T17:51:14Z", "created_at_i": 1749837074, "num_comments": 1, "objectID": "44270609", "points": 3, "story_id": 44270609, "story_text": "We\u2019re considering using an AGPL-licensed database tool as part of our saas.\nAGPL has always felt a bit scary. It\u2019s got a reputation for being \u201cviral\u201d ( in a way that even companies like Google explicitly avoid it in their projects. There\u2019s a lot of FUD and ambiguity around what it means to be compliant \u2014 especially for server-side use. I have read about how companies like iText or MinIO that are kind of abusing the licence to get you to pay for a subscription to their services.<p>Our plan to be compliant is:<p>Fork the AGPL repo and modify it slightly for our workload<p>Publish our changes in a public GitHub repo under the same AGPL license (linking the original repo)<p>Publish that version as an npm package<p>Use that package in our Node.js server<p>Add a clear link to the source on our website (in docs or footer)<p>There\u2019s no private fork \u2014 just a clean public repo that exactly matches what we run in production.<p>Would this be enough to comply with AGPL\u2019s network-use clause? Or do we need to go further, e.g. display the AGPL license explicitly in our app UI, or provide the source in a different way?<p>Has anyone here dealt with this in production? We want to do the right thing, even if it means to not use the project at all.", "title": "Ask HN: How to be compliant with the AGPL licence?", "updated_at": "2025-06-14T03:26:39Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "didierbreedt"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "We've spent the last year building RunOS, a platform that spins up <em>production</em>-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu<p>- Solid Go bindings via libvirt<p>- Excellent GPU passthrough for AI workloads like Ollama<p>- Good isolation/performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;<p>2. Backend selects available server agents<p>3. gRPC commands sent to provision VMs<p>4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)<p>5. Node agents install and connect<p>6. Kubernetes bootstrap with kubeadm + Cilium<p>7. WireGuard mesh established between nodes<p>8. Storage configured (OpenEBS + Longhorn)<p>9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access<p>- Nodes communicate securely even if Kubernetes fails<p>- Simpler troubleshooting with separated layers<p>- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>The platform supports one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, <em>MinIO</em>, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>Managed option: Dedicated servers with fixed 8 CPU/16GB instances. KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it's early access.<p>Self-hosted option: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Working on: Self-managed VM hosts with custom sizing.<p>What's Next<p>The agent code will be open source eventually. One company runs three <em>production</em> clusters already. Common feedback: &quot;I can't believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We're planning weekly updates here on HackerNews about new features, technical challenges, and <em>production</em> lessons learned building RunOS.<p>Questions? Happy to discuss architecture in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC"}}, "_tags": ["story", "author_didierbreedt", "story_45936611", "show_hn"], "author": "didierbreedt", "children": [45940027], "created_at": "2025-11-15T11:01:12Z", "created_at_i": 1763204472, "num_comments": 2, "objectID": "45936611", "points": 2, "story_id": 45936611, "story_text": "We&#x27;ve spent the last year building RunOS, a platform that spins up production-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu<p>- Solid Go bindings via libvirt<p>- Excellent GPU passthrough for AI workloads like Ollama<p>- Good isolation&#x2F;performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;<p>2. Backend selects available server agents<p>3. gRPC commands sent to provision VMs<p>4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)<p>5. Node agents install and connect<p>6. Kubernetes bootstrap with kubeadm + Cilium<p>7. WireGuard mesh established between nodes<p>8. Storage configured (OpenEBS + Longhorn)<p>9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access<p>- Nodes communicate securely even if Kubernetes fails<p>- Simpler troubleshooting with separated layers<p>- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>The platform supports one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>Managed option: Dedicated servers with fixed 8 CPU&#x2F;16GB instances. KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it&#x27;s early access.<p>Self-hosted option: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Working on: Self-managed VM hosts with custom sizing.<p>What&#x27;s Next<p>The agent code will be open source eventually. One company runs three production clusters already. Common feedback: &quot;I can&#x27;t believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We&#x27;re planning weekly updates here on HackerNews about new features, technical challenges, and production lessons learned building RunOS.<p>Questions? Happy to discuss architecture in the comments.", "title": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC", "updated_at": "2025-11-16T05:21:40Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "dib85"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "We've spent the last year building RunOS, a platform that spins up <em>production</em>-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu\n- Solid Go bindings via libvirt\n- Excellent GPU passthrough for AI workloads like Ollama\n- Good isolation/performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;\n2. Backend selects available server agents\n3. gRPC commands sent to provision VMs\n4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)\n5. Node agents install and connect\n6. Kubernetes bootstrap with kubeadm + Cilium\n7. WireGuard mesh established between nodes\n8. Storage configured (OpenEBS + Longhorn)\n9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access\n- Nodes communicate securely even if Kubernetes fails\n- Simpler troubleshooting with separated layers\n- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>We offer one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, <em>MinIO</em>, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>RunOS Cloud: Managed dedicated servers with fixed 8 CPU/16GB instances (free trial credits available). KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it's early access.<p>Bring Your Own Node: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Coming soon: Self-managed VM hosts with custom sizing.<p>What's Next<p>Agent code will be open source. One company runs three <em>production</em> clusters already. Common feedback: &quot;I can't believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We're planning weekly updates here on HackerNews about new features, technical challenges, and <em>production</em> lessons.<p>Try it at runos.com - free trial credits for 8 CPU threads and 16GB memory.<p>Questions? Happy to discuss architecture in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC"}}, "_tags": ["story", "author_dib85", "story_45927988", "show_hn"], "author": "dib85", "created_at": "2025-11-14T15:50:14Z", "created_at_i": 1763135414, "num_comments": 0, "objectID": "45927988", "points": 2, "story_id": 45927988, "story_text": "We&#x27;ve spent the last year building RunOS, a platform that spins up production-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu\n- Solid Go bindings via libvirt\n- Excellent GPU passthrough for AI workloads like Ollama\n- Good isolation&#x2F;performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;\n2. Backend selects available server agents\n3. gRPC commands sent to provision VMs\n4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)\n5. Node agents install and connect\n6. Kubernetes bootstrap with kubeadm + Cilium\n7. WireGuard mesh established between nodes\n8. Storage configured (OpenEBS + Longhorn)\n9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access\n- Nodes communicate securely even if Kubernetes fails\n- Simpler troubleshooting with separated layers\n- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>We offer one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>RunOS Cloud: Managed dedicated servers with fixed 8 CPU&#x2F;16GB instances (free trial credits available). KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it&#x27;s early access.<p>Bring Your Own Node: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Coming soon: Self-managed VM hosts with custom sizing.<p>What&#x27;s Next<p>Agent code will be open source. One company runs three production clusters already. Common feedback: &quot;I can&#x27;t believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We&#x27;re planning weekly updates here on HackerNews about new features, technical challenges, and production lessons.<p>Try it at runos.com - free trial credits for 8 CPU threads and 16GB memory.<p>Questions? Happy to discuss architecture in the comments.", "title": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC", "updated_at": "2025-11-14T18:11:05Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "18nleung"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "Author here with yet another solution for local dev \u2014 let me know your thoughts, would love to trade notes.<p>In brief:<p>Our Docker Compose local dev setup started to break down once we had to model more complicated <em>production</em> behavior locally \u2013 things like table-specific Postgres roles for audit logs and dynamically provisioned databases per-clinical-trial. We were drifting toward a bespoke Bash mess to keep dev and prod in sync.<p>Our core idea was instead to embrace Terraform <i>in the local dev environment</i> too. We were already using Terraform heavily in prod, and Terraform's robust provider ecosystem meant that we could e.g. substitute Docker containers for RDS and <em>MinIO</em> for S3 without deviating too far from our <em>production</em> configuration.<p>The really fun part is how we use Terraform to handle dynamic provisioning, which we need for isolated, per-clinical-trial databases. The way we do it in prod is by giving each clinical trial its own, isolated Terraform state, stored in a cloud storage bucket. By writing an equivalent local Terraform config for this provisioning step, we enable the app to run the <i>same</i> `terraform apply` command as it does in prod to locally to spin up a new database, with the individual state for that new db stored in a local <em>MinIO</em> bucket... which is itself created by the original `terraform apply` that sets up the initial local dev infrastructure.<p>Altogether, Terraform gives us a super high-fidelity local environment that lets us test complex application behavior and infrastructure logic without the full overhead of spinning up a local k8s cluster (which is what I imagine the next best alternative might be?). It's readable, declarative, and required no new tooling on our side since we were already using Docker and Terraform anyways.<p>Curious to hear how other folks are managing complex local dev setups, especially if you're not on Kubernetes!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Terraform for *Local* Dev Infra"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://runharbor.com/blog/2025-09-30-terraform-for-high-fidelity-local-dev-infra"}}, "_tags": ["story", "author_18nleung", "story_45429761", "show_hn"], "author": "18nleung", "created_at": "2025-09-30T19:06:03Z", "created_at_i": 1759259163, "num_comments": 0, "objectID": "45429761", "points": 2, "story_id": 45429761, "story_text": "Author here with yet another solution for local dev \u2014 let me know your thoughts, would love to trade notes.<p>In brief:<p>Our Docker Compose local dev setup started to break down once we had to model more complicated production behavior locally \u2013 things like table-specific Postgres roles for audit logs and dynamically provisioned databases per-clinical-trial. We were drifting toward a bespoke Bash mess to keep dev and prod in sync.<p>Our core idea was instead to embrace Terraform <i>in the local dev environment</i> too. We were already using Terraform heavily in prod, and Terraform&#x27;s robust provider ecosystem meant that we could e.g. substitute Docker containers for RDS and MinIO for S3 without deviating too far from our production configuration.<p>The really fun part is how we use Terraform to handle dynamic provisioning, which we need for isolated, per-clinical-trial databases. The way we do it in prod is by giving each clinical trial its own, isolated Terraform state, stored in a cloud storage bucket. By writing an equivalent local Terraform config for this provisioning step, we enable the app to run the <i>same</i> `terraform apply` command as it does in prod to locally to spin up a new database, with the individual state for that new db stored in a local MinIO bucket... which is itself created by the original `terraform apply` that sets up the initial local dev infrastructure.<p>Altogether, Terraform gives us a super high-fidelity local environment that lets us test complex application behavior and infrastructure logic without the full overhead of spinning up a local k8s cluster (which is what I imagine the next best alternative might be?). It&#x27;s readable, declarative, and required no new tooling on our side since we were already using Docker and Terraform anyways.<p>Curious to hear how other folks are managing complex local dev setups, especially if you&#x27;re not on Kubernetes!", "title": "Show HN: Terraform for *Local* Dev Infra", "updated_at": "2025-09-30T21:19:56Z", "url": "https://runharbor.com/blog/2025-09-30-terraform-for-high-fidelity-local-dev-infra"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "krish_kant"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "Hi HN,<p>We built Supercheck \u2013 an open source platform that combines test automation and uptime monitoring in one self-hosted solution.<p>*The Problem:*\nMost monitoring tools are expensive SaaS products that charge per check. Test automation and monitoring are often separate concerns, using different tools and workflows. We wanted a unified platform where you can write a Playwright test once and use it for both CI/CD testing and <em>production</em> monitoring and use same tool for k6 performance tests too.<p>*What Supercheck Does:*<p>Testing:\n- Playwright browser tests (UI/E2E) with screenshots, traces, videos\n- API tests (HTTP/GraphQL)\n- Database tests (PostgreSQL, MySQL)\n- k6 performance tests\n- Schedule tests via cron or trigger from CI/CD<p>Monitoring:\n- HTTP, Website, Ping, Port monitors\n- Synthetic monitors (run full Playwright tests on a schedule)\n- Multi-region: US East, EU Central, Asia Pacific\n- Threshold-based alerting (consecutive failures before alerting)<p>Extras:\n- AI Create: Generate tests from plain English\n- AI Fix: Automatically analyze failures and suggest fixes\n- Public status pages with incident management\n- Alerts: Slack, Discord, Telegram, Email, Teams, webhooks\n- RBAC with 6 permission levels\n- Browser extensions: Chrome + Edge<p>*Architecture:*\nNext.js frontend, NestJS workers, PostgreSQL, Redis + BullMQ for job queuing, <em>MinIO</em> for artifacts. Each browser test runs in an isolated Playwright context. Workers are stateless \u2013 scale horizontally by adding more containers.<p>*Deploy:*\n```bash\ngit clone <a href=\"https://github.com/supercheck-io/supercheck\" rel=\"nofollow\">https://github.com/supercheck-io/supercheck</a>\ncd supercheck/deploy/docker\n./init-secrets.sh\ndocker compose -f docker-compose.yml up -d\n```<p>One-click deployment with Coolify.<p>*Links:*\n- GitHub: <a href=\"https://github.com/supercheck-io/supercheck\" rel=\"nofollow\">https://github.com/supercheck-io/supercheck</a>\n- Docs: <a href=\"https://supercheck.io/docs\" rel=\"nofollow\">https://supercheck.io/docs</a><p>Happy to answer any questions about the architecture, design decisions, or roadmap."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Supercheck.io \u2013 Open-Source AI-Powered Test Automation and Monitoring"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://supercheck.io/"}}, "_tags": ["story", "author_krish_kant", "story_46801564", "show_hn"], "author": "krish_kant", "children": [46808857], "created_at": "2026-01-28T21:10:46Z", "created_at_i": 1769634646, "num_comments": 1, "objectID": "46801564", "points": 1, "story_id": 46801564, "story_text": "Hi HN,<p>We built Supercheck \u2013 an open source platform that combines test automation and uptime monitoring in one self-hosted solution.<p>*The Problem:*\nMost monitoring tools are expensive SaaS products that charge per check. Test automation and monitoring are often separate concerns, using different tools and workflows. We wanted a unified platform where you can write a Playwright test once and use it for both CI&#x2F;CD testing and production monitoring and use same tool for k6 performance tests too.<p>*What Supercheck Does:*<p>Testing:\n- Playwright browser tests (UI&#x2F;E2E) with screenshots, traces, videos\n- API tests (HTTP&#x2F;GraphQL)\n- Database tests (PostgreSQL, MySQL)\n- k6 performance tests\n- Schedule tests via cron or trigger from CI&#x2F;CD<p>Monitoring:\n- HTTP, Website, Ping, Port monitors\n- Synthetic monitors (run full Playwright tests on a schedule)\n- Multi-region: US East, EU Central, Asia Pacific\n- Threshold-based alerting (consecutive failures before alerting)<p>Extras:\n- AI Create: Generate tests from plain English\n- AI Fix: Automatically analyze failures and suggest fixes\n- Public status pages with incident management\n- Alerts: Slack, Discord, Telegram, Email, Teams, webhooks\n- RBAC with 6 permission levels\n- Browser extensions: Chrome + Edge<p>*Architecture:*\nNext.js frontend, NestJS workers, PostgreSQL, Redis + BullMQ for job queuing, MinIO for artifacts. Each browser test runs in an isolated Playwright context. Workers are stateless \u2013 scale horizontally by adding more containers.<p>*Deploy:*\n```bash\ngit clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;supercheck-io&#x2F;supercheck\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;supercheck-io&#x2F;supercheck</a>\ncd supercheck&#x2F;deploy&#x2F;docker\n.&#x2F;init-secrets.sh\ndocker compose -f docker-compose.yml up -d\n```<p>One-click deployment with Coolify.<p>*Links:*\n- GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;supercheck-io&#x2F;supercheck\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;supercheck-io&#x2F;supercheck</a>\n- Docs: <a href=\"https:&#x2F;&#x2F;supercheck.io&#x2F;docs\" rel=\"nofollow\">https:&#x2F;&#x2F;supercheck.io&#x2F;docs</a><p>Happy to answer any questions about the architecture, design decisions, or roadmap.", "title": "Show HN: Supercheck.io \u2013 Open-Source AI-Powered Test Automation and Monitoring", "updated_at": "2026-01-29T11:38:35Z", "url": "https://supercheck.io/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fahim74"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "I\u2019m validating an idea: a dead-simple database backup tool for indie developers and small SaaS teams.<p>How it works:<p>Connect Postgres/MySQL/Mongo/SQLlike..<p>Click backup \u2192 securely stored (our cloud, or your S3/Wasabi/<em>MinIO</em>)<p>Click restore when you need it<p>Pricing: $1/GB, then ~$0.21/GB/month storage. No tiers, no enterprise upsell.<p>Why?<p>RDS/Cloud SQL snapshots exist, but they lock you into AWS/GCP and aren\u2019t cheap.<p>Many developers on DigitalOcean, Hetzner, Railway, Render, Fly.io, etc. don\u2019t have reliable, automated backups.<p>DIY scripts break, restores often fail, and no one tests them until it\u2019s too late.<p>Questions for HN:<p>Do you currently back up your DB, and how?<p>Would you trust/pay for a vendor-agnostic, usage-based service?<p>What would you want to see before using something like this in <em>production</em>?<p>Not pitching yet \u2014 just looking for brutal feedback before building further."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Simple pay-per-use database backup service (Postgres/MySQL/Mongo/More..)"}}, "_tags": ["story", "author_fahim74", "story_45287692", "ask_hn"], "author": "fahim74", "children": [45370108], "created_at": "2025-09-18T09:55:48Z", "created_at_i": 1758189348, "num_comments": 1, "objectID": "45287692", "points": 1, "story_id": 45287692, "story_text": "I\u2019m validating an idea: a dead-simple database backup tool for indie developers and small SaaS teams.<p>How it works:<p>Connect Postgres&#x2F;MySQL&#x2F;Mongo&#x2F;SQLlike..<p>Click backup \u2192 securely stored (our cloud, or your S3&#x2F;Wasabi&#x2F;MinIO)<p>Click restore when you need it<p>Pricing: $1&#x2F;GB, then ~$0.21&#x2F;GB&#x2F;month storage. No tiers, no enterprise upsell.<p>Why?<p>RDS&#x2F;Cloud SQL snapshots exist, but they lock you into AWS&#x2F;GCP and aren\u2019t cheap.<p>Many developers on DigitalOcean, Hetzner, Railway, Render, Fly.io, etc. don\u2019t have reliable, automated backups.<p>DIY scripts break, restores often fail, and no one tests them until it\u2019s too late.<p>Questions for HN:<p>Do you currently back up your DB, and how?<p>Would you trust&#x2F;pay for a vendor-agnostic, usage-based service?<p>What would you want to see before using something like this in production?<p>Not pitching yet \u2014 just looking for brutal feedback before building further.", "title": "Simple pay-per-use database backup service (Postgres/MySQL/Mongo/More..)", "updated_at": "2025-09-25T07:20:48Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "araju"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "I wanted to learn distributed systems by building something real instead of just reading about them. So I built an image hosting service that automatically tags uploads using GPT-4o-mini vision.<p>What it does: Upload an image \u2192 Celery worker picks it up \u2192 sends to OpenAI Vision API \u2192 tags get saved to PostgreSQL. Takes about 10 seconds, costs ~$0.004/image.<p>Why I built it this way:<p>I kept running into the same architecture decisions at work (swapping providers, handling async tasks, testing background workers) but never had full context. Building end-to-end gave me that.<p>The hardest bug: my tests were passing but <em>production</em> failed. Turned out BackgroundTasks grab new database sessions, so the worker couldn't see test data. Took 45 minutes to figure out it was a code duplication issue between my app and worker initialization.<p>Stack: FastAPI, PostgreSQL, <em>MinIO</em> (S3-compatible), Celery, Redis, Docker<p>Try it: <a href=\"https://chitram.io\" rel=\"nofollow\">https://chitram.io</a><p>Source: <a href=\"https://github.com/abhi10/chitram\" rel=\"nofollow\">https://github.com/abhi10/chitram</a><p>Blog series documenting the build: <a href=\"https://araju.dev/blog\" rel=\"nofollow\">https://araju.dev/blog</a><p>I'm an AI Automation engineer looking for my next role. Happy to answer questions about the architecture or the debugging rabbit holes I fell into."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Chitram \u2013 Open-source image hosting with automatic AI tagging"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://chitram.io"}}, "_tags": ["story", "author_araju", "story_46882027", "show_hn"], "author": "araju", "created_at": "2026-02-04T06:03:26Z", "created_at_i": 1770185006, "num_comments": 0, "objectID": "46882027", "points": 1, "story_id": 46882027, "story_text": "I wanted to learn distributed systems by building something real instead of just reading about them. So I built an image hosting service that automatically tags uploads using GPT-4o-mini vision.<p>What it does: Upload an image \u2192 Celery worker picks it up \u2192 sends to OpenAI Vision API \u2192 tags get saved to PostgreSQL. Takes about 10 seconds, costs ~$0.004&#x2F;image.<p>Why I built it this way:<p>I kept running into the same architecture decisions at work (swapping providers, handling async tasks, testing background workers) but never had full context. Building end-to-end gave me that.<p>The hardest bug: my tests were passing but production failed. Turned out BackgroundTasks grab new database sessions, so the worker couldn&#x27;t see test data. Took 45 minutes to figure out it was a code duplication issue between my app and worker initialization.<p>Stack: FastAPI, PostgreSQL, MinIO (S3-compatible), Celery, Redis, Docker<p>Try it: <a href=\"https:&#x2F;&#x2F;chitram.io\" rel=\"nofollow\">https:&#x2F;&#x2F;chitram.io</a><p>Source: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;abhi10&#x2F;chitram\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;abhi10&#x2F;chitram</a><p>Blog series documenting the build: <a href=\"https:&#x2F;&#x2F;araju.dev&#x2F;blog\" rel=\"nofollow\">https:&#x2F;&#x2F;araju.dev&#x2F;blog</a><p>I&#x27;m an AI Automation engineer looking for my next role. Happy to answer questions about the architecture or the debugging rabbit holes I fell into.", "title": "Show HN: Chitram \u2013 Open-source image hosting with automatic AI tagging", "updated_at": "2026-02-04T06:13:39Z", "url": "https://chitram.io"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jmonegro"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "Scratch.py -- hyper-fast <em>mini</em>-webapp <em>production</em>, in Python"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "http://www.speakeasy.org/~lion/proj/scratch/"}}, "_tags": ["story", "author_jmonegro", "story_790629"], "author": "jmonegro", "children": [790659, 790688, 790693, 790695, 790999, 791148], "created_at": "2009-08-28T02:47:57Z", "created_at_i": 1251427677, "num_comments": 10, "objectID": "790629", "points": 65, "story_id": 790629, "title": "Scratch.py -- hyper-fast mini-webapp production, in Python", "updated_at": "2024-09-19T16:44:49Z", "url": "http://www.speakeasy.org/~lion/proj/scratch/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bootload"}, "story_text": {"matchLevel": "none", "matchedWords": [], "value": ""}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "IPad <em>Mini</em> <em>production</em> has kicked off"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["minio", "production"], "value": "http://techcrunch.com/2012/10/03/ipad-<em>mini</em>-<em>production</em>-has-kicked-off-says-wsj-7-85-inch-lcd-screen-no-retina-display/"}}, "_tags": ["story", "author_bootload", "story_4606613"], "author": "bootload", "children": [4606778, 4606797, 4606803, 4607543, 4607637, 4608721], "created_at": "2012-10-03T09:44:08Z", "created_at_i": 1349257448, "num_comments": 44, "objectID": "4606613", "points": 35, "story_id": 4606613, "story_text": "", "title": "IPad Mini production has kicked off", "updated_at": "2024-09-19T18:52:03Z", "url": "http://techcrunch.com/2012/10/03/ipad-mini-production-has-kicked-off-says-wsj-7-85-inch-lcd-screen-no-retina-display/"}], "hitsPerPage": 15, "nbHits": 66, "nbPages": 5, "page": 0, "params": "query=minio+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 11, "processingTimingsMS": {"_request": {"roundTrip": 23}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 7, "scanning": 2, "total": 10}, "total": 11}, "query": "minio production", "serverTimeMS": 13}}