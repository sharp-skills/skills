{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "universesquid"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "Hey everyone,\nI am trying to figure out how DevOps requirements change with services that build on top of LLMs. I got a couple questions for everyone who actually tried to deploy something into prod.<p>Have you implemented a service that uses <em>langchain</em> and runs in <em>production</em>? What have your experiences been? Did you need to set up extra infrastructure to deploy it? Did you use open source models?"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "Ask HN: Using <em>Langchain</em> in <em>Production</em>"}}, "_tags": ["story", "author_universesquid", "story_35778282", "ask_hn"], "author": "universesquid", "created_at": "2023-05-01T19:57:22Z", "created_at_i": 1682971042, "num_comments": 0, "objectID": "35778282", "points": 3, "story_id": 35778282, "story_text": "Hey everyone,\nI am trying to figure out how DevOps requirements change with services that build on top of LLMs. I got a couple questions for everyone who actually tried to deploy something into prod.<p>Have you implemented a service that uses langchain and runs in production? What have your experiences been? Did you need to set up extra infrastructure to deploy it? Did you use open source models?", "title": "Ask HN: Using Langchain in Production", "updated_at": "2024-09-20T14:03:23Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "strickvl"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "I've spent weeks curating technical implementation details of how companies are actually deploying LLMs and Generative AI in <em>production</em>. The database now contains over 300 case studies with detailed technical summaries (230,000+ words) focusing exclusively on architectural decisions, deployment patterns, and real engineering challenges.<p>Key features:\n* Each case study is technically focused - no marketing fluff\n* 150+ entries from technical conference talks and panels (saving you 100+ hours of video watching)\n* Sophisticated filtering by technical stack, RAG implementations, monitoring solutions etc.\n* Summaries generated consistently using Claude for quick insight extraction\n* All sources remain public and linked for deeper exploration<p>Some unique insights we've found:\n* Common patterns in <em>LangChain</em> <em>production</em> deployments\n* Real-world RAG implementation approaches\n* Emerging best practices in LLM monitoring\n* Novel solutions to prompt engineering workflows\n* <em>Production</em>-tested security measures<p>It's a lot to read so we wrote a blog post summarising the main takeaways here <a href=\"https://www.zenml.io/blog/llmops-lessons-learned-navigating-the-wild-west-of-production-llms\" rel=\"nofollow\">https://www.zenml.io/blog/llmops-lessons-learned-navigating-...</a><p>The database is free and designed to help engineering teams learn from others' practical experiences deploying LLMs. I'm particularly interested in hearing about:\n1. What specific implementation patterns you'd like to see analyzed (contribute more case studies via the link on the database's main page)\n2. Additional case studies you think should be included\n3. How you're handling non-deterministic outputs in <em>production</em><p>Looking forward to your feedback and contributions!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: No-BS Database of 300+ real-world LLM/GenAI <em>production</em> implementations"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.zenml.io/llmops-database"}}, "_tags": ["story", "author_strickvl", "story_42305383", "show_hn"], "author": "strickvl", "created_at": "2024-12-03T12:20:53Z", "created_at_i": 1733228453, "num_comments": 0, "objectID": "42305383", "points": 7, "story_id": 42305383, "story_text": "I&#x27;ve spent weeks curating technical implementation details of how companies are actually deploying LLMs and Generative AI in production. The database now contains over 300 case studies with detailed technical summaries (230,000+ words) focusing exclusively on architectural decisions, deployment patterns, and real engineering challenges.<p>Key features:\n* Each case study is technically focused - no marketing fluff\n* 150+ entries from technical conference talks and panels (saving you 100+ hours of video watching)\n* Sophisticated filtering by technical stack, RAG implementations, monitoring solutions etc.\n* Summaries generated consistently using Claude for quick insight extraction\n* All sources remain public and linked for deeper exploration<p>Some unique insights we&#x27;ve found:\n* Common patterns in LangChain production deployments\n* Real-world RAG implementation approaches\n* Emerging best practices in LLM monitoring\n* Novel solutions to prompt engineering workflows\n* Production-tested security measures<p>It&#x27;s a lot to read so we wrote a blog post summarising the main takeaways here <a href=\"https:&#x2F;&#x2F;www.zenml.io&#x2F;blog&#x2F;llmops-lessons-learned-navigating-the-wild-west-of-production-llms\" rel=\"nofollow\">https:&#x2F;&#x2F;www.zenml.io&#x2F;blog&#x2F;llmops-lessons-learned-navigating-...</a><p>The database is free and designed to help engineering teams learn from others&#x27; practical experiences deploying LLMs. I&#x27;m particularly interested in hearing about:\n1. What specific implementation patterns you&#x27;d like to see analyzed (contribute more case studies via the link on the database&#x27;s main page)\n2. Additional case studies you think should be included\n3. How you&#x27;re handling non-deterministic outputs in production<p>Looking forward to your feedback and contributions!", "title": "Show HN: No-BS Database of 300+ real-world LLM/GenAI production implementations", "updated_at": "2024-12-03T15:10:59Z", "url": "https://www.zenml.io/llmops-database"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jhoxray"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "OneRingAI started as the internal engine of an enterprise agentic platform we've been building for 2+ years. After watching customers hit the same walls with auth, vendor lock-in, and context management over and over, we extracted the core into a standalone open-source library.\nThe two main alternatives didn't fit what we needed in <em>production</em>:<p>- <em>LangChain</em>: Great ecosystem, but the abstraction layers kept growing. By the time you wire up chains, runnables, callbacks, and agents across 50+ packages, you're fighting the framework\n  more than building your product.\n- CrewAI: Clean API, but Python-only and the role-based metaphor breaks down when you need fine-grained control over auth, context windows, or tool failures.<p>OneRingAI is a single TypeScript library (~62K LOC, 20 deps) that treats the boring <em>production</em> problems as first-class concerns:<p>Auth as architecture, not afterthought. A centralized connector registry with built-in OAuth (4 flows, AES-256-GCM storage, 43 vendor templates). This came directly from dealing with\nenterprise SSO and multi-tenant token isolation \u2014 no more scattered env vars or rolling your own token refresh.<p>Per-tool circuit breakers. One flaky Jira API shouldn't crash your entire agent loop. Each tool and connector gets independent failure isolation with retry/backoff. We learned this the\nhard way running agents against dozens of customer SaaS integrations simultaneously.<p>Context that doesn't blow up. Plugin-based context management with token budgeting. InContextMemory puts frequently-accessed state directly in the prompt instead of requiring a retrieval\ncall. Compaction removes tool call/result pairs together so the LLM never sees orphaned context.<p>Actually multi-vendor. 12 LLM providers native, 36 models in a typed registry with pricing and feature flags. Switch vendors by changing a connector name. Run openai-prod and\nopenai-backup side by side. Enterprise customers kept asking for this \u2014 nobody wants to be locked into one provider.<p>Multi-modal built in. Image gen (DALL-E 3, gpt-image-1, Imagen 4), video gen (Sora 2, Veo 3), TTS, STT \u2014 all in the same library. No extra packages.<p>Native MCP support with a registry pattern for managing multiple servers, health checks, and auto tool format conversion.<p>What it's not: it's not a no-code agent builder, and it's not trying to be a framework for every possible AI use case. It's an opinionated library for people building <em>production</em> agent\nsystems in TypeScript who want auth, resilience, and multi-vendor support without duct-taping 15 packages together.<p>2,285 tests, strict TypeScript throughout. The API surface is small on purpose \u2014 Connector.create(), Agent.create(), agent.run().<p>We also built Hosea, an open-source Electron desktop app on top of OneRingAI, if you want to see what a full agent system looks like in practice rather than just reading docs.<p>GitHub: <a href=\"https://github.com/Integrail/oneringai\" rel=\"nofollow\">https://github.com/Integrail/oneringai</a><p>npm: npm i @everworker/oneringai<p>Comparison with alternatives: <a href=\"https://oneringai.io/#comparison\" rel=\"nofollow\">https://oneringai.io/#comparison</a><p>Hosea: <a href=\"https://github.com/Integrail/oneringai/blob/main/apps/hosea/\" rel=\"nofollow\">https://github.com/Integrail/oneringai/blob/main/apps/hosea/</a>...<p>Happy to answer questions about the architecture decisions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: OneRingAI \u2013 Single TypeScript library for multi-vendor AI agents"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://oneringai.io"}}, "_tags": ["story", "author_jhoxray", "story_47046494", "show_hn"], "author": "jhoxray", "children": [47049515], "created_at": "2026-02-17T11:48:29Z", "created_at_i": 1771328909, "num_comments": 1, "objectID": "47046494", "points": 4, "story_id": 47046494, "story_text": "OneRingAI started as the internal engine of an enterprise agentic platform we&#x27;ve been building for 2+ years. After watching customers hit the same walls with auth, vendor lock-in, and context management over and over, we extracted the core into a standalone open-source library.\nThe two main alternatives didn&#x27;t fit what we needed in production:<p>- LangChain: Great ecosystem, but the abstraction layers kept growing. By the time you wire up chains, runnables, callbacks, and agents across 50+ packages, you&#x27;re fighting the framework\n  more than building your product.\n- CrewAI: Clean API, but Python-only and the role-based metaphor breaks down when you need fine-grained control over auth, context windows, or tool failures.<p>OneRingAI is a single TypeScript library (~62K LOC, 20 deps) that treats the boring production problems as first-class concerns:<p>Auth as architecture, not afterthought. A centralized connector registry with built-in OAuth (4 flows, AES-256-GCM storage, 43 vendor templates). This came directly from dealing with\nenterprise SSO and multi-tenant token isolation \u2014 no more scattered env vars or rolling your own token refresh.<p>Per-tool circuit breakers. One flaky Jira API shouldn&#x27;t crash your entire agent loop. Each tool and connector gets independent failure isolation with retry&#x2F;backoff. We learned this the\nhard way running agents against dozens of customer SaaS integrations simultaneously.<p>Context that doesn&#x27;t blow up. Plugin-based context management with token budgeting. InContextMemory puts frequently-accessed state directly in the prompt instead of requiring a retrieval\ncall. Compaction removes tool call&#x2F;result pairs together so the LLM never sees orphaned context.<p>Actually multi-vendor. 12 LLM providers native, 36 models in a typed registry with pricing and feature flags. Switch vendors by changing a connector name. Run openai-prod and\nopenai-backup side by side. Enterprise customers kept asking for this \u2014 nobody wants to be locked into one provider.<p>Multi-modal built in. Image gen (DALL-E 3, gpt-image-1, Imagen 4), video gen (Sora 2, Veo 3), TTS, STT \u2014 all in the same library. No extra packages.<p>Native MCP support with a registry pattern for managing multiple servers, health checks, and auto tool format conversion.<p>What it&#x27;s not: it&#x27;s not a no-code agent builder, and it&#x27;s not trying to be a framework for every possible AI use case. It&#x27;s an opinionated library for people building production agent\nsystems in TypeScript who want auth, resilience, and multi-vendor support without duct-taping 15 packages together.<p>2,285 tests, strict TypeScript throughout. The API surface is small on purpose \u2014 Connector.create(), Agent.create(), agent.run().<p>We also built Hosea, an open-source Electron desktop app on top of OneRingAI, if you want to see what a full agent system looks like in practice rather than just reading docs.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai</a><p>npm: npm i @everworker&#x2F;oneringai<p>Comparison with alternatives: <a href=\"https:&#x2F;&#x2F;oneringai.io&#x2F;#comparison\" rel=\"nofollow\">https:&#x2F;&#x2F;oneringai.io&#x2F;#comparison</a><p>Hosea: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai&#x2F;blob&#x2F;main&#x2F;apps&#x2F;hosea&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai&#x2F;blob&#x2F;main&#x2F;apps&#x2F;hosea&#x2F;</a>...<p>Happy to answer questions about the architecture decisions.", "title": "Show HN: OneRingAI \u2013 Single TypeScript library for multi-vendor AI agents", "updated_at": "2026-02-17T16:45:49Z", "url": "https://oneringai.io"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "iroy2000"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "I built this to test how <em>production</em> <em>LangChain</em> agents handle failures. It injects random exceptions into tool/model calls at configurable rates. Testing agent resilience is hard. This lets you simulate real-world failures (network issues, rate limits, service outages) in a controlled way."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["langchain"], "value": "Show HN: Chaos Monkey middleware for <em>LangChain</em> (v1) agents"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["langchain"], "value": "https://github.com/iroy2000/<em>langchain</em>-chaos-middleware"}}, "_tags": ["story", "author_iroy2000", "story_46047927", "show_hn"], "author": "iroy2000", "created_at": "2025-11-25T17:04:36Z", "created_at_i": 1764090276, "num_comments": 0, "objectID": "46047927", "points": 2, "story_id": 46047927, "story_text": "I built this to test how production LangChain agents handle failures. It injects random exceptions into tool&#x2F;model calls at configurable rates. Testing agent resilience is hard. This lets you simulate real-world failures (network issues, rate limits, service outages) in a controlled way.", "title": "Show HN: Chaos Monkey middleware for LangChain (v1) agents", "updated_at": "2025-11-25T17:28:02Z", "url": "https://github.com/iroy2000/langchain-chaos-middleware"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rhombus01"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "Show HN: Deploy <em>LangChain</em> Apps to <em>Production</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["langchain"], "value": "https://github.com/steamship-core/steamship-<em>langchain</em>"}}, "_tags": ["story", "author_rhombus01", "story_34728147", "show_hn"], "author": "rhombus01", "children": [34728155, 34728194], "created_at": "2023-02-09T17:52:48Z", "created_at_i": 1675965168, "num_comments": 2, "objectID": "34728147", "points": 8, "story_id": 34728147, "title": "Show HN: Deploy LangChain Apps to Production", "updated_at": "2024-09-20T13:13:00Z", "url": "https://github.com/steamship-core/steamship-langchain"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "deepankarm"}, "story_text": {"matchLevel": "none", "matchedWords": [], "value": "- Exposes APIs from function definitions locally as well as on the cloud.\n- Very few lines of code changes, ease of development remains the same as local.\n- Supports both REST &amp; Websocket endpoints\n- Serverless/autoscaling endpoints with automatic tls certs.\n- Real-time streaming, human-in-the-loop support"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "Show HN: <em>Langchain</em>-serve \u2013 <em>Langchain</em> apps on <em>production</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["langchain"], "value": "https://github.com/jina-ai/<em>langchain</em>-serve"}}, "_tags": ["story", "author_deepankarm", "story_35673030", "show_hn"], "author": "deepankarm", "created_at": "2023-04-23T03:21:43Z", "created_at_i": 1682220103, "num_comments": 0, "objectID": "35673030", "points": 4, "story_id": 35673030, "story_text": "- Exposes APIs from function definitions locally as well as on the cloud.\n- Very few lines of code changes, ease of development remains the same as local.\n- Supports both REST &amp; Websocket endpoints\n- Serverless&#x2F;autoscaling endpoints with automatic tls certs.\n- Real-time streaming, human-in-the-loop support", "title": "Show HN: Langchain-serve \u2013 Langchain apps on production", "updated_at": "2024-09-20T13:52:59Z", "url": "https://github.com/jina-ai/langchain-serve"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "EntICOnc"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "Building a <em>Production</em>-Ready <em>LangChain</em> Application with BentoML and OpenLLM"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "https://www.bentoml.com/blog/building-a-<em>production</em>-ready-<em>langchain</em>-application-with-bentoml-and-openllm"}}, "_tags": ["story", "author_EntICOnc", "story_37973541"], "author": "EntICOnc", "created_at": "2023-10-22T07:31:12Z", "created_at_i": 1697959872, "num_comments": 0, "objectID": "37973541", "points": 2, "story_id": 37973541, "title": "Building a Production-Ready LangChain Application with BentoML and OpenLLM", "updated_at": "2024-09-20T15:28:23Z", "url": "https://www.bentoml.com/blog/building-a-production-ready-langchain-application-with-bentoml-and-openllm"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "sherlockxu"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "Building a <em>Production</em>-Ready <em>LangChain</em> Application with BentoML and OpenLLM"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "https://www.bentoml.com/blog/building-a-<em>production</em>-ready-<em>langchain</em>-application-with-bentoml-and-openllm"}}, "_tags": ["story", "author_sherlockxu", "story_37582148"], "author": "sherlockxu", "created_at": "2023-09-20T09:20:40Z", "created_at_i": 1695201640, "num_comments": 0, "objectID": "37582148", "points": 2, "story_id": 37582148, "title": "Building a Production-Ready LangChain Application with BentoML and OpenLLM", "updated_at": "2024-09-20T15:15:12Z", "url": "https://www.bentoml.com/blog/building-a-production-ready-langchain-application-with-bentoml-and-openllm"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "vincentjiang"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "Hi HN,<p>I\u2019m Vincent from Aden. We spent 4 years building ERP automation for construction (PO/invoice reconciliation). We had real enterprise customers but hit a technical wall: Chatbots aren't for real work. Accountants don't want to chat; they want the ledger reconciled while they sleep. They want services, not tools.<p>Existing agent frameworks (<em>LangChain</em>, AutoGPT) failed in <em>production</em> - brittle, looping, and unable to handle messy data. General Computer Use (GCU) frameworks were even worse. My reflections:<p>1. The &quot;Toy App&quot; Ceiling &amp; GCU Trap\nMost frameworks assume synchronous sessions. If the tab closes, state is lost. You can't fit 2 weeks of asynchronous business state into an ephemeral chat session.<p>The GCU hype (agents &quot;looking&quot; at screens) is skeuomorphic. It\u2019s slow (screenshots), expensive (tokens), and fragile (UI changes = crash). It mimics human constraints rather than leveraging machine speed. Real automation should be headless.<p>2. Inversion of Control: OODA &gt; DAGs\nTraditional DAGs are deterministic; if a step fails, the program crashes. In the AI era, the Goal is the law, not the Code. We use an OODA loop to manage stochastic behavior:<p>- Observe: Exceptions are observations (FileNotFound = new state), not crashes.<p>- Orient: Adjust strategy based on Memory and - Traits.<p>- Decide: Generate new code at runtime.<p>- Act: Execute.<p>The topology shouldn't be hardcoded; it should emerge from the task's entropy.<p>3. Reliability: The &quot;Synthetic&quot; SLA\nYou can't guarantee one inference ($k=1$) is correct, but you can guarantee a System of Inference ($k=n$) converges on correctness. Reliability is now a function of compute budget. By wrapping an 80% accurate model in a &quot;Best-of-3&quot; verification loop, we mathematically force the error rate down\u2014trading Latency/Tokens for Certainty.<p>4. Biology &amp; Psychology in Code\n&quot;Hard Logic&quot; can't solve &quot;Soft Problems.&quot; We map cognition to architectural primitives:\nHomeostasis: Solving &quot;Perseveration&quot; (infinite loops) via a &quot;Stress&quot; metric. If an action fails 3x, &quot;neuroplasticity&quot; drops, forcing a strategy shift.\nTraits: Personality as a constraint. &quot;High Conscientiousness&quot; increases verification; &quot;High Risk&quot; executes DROP TABLE without asking.<p>For the industry, we need engineers interested in the intersection of biology, psychology, and distributed systems to help us move beyond brittle scripts. It'd be great to have you roasting my codes and sharing feedback.<p>Repo: <a href=\"https://github.com/adenhq/hive\" rel=\"nofollow\">https://github.com/adenhq/hive</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Agent framework that generates its own topology and evolves at runtime"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/adenhq/hive/blob/main/README.md"}}, "_tags": ["story", "author_vincentjiang", "story_46979781", "show_hn"], "author": "vincentjiang", "children": [46979829, 46982575, 46982586, 46982863, 46983301, 46983313, 46983530, 46984074, 46984427, 46984441, 46984556, 46984721, 46984731, 46984748, 46984927, 46985003, 46985026, 46985061, 46985133, 46985184, 46985231, 46985243, 46985278, 46985290, 46985302, 46985463, 46985688, 46985739, 46985824, 46986043, 46986266, 46986448, 46986672, 46986893, 46987006, 46988099, 46995150, 47021914, 47021920, 47023564, 47023581, 47034782], "created_at": "2026-02-11T19:39:43Z", "created_at_i": 1770838783, "num_comments": 36, "objectID": "46979781", "points": 107, "story_id": 46979781, "story_text": "Hi HN,<p>I\u2019m Vincent from Aden. We spent 4 years building ERP automation for construction (PO&#x2F;invoice reconciliation). We had real enterprise customers but hit a technical wall: Chatbots aren&#x27;t for real work. Accountants don&#x27;t want to chat; they want the ledger reconciled while they sleep. They want services, not tools.<p>Existing agent frameworks (LangChain, AutoGPT) failed in production - brittle, looping, and unable to handle messy data. General Computer Use (GCU) frameworks were even worse. My reflections:<p>1. The &quot;Toy App&quot; Ceiling &amp; GCU Trap\nMost frameworks assume synchronous sessions. If the tab closes, state is lost. You can&#x27;t fit 2 weeks of asynchronous business state into an ephemeral chat session.<p>The GCU hype (agents &quot;looking&quot; at screens) is skeuomorphic. It\u2019s slow (screenshots), expensive (tokens), and fragile (UI changes = crash). It mimics human constraints rather than leveraging machine speed. Real automation should be headless.<p>2. Inversion of Control: OODA &gt; DAGs\nTraditional DAGs are deterministic; if a step fails, the program crashes. In the AI era, the Goal is the law, not the Code. We use an OODA loop to manage stochastic behavior:<p>- Observe: Exceptions are observations (FileNotFound = new state), not crashes.<p>- Orient: Adjust strategy based on Memory and - Traits.<p>- Decide: Generate new code at runtime.<p>- Act: Execute.<p>The topology shouldn&#x27;t be hardcoded; it should emerge from the task&#x27;s entropy.<p>3. Reliability: The &quot;Synthetic&quot; SLA\nYou can&#x27;t guarantee one inference ($k=1$) is correct, but you can guarantee a System of Inference ($k=n$) converges on correctness. Reliability is now a function of compute budget. By wrapping an 80% accurate model in a &quot;Best-of-3&quot; verification loop, we mathematically force the error rate down\u2014trading Latency&#x2F;Tokens for Certainty.<p>4. Biology &amp; Psychology in Code\n&quot;Hard Logic&quot; can&#x27;t solve &quot;Soft Problems.&quot; We map cognition to architectural primitives:\nHomeostasis: Solving &quot;Perseveration&quot; (infinite loops) via a &quot;Stress&quot; metric. If an action fails 3x, &quot;neuroplasticity&quot; drops, forcing a strategy shift.\nTraits: Personality as a constraint. &quot;High Conscientiousness&quot; increases verification; &quot;High Risk&quot; executes DROP TABLE without asking.<p>For the industry, we need engineers interested in the intersection of biology, psychology, and distributed systems to help us move beyond brittle scripts. It&#x27;d be great to have you roasting my codes and sharing feedback.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;adenhq&#x2F;hive\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;adenhq&#x2F;hive</a>", "title": "Show HN: Agent framework that generates its own topology and evolves at runtime", "updated_at": "2026-02-25T11:27:30Z", "url": "https://github.com/adenhq/hive/blob/main/README.md"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "zachllama"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "I worked in building LLM applications for last 5 years (before the OpenAI hype began). Here is my list of go to frameworks<p>Reliable<p>1. Deepspeed - consolidates lots of good training and inference techniques. Really good set of researchers and community backing this project<p>2. HuggingFace - It has convenience of trying out multiple models and datasets relatively quickly. The API is hard to read through, but the community is really good and helpful.<p>3. FasterTransformer + Triton Server - best for inference. Steep learning curve, but worth it. I often use Deepspeed inference with Triton server - takes some work to integrate but worth the effort.<p>Good for reference only:<p>1. <em>Langchain</em>, LLamaIndex: Not currently <em>production</em> quality and feel more hacky and prototyping quality. Good to reference to understand interfaces, new techniques. But take what you need and write your own code<p>2. LMFlow: really good project. I am tracking how it evolves. I use it as a reference to build out finetuning, rlhf style workflows.<p>Avoid completely:<p>1. pytorch-lightning / lightning: outdated and too many bugs. Not for prototyping or <em>production</em>. you will waste more time trying to understand why it doesn't work instead of doing your work. Almost no community to help around this anymore.<p>2. Tensorflow: outdated. Don't sink time into this as its quickly being replaced everywhere. Pick up new concepts from Jax and see how to apply them in pytorch.<p>What are some other frameworks that you use?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "LLM Training/Application Frameworks to use and avoid"}}, "_tags": ["story", "author_zachllama", "story_36749450", "ask_hn"], "author": "zachllama", "children": [36750816, 36819391], "created_at": "2023-07-16T17:27:06Z", "created_at_i": 1689528426, "num_comments": 2, "objectID": "36749450", "points": 8, "story_id": 36749450, "story_text": "I worked in building LLM applications for last 5 years (before the OpenAI hype began). Here is my list of go to frameworks<p>Reliable<p>1. Deepspeed - consolidates lots of good training and inference techniques. Really good set of researchers and community backing this project<p>2. HuggingFace - It has convenience of trying out multiple models and datasets relatively quickly. The API is hard to read through, but the community is really good and helpful.<p>3. FasterTransformer + Triton Server - best for inference. Steep learning curve, but worth it. I often use Deepspeed inference with Triton server - takes some work to integrate but worth the effort.<p>Good for reference only:<p>1. Langchain, LLamaIndex: Not currently production quality and feel more hacky and prototyping quality. Good to reference to understand interfaces, new techniques. But take what you need and write your own code<p>2. LMFlow: really good project. I am tracking how it evolves. I use it as a reference to build out finetuning, rlhf style workflows.<p>Avoid completely:<p>1. pytorch-lightning &#x2F; lightning: outdated and too many bugs. Not for prototyping or production. you will waste more time trying to understand why it doesn&#x27;t work instead of doing your work. Almost no community to help around this anymore.<p>2. Tensorflow: outdated. Don&#x27;t sink time into this as its quickly being replaced everywhere. Pick up new concepts from Jax and see how to apply them in pytorch.<p>What are some other frameworks that you use?", "title": "LLM Training/Application Frameworks to use and avoid", "updated_at": "2024-09-20T14:38:20Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jacky2wong"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "With the number of ask your PDF SaaS solutions, I noticed there weren't any solutions focused on people who wanted to build it for their business/SMB. Most of them also weren't <em>production</em>-ready (requiring a lot of work to productionize applications like <em>LangChain</em>). We therefore built a solution that provided  Helicone-like observability that handled everything out of the box. We built PDFAgent to combine the capabilities of asking your PDF with built-in observability on top of Twilix\u2019s RAG infrastructure. It handles parsing, cleaning, splitting up your PDF to provide useful embeddings (the user just needs to submit a publicly readable URL link) and a MixPanel-like interface to monitor and observe queries."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: PDFAgent with Observability"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/mr-gpt/PDFAgent"}}, "_tags": ["story", "author_jacky2wong", "story_36843507", "show_hn"], "author": "jacky2wong", "created_at": "2023-07-24T05:41:22Z", "created_at_i": 1690177282, "num_comments": 0, "objectID": "36843507", "points": 4, "story_id": 36843507, "story_text": "With the number of ask your PDF SaaS solutions, I noticed there weren&#x27;t any solutions focused on people who wanted to build it for their business&#x2F;SMB. Most of them also weren&#x27;t production-ready (requiring a lot of work to productionize applications like LangChain). We therefore built a solution that provided  Helicone-like observability that handled everything out of the box. We built PDFAgent to combine the capabilities of asking your PDF with built-in observability on top of Twilix\u2019s RAG infrastructure. It handles parsing, cleaning, splitting up your PDF to provide useful embeddings (the user just needs to submit a publicly readable URL link) and a MixPanel-like interface to monitor and observe queries.", "title": "Show HN: PDFAgent with Observability", "updated_at": "2024-09-20T14:37:42Z", "url": "https://github.com/mr-gpt/PDFAgent"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "exordex"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "Hey HN,\n                                                                                                                                      If you're building <em>LangChain</em> agents, you've probably seen them break in creative ways - prompt injection bypassing your chain logic, tools getting called with bad parameters, or cascading failures when an API times out mid-chain.<p>I built Khaos to test these failure modes before <em>production</em>.<p>Example <em>LangChain</em> agent:\n  ```python\n  from <em>langchain</em>.agents import AgentExecutor, create_openai_functions_agent\n  from khaos import khaosagent<p><pre><code>  @khaosagent(name=&quot;research-agent&quot;, framework=&quot;langgraph&quot;)\n  def agent(query: str) -&gt; dict:\n      executor = AgentExecutor(agent=agent, tools=tools)\n      result = executor.invoke({&quot;input&quot;: query})\n      return {&quot;response&quot;: result[&quot;output&quot;]}\n</code></pre>\nTest it:\n  pip install khaos-agent\n  khaos discover\n  khaos run research-agent --pack security<p>Khaos injects:\n  - 242+ security attacks - Prompt injection variations that bypass <em>LangChain</em>'s prompt templates\n  - Tool misuse - Malicious parameters in tool calls (e.g., os.system injection in code execution tools)\n  - Chain failures - What happens when your 3rd step in a 5-step chain times out?\n  - LLM faults - Rate limits, token overflows, model unavailability<p><pre><code>  Why this matters for <em>LangChain</em> specifically:\n\n  <em>LangChain</em>'s abstraction layers can hide vulnerabilities:\n  - Prompt templates can still be injected via tool outputs\n  - AgentExecutor doesn't validate tool parameters\n  - Chains fail silently or propagate corrupted state\n  - ReAct/Plan-and-Execute patterns have unique attack surfaces\n\n  Works with LangGraph, LCEL chains, and classic <em>LangChain</em> agents. Auto-instruments your chains to inject faults at each step.\n\n  Repo: https://github.com/ExordexLabs/khaos-sdk\n  Examples: https://github.com/ExordexLabs/khaos-examples/tree/master/code-execution-agent</code></pre>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["langchain"], "value": "<em>LangChain</em> Agent Testing Guide Tool (Free)"}}, "_tags": ["story", "author_exordex", "story_47004292", "ask_hn"], "author": "exordex", "created_at": "2026-02-13T16:07:25Z", "created_at_i": 1770998845, "num_comments": 0, "objectID": "47004292", "points": 1, "story_id": 47004292, "story_text": "Hey HN,\n                                                                                                                                      If you&#x27;re building LangChain agents, you&#x27;ve probably seen them break in creative ways - prompt injection bypassing your chain logic, tools getting called with bad parameters, or cascading failures when an API times out mid-chain.<p>I built Khaos to test these failure modes before production.<p>Example LangChain agent:\n  ```python\n  from langchain.agents import AgentExecutor, create_openai_functions_agent\n  from khaos import khaosagent<p><pre><code>  @khaosagent(name=&quot;research-agent&quot;, framework=&quot;langgraph&quot;)\n  def agent(query: str) -&gt; dict:\n      executor = AgentExecutor(agent=agent, tools=tools)\n      result = executor.invoke({&quot;input&quot;: query})\n      return {&quot;response&quot;: result[&quot;output&quot;]}\n</code></pre>\nTest it:\n  pip install khaos-agent\n  khaos discover\n  khaos run research-agent --pack security<p>Khaos injects:\n  - 242+ security attacks - Prompt injection variations that bypass LangChain&#x27;s prompt templates\n  - Tool misuse - Malicious parameters in tool calls (e.g., os.system injection in code execution tools)\n  - Chain failures - What happens when your 3rd step in a 5-step chain times out?\n  - LLM faults - Rate limits, token overflows, model unavailability<p><pre><code>  Why this matters for LangChain specifically:\n\n  LangChain&#x27;s abstraction layers can hide vulnerabilities:\n  - Prompt templates can still be injected via tool outputs\n  - AgentExecutor doesn&#x27;t validate tool parameters\n  - Chains fail silently or propagate corrupted state\n  - ReAct&#x2F;Plan-and-Execute patterns have unique attack surfaces\n\n  Works with LangGraph, LCEL chains, and classic LangChain agents. Auto-instruments your chains to inject faults at each step.\n\n  Repo: https:&#x2F;&#x2F;github.com&#x2F;ExordexLabs&#x2F;khaos-sdk\n  Examples: https:&#x2F;&#x2F;github.com&#x2F;ExordexLabs&#x2F;khaos-examples&#x2F;tree&#x2F;master&#x2F;code-execution-agent</code></pre>", "title": "LangChain Agent Testing Guide Tool (Free)", "updated_at": "2026-02-13T16:09:21Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "EntICOnc"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "The <em>LangChain</em> and Vector Databases in <em>Production</em> Course"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.i-programmer.info/news/105-artificial-intelligence/16460-master-llms-on-data-for-free.html"}}, "_tags": ["story", "author_EntICOnc", "story_36772489"], "author": "EntICOnc", "created_at": "2023-07-18T14:06:43Z", "created_at_i": 1689689203, "num_comments": 0, "objectID": "36772489", "points": 2, "story_id": 36772489, "title": "The LangChain and Vector Databases in Production Course", "updated_at": "2024-09-20T14:40:55Z", "url": "https://www.i-programmer.info/news/105-artificial-intelligence/16460-master-llms-on-data-for-free.html"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "toshvelaga"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "Created an app that connect OpenAI to my <em>production</em> postgres DB using <em>langchain</em>.<p>App: https://askmydb.vercel.app/<p>Check out the demo: https://www.loom.com/share/108e23873cfc41edbb8923cc86edccb3<p>Built using Next JS, Vercel, AWS Lambda and <em>langchain</em>. Everything is stored locally on the user's computer."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Connected OpenAI to My Postgres DB"}}, "_tags": ["story", "author_toshvelaga", "story_35781371", "ask_hn"], "author": "toshvelaga", "children": [35821040], "created_at": "2023-05-02T00:24:10Z", "created_at_i": 1682987050, "num_comments": 1, "objectID": "35781371", "points": 8, "story_id": 35781371, "story_text": "Created an app that connect OpenAI to my production postgres DB using langchain.<p>App: https:&#x2F;&#x2F;askmydb.vercel.app&#x2F;<p>Check out the demo: https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;108e23873cfc41edbb8923cc86edccb3<p>Built using Next JS, Vercel, AWS Lambda and langchain. Everything is stored locally on the user&#x27;s computer.", "title": "Connected OpenAI to My Postgres DB", "updated_at": "2024-09-20T14:03:40Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fazlerocks"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["langchain", "production"], "value": "I've been building AI applications using Next.js, GPT, and <em>Langchain</em>. As I'm approaching <em>production</em> scale, I'm curious how others are handling deployment infrastructure.<p>Current stack:\n- Next.js on Vercel\n- Serverless functions for AI/LLM endpoints\n- Pinecone for vector storage<p>Questions for those running AI in <em>production</em>:<p>1. What's your serverless infrastructure choice? (Vercel/Cloud Run/Lambda)<p>2. How are you handling state management for long-running agent tasks?<p>3. What's your approach to cost optimization with LLM API calls?<p>4. Are you self-hosting any components?<p>5. How are you handling vector store scaling?<p>Particularly interested in hearing from teams who've scaled beyond prototype stage. Have you hit any unexpected limitations with serverless for AI workloads?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: What's your serverless stack for AI/LLM apps in <em>production</em>?"}}, "_tags": ["story", "author_fazlerocks", "story_42659704", "ask_hn"], "author": "fazlerocks", "children": [42660081], "created_at": "2025-01-10T20:22:57Z", "created_at_i": 1736540577, "num_comments": 3, "objectID": "42659704", "points": 3, "story_id": 42659704, "story_text": "I&#x27;ve been building AI applications using Next.js, GPT, and Langchain. As I&#x27;m approaching production scale, I&#x27;m curious how others are handling deployment infrastructure.<p>Current stack:\n- Next.js on Vercel\n- Serverless functions for AI&#x2F;LLM endpoints\n- Pinecone for vector storage<p>Questions for those running AI in production:<p>1. What&#x27;s your serverless infrastructure choice? (Vercel&#x2F;Cloud Run&#x2F;Lambda)<p>2. How are you handling state management for long-running agent tasks?<p>3. What&#x27;s your approach to cost optimization with LLM API calls?<p>4. Are you self-hosting any components?<p>5. How are you handling vector store scaling?<p>Particularly interested in hearing from teams who&#x27;ve scaled beyond prototype stage. Have you hit any unexpected limitations with serverless for AI workloads?", "title": "Ask HN: What's your serverless stack for AI/LLM apps in production?", "updated_at": "2025-01-12T21:52:01Z"}], "hitsPerPage": 15, "nbHits": 101, "nbPages": 7, "page": 0, "params": "query=langchain+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 13, "processingTimingsMS": {"_request": {"roundTrip": 23}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 10, "scanning": 1, "total": 12}, "total": 13}, "query": "langchain production", "serverTimeMS": 15}}