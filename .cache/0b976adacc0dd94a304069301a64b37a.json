{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "greatNespresso"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sequelize", "production"], "value": "Hi folks,<p>Having played for long with Node on toy projects (mainly web targeted as the question suggests) I have always enjoyed the freedom of composing packages for everything, in a light Flaskish way and it turned out to always be sufficient for my needs.<p>Today, however, I am considering launching a more ambitious app, and I am feeling doubtful about my koa + <em>sequelize</em> + passport traditionnal backend setup for this use case.<p>Still, if React and Vue owns all the attention on the web right now, I haven't managed to find out a thread on the usual/accepted Node stack for backend, currently in use on <em>production</em> web app, as the prod and cons associated.<p>Are Frameworks like Meteor the usual way of going as in other languages like Python or Ruby, is MEAN still a/THE thing for Node considering the drawbacks associated with NoSQL, do you rather externalize services to PAAS like Firebase ?<p>Having, again, no experience with Node business apps, I would be eagerly curious and thankful for you to share your stack and opinion :)<p>Thanks !"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: Node stack for <em>production</em> web app in 2018"}}, "_tags": ["story", "author_greatNespresso", "story_17509748", "ask_hn"], "author": "greatNespresso", "children": [17510922, 17512423], "created_at": "2018-07-11T19:45:46Z", "created_at_i": 1531338346, "num_comments": 4, "objectID": "17509748", "points": 3, "story_id": 17509748, "story_text": "Hi folks,<p>Having played for long with Node on toy projects (mainly web targeted as the question suggests) I have always enjoyed the freedom of composing packages for everything, in a light Flaskish way and it turned out to always be sufficient for my needs.<p>Today, however, I am considering launching a more ambitious app, and I am feeling doubtful about my koa + sequelize + passport traditionnal backend setup for this use case.<p>Still, if React and Vue owns all the attention on the web right now, I haven&#x27;t managed to find out a thread on the usual&#x2F;accepted Node stack for backend, currently in use on production web app, as the prod and cons associated.<p>Are Frameworks like Meteor the usual way of going as in other languages like Python or Ruby, is MEAN still a&#x2F;THE thing for Node considering the drawbacks associated with NoSQL, do you rather externalize services to PAAS like Firebase ?<p>Having, again, no experience with Node business apps, I would be eagerly curious and thankful for you to share your stack and opinion :)<p>Thanks !", "title": "Ask HN: Node stack for production web app in 2018", "updated_at": "2024-09-20T02:42:52Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "levkk"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sequelize", "production"], "value": "Hey HN! Lev and Justin here, authors of PgDog (<a href=\"https://pgdog.dev/\">https://pgdog.dev/</a>), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that works without requiring application code changes or database migrations.<p>Our post from last year: <a href=\"https://news.ycombinator.com/item?id=44099187\">https://news.ycombinator.com/item?id=44099187</a><p>The most important update: we are in <em>production</em>. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway.<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE/DROP/ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important because most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, <em>Sequelize</em>, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth a look.<p>If you\u2019re like us and prefer integers to UUIDs for your primary keys, we built a cross-shard unique sequence, directly inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, we can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a promoted primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with INSERT/UPDATE statements, but if you still prefer to handle your read/write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python/Ruby/Go apps, this helps by reducing memory usage, I/O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by an application crash, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending the Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off/on, so should you choose to give it a try, you can do so at your own pace. Our docs (<a href=\"https://docs.pgdog.dev\">https://docs.pgdog.dev</a>) should help too.<p>Thanks for reading and happy hacking!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: PgDog \u2013 Scale Postgres without changing the app"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/pgdogdev/pgdog"}}, "_tags": ["story", "author_levkk", "story_47123631", "show_hn"], "author": "levkk", "children": [47125519, 47125908, 47126058, 47126391, 47126668, 47126736, 47126921, 47127155, 47127202, 47127712, 47128494, 47128649, 47130284, 47130616, 47131018, 47131119, 47131282, 47132290, 47132869, 47133196, 47133746, 47135830, 47149003, 47154215], "created_at": "2026-02-23T15:33:24Z", "created_at_i": 1771860804, "num_comments": 62, "objectID": "47123631", "points": 322, "story_id": 47123631, "story_text": "Hey HN! Lev and Justin here, authors of PgDog (<a href=\"https:&#x2F;&#x2F;pgdog.dev&#x2F;\">https:&#x2F;&#x2F;pgdog.dev&#x2F;</a>), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that works without requiring application code changes or database migrations.<p>Our post from last year: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44099187\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44099187</a><p>The most important update: we are in production. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway.<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE&#x2F;DROP&#x2F;ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important because most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, Sequelize, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth a look.<p>If you\u2019re like us and prefer integers to UUIDs for your primary keys, we built a cross-shard unique sequence, directly inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, we can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a promoted primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with INSERT&#x2F;UPDATE statements, but if you still prefer to handle your read&#x2F;write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python&#x2F;Ruby&#x2F;Go apps, this helps by reducing memory usage, I&#x2F;O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by an application crash, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending the Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off&#x2F;on, so should you choose to give it a try, you can do so at your own pace. Our docs (<a href=\"https:&#x2F;&#x2F;docs.pgdog.dev\">https:&#x2F;&#x2F;docs.pgdog.dev</a>) should help too.<p>Thanks for reading and happy hacking!", "title": "Show HN: PgDog \u2013 Scale Postgres without changing the app", "updated_at": "2026-02-27T15:48:13Z", "url": "https://github.com/pgdogdev/pgdog"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "levkk"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sequelize", "production"], "value": "Hey HN!<p>Lev and Justin here, authors of PgDog (https://github.com/pgdogdev/pgdog), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that doesn\u2019t require application code changes or database migrations to work.<p>Our post from last year: https://news.ycombinator.com/item?id=44099187<p>The most important update: we are in <em>production</em>. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway:<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE/DROP/ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important since most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, <em>Sequelize</em>, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth another look.<p>If you\u2019re like us and prefer integers to UUIDs for your primary keys, we built a cross-shard unique sequence inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, PgDog can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a new primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with insert/update statements, but if you still prefer to handle your read/write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python/Ruby/Go apps, this helps by reducing memory usage, I/O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by PgBouncer, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending a Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off/on, so should you choose to give it a try, you can do so at your own pace. Our docs (https://docs.pgdog.dev) should help too.<p>Thanks for reading and happy hacking!<p>Lev &amp; Justin"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "PgDog: Connection pooler, load balancer and sharder for PostgreSQL"}}, "_tags": ["story", "author_levkk", "story_47048265", "ask_hn"], "author": "levkk", "created_at": "2026-02-17T15:05:55Z", "created_at_i": 1771340755, "num_comments": 0, "objectID": "47048265", "points": 1, "story_id": 47048265, "story_text": "Hey HN!<p>Lev and Justin here, authors of PgDog (https:&#x2F;&#x2F;github.com&#x2F;pgdogdev&#x2F;pgdog), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that doesn\u2019t require application code changes or database migrations to work.<p>Our post from last year: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44099187<p>The most important update: we are in production. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway:<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE&#x2F;DROP&#x2F;ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important since most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, Sequelize, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth another look.<p>If you\u2019re like us and prefer integers to UUIDs for your primary keys, we built a cross-shard unique sequence inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, PgDog can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a new primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with insert&#x2F;update statements, but if you still prefer to handle your read&#x2F;write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python&#x2F;Ruby&#x2F;Go apps, this helps by reducing memory usage, I&#x2F;O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by PgBouncer, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending a Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off&#x2F;on, so should you choose to give it a try, you can do so at your own pace. Our docs (https:&#x2F;&#x2F;docs.pgdog.dev) should help too.<p>Thanks for reading and happy hacking!<p>Lev &amp; Justin", "title": "PgDog: Connection pooler, load balancer and sharder for PostgreSQL", "updated_at": "2026-02-17T15:11:18Z"}], "hitsPerPage": 15, "nbHits": 3, "nbPages": 1, "page": 0, "params": "query=sequelize+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 10, "processingTimingsMS": {"_request": {"roundTrip": 21}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 8, "scanning": 1, "total": 10}, "total": 10}, "query": "sequelize production", "serverTimeMS": 12}}