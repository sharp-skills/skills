{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "dormstern"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "It reads Docker Compose, Helm charts, K8s manifests, and Spring Boot configs, extracts every network dependency, and generates per-service NetworkPolicies.\nTested against real <em>production</em> stacks:<p><em>Sentry</em> self-hosted (70+ services): 411 dependencies, 71 policies, 11ms\nPostHog (25+ services): 23 dependencies, 12 policies, 128ms<p>Key design decisions:<p>Static analysis only. No agents, no cluster access, no observation period. Works offline.\nAI is optional. Rule-based parsers handle the core. --ai adds Ollama (local) or Gemini (cloud) for edge cases.\nInteractive TUI lets you review every dependency before generating YAML.\nPer-service output with both ingress AND egress rules.<p>The thesis: your configs already declare every dependency. Why are we paying for 30-60 day observation periods and runtime agents?\nWritten in Go, MIT licensed. Would love feedback from anyone running NetworkPolicies in <em>production</em>."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Segspec (CLI) K8s NetworkPolicies from App Configs (Go)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/dormstern/segspec"}}, "_tags": ["story", "author_dormstern", "story_47092058", "show_hn"], "author": "dormstern", "children": [47092143], "created_at": "2026-02-20T18:45:00Z", "created_at_i": 1771613100, "num_comments": 1, "objectID": "47092058", "points": 1, "story_id": 47092058, "story_text": "It reads Docker Compose, Helm charts, K8s manifests, and Spring Boot configs, extracts every network dependency, and generates per-service NetworkPolicies.\nTested against real production stacks:<p>Sentry self-hosted (70+ services): 411 dependencies, 71 policies, 11ms\nPostHog (25+ services): 23 dependencies, 12 policies, 128ms<p>Key design decisions:<p>Static analysis only. No agents, no cluster access, no observation period. Works offline.\nAI is optional. Rule-based parsers handle the core. --ai adds Ollama (local) or Gemini (cloud) for edge cases.\nInteractive TUI lets you review every dependency before generating YAML.\nPer-service output with both ingress AND egress rules.<p>The thesis: your configs already declare every dependency. Why are we paying for 30-60 day observation periods and runtime agents?\nWritten in Go, MIT licensed. Would love feedback from anyone running NetworkPolicies in production.", "title": "Show HN: Segspec (CLI) K8s NetworkPolicies from App Configs (Go)", "updated_at": "2026-02-20T19:14:51Z", "url": "https://github.com/dormstern/segspec"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "paulmbw"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "If you are building a mobile app with Expo and React Native, Launchtoday can help you reduce development time by up to 80% and accelerate your app's launch.<p>Launchtoday provides a comprehensive boilerplate with essential integrations, minimizing time to market:<p>- Built with Expo: Develop universal apps with support for Web, iOS, and Android<p>- Supabase Integration: Manage authentication and data storage with a Postgres database (AWS integration is in progress).<p>- Push Notifications: Utilise Expo and Firebase to engage your users effectively.<p>- Payment Solutions: Integrate Stripe and RevenueCat, including support for Apple/Google Pay.<p>- Chat Functionality: Enable chat features with Stream.\nPrivacy-Focused Analytics: Implement privacy-conscious analytics with Metricalp (receive a 20% discount on any plan with Launchtoday).<p>- Monitoring: Monitor app health in <em>production</em> with <em>Sentry</em>.<p>Here is a recent testimmonial on how Launchtoday reduced the time to market:<p>\u201cLaunchtoday significantly reduced our development time, saving us at least 20 hours during our app\u2019s migration to React Native. Within an hour, we had authentication and the basic UI set up efficiently.\u201d<p>Optimise your development process with Launchtoday and bring your app to market faster.<p>If you are a YC startup building mobile apps, contact me directly (via DM) on X for a discount: <a href=\"https://x.com/paulwaweruco\" rel=\"nofollow\">https://x.com/paulwaweruco</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Reduce your app's time to market with Launchtoday"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://launchtoday.dev/"}}, "_tags": ["story", "author_paulmbw", "story_40749108", "show_hn"], "author": "paulmbw", "created_at": "2024-06-21T12:55:04Z", "created_at_i": 1718974504, "num_comments": 0, "objectID": "40749108", "points": 1, "story_id": 40749108, "story_text": "If you are building a mobile app with Expo and React Native, Launchtoday can help you reduce development time by up to 80% and accelerate your app&#x27;s launch.<p>Launchtoday provides a comprehensive boilerplate with essential integrations, minimizing time to market:<p>- Built with Expo: Develop universal apps with support for Web, iOS, and Android<p>- Supabase Integration: Manage authentication and data storage with a Postgres database (AWS integration is in progress).<p>- Push Notifications: Utilise Expo and Firebase to engage your users effectively.<p>- Payment Solutions: Integrate Stripe and RevenueCat, including support for Apple&#x2F;Google Pay.<p>- Chat Functionality: Enable chat features with Stream.\nPrivacy-Focused Analytics: Implement privacy-conscious analytics with Metricalp (receive a 20% discount on any plan with Launchtoday).<p>- Monitoring: Monitor app health in production with Sentry.<p>Here is a recent testimmonial on how Launchtoday reduced the time to market:<p>\u201cLaunchtoday significantly reduced our development time, saving us at least 20 hours during our app\u2019s migration to React Native. Within an hour, we had authentication and the basic UI set up efficiently.\u201d<p>Optimise your development process with Launchtoday and bring your app to market faster.<p>If you are a YC startup building mobile apps, contact me directly (via DM) on X for a discount: <a href=\"https:&#x2F;&#x2F;x.com&#x2F;paulwaweruco\" rel=\"nofollow\">https:&#x2F;&#x2F;x.com&#x2F;paulwaweruco</a>", "title": "Show HN: Reduce your app's time to market with Launchtoday", "updated_at": "2024-09-20T17:16:44Z", "url": "https://launchtoday.dev/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "art049"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "Hi HN! We\u2019re Arthur and Adrien from CodSpeed. We\u2019re building a tool measuring software performance before any <em>production</em> deployment, catching performance regressions before they hit <em>production</em> environments and reporting performance changes directly in Pull Request comments. It\u2019s kind of like Codecov but for performance measurement.<p>Today, the go to solution to measure performance is probably to use an APM(DataDog, <em>Sentry</em>, \u2026), continuously analyzing your <em>production</em> environment. However, since those solutions are operating on real environments they need real users to experience poor performance in order to report issues and unfortunately, performance remains an afterthought appearing only at the end of the development cycle.<p>Another possibility to measure performance is to create benchmarks while developing and to run them on a regular basis to have an idea of the performance trend of your project. However, with this approach, the variance in the results creates a lot of noise and it\u2019s rarely possible to compare your results with the ones from a co-worker or a <em>production</em> environment.<p>To make consistent performance measurement as easy as unit testing and fully integrated in CI workflows, we chose a benchmark based solution. And, to eliminate the usual variance associated with running them, we measure the number of instructions and memory/cache accesses through CPU instrumentation performed with Valgrind. This approach gives repeatable and consistent results that couldn\u2019t be obtained with a time based statistical approach, especially in extremely noisy CI and cloud environments.<p>We have been in closed beta for a few months, already being used by popular open-source projects such as Prisma and Pydantic. Notably, CodSpeed helped Pydantic through their Rust migration, empowering them to make the library 17x faster: <a href=\"https://docs.pydantic.dev/latest/blog/pydantic-v2/#performance\" rel=\"nofollow noreferrer\">https://docs.pydantic.dev/latest/blog/pydantic-v2/#performan...</a><p>Today, we\u2019re super excited to finally make the product available to everyone. We currently support Python, Node.js and Rust and are looking forward to integrate with more languages soon.<p>The product is and will be free forever for open-source projects. Also, we have a per-seat pricing for private repository usage.\nWe have a lot of exciting features planned regarding additional integrations, such as Database and GPU integrations that should come in upcoming months.<p>Don\u2019t hesitate to try out the product and give your honest feedback. We\u2019re looking forward to your comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: CodSpeed \u2013 Continuous Performance Measurement"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://codspeed.io/"}}, "_tags": ["story", "author_art049", "story_36682012", "show_hn"], "author": "art049", "children": [36682013, 36682668, 36683693, 36715126], "created_at": "2023-07-11T15:02:20Z", "created_at_i": 1689087740, "num_comments": 3, "objectID": "36682012", "points": 32, "story_id": 36682012, "story_text": "Hi HN! We\u2019re Arthur and Adrien from CodSpeed. We\u2019re building a tool measuring software performance before any production deployment, catching performance regressions before they hit production environments and reporting performance changes directly in Pull Request comments. It\u2019s kind of like Codecov but for performance measurement.<p>Today, the go to solution to measure performance is probably to use an APM(DataDog, Sentry, \u2026), continuously analyzing your production environment. However, since those solutions are operating on real environments they need real users to experience poor performance in order to report issues and unfortunately, performance remains an afterthought appearing only at the end of the development cycle.<p>Another possibility to measure performance is to create benchmarks while developing and to run them on a regular basis to have an idea of the performance trend of your project. However, with this approach, the variance in the results creates a lot of noise and it\u2019s rarely possible to compare your results with the ones from a co-worker or a production environment.<p>To make consistent performance measurement as easy as unit testing and fully integrated in CI workflows, we chose a benchmark based solution. And, to eliminate the usual variance associated with running them, we measure the number of instructions and memory&#x2F;cache accesses through CPU instrumentation performed with Valgrind. This approach gives repeatable and consistent results that couldn\u2019t be obtained with a time based statistical approach, especially in extremely noisy CI and cloud environments.<p>We have been in closed beta for a few months, already being used by popular open-source projects such as Prisma and Pydantic. Notably, CodSpeed helped Pydantic through their Rust migration, empowering them to make the library 17x faster: <a href=\"https:&#x2F;&#x2F;docs.pydantic.dev&#x2F;latest&#x2F;blog&#x2F;pydantic-v2&#x2F;#performance\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;docs.pydantic.dev&#x2F;latest&#x2F;blog&#x2F;pydantic-v2&#x2F;#performan...</a><p>Today, we\u2019re super excited to finally make the product available to everyone. We currently support Python, Node.js and Rust and are looking forward to integrate with more languages soon.<p>The product is and will be free forever for open-source projects. Also, we have a per-seat pricing for private repository usage.\nWe have a lot of exciting features planned regarding additional integrations, such as Database and GPU integrations that should come in upcoming months.<p>Don\u2019t hesitate to try out the product and give your honest feedback. We\u2019re looking forward to your comments!", "title": "Show HN: CodSpeed \u2013 Continuous Performance Measurement", "updated_at": "2024-09-20T14:39:18Z", "url": "https://codspeed.io/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pranav9"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "Hey HN,<p>I've been building Aether, a background agent that takes <em>production</em> errors from <em>Sentry</em> and attempts to turn them into verified pull requests.<p>When a new error hits your <em>Sentry</em> project:<p>1. <em>Sentry</em> webhook fires with the stack trace, breadcrumbs, and context\n2. Aether spins up an isolated Fly.io VM and clones the repo at the relevant commit\n3. Agent analyzes the stack trace, reproduces the issue, proposes a fix\n4. Starts the dev server, re-runs tests, and can verify the running app with Playwright (headless Chromium is pre-installed in every VM)\n5. A review pass evaluates the diff before a PR is opened\n6. Pushes to a feature branch and opens a GitHub PR, but only if verification succeeds\n7. If CI fails, it retries once with the failure logs. If it fails again, the task is marked failed. No infinite loops.<p>Why full VMs instead of worktrees? Each task runs in its own isolated machine with a real filesystem, real process model, real network stack. It can `npm install`, run a dev server on port 3000, and Playwright can hit `localhost:3000` because it's an actual environment, not a sandbox. Since each task is its own VM, preview URLs are exposed per task via a gateway proxy so you can inspect the running app while the agent works. VMs shut down shortly after the task completes.<p>There's a simple multi-agent setup: a solver proposes the fix, a review agent evaluates the diff, and the fix has to survive re-execution in a clean isolated environment before a PR gets opened. Not claiming formal guarantees here, just requiring the fix to actually execute successfully in a reproducible environment before it touches your repo.<p>Limitations:<p>- Works best on well-tested codebases where &quot;reproduce and verify&quot; is meaningful\n- If reproduction isn't deterministic, results degrade\n- CI retry is capped at one automatic attempt\n- Code review is model-driven, not an architectural enforcement layer\n- BYOK only, you bring your own API key via OpenRouter. No markup on model costs but it's not super cheap to run\n- <em>Sentry</em> integration is built but waiting on approval from <em>Sentry</em>, coming soon\n- CLI is also coming soon<p>Bug fixing is the main focus but it's built on top of a general-purpose background agents system that works today. The agent is still great at general coding tasks. You can give the agent tasks from a full web IDE with a code editor, terminal, file tree, and agent chat panel. CLI is coming soon too (`aether run &quot;add auth to the API&quot;`). Each task gets its own isolated VM with shareable preview URLs so you can hand someone a link to see exactly what the agent built. Similar to Cursor background agents but running in the cloud with full environment isolation instead of local worktrees.<p>Stack: Go API (Chi), Fly.io VMs, React 19 + Vite frontend, Bun workspace service inside each VM, Supabase for auth/db/realtime, Playwright + Chromium preinstalled on each VM.<p>Self-serve right now: GitHub OAuth, connect a repo, and go via the web IDE. <em>Sentry</em> and CLI coming soon.<p>Would value feedback from engineers who deal with <em>production</em> debugging regularly, or frequently use background agents. Where would this break, and what would make you trust it?<p>Landing page: <a href=\"https://www.runaether.dev\" rel=\"nofollow\">https://www.runaether.dev</a>\nTry it: <a href=\"https://app.runaether.dev\" rel=\"nofollow\">https://app.runaether.dev</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Aether \u2013 Background agents that fix bugs in isolated VMs, opens PRs"}}, "_tags": ["story", "author_pranav9", "story_47081472", "show_hn"], "author": "pranav9", "children": [47081704, 47082242, 47082789, 47084861, 47094474], "created_at": "2026-02-19T23:43:54Z", "created_at_i": 1771544634, "num_comments": 6, "objectID": "47081472", "points": 8, "story_id": 47081472, "story_text": "Hey HN,<p>I&#x27;ve been building Aether, a background agent that takes production errors from Sentry and attempts to turn them into verified pull requests.<p>When a new error hits your Sentry project:<p>1. Sentry webhook fires with the stack trace, breadcrumbs, and context\n2. Aether spins up an isolated Fly.io VM and clones the repo at the relevant commit\n3. Agent analyzes the stack trace, reproduces the issue, proposes a fix\n4. Starts the dev server, re-runs tests, and can verify the running app with Playwright (headless Chromium is pre-installed in every VM)\n5. A review pass evaluates the diff before a PR is opened\n6. Pushes to a feature branch and opens a GitHub PR, but only if verification succeeds\n7. If CI fails, it retries once with the failure logs. If it fails again, the task is marked failed. No infinite loops.<p>Why full VMs instead of worktrees? Each task runs in its own isolated machine with a real filesystem, real process model, real network stack. It can `npm install`, run a dev server on port 3000, and Playwright can hit `localhost:3000` because it&#x27;s an actual environment, not a sandbox. Since each task is its own VM, preview URLs are exposed per task via a gateway proxy so you can inspect the running app while the agent works. VMs shut down shortly after the task completes.<p>There&#x27;s a simple multi-agent setup: a solver proposes the fix, a review agent evaluates the diff, and the fix has to survive re-execution in a clean isolated environment before a PR gets opened. Not claiming formal guarantees here, just requiring the fix to actually execute successfully in a reproducible environment before it touches your repo.<p>Limitations:<p>- Works best on well-tested codebases where &quot;reproduce and verify&quot; is meaningful\n- If reproduction isn&#x27;t deterministic, results degrade\n- CI retry is capped at one automatic attempt\n- Code review is model-driven, not an architectural enforcement layer\n- BYOK only, you bring your own API key via OpenRouter. No markup on model costs but it&#x27;s not super cheap to run\n- Sentry integration is built but waiting on approval from Sentry, coming soon\n- CLI is also coming soon<p>Bug fixing is the main focus but it&#x27;s built on top of a general-purpose background agents system that works today. The agent is still great at general coding tasks. You can give the agent tasks from a full web IDE with a code editor, terminal, file tree, and agent chat panel. CLI is coming soon too (`aether run &quot;add auth to the API&quot;`). Each task gets its own isolated VM with shareable preview URLs so you can hand someone a link to see exactly what the agent built. Similar to Cursor background agents but running in the cloud with full environment isolation instead of local worktrees.<p>Stack: Go API (Chi), Fly.io VMs, React 19 + Vite frontend, Bun workspace service inside each VM, Supabase for auth&#x2F;db&#x2F;realtime, Playwright + Chromium preinstalled on each VM.<p>Self-serve right now: GitHub OAuth, connect a repo, and go via the web IDE. Sentry and CLI coming soon.<p>Would value feedback from engineers who deal with production debugging regularly, or frequently use background agents. Where would this break, and what would make you trust it?<p>Landing page: <a href=\"https:&#x2F;&#x2F;www.runaether.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;www.runaether.dev</a>\nTry it: <a href=\"https:&#x2F;&#x2F;app.runaether.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;app.runaether.dev</a>", "title": "Show HN: Aether \u2013 Background agents that fix bugs in isolated VMs, opens PRs", "updated_at": "2026-02-21T21:50:06Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "joeyespo"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "How to fix errors in <em>production</em> with GitHub and <em>Sentry</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "https://github.com/blog/2388-how-to-fix-errors-in-<em>production</em>-with-github-and-<em>sentry</em>"}}, "_tags": ["story", "author_joeyespo", "story_14665414"], "author": "joeyespo", "created_at": "2017-06-29T17:58:08Z", "created_at_i": 1498759088, "num_comments": 0, "objectID": "14665414", "points": 1, "story_id": 14665414, "title": "How to fix errors in production with GitHub and Sentry", "updated_at": "2024-09-20T01:01:01Z", "url": "https://github.com/blog/2388-how-to-fix-errors-in-production-with-github-and-sentry"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rhinorackattack"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "Disclaimer - the goal isn't to build a fully fledged company from this. Rather build a project that developers/startups will find useful as well as generate some income for myself.<p>I\u2019m working on a project to streamline the setup of tech stacks for startups. There are countless free repos on GitHub designed to help startups hit the ground running, but they usually just give you the code, leaving a lot of configuration work to be done. My solution is different: I want to offer a unified platform that integrates with a range of 3rd-party vendors and automatically sets up their cloud environments using API keys or OAuth.<p>The Vision:\nIn my dashboard, users will provide the necessary API keys, and we'll take care of the rest, automatically configuring and deploying the entire tech stack. At the end of the process, they'll have a backend and frontend deployed on AWS, a working RDS, and everything fully terraformed. Think of it as a one-click solution to build and deploy a comprehensive, ready-to-go environment.<p>What\u2019s in the box:<p>The repo is comprised of a monorepo with a FE with a basic auth page for login/logout, backend, shared types, an admin FE dashboard and a whole bunch of necessary and modern technologies to get your *SaaS portal* up and running. (SEO is available but the expectation is that this product is behind a paywall/authwall.<p>The Code:<p>- Monorepo with FE, BE and shared types\n- React + Next.js (frontend)\n- pnpm\n- Nest.js (server)\n- Tailwind\n- Material UI\n- Apollo (GraphQL)\n- Jest (Testing)\n- Typescript + ESLint + Prettier + Husky\n- Turbo\n- TypeORM\n- Segment\n- Database migrations\n- Docker\n- Logging (Pino.js)<p>Configuration:<p>- Terraform\n- GitHub Actions (CI/CD)\n- Local DB env with docker compose<p>Deployment (3rd Parties):<p>- <em>Sentry</em>\n- Auth0\n- Analytics\n- Stripe\n- Managed staging/<em>production</em> on AWS\n- Storage with S3\n- Langchain (ChatGPT)\n- Feature Flags (LaunchDarkly)\n- Mail (mailgun) + Notifications (SNS)\n- Postgres (RDS)\n- Lambdas with SAM<p>Features:<p>- Shared types\n- Langchain\n- Admin dashboard\n- CLI\n- Docs<p>Extras:<p>- Storybook\n- SEO\n- i18n<p>The Value Proposition:<p>- Unified Setup: Quickly deploy a <em>production</em>-ready tech stack with minimal configuration.\n- Reduced Complexity: Even with bots and automations, setting up AWS and other tools can be complex. This tool simplifies the entire process.<p>Why Build This:<p>- Complexity Management: Managing roles and deployments across various cloud platforms can be a headache. This tool pulls together all the essential components and simplifies the deployment process.\n- Engineer-Level Setup: You typically need an engineer to set up AWS, but this solution makes it trivial for non-tech founders to deploy a professional-grade infrastructure across a wide range of necessary services to run your start up.<p>I'm looking for feedback and validation on this idea. What do you think?<p>Some Initial Questions:<p>- I\u2019m not presenting vendor alternatives for the MVP. Maybe later I can present more vendor options, but for now I\u2019m trying to take the vendor decision-making process away from the startups so they can just focus on building the products. Will this present some problems?\n- I\u2019ve chosen AWS because startups often have to spend time learning about AWS and having users, env, compute, DBs, storage etc auto set up will allow them to focus on building their product and not on complex infrastructure. Once they\u2019re in the ecosystem, they can leverage more of it. I know there are more modern and streamlined alternatives to some of these services e.g. Vercel, Cloudflare, Firebase, Railway, Digital Ocean app platform, Render, Aptible; or when it comes to databases; planetDB or SurrealDB etc.<p>Business Model:<p>- Offer the code for free, once off fee for auto deployment and set up"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Idea Validation"}}, "_tags": ["story", "author_rhinorackattack", "story_40314582", "ask_hn"], "author": "rhinorackattack", "children": [40314703, 40314823, 40380254], "created_at": "2024-05-10T01:08:44Z", "created_at_i": 1715303324, "num_comments": 3, "objectID": "40314582", "points": 1, "story_id": 40314582, "story_text": "Disclaimer - the goal isn&#x27;t to build a fully fledged company from this. Rather build a project that developers&#x2F;startups will find useful as well as generate some income for myself.<p>I\u2019m working on a project to streamline the setup of tech stacks for startups. There are countless free repos on GitHub designed to help startups hit the ground running, but they usually just give you the code, leaving a lot of configuration work to be done. My solution is different: I want to offer a unified platform that integrates with a range of 3rd-party vendors and automatically sets up their cloud environments using API keys or OAuth.<p>The Vision:\nIn my dashboard, users will provide the necessary API keys, and we&#x27;ll take care of the rest, automatically configuring and deploying the entire tech stack. At the end of the process, they&#x27;ll have a backend and frontend deployed on AWS, a working RDS, and everything fully terraformed. Think of it as a one-click solution to build and deploy a comprehensive, ready-to-go environment.<p>What\u2019s in the box:<p>The repo is comprised of a monorepo with a FE with a basic auth page for login&#x2F;logout, backend, shared types, an admin FE dashboard and a whole bunch of necessary and modern technologies to get your *SaaS portal* up and running. (SEO is available but the expectation is that this product is behind a paywall&#x2F;authwall.<p>The Code:<p>- Monorepo with FE, BE and shared types\n- React + Next.js (frontend)\n- pnpm\n- Nest.js (server)\n- Tailwind\n- Material UI\n- Apollo (GraphQL)\n- Jest (Testing)\n- Typescript + ESLint + Prettier + Husky\n- Turbo\n- TypeORM\n- Segment\n- Database migrations\n- Docker\n- Logging (Pino.js)<p>Configuration:<p>- Terraform\n- GitHub Actions (CI&#x2F;CD)\n- Local DB env with docker compose<p>Deployment (3rd Parties):<p>- Sentry\n- Auth0\n- Analytics\n- Stripe\n- Managed staging&#x2F;production on AWS\n- Storage with S3\n- Langchain (ChatGPT)\n- Feature Flags (LaunchDarkly)\n- Mail (mailgun) + Notifications (SNS)\n- Postgres (RDS)\n- Lambdas with SAM<p>Features:<p>- Shared types\n- Langchain\n- Admin dashboard\n- CLI\n- Docs<p>Extras:<p>- Storybook\n- SEO\n- i18n<p>The Value Proposition:<p>- Unified Setup: Quickly deploy a production-ready tech stack with minimal configuration.\n- Reduced Complexity: Even with bots and automations, setting up AWS and other tools can be complex. This tool simplifies the entire process.<p>Why Build This:<p>- Complexity Management: Managing roles and deployments across various cloud platforms can be a headache. This tool pulls together all the essential components and simplifies the deployment process.\n- Engineer-Level Setup: You typically need an engineer to set up AWS, but this solution makes it trivial for non-tech founders to deploy a professional-grade infrastructure across a wide range of necessary services to run your start up.<p>I&#x27;m looking for feedback and validation on this idea. What do you think?<p>Some Initial Questions:<p>- I\u2019m not presenting vendor alternatives for the MVP. Maybe later I can present more vendor options, but for now I\u2019m trying to take the vendor decision-making process away from the startups so they can just focus on building the products. Will this present some problems?\n- I\u2019ve chosen AWS because startups often have to spend time learning about AWS and having users, env, compute, DBs, storage etc auto set up will allow them to focus on building their product and not on complex infrastructure. Once they\u2019re in the ecosystem, they can leverage more of it. I know there are more modern and streamlined alternatives to some of these services e.g. Vercel, Cloudflare, Firebase, Railway, Digital Ocean app platform, Render, Aptible; or when it comes to databases; planetDB or SurrealDB etc.<p>Business Model:<p>- Offer the code for free, once off fee for auto deployment and set up", "title": "Ask HN: Idea Validation", "updated_at": "2025-02-26T22:31:53Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Dimittri"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https://sonarly.com\">https://sonarly.com</a>), an AI engineer for <em>production</em>. It connects to your observability tools like <em>Sentry</em>, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here's a demo: <a href=\"https://www.youtube.com/watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https://www.youtube.com/watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from <em>production</em> alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up <em>Sentry</em>, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it's a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in <em>production</em>, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can't catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a <em>Sentry</em>-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend <em>Sentry</em> SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their <em>Sentry</em> config to send data to us in addition to <em>Sentry</em>.<p>We wanted to build an interface where you don't need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don't want to add a new tracker or change their monitoring stack, as these platforms do the job they're supposed to do. So we decided to build above them. Now we connect to tools like <em>Sentry</em>, Datadog, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific <em>Sentry</em> issue and relevant logs fetched via MCP (mostly using grep on Datadog/Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the <em>production</em> system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using <em>Sentry</em> was receiving ~180 alerts/day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don't look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180/day to 50/day (by grouping issues) and giving a severity based on the impact on the user/infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https://sonarly.com\">https://sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like <em>Sentry</em> (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I'll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything's constructive!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your <em>production</em> alerts"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://sonarly.com/"}}, "_tags": ["story", "author_Dimittri", "story_47049776", "launch_hn"], "author": "Dimittri", "children": [47052409, 47054509, 47055028, 47055342, 47061697], "created_at": "2026-02-17T17:03:09Z", "created_at_i": 1771347789, "num_comments": 17, "objectID": "47049776", "points": 30, "story_id": 47049776, "story_text": "Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>), an AI engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here&#x27;s a demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it&#x27;s a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in production, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can&#x27;t catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don&#x27;t need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don&#x27;t want to add a new tracker or change their monitoring stack, as these platforms do the job they&#x27;re supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog&#x2F;Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts&#x2F;day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don&#x27;t look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180&#x2F;day to 50&#x2F;day (by grouping issues) and giving a severity based on the impact on the user&#x2F;infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I&#x27;ll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything&#x27;s constructive!", "title": "Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your production alerts", "updated_at": "2026-02-24T03:55:09Z", "url": "https://sonarly.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "skoshx"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "Hey HN! I\u2019m Rasmus, co-founder of Flytrap. Flytrap is a debugging tool for reproducing and fixing <em>production</em> bugs faster.<p>When launching a product quickly, there is bound to be some bugs. Tools like <em>Sentry</em> might help combat this, but when a bug lands in <em>Sentry</em>, all you see is the Red Carpet of Doom (the stack trace).<p>We were frustrated with the poor developer-experience of current debugging tools, and decided to build Flytrap; a fast debugging tool that quickly allows developers to understand in detail what happened leading up to a bug, and focuses on reproducibility in the developer environment.<p>- Key features<p>1. Detailed context; See the inputs and outputs of all function calls leading up to the bug<p>2. Reproducibility; Reproduce the bug on your local development environment in under a minute<p>3. Security; All capture data encrypted during transit and at rest.<p>- Links<p>Home page: <a href=\"https://www.useflytrap.com\" rel=\"nofollow noreferrer\">https://www.useflytrap.com</a><p>Docs: <a href=\"https://docs.useflytrap.com\" rel=\"nofollow noreferrer\">https://docs.useflytrap.com</a><p>GitHub: <a href=\"https://github.com/useflytrap/flytrap-js\">https://github.com/useflytrap/flytrap-js</a><p>What issues have you had with fixing bugs in <em>production</em>? We would love to hear your ideas, thoughts, experiences and feedback!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Flytrap \u2013 Debugging tool for fixing <em>production</em> bugs"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.useflytrap.com"}}, "_tags": ["story", "author_skoshx", "story_36599472", "show_hn"], "author": "skoshx", "children": [36599678, 36613888], "created_at": "2023-07-05T12:31:21Z", "created_at_i": 1688560281, "num_comments": 2, "objectID": "36599472", "points": 13, "story_id": 36599472, "story_text": "Hey HN! I\u2019m Rasmus, co-founder of Flytrap. Flytrap is a debugging tool for reproducing and fixing production bugs faster.<p>When launching a product quickly, there is bound to be some bugs. Tools like Sentry might help combat this, but when a bug lands in Sentry, all you see is the Red Carpet of Doom (the stack trace).<p>We were frustrated with the poor developer-experience of current debugging tools, and decided to build Flytrap; a fast debugging tool that quickly allows developers to understand in detail what happened leading up to a bug, and focuses on reproducibility in the developer environment.<p>- Key features<p>1. Detailed context; See the inputs and outputs of all function calls leading up to the bug<p>2. Reproducibility; Reproduce the bug on your local development environment in under a minute<p>3. Security; All capture data encrypted during transit and at rest.<p>- Links<p>Home page: <a href=\"https:&#x2F;&#x2F;www.useflytrap.com\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.useflytrap.com</a><p>Docs: <a href=\"https:&#x2F;&#x2F;docs.useflytrap.com\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;docs.useflytrap.com</a><p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;useflytrap&#x2F;flytrap-js\">https:&#x2F;&#x2F;github.com&#x2F;useflytrap&#x2F;flytrap-js</a><p>What issues have you had with fixing bugs in production? We would love to hear your ideas, thoughts, experiences and feedback!", "title": "Show HN: Flytrap \u2013 Debugging tool for fixing production bugs", "updated_at": "2024-09-20T14:31:33Z", "url": "https://www.useflytrap.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "parthi"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "We're a small startup running Node for our backend. Using a combination of LogDNA, FullStory, <em>Sentry</em> and Mixpanel across frontend and backend.<p>Problems I face:<p>1. Can't trace user journey: User reports a bug. I look through logs with a timestamp of the user report, but can't trace the user journey, especially with lots of asynchronous operations going on in parallel<p>2. Insufficient logging: I can identify the error being thrown but can't identify the cause. Would want to see logs of surrounding code which we maybe didn't log<p>To solve this, I've resorted to adding logging around the suspected problem path, pushing to <em>production</em> and asking the user to try again (very annoying for them)<p>Any best practices I'm missing or is this really the best way to be debugging issues in prod?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: How do I debug my back end in <em>production</em>?"}}, "_tags": ["story", "author_parthi", "story_23393730", "ask_hn"], "author": "parthi", "children": [23394455, 23395195], "created_at": "2020-06-02T17:41:24Z", "created_at_i": 1591119684, "num_comments": 5, "objectID": "23393730", "points": 6, "story_id": 23393730, "story_text": "We&#x27;re a small startup running Node for our backend. Using a combination of LogDNA, FullStory, Sentry and Mixpanel across frontend and backend.<p>Problems I face:<p>1. Can&#x27;t trace user journey: User reports a bug. I look through logs with a timestamp of the user report, but can&#x27;t trace the user journey, especially with lots of asynchronous operations going on in parallel<p>2. Insufficient logging: I can identify the error being thrown but can&#x27;t identify the cause. Would want to see logs of surrounding code which we maybe didn&#x27;t log<p>To solve this, I&#x27;ve resorted to adding logging around the suspected problem path, pushing to production and asking the user to try again (very annoying for them)<p>Any best practices I&#x27;m missing or is this really the best way to be debugging issues in prod?", "title": "Ask HN: How do I debug my back end in production?", "updated_at": "2024-09-20T06:20:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "skoshx"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "Logging and tools like <em>Sentry</em> are a thing of the past.<p>A while back on a night out in northern Norway, I had to start debugging a critical <em>production</em> bug that broke the payment flow of my SaaS product. I had limited time to fix the bug or I would have lost about ~1K profit. Super stressful.<p>I had logging and <em>Sentry</em> in place, but neither helped me reproduce or find the root cause of the bug.<p>Ever since, I started thinking; why can\u2019t we just have a tool that you setup once, and that allows us to reproduce every function call and function that the user ran before the bug?<p>This is how the idea for Flytrap was born.<p>Flytrap is the fastest debugging tool for JavaScript projects. Just set it up in 5 minutes. No logging needed. When viewing bugs on Flytrap, it creates a visualisation of your code, and displays the bugs RIGHT IN YOUR CODE. Each function and function call in your code can be inspected for their input and output values, to gain a deep understanding of what went wrong.<p>Then, to reproduce any bug, just copy the ID of your bug, place it in your Flytrap config file, and boom, the values of the end-user will be injected in your local development environment!<p>Links:<p>Home page: <a href=\"https://www.useflytrap.com\" rel=\"nofollow noreferrer\">https://www.useflytrap.com</a>\nDocs: <a href=\"https://docs.useflytrap.com/\" rel=\"nofollow noreferrer\">https://docs.useflytrap.com/</a>\nGitHub: <a href=\"https://github.com/useflytrap/flytrap-js\">https://github.com/useflytrap/flytrap-js</a><p>I would be happy to hear what issues you have had with fixing bugs in <em>production</em>. Ideas, experiences and feedback are much appreciated!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: I built a tool that helps fixing JavaScript <em>production</em> bugs much faster"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.useflytrap.com/"}}, "_tags": ["story", "author_skoshx", "story_37064114", "show_hn"], "author": "skoshx", "children": [37065516], "created_at": "2023-08-09T15:24:19Z", "created_at_i": 1691594659, "num_comments": 1, "objectID": "37064114", "points": 5, "story_id": 37064114, "story_text": "Logging and tools like Sentry are a thing of the past.<p>A while back on a night out in northern Norway, I had to start debugging a critical production bug that broke the payment flow of my SaaS product. I had limited time to fix the bug or I would have lost about ~1K profit. Super stressful.<p>I had logging and Sentry in place, but neither helped me reproduce or find the root cause of the bug.<p>Ever since, I started thinking; why can\u2019t we just have a tool that you setup once, and that allows us to reproduce every function call and function that the user ran before the bug?<p>This is how the idea for Flytrap was born.<p>Flytrap is the fastest debugging tool for JavaScript projects. Just set it up in 5 minutes. No logging needed. When viewing bugs on Flytrap, it creates a visualisation of your code, and displays the bugs RIGHT IN YOUR CODE. Each function and function call in your code can be inspected for their input and output values, to gain a deep understanding of what went wrong.<p>Then, to reproduce any bug, just copy the ID of your bug, place it in your Flytrap config file, and boom, the values of the end-user will be injected in your local development environment!<p>Links:<p>Home page: <a href=\"https:&#x2F;&#x2F;www.useflytrap.com\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.useflytrap.com</a>\nDocs: <a href=\"https:&#x2F;&#x2F;docs.useflytrap.com&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;docs.useflytrap.com&#x2F;</a>\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;useflytrap&#x2F;flytrap-js\">https:&#x2F;&#x2F;github.com&#x2F;useflytrap&#x2F;flytrap-js</a><p>I would be happy to hear what issues you have had with fixing bugs in production. Ideas, experiences and feedback are much appreciated!", "title": "Show HN: I built a tool that helps fixing JavaScript production bugs much faster", "updated_at": "2024-09-20T14:51:37Z", "url": "https://www.useflytrap.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Dimittri"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "Hi all, I've been working on this devtool for 1 month now for myself at first and I'll be curious to see if it's something that could work for you as well.\nSo basically, it detects bugs in your website in <em>production</em> from real user sessions, an llm clusters them by severity and it provides the complete context of the issue that you can copy-paste into your coding agent to fix it in one go.\nWhy did I create it?\nI've been shipping fast with tools like Cursor and Claude Code. The problem? When bugs happen in <em>production</em>, these tools have zero context about what actually went wrong. <em>Sentry</em> is powerful but way too complex - I just want to copy context and paste it into my copilot.\nHow to install it?\nBasically it's a JS tracker to add to your website\nI'm very curious about your ideas about this tool or similar tools available if you know some of them.\nLooking for feedback on the whole concept and UX. Is this something you'd actually use? What's missing?\nHappy to share more technical details for those interested.\nThanks!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Give your coding agent <em>production</em> bug context"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://sonarly.dev/"}}, "_tags": ["story", "author_Dimittri", "story_45909113", "show_hn"], "author": "Dimittri", "created_at": "2025-11-13T00:58:19Z", "created_at_i": 1762995499, "num_comments": 0, "objectID": "45909113", "points": 5, "story_id": 45909113, "story_text": "Hi all, I&#x27;ve been working on this devtool for 1 month now for myself at first and I&#x27;ll be curious to see if it&#x27;s something that could work for you as well.\nSo basically, it detects bugs in your website in production from real user sessions, an llm clusters them by severity and it provides the complete context of the issue that you can copy-paste into your coding agent to fix it in one go.\nWhy did I create it?\nI&#x27;ve been shipping fast with tools like Cursor and Claude Code. The problem? When bugs happen in production, these tools have zero context about what actually went wrong. Sentry is powerful but way too complex - I just want to copy context and paste it into my copilot.\nHow to install it?\nBasically it&#x27;s a JS tracker to add to your website\nI&#x27;m very curious about your ideas about this tool or similar tools available if you know some of them.\nLooking for feedback on the whole concept and UX. Is this something you&#x27;d actually use? What&#x27;s missing?\nHappy to share more technical details for those interested.\nThanks!", "title": "Show HN: Give your coding agent production bug context", "updated_at": "2025-11-13T02:11:44Z", "url": "https://sonarly.dev/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mike210"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "Everyone hates <em>production</em> bugs. Users get frustrated.  Engineers get paged.  Development gets delayed.  But what if many of those bugs could be fixed automatically?<p>Enter Paladin, a tool I built that automatically sends you a pull request to fix bugs shortly after they occur.<p>A little over a month ago, I posted my hacky but effective AI setup for fixing <em>production</em> bugs to Reddit (<a href=\"https://redd.it/1jibmtc\" rel=\"nofollow\">https://redd.it/1jibmtc</a>).  300+ devs messaged me or commented wanting to try it out, so I\u2019ve been spending the past weeks refining it into Paladin, and excited to release it today!<p>How it works:<p>Paladin hooks into your application\u2019s error handling with an SDK, triggering a \u201crun\u201d when an exception is thrown.  During the run, Paladin pulls your code on Github and uses LLMs to fix the error, sending you the fix as a PR over Slack in ~90 seconds.   Here\u2019s a two minute demo: <a href=\"https://youtu.be/0bm8nq99Nrw\" rel=\"nofollow\">https://youtu.be/0bm8nq99Nrw</a>.<p>In early testing, Paladin solves over 55% of real <em>production</em> errors on the first try and makes useful progress on many others.  It\u2019s able to do well by supplying deep context to the LLMs: the stack trace, execution state, repo code, and more.  When it works well, it allows you to fix bugs more quickly, meaning less downtime for users and saved engineering time.<p>Eliminating context switching has been an unexpected win for me, because I work best in long, focused stretches.  When a bug hits affecting real users,  I have to drop everything mid-feature to stash changes, debug, and mentally shift contexts, and then try to return. I\u2019ve found PR reviews and tweaks to be much less disruptive.<p>Getting started (Free, no card required)\n1. Sign up at <a href=\"https://app.paladin.run/signup\" rel=\"nofollow\">https://app.paladin.run/signup</a>\n2. Follow instructions to connect your Github and Slack (or just email)\n3. Choose and install the correct SDK into your app\n4. Configure to send errors to Paladin\n5. Done!<p>Paladin supports React, React Native, Laravel, Flutter, Django, Node, Next, Vanilla Javascript, Express, FastAPI, PHP, Vanilla Python, Nest, Vue, Android, iOS, Rails, Flask, and many more  thanks to <em>Sentry</em>\u2019s MIT licensed client SDKs (your errors do not go to <em>Sentry</em>, they are just used to capture errors).  If you have a client and server, I\u2019d start with your server.<p>Notes on privacy, performance, and future plans below:<p>Paladin will never abuse repo access for any type of training or sharing, and only pulls it for making fixes.  An LLM provider (Google/Anthropic/OpenAI) processes part of your code, so if you can\u2019t use tools like Cursor/Windsurf, you probably can\u2019t use Paladin.<p>On performance, my personal set is admittedly very limited, but I think the performance makes sense to me given current bests on benchmarks like Aider Polyglot and SWE-bench Verified.  I\u2019d expect these numbers to get much better as models progress. I\u2019d also expect Paladin to fall short where current frontier LLMs do: uncommon frameworks, libraries or languages.<p>In the future, I am planning on having two usage options:\n- Free: if you bring your own OpenRouter API key\n- Paid: if Paladin pays for the model costs<p>Really looking forward to hearing feedback and ideas!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Paladin \u2013 An AI trigger to fix your sh*t <em>production</em> bugs"}}, "_tags": ["story", "author_mike210", "story_43906288", "show_hn"], "author": "mike210", "created_at": "2025-05-06T15:32:21Z", "created_at_i": 1746545541, "num_comments": 0, "objectID": "43906288", "points": 5, "story_id": 43906288, "story_text": "Everyone hates production bugs. Users get frustrated.  Engineers get paged.  Development gets delayed.  But what if many of those bugs could be fixed automatically?<p>Enter Paladin, a tool I built that automatically sends you a pull request to fix bugs shortly after they occur.<p>A little over a month ago, I posted my hacky but effective AI setup for fixing production bugs to Reddit (<a href=\"https:&#x2F;&#x2F;redd.it&#x2F;1jibmtc\" rel=\"nofollow\">https:&#x2F;&#x2F;redd.it&#x2F;1jibmtc</a>).  300+ devs messaged me or commented wanting to try it out, so I\u2019ve been spending the past weeks refining it into Paladin, and excited to release it today!<p>How it works:<p>Paladin hooks into your application\u2019s error handling with an SDK, triggering a \u201crun\u201d when an exception is thrown.  During the run, Paladin pulls your code on Github and uses LLMs to fix the error, sending you the fix as a PR over Slack in ~90 seconds.   Here\u2019s a two minute demo: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;0bm8nq99Nrw\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;0bm8nq99Nrw</a>.<p>In early testing, Paladin solves over 55% of real production errors on the first try and makes useful progress on many others.  It\u2019s able to do well by supplying deep context to the LLMs: the stack trace, execution state, repo code, and more.  When it works well, it allows you to fix bugs more quickly, meaning less downtime for users and saved engineering time.<p>Eliminating context switching has been an unexpected win for me, because I work best in long, focused stretches.  When a bug hits affecting real users,  I have to drop everything mid-feature to stash changes, debug, and mentally shift contexts, and then try to return. I\u2019ve found PR reviews and tweaks to be much less disruptive.<p>Getting started (Free, no card required)\n1. Sign up at <a href=\"https:&#x2F;&#x2F;app.paladin.run&#x2F;signup\" rel=\"nofollow\">https:&#x2F;&#x2F;app.paladin.run&#x2F;signup</a>\n2. Follow instructions to connect your Github and Slack (or just email)\n3. Choose and install the correct SDK into your app\n4. Configure to send errors to Paladin\n5. Done!<p>Paladin supports React, React Native, Laravel, Flutter, Django, Node, Next, Vanilla Javascript, Express, FastAPI, PHP, Vanilla Python, Nest, Vue, Android, iOS, Rails, Flask, and many more  thanks to Sentry\u2019s MIT licensed client SDKs (your errors do not go to Sentry, they are just used to capture errors).  If you have a client and server, I\u2019d start with your server.<p>Notes on privacy, performance, and future plans below:<p>Paladin will never abuse repo access for any type of training or sharing, and only pulls it for making fixes.  An LLM provider (Google&#x2F;Anthropic&#x2F;OpenAI) processes part of your code, so if you can\u2019t use tools like Cursor&#x2F;Windsurf, you probably can\u2019t use Paladin.<p>On performance, my personal set is admittedly very limited, but I think the performance makes sense to me given current bests on benchmarks like Aider Polyglot and SWE-bench Verified.  I\u2019d expect these numbers to get much better as models progress. I\u2019d also expect Paladin to fall short where current frontier LLMs do: uncommon frameworks, libraries or languages.<p>In the future, I am planning on having two usage options:\n- Free: if you bring your own OpenRouter API key\n- Paid: if Paladin pays for the model costs<p>Really looking forward to hearing feedback and ideas!", "title": "Show HN: Paladin \u2013 An AI trigger to fix your sh*t production bugs", "updated_at": "2025-05-06T16:19:46Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kwhinnery"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["sentry"], "value": "<em>Sentry</em> AI Autofix"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "https://techcrunch.com/2024/03/20/sentrys-ai-powered-autofix-helps-developers-quickly-debug-and-fix-their-<em>production</em>-code/"}}, "_tags": ["story", "author_kwhinnery", "story_39770793"], "author": "kwhinnery", "created_at": "2024-03-20T19:14:52Z", "created_at_i": 1710962092, "num_comments": 0, "objectID": "39770793", "points": 5, "story_id": 39770793, "title": "Sentry AI Autofix", "updated_at": "2024-09-20T16:40:33Z", "url": "https://techcrunch.com/2024/03/20/sentrys-ai-powered-autofix-helps-developers-quickly-debug-and-fix-their-production-code/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ij23"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Hi HN - \nIshaan and Krrish here from BerriAI. We\u2019ve built a hallucination monitoring tool for LLM Apps in <em>production</em>, that can instantly identify language mistranslations (responding to a user in the incorrect language) and inventing new information errors (answering from information not in the prompt). <i>Live demo here</i>: <a href=\"https://logs.berri.ai/\">https://logs.berri.ai/</a><p>We served over 1m+ chatGPT queries with our initial \u2018chat with your data\u2019 app. However, we had no ability to tell how any of the technical changes we made (e.g. moving from llama index to our own retrieval/qa system) impacted our users in <em>production</em>.<p>Berri is super easy to integrate into your system - we added it to our previous product with just 2 lines of code!<p>It\u2019s super early days and we\u2019re looking for others like us - people in <em>production</em> - pushing changes but unsure if/how they\u2019re actually solving issues / improving their system over time.<p>Thanks for taking the time to read this, we\u2019re really happy to be posting here :)<p>Krrish and Ishaan"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["sentry"], "value": "Show HN: BerriAI \u2013 Monitor Hallucinations in LLMs (<em>Sentry</em> for LLM Apps)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://logs.berri.ai/"}}, "_tags": ["story", "author_ij23", "story_36253098", "show_hn"], "author": "ij23", "created_at": "2023-06-09T04:22:08Z", "created_at_i": 1686284528, "num_comments": 0, "objectID": "36253098", "points": 4, "story_id": 36253098, "story_text": "Hi HN - \nIshaan and Krrish here from BerriAI. We\u2019ve built a hallucination monitoring tool for LLM Apps in production, that can instantly identify language mistranslations (responding to a user in the incorrect language) and inventing new information errors (answering from information not in the prompt). <i>Live demo here</i>: <a href=\"https:&#x2F;&#x2F;logs.berri.ai&#x2F;\">https:&#x2F;&#x2F;logs.berri.ai&#x2F;</a><p>We served over 1m+ chatGPT queries with our initial \u2018chat with your data\u2019 app. However, we had no ability to tell how any of the technical changes we made (e.g. moving from llama index to our own retrieval&#x2F;qa system) impacted our users in production.<p>Berri is super easy to integrate into your system - we added it to our previous product with just 2 lines of code!<p>It\u2019s super early days and we\u2019re looking for others like us - people in production - pushing changes but unsure if&#x2F;how they\u2019re actually solving issues &#x2F; improving their system over time.<p>Thanks for taking the time to read this, we\u2019re really happy to be posting here :)<p>Krrish and Ishaan", "title": "Show HN: BerriAI \u2013 Monitor Hallucinations in LLMs (Sentry for LLM Apps)", "updated_at": "2024-09-20T14:15:21Z", "url": "https://logs.berri.ai/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bumpymark"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["sentry", "production"], "value": "Hey HN, I built Bugmail because I screwed up.\nI was working on a side project and  had some bugs in <em>production</em> I didn't know about. when  I found out, I'd already lost users(at least the few i had), and the funny part is , no one gave feedback if stuff was working or not.\nSo I looked for something to track errors , i tried  <em>Sentry</em> and it was  way too complex for what I needed. I didn't want to configure alert rules or navigate dashboard hell. i just wanted an email when something breaks with the stack trace, breadcrumbs, and who it happened to.\nCouldn't find anything simple enough, so I built Bugmail.\nWhat it does:<p>Errors show up in a Gmail-style inbox\nYou get the stack trace, user context, and what they were doing\nThat's it. No config files, no complex dashboards<p>It's basically emails me when shit breaks but actually usable. Built it for my own projects, figured other indie devs might want the same thing.<p>Try it here : <a href=\"https://bugmail.site\" rel=\"nofollow\">https://bugmail.site</a><p>would love your honest feedback , is this something you'd use ?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Bugmail \u2013 the easiest way to catch and fix <em>production</em> bugs"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.bugmail.site/"}}, "_tags": ["story", "author_bumpymark", "story_46189573", "show_hn"], "author": "bumpymark", "created_at": "2025-12-08T07:55:13Z", "created_at_i": 1765180513, "num_comments": 0, "objectID": "46189573", "points": 2, "story_id": 46189573, "story_text": "Hey HN, I built Bugmail because I screwed up.\nI was working on a side project and  had some bugs in production I didn&#x27;t know about. when  I found out, I&#x27;d already lost users(at least the few i had), and the funny part is , no one gave feedback if stuff was working or not.\nSo I looked for something to track errors , i tried  Sentry and it was  way too complex for what I needed. I didn&#x27;t want to configure alert rules or navigate dashboard hell. i just wanted an email when something breaks with the stack trace, breadcrumbs, and who it happened to.\nCouldn&#x27;t find anything simple enough, so I built Bugmail.\nWhat it does:<p>Errors show up in a Gmail-style inbox\nYou get the stack trace, user context, and what they were doing\nThat&#x27;s it. No config files, no complex dashboards<p>It&#x27;s basically emails me when shit breaks but actually usable. Built it for my own projects, figured other indie devs might want the same thing.<p>Try it here : <a href=\"https:&#x2F;&#x2F;bugmail.site\" rel=\"nofollow\">https:&#x2F;&#x2F;bugmail.site</a><p>would love your honest feedback , is this something you&#x27;d use ?", "title": "Show HN: Bugmail \u2013 the easiest way to catch and fix production bugs", "updated_at": "2025-12-08T08:08:48Z", "url": "https://www.bugmail.site/"}], "hitsPerPage": 15, "nbHits": 69, "nbPages": 5, "page": 0, "params": "query=sentry+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 20, "processingTimingsMS": {"_request": {"roundTrip": 21}, "afterFetch": {"format": {"highlighting": 1, "total": 2}}, "fetch": {"query": 12, "scanning": 6, "total": 19}, "total": 20}, "query": "sentry production", "serverTimeMS": 23}}