{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "zachllama"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "I worked in building LLM applications for last 5 years (before the OpenAI hype began). Here is my list of go to frameworks<p>Reliable<p>1. Deepspeed - consolidates lots of good training and inference techniques. Really good set of researchers and community backing this project<p>2. HuggingFace - It has convenience of trying out multiple models and datasets relatively quickly. The API is hard to read through, but the community is really good and helpful.<p>3. FasterTransformer + Triton Server - best for inference. Steep learning curve, but worth it. I often use Deepspeed inference with Triton server - takes some work to integrate but worth the effort.<p>Good for reference only:<p>1. Langchain, <em>LLamaIndex</em>: Not currently <em>production</em> quality and feel more hacky and prototyping quality. Good to reference to understand interfaces, new techniques. But take what you need and write your own code<p>2. LMFlow: really good project. I am tracking how it evolves. I use it as a reference to build out finetuning, rlhf style workflows.<p>Avoid completely:<p>1. pytorch-lightning / lightning: outdated and too many bugs. Not for prototyping or <em>production</em>. you will waste more time trying to understand why it doesn't work instead of doing your work. Almost no community to help around this anymore.<p>2. Tensorflow: outdated. Don't sink time into this as its quickly being replaced everywhere. Pick up new concepts from Jax and see how to apply them in pytorch.<p>What are some other frameworks that you use?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "LLM Training/Application Frameworks to use and avoid"}}, "_tags": ["story", "author_zachllama", "story_36749450", "ask_hn"], "author": "zachllama", "children": [36750816, 36819391], "created_at": "2023-07-16T17:27:06Z", "created_at_i": 1689528426, "num_comments": 2, "objectID": "36749450", "points": 8, "story_id": 36749450, "story_text": "I worked in building LLM applications for last 5 years (before the OpenAI hype began). Here is my list of go to frameworks<p>Reliable<p>1. Deepspeed - consolidates lots of good training and inference techniques. Really good set of researchers and community backing this project<p>2. HuggingFace - It has convenience of trying out multiple models and datasets relatively quickly. The API is hard to read through, but the community is really good and helpful.<p>3. FasterTransformer + Triton Server - best for inference. Steep learning curve, but worth it. I often use Deepspeed inference with Triton server - takes some work to integrate but worth the effort.<p>Good for reference only:<p>1. Langchain, LLamaIndex: Not currently production quality and feel more hacky and prototyping quality. Good to reference to understand interfaces, new techniques. But take what you need and write your own code<p>2. LMFlow: really good project. I am tracking how it evolves. I use it as a reference to build out finetuning, rlhf style workflows.<p>Avoid completely:<p>1. pytorch-lightning &#x2F; lightning: outdated and too many bugs. Not for prototyping or production. you will waste more time trying to understand why it doesn&#x27;t work instead of doing your work. Almost no community to help around this anymore.<p>2. Tensorflow: outdated. Don&#x27;t sink time into this as its quickly being replaced everywhere. Pick up new concepts from Jax and see how to apply them in pytorch.<p>What are some other frameworks that you use?", "title": "LLM Training/Application Frameworks to use and avoid", "updated_at": "2024-09-20T14:38:20Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "vikp"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "I built Endless Academy because I was frustrated by not having the right vocabulary to search Google/ask ChatGPT when learning new topics.<p>Endless (<a href=\"https://www.endless.academy\" rel=\"nofollow noreferrer\">https://www.endless.academy</a>) will build a course for you, on any topic.  You can customize what's taught and the teaching style. It's an efficient way to learn something enough to start using it.<p>There are a LOT of projects like this.  Endless is different in important ways:<p><pre><code>  - It's block-based, like Notion.  You can use this to customize the teaching style, by adding video, exercise, or other blocks.\n  - Courses are dynamic.  You can add new blocks to an existing course, and even edit the course.\n  - AI chat blocks let you get help.  I think chat should be integrated into the flow of the course, not separated into a different window.  This lets you have multiple chats going, and preserve the context in each one.\n  - Steer how the AI develops the course by editing the outline.\n</code></pre>\nEndless works by leveraging multiple AI models:<p><pre><code>  - Endless looks up information from high-quality educational resources (you can see the sources)\n  - It uses that information to prompt an LLM to write the learning blocks\n  - If necessary, it uses other models and services to post-process each block (for example, to find a video)\n</code></pre>\nThe backend is FastAPI, and the frontend is SvelteKit.  Some learnings:<p><pre><code>  - I wanted the interface to feel dynamic, with information from the LLM streaming in realtime.  This meant I had to rip everything apart halfway through and make the backend mostly async (a paradigm I hadn't used before).\n  - SvelteKit is fantastic - everything being plain Javascript/HTML makes creating components fast.  I only ran into a couple of issues where the magic made it hard to understand errors.  That said, there are so many off the shelf React components that overall productivity may not have been that different.\n  - I used TipTap for the block editor, and I mostly like it.  My main issue is handling state across multiple TipTap instances interspersed with Svelte components.  The &quot;correct&quot; way seems to be using nodeviews and a single TipTap instance, but that trades off data model flexibility (since TipTap imposes its own schema).\n  - I did not use Langchain/<em>LlamaIndex</em>/etc.  In <em>production</em>, the need for reliability and observability meant that I just wrote my own light wrappers instead.</code></pre>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Endless \u2013 Learn anything with personalized AI and blocks"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.endless.academy"}}, "_tags": ["story", "author_vikp", "story_36668966", "show_hn"], "author": "vikp", "created_at": "2023-07-10T16:44:19Z", "created_at_i": 1689007459, "num_comments": 0, "objectID": "36668966", "points": 1, "story_id": 36668966, "story_text": "I built Endless Academy because I was frustrated by not having the right vocabulary to search Google&#x2F;ask ChatGPT when learning new topics.<p>Endless (<a href=\"https:&#x2F;&#x2F;www.endless.academy\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.endless.academy</a>) will build a course for you, on any topic.  You can customize what&#x27;s taught and the teaching style. It&#x27;s an efficient way to learn something enough to start using it.<p>There are a LOT of projects like this.  Endless is different in important ways:<p><pre><code>  - It&#x27;s block-based, like Notion.  You can use this to customize the teaching style, by adding video, exercise, or other blocks.\n  - Courses are dynamic.  You can add new blocks to an existing course, and even edit the course.\n  - AI chat blocks let you get help.  I think chat should be integrated into the flow of the course, not separated into a different window.  This lets you have multiple chats going, and preserve the context in each one.\n  - Steer how the AI develops the course by editing the outline.\n</code></pre>\nEndless works by leveraging multiple AI models:<p><pre><code>  - Endless looks up information from high-quality educational resources (you can see the sources)\n  - It uses that information to prompt an LLM to write the learning blocks\n  - If necessary, it uses other models and services to post-process each block (for example, to find a video)\n</code></pre>\nThe backend is FastAPI, and the frontend is SvelteKit.  Some learnings:<p><pre><code>  - I wanted the interface to feel dynamic, with information from the LLM streaming in realtime.  This meant I had to rip everything apart halfway through and make the backend mostly async (a paradigm I hadn&#x27;t used before).\n  - SvelteKit is fantastic - everything being plain Javascript&#x2F;HTML makes creating components fast.  I only ran into a couple of issues where the magic made it hard to understand errors.  That said, there are so many off the shelf React components that overall productivity may not have been that different.\n  - I used TipTap for the block editor, and I mostly like it.  My main issue is handling state across multiple TipTap instances interspersed with Svelte components.  The &quot;correct&quot; way seems to be using nodeviews and a single TipTap instance, but that trades off data model flexibility (since TipTap imposes its own schema).\n  - I did not use Langchain&#x2F;LlamaIndex&#x2F;etc.  In production, the need for reliability and observability meant that I just wrote my own light wrappers instead.</code></pre>", "title": "Show HN: Endless \u2013 Learn anything with personalized AI and blocks", "updated_at": "2024-09-20T14:37:42Z", "url": "https://www.endless.academy"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "behnamoh"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["llamaindex"], "value": "I've come across many LLM frameworks: Langchain, <em>LlamaIndex</em>, LMQL, guidance, Marvin, Instructor, etc. There's a lot of overlap between them and I don't know if any of them actually adds a value to LLM workflows in a way that's maintainable and robust. So far, I've been able to just build my own little libraries to use in some LLM applications (no RAG), but as I consider the more recent advancements in the field (guaranteed function calling, better RAG, agents and tool use, etc.), I wonder if using one of these frameworks would be a better approach compared to building everything on my own.<p>I appreciate your thoughts and comments on this!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "ChatGPT is 1yo now. Which LLM framework(s) do you use in <em>production</em> and why?"}}, "_tags": ["story", "author_behnamoh", "story_38518890", "ask_hn"], "author": "behnamoh", "created_at": "2023-12-04T16:00:35Z", "created_at_i": 1701705635, "num_comments": 0, "objectID": "38518890", "points": 8, "story_id": 38518890, "story_text": "I&#x27;ve come across many LLM frameworks: Langchain, LlamaIndex, LMQL, guidance, Marvin, Instructor, etc. There&#x27;s a lot of overlap between them and I don&#x27;t know if any of them actually adds a value to LLM workflows in a way that&#x27;s maintainable and robust. So far, I&#x27;ve been able to just build my own little libraries to use in some LLM applications (no RAG), but as I consider the more recent advancements in the field (guaranteed function calling, better RAG, agents and tool use, etc.), I wonder if using one of these frameworks would be a better approach compared to building everything on my own.<p>I appreciate your thoughts and comments on this!", "title": "ChatGPT is 1yo now. Which LLM framework(s) do you use in production and why?", "updated_at": "2024-09-20T15:47:20Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "secsamai"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "Hi HN,<p>A few weeks ago, we stealth launched SEC Insights on Product Hunt and were thrilled to reach the top 5!<p>What is SEC Insights? It harnesses the Retrieval Augmented Generation (RAG) capabilities of <em>LlamaIndex</em> to provide answers about SEC 10-K &amp; 10-Q documents. The enthusiastic reception we received on Product Hunt prompted us to open-source our project. Now, developers can use our project either as a reference or they can fork it entirely for their LLM-powered applications.<p>We at <em>LlamaIndex</em> built this to help developers move their LLM app ideas from prototype to <em>production</em>. We're very open to feedback, Github issues, and PRs!<p>Also, keep an eye on our YouTube page for a walkthrough and setup tutorial we have in the works!<p><a href=\"https://www.youtube.com/@LlamaIndex\">https://www.youtube.com/@<em>LlamaIndex</em></a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["llamaindex"], "value": "Show HN: secinsights.ai \u2013 An open-source full-stack app using <em>LlamaIndex</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/run-llama/sec-insights"}}, "_tags": ["story", "author_secsamai", "story_37407918", "show_hn"], "author": "secsamai", "created_at": "2023-09-06T17:07:43Z", "created_at_i": 1694020063, "num_comments": 0, "objectID": "37407918", "points": 7, "story_id": 37407918, "story_text": "Hi HN,<p>A few weeks ago, we stealth launched SEC Insights on Product Hunt and were thrilled to reach the top 5!<p>What is SEC Insights? It harnesses the Retrieval Augmented Generation (RAG) capabilities of LlamaIndex to provide answers about SEC 10-K &amp; 10-Q documents. The enthusiastic reception we received on Product Hunt prompted us to open-source our project. Now, developers can use our project either as a reference or they can fork it entirely for their LLM-powered applications.<p>We at LlamaIndex built this to help developers move their LLM app ideas from prototype to production. We&#x27;re very open to feedback, Github issues, and PRs!<p>Also, keep an eye on our YouTube page for a walkthrough and setup tutorial we have in the works!<p><a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;@LlamaIndex\">https:&#x2F;&#x2F;www.youtube.com&#x2F;@LlamaIndex</a>", "title": "Show HN: secinsights.ai \u2013 An open-source full-stack app using LlamaIndex", "updated_at": "2024-09-20T14:57:39Z", "url": "https://github.com/run-llama/sec-insights"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kmassimilian"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["llamaindex"], "value": "Been working on a robust Q&amp;A app for enterprise. I used <em>llamaindex</em> (+langchain) as a pipeline. Started using Chroma for my vector db, which worked pretty well, but I realized that my app runs faster when I store the indices in an S3 bucket rather than use Chroma to store my embeddings and generate the index from these embeddings at query time. Are there tradeoffs I'm making in using a pre-built index in S3 rather than a vector db to stash embeddings? Has anyone come across this kind of consideration? I've looked at Weaviate (offers hybrid search) but haven't decided to retool code based around it. Basically, I'm just looking for whichever implementation will result in the fastest response times (knowledge base size is 'large' ~40GB).<p>RE Weaviate, this looks interesting: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb<p>Further and related, has anyone tried to embed a larger amount of data before? I estimated total time using CPU ~29 hours. With GPU I've seen demos reducing this to minutes. https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-<em>llamaindex</em>-ray<p>TY"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Design considerations for RAG application in <em>production</em> mode"}}, "_tags": ["story", "author_kmassimilian", "story_37123940", "ask_hn"], "author": "kmassimilian", "created_at": "2023-08-14T17:34:27Z", "created_at_i": 1692034467, "num_comments": 0, "objectID": "37123940", "points": 3, "story_id": 37123940, "story_text": "Been working on a robust Q&amp;A app for enterprise. I used llamaindex (+langchain) as a pipeline. Started using Chroma for my vector db, which worked pretty well, but I realized that my app runs faster when I store the indices in an S3 bucket rather than use Chroma to store my embeddings and generate the index from these embeddings at query time. Are there tradeoffs I&#x27;m making in using a pre-built index in S3 rather than a vector db to stash embeddings? Has anyone come across this kind of consideration? I&#x27;ve looked at Weaviate (offers hybrid search) but haven&#x27;t decided to retool code based around it. Basically, I&#x27;m just looking for whichever implementation will result in the fastest response times (knowledge base size is &#x27;large&#x27; ~40GB).<p>RE Weaviate, this looks interesting: https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-cookbook&#x2F;blob&#x2F;main&#x2F;examples&#x2F;vector_databases&#x2F;weaviate&#x2F;hybrid-search-with-weaviate-and-openai.ipynb<p>Further and related, has anyone tried to embed a larger amount of data before? I estimated total time using CPU ~29 hours. With GPU I&#x27;ve seen demos reducing this to minutes. https:&#x2F;&#x2F;www.anyscale.com&#x2F;blog&#x2F;build-and-scale-a-powerful-query-engine-with-llamaindex-ray<p>TY", "title": "Design considerations for RAG application in production mode", "updated_at": "2024-09-20T14:47:35Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "manili"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "I'm curious to know, based on your experience in <em>production</em>, how much does Chain-of-Though Reasoning typically cost in terms of tokens for frameworks like <em>LlamaIndex</em>, LangChain, CrewAI, etc.?<p>I understand it depends on many different factors including the complexity of the product and the architecture of the agents involved, but I'd love to hear about your experiences in creating real-world-class application."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["llamaindex"], "value": "How much does CoT Reasoning typically cost for frameworks like <em>LlamaIndex</em>?"}}, "_tags": ["story", "author_manili", "story_41538848", "ask_hn"], "author": "manili", "children": [41539834], "created_at": "2024-09-14T10:32:24Z", "created_at_i": 1726309944, "num_comments": 4, "objectID": "41538848", "points": 2, "story_id": 41538848, "story_text": "I&#x27;m curious to know, based on your experience in production, how much does Chain-of-Though Reasoning typically cost in terms of tokens for frameworks like LlamaIndex, LangChain, CrewAI, etc.?<p>I understand it depends on many different factors including the complexity of the product and the architecture of the agents involved, but I&#x27;d love to hear about your experiences in creating real-world-class application.", "title": "How much does CoT Reasoning typically cost for frameworks like LlamaIndex?", "updated_at": "2024-09-20T17:48:37Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justintorre75"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "Hi HN - Justin, Scott, and Barak here. We're excited to introduce Helicone (<a href=\"https://www.helicone.ai\">https://www.helicone.ai</a>) an open-source logging solution for OpenAi applications. Helicone's one-line integration logs the prompts, completions, latencies, and costs of your OpenAI requests. It currently works with GPT, and can be integrated with one line of code. There\u2019s a demo at <a href=\"https://www.helicone.ai/video\">https://www.helicone.ai/video</a>.<p>Helicone's core technology is a proxy that routes all your OpenAI requests through our edge-deployed Cloudflare Workers. These workers are incredibly reliable and cause no discernible latency impact in <em>production</em> environments. As a proxy, we offer more than just observability: we provide caching and prompt formatting, and we'll soon add user rate limiting and model provider back off to make sure your app is still up when OpenAI is down.<p>Our web application then provides insights into key metrics, such as which users are disproportionately driving costs and what is the token usage broken down by prompts. You can filter this data based on custom logic and export it to other destinations.<p>Getting started with Helicone is quick and easy, regardless of the OpenAI SDK you use. Our proxy-based solution does not require a third party package\u2014simply change your request's base URL from <a href=\"https://api.openai.com/v1\" rel=\"nofollow\">https://api.openai.com/v1</a> to <a href=\"https://oai.hconeai.com/v1\" rel=\"nofollow\">https://oai.hconeai.com/v1</a>. Helicone can be integrated with LangChain, <em>LLama Index</em>, and all other OpenAI native libraries. (<a href=\"https://docs.helicone.ai/quickstart/integrate-in-one-line-of-code\">https://docs.helicone.ai/quickstart/integrate-in-one-line-of...</a>)<p>We have exciting new features coming up, one of which is an API to log user feedback. For instance, if you're developing a tool like GitHub Copilot, you can log when a user accepted or rejected a suggestion. Helicone will then aggregate your result quality into metrics and make finetuning suggestions for when you can save costs or improve performance.<p>Before launching Helicone, we developed several projects with GPT-3, including airapbattle.com, tabletalk.ai, and dreamsubmarine.com. For each project, we used a beta version of Helicone which gave us instant visibility into user engagement and result quality issues. As we talked to more builders and companies, we realized they were spending too much time building in-house solutions like this and that existing analytics products were not tailored to inference endpoints like GPT-3.<p>Helicone is developed under the Common Clause V1.0 w/ Apache 2.0 license so that you can use Helicone within your own infrastructure. If you do not want to self-host, we provide a hosted solution with 1k requests free per month to try our product. If you exceed that we offer a paid subscription as well, and you can view our pricing at <a href=\"https://www.helicone.ai/pricing\">https://www.helicone.ai/pricing</a>.<p>We're thrilled to introduce Helicone to the HackerNews community and would love to hear your thoughts, ideas, and experiences related to LLM logging and analytics. We're eager to engage in meaningful discussions, so please don't hesitate to share your insights and feedback with us!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Helicone.ai (YC W23) \u2013 Open-source logging for OpenAI"}}, "_tags": ["story", "author_justintorre75", "story_35279155", "launch_hn"], "author": "justintorre75", "children": [35279926, 35279945, 35279965, 35279980, 35280417, 35280653, 35280758, 35281022, 35281129, 35281256, 35281382, 35281593, 35282424, 35282515, 35282945, 35283225, 35283256, 35283690, 35283860, 35284214, 35284226, 35284442, 35284726, 35284729, 35285199, 35285816, 35286211, 35286461], "created_at": "2023-03-23T18:25:45Z", "created_at_i": 1679595945, "num_comments": 72, "objectID": "35279155", "points": 166, "story_id": 35279155, "story_text": "Hi HN - Justin, Scott, and Barak here. We&#x27;re excited to introduce Helicone (<a href=\"https:&#x2F;&#x2F;www.helicone.ai\">https:&#x2F;&#x2F;www.helicone.ai</a>) an open-source logging solution for OpenAi applications. Helicone&#x27;s one-line integration logs the prompts, completions, latencies, and costs of your OpenAI requests. It currently works with GPT, and can be integrated with one line of code. There\u2019s a demo at <a href=\"https:&#x2F;&#x2F;www.helicone.ai&#x2F;video\">https:&#x2F;&#x2F;www.helicone.ai&#x2F;video</a>.<p>Helicone&#x27;s core technology is a proxy that routes all your OpenAI requests through our edge-deployed Cloudflare Workers. These workers are incredibly reliable and cause no discernible latency impact in production environments. As a proxy, we offer more than just observability: we provide caching and prompt formatting, and we&#x27;ll soon add user rate limiting and model provider back off to make sure your app is still up when OpenAI is down.<p>Our web application then provides insights into key metrics, such as which users are disproportionately driving costs and what is the token usage broken down by prompts. You can filter this data based on custom logic and export it to other destinations.<p>Getting started with Helicone is quick and easy, regardless of the OpenAI SDK you use. Our proxy-based solution does not require a third party package\u2014simply change your request&#x27;s base URL from <a href=\"https:&#x2F;&#x2F;api.openai.com&#x2F;v1\" rel=\"nofollow\">https:&#x2F;&#x2F;api.openai.com&#x2F;v1</a> to <a href=\"https:&#x2F;&#x2F;oai.hconeai.com&#x2F;v1\" rel=\"nofollow\">https:&#x2F;&#x2F;oai.hconeai.com&#x2F;v1</a>. Helicone can be integrated with LangChain, LLama Index, and all other OpenAI native libraries. (<a href=\"https:&#x2F;&#x2F;docs.helicone.ai&#x2F;quickstart&#x2F;integrate-in-one-line-of-code\">https:&#x2F;&#x2F;docs.helicone.ai&#x2F;quickstart&#x2F;integrate-in-one-line-of...</a>)<p>We have exciting new features coming up, one of which is an API to log user feedback. For instance, if you&#x27;re developing a tool like GitHub Copilot, you can log when a user accepted or rejected a suggestion. Helicone will then aggregate your result quality into metrics and make finetuning suggestions for when you can save costs or improve performance.<p>Before launching Helicone, we developed several projects with GPT-3, including airapbattle.com, tabletalk.ai, and dreamsubmarine.com. For each project, we used a beta version of Helicone which gave us instant visibility into user engagement and result quality issues. As we talked to more builders and companies, we realized they were spending too much time building in-house solutions like this and that existing analytics products were not tailored to inference endpoints like GPT-3.<p>Helicone is developed under the Common Clause V1.0 w&#x2F; Apache 2.0 license so that you can use Helicone within your own infrastructure. If you do not want to self-host, we provide a hosted solution with 1k requests free per month to try our product. If you exceed that we offer a paid subscription as well, and you can view our pricing at <a href=\"https:&#x2F;&#x2F;www.helicone.ai&#x2F;pricing\">https:&#x2F;&#x2F;www.helicone.ai&#x2F;pricing</a>.<p>We&#x27;re thrilled to introduce Helicone to the HackerNews community and would love to hear your thoughts, ideas, and experiences related to LLM logging and analytics. We&#x27;re eager to engage in meaningful discussions, so please don&#x27;t hesitate to share your insights and feedback with us!", "title": "Launch HN: Helicone.ai (YC W23) \u2013 Open-source logging for OpenAI", "updated_at": "2024-09-20T13:41:41Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "calebkaiser"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "Hey HN! I'm Caleb, one of the contributors to Opik, a new open source framework for LLM evaluations.<p>Over the last few months, my colleagues and I have been working on a project to solve what we see as the most painful parts of writing evals for an LLM application. For this initial release, we've focused on a few core features that we think are the most essential:<p>- Simplifying the implementation of more complex LLM-based evaluation metrics, like Hallucination and Moderation.<p>- Enabling step-by-step tracking, such that you can test and debug each individual component of your LLM application, even in more complex multi-agent architectures.<p>- Exposing an API for &quot;model unit tests&quot; (built on Pytest), to allow you to run evals as part of your CI/CD pipelines<p>- Providing an easy UI for scoring, annotating, and versioning your logged LLM data, for further evaluation or training.<p>It's often hard to feel like you can trust an LLM application in <em>production</em>, not just because of the stochastic nature of the model, but because of the opaqueness of the application itself. Our belief is that with better tooling for evaluations, we can meaningfully improve this situation, and unlock a new wave of LLM applications.<p>You can run Opik locally, or with a free API key via our cloud platform. You can use it with any model server or hosted model, but we currently have a built-in integration with the OpenAI Python library, which means it automatically works not just with OpenAI models, but with any model served via a compatible model server (ollama, vLLM, etc). Opik also currently has out-of-the-box integrations with LangChain, <em>LlamaIndex</em>, Ragas, and a few other popular tools.<p>This is our initial release of Opik, so if you have any feedback or questions, I'd love to hear them!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Opik, an open source LLM evaluation framework"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/comet-ml/opik"}}, "_tags": ["story", "author_calebkaiser", "story_41567192", "show_hn"], "author": "calebkaiser", "children": [41567374, 41568516, 41571313, 41575655, 41579948, 41597605], "created_at": "2024-09-17T13:01:44Z", "created_at_i": 1726578104, "num_comments": 15, "objectID": "41567192", "points": 86, "story_id": 41567192, "story_text": "Hey HN! I&#x27;m Caleb, one of the contributors to Opik, a new open source framework for LLM evaluations.<p>Over the last few months, my colleagues and I have been working on a project to solve what we see as the most painful parts of writing evals for an LLM application. For this initial release, we&#x27;ve focused on a few core features that we think are the most essential:<p>- Simplifying the implementation of more complex LLM-based evaluation metrics, like Hallucination and Moderation.<p>- Enabling step-by-step tracking, such that you can test and debug each individual component of your LLM application, even in more complex multi-agent architectures.<p>- Exposing an API for &quot;model unit tests&quot; (built on Pytest), to allow you to run evals as part of your CI&#x2F;CD pipelines<p>- Providing an easy UI for scoring, annotating, and versioning your logged LLM data, for further evaluation or training.<p>It&#x27;s often hard to feel like you can trust an LLM application in production, not just because of the stochastic nature of the model, but because of the opaqueness of the application itself. Our belief is that with better tooling for evaluations, we can meaningfully improve this situation, and unlock a new wave of LLM applications.<p>You can run Opik locally, or with a free API key via our cloud platform. You can use it with any model server or hosted model, but we currently have a built-in integration with the OpenAI Python library, which means it automatically works not just with OpenAI models, but with any model served via a compatible model server (ollama, vLLM, etc). Opik also currently has out-of-the-box integrations with LangChain, LlamaIndex, Ragas, and a few other popular tools.<p>This is our initial release of Opik, so if you have any feedback or questions, I&#x27;d love to hear them!", "title": "Show HN: Opik, an open source LLM evaluation framework", "updated_at": "2025-07-24T12:54:05Z", "url": "https://github.com/comet-ml/opik"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jgilhuly"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "LangGraph and <em>LlamaIndex</em> Workflows are generating a lot of buzz right now, and we wanted to see how they measure up in practice versus just writing the code. To do that, we took a straightforward agent architecture\u2014one we've built and deployed in code without a framework\u2014and implemented it using LangGraph and Workflows. Our main goal was to explore how these frameworks translate a simple agent design into their abstractions and assess the impact on the development and debugging process.<p>We want to share our findings with the community, providing practical examples and honest observations about these frameworks where they introduce friction and where they shine. There\u2019s a lot of hype out there, and we hope to offer some clarity with real code examples and unbiased perspectives.<p>For context, we\u2019ve been running our own Co-pilot agent/assistant in <em>production</em> for about eight months. We\u2019ve also helped clients troubleshoot their assistants at scale, so we\u2019ve seen a wide range of use cases and challenges.<p>The architecture we tested is a single-tier LLM router\u2014a pattern we often see in various client implementations. It involves a single LLM router that uses function calling to route tasks or skills, which might include another LLM call before returning control to the router. It\u2019s a simple but versatile pattern.<p>Here\u2019s a Towards Data Science write up we did on the project: https://towardsdatascience.com/choosing-between-llm-agent-frameworks-69019493b259<p>Full code: https://github.com/Arize-ai/phoenix/tree/main/examples/agent_framework_comparison<p>Hot take #1: For experienced developers, framework abstractions can add unnecessary complexity.\nHot take #2: Built-in parallelism, while promising, can complicate debugging a lot.\nHot Take #3: In environments with less experienced development teams that have no scaffolding, these frameworks could offer some useful structure. At least in the POC phase.<p>We\u2019re repeating this process now with CrewAI and Autogen - learnings to follow soon.<p>And if you want to deep dive into the logs of any of these, we\u2019ve published the traces captured with Arize Phoenix here.\nPure code: https://phoenix-demo.arize.com/projects/UHJvamVjdDo2\nLangGraph: https://phoenix-demo.arize.com/projects/UHJvamVjdDoy\n<em>LlamaIndex</em> Workflows: https://phoenix-demo.arize.com/projects/UHJvamVjdDo1<p>We\u2019re curious to hear what others think. What\u2019s been your experience with these frameworks, and how do they compare to rolling your own agent solutions?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Should I Use a Framework to Build an Agent? Code vs. LangGraph vs. Workflows"}}, "_tags": ["story", "author_jgilhuly", "story_41722243", "ask_hn"], "author": "jgilhuly", "children": [41722421, 41722638, 41723209], "created_at": "2024-10-02T16:23:36Z", "created_at_i": 1727886216, "num_comments": 6, "objectID": "41722243", "points": 21, "story_id": 41722243, "story_text": "LangGraph and LlamaIndex Workflows are generating a lot of buzz right now, and we wanted to see how they measure up in practice versus just writing the code. To do that, we took a straightforward agent architecture\u2014one we&#x27;ve built and deployed in code without a framework\u2014and implemented it using LangGraph and Workflows. Our main goal was to explore how these frameworks translate a simple agent design into their abstractions and assess the impact on the development and debugging process.<p>We want to share our findings with the community, providing practical examples and honest observations about these frameworks where they introduce friction and where they shine. There\u2019s a lot of hype out there, and we hope to offer some clarity with real code examples and unbiased perspectives.<p>For context, we\u2019ve been running our own Co-pilot agent&#x2F;assistant in production for about eight months. We\u2019ve also helped clients troubleshoot their assistants at scale, so we\u2019ve seen a wide range of use cases and challenges.<p>The architecture we tested is a single-tier LLM router\u2014a pattern we often see in various client implementations. It involves a single LLM router that uses function calling to route tasks or skills, which might include another LLM call before returning control to the router. It\u2019s a simple but versatile pattern.<p>Here\u2019s a Towards Data Science write up we did on the project: https:&#x2F;&#x2F;towardsdatascience.com&#x2F;choosing-between-llm-agent-frameworks-69019493b259<p>Full code: https:&#x2F;&#x2F;github.com&#x2F;Arize-ai&#x2F;phoenix&#x2F;tree&#x2F;main&#x2F;examples&#x2F;agent_framework_comparison<p>Hot take #1: For experienced developers, framework abstractions can add unnecessary complexity.\nHot take #2: Built-in parallelism, while promising, can complicate debugging a lot.\nHot Take #3: In environments with less experienced development teams that have no scaffolding, these frameworks could offer some useful structure. At least in the POC phase.<p>We\u2019re repeating this process now with CrewAI and Autogen - learnings to follow soon.<p>And if you want to deep dive into the logs of any of these, we\u2019ve published the traces captured with Arize Phoenix here.\nPure code: https:&#x2F;&#x2F;phoenix-demo.arize.com&#x2F;projects&#x2F;UHJvamVjdDo2\nLangGraph: https:&#x2F;&#x2F;phoenix-demo.arize.com&#x2F;projects&#x2F;UHJvamVjdDoy\nLlamaIndex Workflows: https:&#x2F;&#x2F;phoenix-demo.arize.com&#x2F;projects&#x2F;UHJvamVjdDo1<p>We\u2019re curious to hear what others think. What\u2019s been your experience with these frameworks, and how do they compare to rolling your own agent solutions?", "title": "Should I Use a Framework to Build an Agent? Code vs. LangGraph vs. Workflows", "updated_at": "2025-04-14T01:24:19Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "behnamoh"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "Post ChatGPT, there were many interesting papers on the properties and features of large language models. The pace of publication was so fast that every week we could see at least 1-2 papers that would make the news.<p>On the open-source side, we had some low quality models and there was so much experimentation to see what works and what doesn't. Soon we got llama and llama 2 and a plethora of models to play with. So many projects started around them, many got abandoned shortly.<p>Now that I reflect on what happened in 2023, despite all the progress that was made, I feel like the pace of growth has decreased. In early 2023 I legit thought &quot;this is how singularity feels like; every 2-3 days we'll have something new and exciting&quot;.<p>But now things are more settled. We still get new models every day and Mixtral is still amazing. But something seems off. No company (not even Google) was able to make something better than GPT-4. So many Chat UI and wrapper projects are abandoned (Github can be a scary place...), and we're not much further in our way to understand what the heck happens in these models than we were a year ago. In addition, it's become clear that GPT-4-level intelligence might be the best we can extract from the current LLM technology, and no one takes AGI seriously anymore.<p>Edit: I should add that Langchain, <em>LlamaIndex</em> and so many other &quot;frameworks&quot; built around LLMs that used to be all the rage now are evidently useless in <em>production</em>. RAG is still RAG, and no matter the tricks you play to make it &quot;smarter&quot;, it's still just RAG. Langchain and similar frameworks cause more problems than they solve, and the tech debt is horrible. Vector databases are the same. Most are fighting for that sweet VC money, and their features are essentially the same. So many startups suddenly went out of business after OpenAI's first DevDay; so many more will perish after the second DevDay. It's unclear how $$$ VC will be allocated in 2024 given that the safest bet to make money off of AI was and still is OpenAI, not Google, not these third-party frameworks and libraries, not some wrapper around OpenAI's API with a nice UI and shiny website.<p>What do you think about all this?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Is GenAI hype declining or are low-hanging fruits gone?"}}, "_tags": ["story", "author_behnamoh", "story_38898840", "ask_hn"], "author": "behnamoh", "children": [38898883, 38899008, 38899149, 38899176, 38899505, 38900855], "created_at": "2024-01-07T05:48:49Z", "created_at_i": 1704606529, "num_comments": 7, "objectID": "38898840", "points": 11, "story_id": 38898840, "story_text": "Post ChatGPT, there were many interesting papers on the properties and features of large language models. The pace of publication was so fast that every week we could see at least 1-2 papers that would make the news.<p>On the open-source side, we had some low quality models and there was so much experimentation to see what works and what doesn&#x27;t. Soon we got llama and llama 2 and a plethora of models to play with. So many projects started around them, many got abandoned shortly.<p>Now that I reflect on what happened in 2023, despite all the progress that was made, I feel like the pace of growth has decreased. In early 2023 I legit thought &quot;this is how singularity feels like; every 2-3 days we&#x27;ll have something new and exciting&quot;.<p>But now things are more settled. We still get new models every day and Mixtral is still amazing. But something seems off. No company (not even Google) was able to make something better than GPT-4. So many Chat UI and wrapper projects are abandoned (Github can be a scary place...), and we&#x27;re not much further in our way to understand what the heck happens in these models than we were a year ago. In addition, it&#x27;s become clear that GPT-4-level intelligence might be the best we can extract from the current LLM technology, and no one takes AGI seriously anymore.<p>Edit: I should add that Langchain, LlamaIndex and so many other &quot;frameworks&quot; built around LLMs that used to be all the rage now are evidently useless in production. RAG is still RAG, and no matter the tricks you play to make it &quot;smarter&quot;, it&#x27;s still just RAG. Langchain and similar frameworks cause more problems than they solve, and the tech debt is horrible. Vector databases are the same. Most are fighting for that sweet VC money, and their features are essentially the same. So many startups suddenly went out of business after OpenAI&#x27;s first DevDay; so many more will perish after the second DevDay. It&#x27;s unclear how $$$ VC will be allocated in 2024 given that the safest bet to make money off of AI was and still is OpenAI, not Google, not these third-party frameworks and libraries, not some wrapper around OpenAI&#x27;s API with a nice UI and shiny website.<p>What do you think about all this?", "title": "Ask HN: Is GenAI hype declining or are low-hanging fruits gone?", "updated_at": "2024-09-20T16:08:14Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "patethegreat"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "Hey everyone,\nI\u2019m considering starting a new open source project and wanted to see if anyone else thinks the idea could be useful. The concept is simple: an open source LLM log router that works like Segment\u2014but specifically for LLM logs. It would let you easily route logs to different analytics, eval, monitoring, and data warehouse platforms (think LangFuse, Gentrace, Lattitude, etc.) so you can leverage the strengths of each without needing separate integrations. I\u2019ve run into a few recurring challenges when integrating multiple eval and monitoring tools.<p>Conflicting Integrations:\nMany tools use their own forks of popular packages (like the OpenAI SDK or LangChain), which often conflict\u2014making it nearly impossible to use them together.<p>Inconsistent Prompt Templating:\nSometimes different tools require different prompt formats, complicating the process of switching between them or using multiple tools simultaneously.<p>Data Migration Challenges:\nMoving logs between systems is a hassle. Testing a new tool often means generating new data or deploying changes in <em>production</em>, making it hard to evaluate if switching is worthwhile.<p>While some solutions exist (such as LangChain and LiteLLM integrations with various eval platforms), they don\u2019t fully address these issues\u2014especially data migration and integrating with multiple tools at once. To solve this, I\u2019m toying with the idea of a new open source project\u2014a lightweight, self-hosted server that acts as a \u201cSegment for LLM traces and logs\u201d. Here are some of the features I envision.<p>Self-hosted logging server:\nSend LLM traces to a single endpoint without impacting your app\u2019s performance.<p>Centralized aggregation and routing:\nGather traces and logs on a single server, then forward them to any destination you choose (evals, analytics, monitoring, alerting, data warehouses, etc).<p>Lightweight Framework integrations:\nSupport for popular frameworks and SDKs (LangChain, LiteLLM, OpenAI SDK, <em>LlamaIndex</em>, etc.) with integrations that never block your event loop\u2014so even if the logging server or a destination platform goes down your app continues to function as expected.<p>Easy configuration:\nA simple interface for managing data sources and destinations without making code changes or redeploying your application.<p>Data portability:\nStore logs in a database with the option to re-export to new tools in the future\u2014ensuring you never face vendor lock-in.<p>Custom integrations:\nWebhooks to easily set up your own custom destinations.<p>I\u2019d love to hear if you\u2019ve experienced any similar issues integrating with multiple LLM monitoring, eval, and analytics platforms. If so, how did you address them? Do you see value in an open source data router like this? Please share your thoughts in the comments, and if you\u2019re interested in contributing or using a project like this in the future, it\u2019d be super helpful if you could fill out this survey.\nhttps://yk1m5yevl9j.typeform.com/to/cQdxF6bN<p>Thanks in advance for your feedback!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Segment for LLM Traces? Seeking Feedback on an Open Source LLM Log Router"}}, "_tags": ["story", "author_patethegreat", "story_43133994", "ask_hn"], "author": "patethegreat", "children": [43134192, 43136334, 43148829], "created_at": "2025-02-21T22:44:57Z", "created_at_i": 1740177897, "num_comments": 2, "objectID": "43133994", "points": 11, "story_id": 43133994, "story_text": "Hey everyone,\nI\u2019m considering starting a new open source project and wanted to see if anyone else thinks the idea could be useful. The concept is simple: an open source LLM log router that works like Segment\u2014but specifically for LLM logs. It would let you easily route logs to different analytics, eval, monitoring, and data warehouse platforms (think LangFuse, Gentrace, Lattitude, etc.) so you can leverage the strengths of each without needing separate integrations. I\u2019ve run into a few recurring challenges when integrating multiple eval and monitoring tools.<p>Conflicting Integrations:\nMany tools use their own forks of popular packages (like the OpenAI SDK or LangChain), which often conflict\u2014making it nearly impossible to use them together.<p>Inconsistent Prompt Templating:\nSometimes different tools require different prompt formats, complicating the process of switching between them or using multiple tools simultaneously.<p>Data Migration Challenges:\nMoving logs between systems is a hassle. Testing a new tool often means generating new data or deploying changes in production, making it hard to evaluate if switching is worthwhile.<p>While some solutions exist (such as LangChain and LiteLLM integrations with various eval platforms), they don\u2019t fully address these issues\u2014especially data migration and integrating with multiple tools at once. To solve this, I\u2019m toying with the idea of a new open source project\u2014a lightweight, self-hosted server that acts as a \u201cSegment for LLM traces and logs\u201d. Here are some of the features I envision.<p>Self-hosted logging server:\nSend LLM traces to a single endpoint without impacting your app\u2019s performance.<p>Centralized aggregation and routing:\nGather traces and logs on a single server, then forward them to any destination you choose (evals, analytics, monitoring, alerting, data warehouses, etc).<p>Lightweight Framework integrations:\nSupport for popular frameworks and SDKs (LangChain, LiteLLM, OpenAI SDK, LlamaIndex, etc.) with integrations that never block your event loop\u2014so even if the logging server or a destination platform goes down your app continues to function as expected.<p>Easy configuration:\nA simple interface for managing data sources and destinations without making code changes or redeploying your application.<p>Data portability:\nStore logs in a database with the option to re-export to new tools in the future\u2014ensuring you never face vendor lock-in.<p>Custom integrations:\nWebhooks to easily set up your own custom destinations.<p>I\u2019d love to hear if you\u2019ve experienced any similar issues integrating with multiple LLM monitoring, eval, and analytics platforms. If so, how did you address them? Do you see value in an open source data router like this? Please share your thoughts in the comments, and if you\u2019re interested in contributing or using a project like this in the future, it\u2019d be super helpful if you could fill out this survey.\nhttps:&#x2F;&#x2F;yk1m5yevl9j.typeform.com&#x2F;to&#x2F;cQdxF6bN<p>Thanks in advance for your feedback!", "title": "Segment for LLM Traces? Seeking Feedback on an Open Source LLM Log Router", "updated_at": "2025-02-25T07:07:46Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tifa2up"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "Hi HN,<p>We built one of the largest RAG set-ups that exist toady with Usul.ai (6B tokens). We started by using langchain and <em>llamaindex</em>, they were able to get us to a prototype in a couple of days, but took 3 months of taking pieces apart and optimizing them to make it perform well at such large scale.<p>We put all of these learning into an MIT licensed open-source project \u2014 Agentset. Our goal to let people get <em>production</em> quality RAG w/o having to understand or optimize the underlying pieces. It supports 22 file formats, agentic search, deep research, citations, and a UI out of the box.<p>Happy to answer any questions about our journey or the product."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Agentset \u2013 Open-source RAG with vector DB, embeddings, and API built-in"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/agentset-ai/agentset"}}, "_tags": ["story", "author_tifa2up", "story_45615751", "show_hn"], "author": "tifa2up", "created_at": "2025-10-17T12:03:58Z", "created_at_i": 1760702638, "num_comments": 0, "objectID": "45615751", "points": 6, "story_id": 45615751, "story_text": "Hi HN,<p>We built one of the largest RAG set-ups that exist toady with Usul.ai (6B tokens). We started by using langchain and llamaindex, they were able to get us to a prototype in a couple of days, but took 3 months of taking pieces apart and optimizing them to make it perform well at such large scale.<p>We put all of these learning into an MIT licensed open-source project \u2014 Agentset. Our goal to let people get production quality RAG w&#x2F;o having to understand or optimize the underlying pieces. It supports 22 file formats, agentic search, deep research, citations, and a UI out of the box.<p>Happy to answer any questions about our journey or the product.", "title": "Show HN: Agentset \u2013 Open-source RAG with vector DB, embeddings, and API built-in", "updated_at": "2026-01-26T12:26:23Z", "url": "https://github.com/agentset-ai/agentset"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ij23"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "Hi HN - \nIshaan and Krrish here from BerriAI. We\u2019ve built a hallucination monitoring tool for LLM Apps in <em>production</em>, that can instantly identify language mistranslations (responding to a user in the incorrect language) and inventing new information errors (answering from information not in the prompt). <i>Live demo here</i>: <a href=\"https://logs.berri.ai/\">https://logs.berri.ai/</a><p>We served over 1m+ chatGPT queries with our initial \u2018chat with your data\u2019 app. However, we had no ability to tell how any of the technical changes we made (e.g. moving from <em>llama index</em> to our own retrieval/qa system) impacted our users in <em>production</em>.<p>Berri is super easy to integrate into your system - we added it to our previous product with just 2 lines of code!<p>It\u2019s super early days and we\u2019re looking for others like us - people in <em>production</em> - pushing changes but unsure if/how they\u2019re actually solving issues / improving their system over time.<p>Thanks for taking the time to read this, we\u2019re really happy to be posting here :)<p>Krrish and Ishaan"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: BerriAI \u2013 Monitor Hallucinations in LLMs (Sentry for LLM Apps)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://logs.berri.ai/"}}, "_tags": ["story", "author_ij23", "story_36253098", "show_hn"], "author": "ij23", "created_at": "2023-06-09T04:22:08Z", "created_at_i": 1686284528, "num_comments": 0, "objectID": "36253098", "points": 4, "story_id": 36253098, "story_text": "Hi HN - \nIshaan and Krrish here from BerriAI. We\u2019ve built a hallucination monitoring tool for LLM Apps in production, that can instantly identify language mistranslations (responding to a user in the incorrect language) and inventing new information errors (answering from information not in the prompt). <i>Live demo here</i>: <a href=\"https:&#x2F;&#x2F;logs.berri.ai&#x2F;\">https:&#x2F;&#x2F;logs.berri.ai&#x2F;</a><p>We served over 1m+ chatGPT queries with our initial \u2018chat with your data\u2019 app. However, we had no ability to tell how any of the technical changes we made (e.g. moving from llama index to our own retrieval&#x2F;qa system) impacted our users in production.<p>Berri is super easy to integrate into your system - we added it to our previous product with just 2 lines of code!<p>It\u2019s super early days and we\u2019re looking for others like us - people in production - pushing changes but unsure if&#x2F;how they\u2019re actually solving issues &#x2F; improving their system over time.<p>Thanks for taking the time to read this, we\u2019re really happy to be posting here :)<p>Krrish and Ishaan", "title": "Show HN: BerriAI \u2013 Monitor Hallucinations in LLMs (Sentry for LLM Apps)", "updated_at": "2024-09-20T14:15:21Z", "url": "https://logs.berri.ai/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "david1542"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "Hey all,<p>I\u2019ve been working on an AI agent system over the past year that connects to internal company tools like Slack, GitHub, Notion, etc, to help investigate <em>production</em> incidents. The agent needs context, so we built a system that ingests this data, processes it, and builds a structured knowledge graph (kind of a mix of RAG and GraphRAG).<p>What we didn\u2019t expect was just how much infra work that would require.<p>We ended up:<p>- Using <em>LlamaIndex</em>'s OS abstractions for chunking, embedding and retrieval.<p>- Adopting Chroma as the vector store.<p>- Writing custom integrations for Slack/GitHub/Notion. We used LlamaHub here for the actual querying, although some parts were a bit unmaintained and we had to fork + fix. We could\u2019ve used Nango or Airbyte tbh but eventually didn't do that.<p>- Building an auto-refresh pipeline to sync data every few hours and do diffs based on timestamps. This was pretty hard as well.<p>- Handling security and privacy (most customers needed to keep data in their own environments).<p>- Handling scale - some orgs had hundreds of thousands of documents across different tools.<p>It became clear we were spending a lot more time on data infrastructure than on the actual agent logic. I think it might be ok for a company that interacts with customers' data, but definitely we felt like we were dealing with a lot of non-core work.<p>So I\u2019m curious: for folks building LLM apps that connect to company systems, how are you approaching this? Are you building it all from scratch too? Using open-source tools? Is there something obvious we\u2019re missing?<p>Would really appreciate hearing how others are tackling this part of the stack."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: How do you build per-user RAG/GraphRAG"}}, "_tags": ["story", "author_david1542", "story_43772702", "ask_hn"], "author": "david1542", "children": [43772828, 43772937], "created_at": "2025-04-23T14:34:23Z", "created_at_i": 1745418863, "num_comments": 4, "objectID": "43772702", "points": 2, "story_id": 43772702, "story_text": "Hey all,<p>I\u2019ve been working on an AI agent system over the past year that connects to internal company tools like Slack, GitHub, Notion, etc, to help investigate production incidents. The agent needs context, so we built a system that ingests this data, processes it, and builds a structured knowledge graph (kind of a mix of RAG and GraphRAG).<p>What we didn\u2019t expect was just how much infra work that would require.<p>We ended up:<p>- Using LlamaIndex&#x27;s OS abstractions for chunking, embedding and retrieval.<p>- Adopting Chroma as the vector store.<p>- Writing custom integrations for Slack&#x2F;GitHub&#x2F;Notion. We used LlamaHub here for the actual querying, although some parts were a bit unmaintained and we had to fork + fix. We could\u2019ve used Nango or Airbyte tbh but eventually didn&#x27;t do that.<p>- Building an auto-refresh pipeline to sync data every few hours and do diffs based on timestamps. This was pretty hard as well.<p>- Handling security and privacy (most customers needed to keep data in their own environments).<p>- Handling scale - some orgs had hundreds of thousands of documents across different tools.<p>It became clear we were spending a lot more time on data infrastructure than on the actual agent logic. I think it might be ok for a company that interacts with customers&#x27; data, but definitely we felt like we were dealing with a lot of non-core work.<p>So I\u2019m curious: for folks building LLM apps that connect to company systems, how are you approaching this? Are you building it all from scratch too? Using open-source tools? Is there something obvious we\u2019re missing?<p>Would really appreciate hearing how others are tackling this part of the stack.", "title": "Ask HN: How do you build per-user RAG/GraphRAG", "updated_at": "2025-04-23T18:47:25Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "retrovrv"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["llamaindex", "production"], "value": "Hey HN! Long-time reader, occasional commenter here. I wanted to share something my team and I have been building to solve our own frustrations.<p>We've all seen the explosion of prompt engineering tools lately. While playing around in playgrounds is fun, when we tried to take our AI prompts to <em>production</em>, we hit a wall. I'm guessing many of you have experienced similar pain points.<p>We kept hitting questions nobody seemed to be answering: How do you version control thousands of prompts? How do you handle multiple <em>production</em> deployments? How do you scale from prototype to millions of requests per day? How do you collaborate across hundreds of engineers without stepping on each other's toes?<p>So we built Portkey's Prompt Engineering Studio - a complete toolkit designed specifically for productionizing AI prompts across 1600+ models.<p>Some technical details that make our approach different:<p>- High-performance infrastructure: We've deployed prompts as large as 500,000 tokens with <em>production</em>-level latency\n- Git-like version control with instant rollbacks for prompt deployments\n- Mustache templating system for parameterization and reusable snippets\n- Publish/release flow with proper dev/staging/prod environments\n- Real-time analytics tracking prompt performance, latency, and token usage\n- Native integrations with Langchain, <em>Llamaindex</em>, and Promptfoo<p>The scaling capabilities have enabled some impressive use cases:<p>- A content company running 500+ prompts across 700+ websites\n- A tech firm that cut deployment times from 3 days to near-instant\n- Education platforms with hundreds of non-technical creators building AI workflows<p>Our platform has processed hundreds of millions of prompt completion requests already, with over 10,000 prompts deployed to <em>production</em> environments.<p>We think the HN community will especially appreciate our approach to bringing software engineering best practices to AI development!<p>You can try it yourself at prompt.new<p>I'd genuinely love to hear how others in the community are handling these challenges, what you think of our approach, or any other feedback you might have. This community has been invaluable in shaping how we think about developer tools."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Prompt Engineering Studio \u2013 Toolkit for deploying AI prompts at scale"}}, "_tags": ["story", "author_retrovrv", "story_43396713", "show_hn"], "author": "retrovrv", "children": [43396728, 43396763], "created_at": "2025-03-18T07:52:21Z", "created_at_i": 1742284341, "num_comments": 4, "objectID": "43396713", "points": 2, "story_id": 43396713, "story_text": "Hey HN! Long-time reader, occasional commenter here. I wanted to share something my team and I have been building to solve our own frustrations.<p>We&#x27;ve all seen the explosion of prompt engineering tools lately. While playing around in playgrounds is fun, when we tried to take our AI prompts to production, we hit a wall. I&#x27;m guessing many of you have experienced similar pain points.<p>We kept hitting questions nobody seemed to be answering: How do you version control thousands of prompts? How do you handle multiple production deployments? How do you scale from prototype to millions of requests per day? How do you collaborate across hundreds of engineers without stepping on each other&#x27;s toes?<p>So we built Portkey&#x27;s Prompt Engineering Studio - a complete toolkit designed specifically for productionizing AI prompts across 1600+ models.<p>Some technical details that make our approach different:<p>- High-performance infrastructure: We&#x27;ve deployed prompts as large as 500,000 tokens with production-level latency\n- Git-like version control with instant rollbacks for prompt deployments\n- Mustache templating system for parameterization and reusable snippets\n- Publish&#x2F;release flow with proper dev&#x2F;staging&#x2F;prod environments\n- Real-time analytics tracking prompt performance, latency, and token usage\n- Native integrations with Langchain, Llamaindex, and Promptfoo<p>The scaling capabilities have enabled some impressive use cases:<p>- A content company running 500+ prompts across 700+ websites\n- A tech firm that cut deployment times from 3 days to near-instant\n- Education platforms with hundreds of non-technical creators building AI workflows<p>Our platform has processed hundreds of millions of prompt completion requests already, with over 10,000 prompts deployed to production environments.<p>We think the HN community will especially appreciate our approach to bringing software engineering best practices to AI development!<p>You can try it yourself at prompt.new<p>I&#x27;d genuinely love to hear how others in the community are handling these challenges, what you think of our approach, or any other feedback you might have. This community has been invaluable in shaping how we think about developer tools.", "title": "Show HN: Prompt Engineering Studio \u2013 Toolkit for deploying AI prompts at scale", "updated_at": "2026-02-26T03:39:32Z"}], "hitsPerPage": 15, "nbHits": 20, "nbPages": 2, "page": 0, "params": "query=llamaindex+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 12, "processingTimingsMS": {"_request": {"roundTrip": 14}, "afterFetch": {"format": {"highlighting": 1, "total": 2}}, "fetch": {"query": 9, "scanning": 1, "total": 11}, "total": 12}, "query": "llamaindex production", "serverTimeMS": 14}}