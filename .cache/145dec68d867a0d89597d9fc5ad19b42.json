{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "lokahdev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "I built [VectorLiteDB (<a href=\"https://github.com/vectorlitedb/vectorlitedb\" rel=\"nofollow\">https://github.com/vectorlitedb/vectorlitedb</a>)<p>\u2014 a simple, embedded vector database that stores everything in a single file, just like SQLite.<p>The problem:<p>If you\u2019re a developer building AI apps, you usually have two choices for vector search<p>- Set up a server (e.g. Chroma, Weaviate)  \n- Use a cloud service (e.g. <em>Pinecone</em>)<p>That works for <em>production</em>, but it\u2019s overkill when you just want to:<p>- Quickly prototype with embeddings  \n- Run offline without cloud dependencies  \n- Keep your data portable in a single file<p>The inspiration was *SQLite* during development \u2014 simple, local, and reliable.<p>The solution:<p>So I built VectorLiteDB<p>- Single-file, embedded, no server  \n- Stores vectors + metadata, persists to disk  \n- Supports cosine / L2 / dot similarity  \n- Works offline, ~100ms for 10K vectors  \n- Perfect for local RAG, prototyping or personal AI memory<p>Feedback on both the tool and the approach would be really helpful.<p>- Is this something that would be useful\n- Use cases you\u2019d try this for"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: VectorLiteDB \u2013 a vector DB for local dev, like SQLite but for vectors"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/vectorlitedb/vectorlitedb"}}, "_tags": ["story", "author_lokahdev", "story_45319922", "show_hn"], "author": "lokahdev", "children": [45322272, 45336263], "created_at": "2025-09-21T03:54:25Z", "created_at_i": 1758426865, "num_comments": 4, "objectID": "45319922", "points": 13, "story_id": 45319922, "story_text": "I built [VectorLiteDB (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;vectorlitedb&#x2F;vectorlitedb\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vectorlitedb&#x2F;vectorlitedb</a>)<p>\u2014 a simple, embedded vector database that stores everything in a single file, just like SQLite.<p>The problem:<p>If you\u2019re a developer building AI apps, you usually have two choices for vector search<p>- Set up a server (e.g. Chroma, Weaviate)  \n- Use a cloud service (e.g. Pinecone)<p>That works for production, but it\u2019s overkill when you just want to:<p>- Quickly prototype with embeddings  \n- Run offline without cloud dependencies  \n- Keep your data portable in a single file<p>The inspiration was *SQLite* during development \u2014 simple, local, and reliable.<p>The solution:<p>So I built VectorLiteDB<p>- Single-file, embedded, no server  \n- Stores vectors + metadata, persists to disk  \n- Supports cosine &#x2F; L2 &#x2F; dot similarity  \n- Works offline, ~100ms for 10K vectors  \n- Perfect for local RAG, prototyping or personal AI memory<p>Feedback on both the tool and the approach would be really helpful.<p>- Is this something that would be useful\n- Use cases you\u2019d try this for", "title": "Show HN: VectorLiteDB \u2013 a vector DB for local dev, like SQLite but for vectors", "updated_at": "2025-09-26T11:39:07Z", "url": "https://github.com/vectorlitedb/vectorlitedb"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mimchak"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "I launched Embex two weeks ago and hit 9,000 downloads (7K PyPI, 2K npm).<p>Embex is a universal ORM for vector databases. One API that works across LanceDB, Qdrant, <em>Pinecone</em>, Chroma, PgVector, Milvus, and Weaviate.<p>The problem: Every vector database has a different API. Switching from <em>Pinecone</em> to Qdrant means rewriting your data layer.<p>Example:\n```python\n# Works with ANY provider\nclient = await EmbexClient.new_async(provider=&quot;lancedb&quot;, url=&quot;./data&quot;)\nawait client.insert(&quot;products&quot;, vectors)\nresults = await client.search(&quot;products&quot;, vector=query, top_k=5)<p># Switch to Qdrant? Change one line:\nclient = await EmbexClient.new_async(provider=&quot;qdrant&quot;, url=&quot;http://localhost:6333&quot;)\n```<p>Built with Rust core + SIMD acceleration (4x faster than pure Python/JS). Available for Python and Node.js.<p>What happened after launch:\n- Published to PyPI and npm\n- Downloads started coming in organically\n- 9K downloads in 2 weeks<p>I'm honestly not sure where most of the traffic came from. PyPI/npm search probably, but I haven't dug into the analytics deeply. I just made one LinkedIn post which didn't get any likes/comments.<p>Start local with LanceDB (embedded, zero Docker), then switch to <em>production</em> databases (Qdrant, <em>Pinecone</em>, Milvus) without changing code.<p>GitHub: <a href=\"https://github.com/bridgerust/bridgerust\" rel=\"nofollow\">https://github.com/bridgerust/bridgerust</a>\nDocs: <a href=\"https://bridgerust.dev/embex/introduction\" rel=\"nofollow\">https://bridgerust.dev/embex/introduction</a><p>Would love feedback on:\n- API design decisions\n- Which databases to support next\n- Performance optimizations<p>Built using BridgeRust, a framework for creating cross-language Rust libraries (also open source in this repo)."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Embex \u2013 9K organic downloads in 2 weeks with zero marketing"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.bridgerust.dev/embex/introduction/"}}, "_tags": ["story", "author_mimchak", "story_46571103", "show_hn"], "author": "mimchak", "created_at": "2026-01-10T23:37:26Z", "created_at_i": 1768088246, "num_comments": 0, "objectID": "46571103", "points": 2, "story_id": 46571103, "story_text": "I launched Embex two weeks ago and hit 9,000 downloads (7K PyPI, 2K npm).<p>Embex is a universal ORM for vector databases. One API that works across LanceDB, Qdrant, Pinecone, Chroma, PgVector, Milvus, and Weaviate.<p>The problem: Every vector database has a different API. Switching from Pinecone to Qdrant means rewriting your data layer.<p>Example:\n```python\n# Works with ANY provider\nclient = await EmbexClient.new_async(provider=&quot;lancedb&quot;, url=&quot;.&#x2F;data&quot;)\nawait client.insert(&quot;products&quot;, vectors)\nresults = await client.search(&quot;products&quot;, vector=query, top_k=5)<p># Switch to Qdrant? Change one line:\nclient = await EmbexClient.new_async(provider=&quot;qdrant&quot;, url=&quot;http:&#x2F;&#x2F;localhost:6333&quot;)\n```<p>Built with Rust core + SIMD acceleration (4x faster than pure Python&#x2F;JS). Available for Python and Node.js.<p>What happened after launch:\n- Published to PyPI and npm\n- Downloads started coming in organically\n- 9K downloads in 2 weeks<p>I&#x27;m honestly not sure where most of the traffic came from. PyPI&#x2F;npm search probably, but I haven&#x27;t dug into the analytics deeply. I just made one LinkedIn post which didn&#x27;t get any likes&#x2F;comments.<p>Start local with LanceDB (embedded, zero Docker), then switch to production databases (Qdrant, Pinecone, Milvus) without changing code.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;bridgerust&#x2F;bridgerust\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;bridgerust&#x2F;bridgerust</a>\nDocs: <a href=\"https:&#x2F;&#x2F;bridgerust.dev&#x2F;embex&#x2F;introduction\" rel=\"nofollow\">https:&#x2F;&#x2F;bridgerust.dev&#x2F;embex&#x2F;introduction</a><p>Would love feedback on:\n- API design decisions\n- Which databases to support next\n- Performance optimizations<p>Built using BridgeRust, a framework for creating cross-language Rust libraries (also open source in this repo).", "title": "Show HN: Embex \u2013 9K organic downloads in 2 weeks with zero marketing", "updated_at": "2026-01-11T08:55:11Z", "url": "https://www.bridgerust.dev/embex/introduction/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tikkun"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "The dev LLM stack<p>-   OpenAI + <em>Pinecone</em> + GPT-Index or Langchain<p>-   Perhaps also dust.tt for playing around with prompts, kinda like a more advanced gpt playground -- &lt;https://docs.dust.tt/quickstart&gt;<p>The <em>production</em> LLM stack<p>-   The dev stack<p>-   OpenAI + <em>Pinecone</em> + GPT-Index or Langchain<p>-   arXiv for finding new research to build on<p>-   Prompt platforms such as Humanloop<p>-   ML frameworks such as PyTorch, Keras, Tensorflow<p>-   MLOps tools such as MLflow, Kubeflow, Metaflow, Airflow, Seldon Core, TFServing<p>Example OpenAI Projects<p>-   &lt;https://github.com/openai/openai-cookbook&gt;<p>-   &lt;https://gpt-index.readthedocs.io/en/latest/gallery/app_showcase.html&gt;<p>-   &lt;https://langchain.readthedocs.io/en/latest/gallery.html&gt;<p>-   &lt;https://gkogan.notion.site/gkogan/The-OP-Stack-aafcab0005e3445a8ad8491aac80446c&gt;<p>-   &lt;https://github.com/transitive-bullshit/yt-semantic-search&gt;<p>What OpenAI/MSFT should do<p>-   Fund &quot;AI white mirror&quot; -- a tv show that has beautiful visions a future where intelligence costs ~0<p>Things you'll probably discover<p>-   Embeddings work ok, but not great, from a user perspective. As a developer they're great to work with. As a user, the results aren't ranked quite right. Embeddings use cases will be better with GPT-4 or GPT-4.5.<p>-   All of the obvious gpt apps will be built. We'll get hundreds of basic gpt wrapper apps (and some of them will be big businesses!), hundreds of basic embeddings search apps. If someone can think of the idea and make it without needing specific relationships, credibility, or experience, then it'll probably exist by Summer 2023.<p>-   The developer energy in this space is intense. Adults are going to hackathons to build ai apps. This is awesome.<p>-   Devs using gpt will soon be a large enough market that startups will exist and succeed just by selling to developers that are using gpt-3 in <em>production</em>. We already saw it a little bit, but we'll get many more startups here.<p>-   How could AI not be better than me at all computer based things within 10 years?<p>-   AI is kinda like a kid. When they're young, they're not that smart. Then all of a sudden, they've gotten enough training data, and their brain (compute!) has grown, and they're doing useful stuff. This is related to why people will say that building models can feel frustrating because it doesn't work well for ages and then all of a sudden it works (CEO of Oasis said this, CTO of OpenAI said this, and Instagram co-founder said this).<p>Would love input and feedback on this. I have similar things that I'm going to submit, covering what builders and engineers should do, what vector database to use, why no one else made ChatGPT before OpenAI, things holding ai powered apps back, and some other stuff like that. If you want a preview and are happy to give feedback, then email is in my profile."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Dev LLM stack, <em>production</em> LLM stack, example projects, & things you'll discover"}}, "_tags": ["story", "author_tikkun", "story_34726647", "ask_hn"], "author": "tikkun", "children": [34731147], "created_at": "2023-02-09T16:36:15Z", "created_at_i": 1675960575, "num_comments": 2, "objectID": "34726647", "points": 10, "story_id": 34726647, "story_text": "The dev LLM stack<p>-   OpenAI + Pinecone + GPT-Index or Langchain<p>-   Perhaps also dust.tt for playing around with prompts, kinda like a more advanced gpt playground -- &lt;https:&#x2F;&#x2F;docs.dust.tt&#x2F;quickstart&gt;<p>The production LLM stack<p>-   The dev stack<p>-   OpenAI + Pinecone + GPT-Index or Langchain<p>-   arXiv for finding new research to build on<p>-   Prompt platforms such as Humanloop<p>-   ML frameworks such as PyTorch, Keras, Tensorflow<p>-   MLOps tools such as MLflow, Kubeflow, Metaflow, Airflow, Seldon Core, TFServing<p>Example OpenAI Projects<p>-   &lt;https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-cookbook&gt;<p>-   &lt;https:&#x2F;&#x2F;gpt-index.readthedocs.io&#x2F;en&#x2F;latest&#x2F;gallery&#x2F;app_showcase.html&gt;<p>-   &lt;https:&#x2F;&#x2F;langchain.readthedocs.io&#x2F;en&#x2F;latest&#x2F;gallery.html&gt;<p>-   &lt;https:&#x2F;&#x2F;gkogan.notion.site&#x2F;gkogan&#x2F;The-OP-Stack-aafcab0005e3445a8ad8491aac80446c&gt;<p>-   &lt;https:&#x2F;&#x2F;github.com&#x2F;transitive-bullshit&#x2F;yt-semantic-search&gt;<p>What OpenAI&#x2F;MSFT should do<p>-   Fund &quot;AI white mirror&quot; -- a tv show that has beautiful visions a future where intelligence costs ~0<p>Things you&#x27;ll probably discover<p>-   Embeddings work ok, but not great, from a user perspective. As a developer they&#x27;re great to work with. As a user, the results aren&#x27;t ranked quite right. Embeddings use cases will be better with GPT-4 or GPT-4.5.<p>-   All of the obvious gpt apps will be built. We&#x27;ll get hundreds of basic gpt wrapper apps (and some of them will be big businesses!), hundreds of basic embeddings search apps. If someone can think of the idea and make it without needing specific relationships, credibility, or experience, then it&#x27;ll probably exist by Summer 2023.<p>-   The developer energy in this space is intense. Adults are going to hackathons to build ai apps. This is awesome.<p>-   Devs using gpt will soon be a large enough market that startups will exist and succeed just by selling to developers that are using gpt-3 in production. We already saw it a little bit, but we&#x27;ll get many more startups here.<p>-   How could AI not be better than me at all computer based things within 10 years?<p>-   AI is kinda like a kid. When they&#x27;re young, they&#x27;re not that smart. Then all of a sudden, they&#x27;ve gotten enough training data, and their brain (compute!) has grown, and they&#x27;re doing useful stuff. This is related to why people will say that building models can feel frustrating because it doesn&#x27;t work well for ages and then all of a sudden it works (CEO of Oasis said this, CTO of OpenAI said this, and Instagram co-founder said this).<p>Would love input and feedback on this. I have similar things that I&#x27;m going to submit, covering what builders and engineers should do, what vector database to use, why no one else made ChatGPT before OpenAI, things holding ai powered apps back, and some other stuff like that. If you want a preview and are happy to give feedback, then email is in my profile.", "title": "Dev LLM stack, production LLM stack, example projects, & things you'll discover", "updated_at": "2024-09-20T13:12:44Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "alokjnv10"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "Hi Alok,<p>We\u2019re writing to inform you about an incident that impacted your index on the Starter (free) plan.<p>At 15:00 EST today, a script meant to delete inactive indexes in the Starter (free) tier was inadvertently run on certain active indexes on the Starter (free) plan. Our engineering team immediately halted index operations to investigate the issue.<p>At 16:30 EST all operations returned to normal. A large fraction of the affected indexes were restored. Indexes and operations for users on paid plans (Standard, Enterprise, or Enterprise Dedicated) were not affected.<p>According to our logs, unfortunately your index was deleted by the script. Our engineers were able to recover some indexes already. We are still working on recovering the rest. At this point, we cannot guarantee we will be able to recover all indexes.<p>We are deeply sorry for the inconvenience caused by this incident. While our Starter (free) tier is offered without availability guarantees, we recognize there are many free users who nonetheless depend on <em>Pinecone</em> to power their applications and projects. Earning and keeping your trust is a top priority for us, which is why we\u2019re committed to making this right.<p>Here are the steps we\u2019re taking:<p>We are continuing work to restore as many affected indexes as possible. There is no action needed on your behalf.\nWe will share a detailed post-mortem in a few days, also outlining steps we\u2019ve taken to prevent such incidents in the future.\nWe are increasing your pod limit on the Starter (free) plan to 3 pods (from 1). As a result, if you choose to recreate your index yourself, it will not interfere with the restoration efforts. You do not have to do anything to get this benefit.\nWe are offering $100 in credits for affected users who wish to upgrade to a paid plan, which is recommended for <em>production</em> applications. We will automatically apply this credit to you when you upgrade in the next 30 days. There is no need to notify us or ask for the credits. At the end of the billing cycle, you will simply not be charged for the first $100 of usage.\nWe will continue to promptly share any updates related to this incident with you.\nOnce again, we deeply regret this incident and will continue working hard to make it right. If you have any questions or concerns you can reply to this email (support@<em>pinecone</em>.io) or contact your Customer Success Engineer directly. For the latest system status visit our status page.<p>\u2014 Team <em>Pinecone</em>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pinecone"], "value": "Incident that affected your <em>Pinecone</em> index"}}, "_tags": ["story", "author_alokjnv10", "story_34992170", "ask_hn"], "author": "alokjnv10", "children": [34993289], "created_at": "2023-03-02T04:55:04Z", "created_at_i": 1677732904, "num_comments": 1, "objectID": "34992170", "points": 7, "story_id": 34992170, "story_text": "Hi Alok,<p>We\u2019re writing to inform you about an incident that impacted your index on the Starter (free) plan.<p>At 15:00 EST today, a script meant to delete inactive indexes in the Starter (free) tier was inadvertently run on certain active indexes on the Starter (free) plan. Our engineering team immediately halted index operations to investigate the issue.<p>At 16:30 EST all operations returned to normal. A large fraction of the affected indexes were restored. Indexes and operations for users on paid plans (Standard, Enterprise, or Enterprise Dedicated) were not affected.<p>According to our logs, unfortunately your index was deleted by the script. Our engineers were able to recover some indexes already. We are still working on recovering the rest. At this point, we cannot guarantee we will be able to recover all indexes.<p>We are deeply sorry for the inconvenience caused by this incident. While our Starter (free) tier is offered without availability guarantees, we recognize there are many free users who nonetheless depend on Pinecone to power their applications and projects. Earning and keeping your trust is a top priority for us, which is why we\u2019re committed to making this right.<p>Here are the steps we\u2019re taking:<p>We are continuing work to restore as many affected indexes as possible. There is no action needed on your behalf.\nWe will share a detailed post-mortem in a few days, also outlining steps we\u2019ve taken to prevent such incidents in the future.\nWe are increasing your pod limit on the Starter (free) plan to 3 pods (from 1). As a result, if you choose to recreate your index yourself, it will not interfere with the restoration efforts. You do not have to do anything to get this benefit.\nWe are offering $100 in credits for affected users who wish to upgrade to a paid plan, which is recommended for production applications. We will automatically apply this credit to you when you upgrade in the next 30 days. There is no need to notify us or ask for the credits. At the end of the billing cycle, you will simply not be charged for the first $100 of usage.\nWe will continue to promptly share any updates related to this incident with you.\nOnce again, we deeply regret this incident and will continue working hard to make it right. If you have any questions or concerns you can reply to this email (support@pinecone.io) or contact your Customer Success Engineer directly. For the latest system status visit our status page.<p>\u2014 Team Pinecone", "title": "Incident that affected your Pinecone index", "updated_at": "2024-09-20T13:31:46Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "andrewcamel"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "Just a reminder to not store all your expensive eggs (embeddings) in one basket! Bad mistake on my part.<p>Something to be said for <em>Pinecone</em> on this... if your core business is to be the &quot;<em>production</em>-grade hosted vector DB&quot;, probably best to reallocate some of your R&amp;D budget to implementing basic controls that prevent someone running a script that can wipe the entire <em>production</em> DB. Undermines a lot of investment they've made into their reputation and brand.<p>https://community.<em>pinecone</em>.io/t/did-anyone-else-just-have-their-index-deleted-due-to-inactivity/704<p>https://community.<em>pinecone</em>.io/t/march-1st-incident-that-affected-indexes-on-the-starter-free-plan/724/2"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pinecone"], "value": "<em>Pinecone</em> just wiped every free user account (by accident)"}}, "_tags": ["story", "author_andrewcamel", "story_35002864", "ask_hn"], "author": "andrewcamel", "children": [35013707], "created_at": "2023-03-02T22:17:37Z", "created_at_i": 1677795457, "num_comments": 1, "objectID": "35002864", "points": 5, "story_id": 35002864, "story_text": "Just a reminder to not store all your expensive eggs (embeddings) in one basket! Bad mistake on my part.<p>Something to be said for Pinecone on this... if your core business is to be the &quot;production-grade hosted vector DB&quot;, probably best to reallocate some of your R&amp;D budget to implementing basic controls that prevent someone running a script that can wipe the entire production DB. Undermines a lot of investment they&#x27;ve made into their reputation and brand.<p>https:&#x2F;&#x2F;community.pinecone.io&#x2F;t&#x2F;did-anyone-else-just-have-their-index-deleted-due-to-inactivity&#x2F;704<p>https:&#x2F;&#x2F;community.pinecone.io&#x2F;t&#x2F;march-1st-incident-that-affected-indexes-on-the-starter-free-plan&#x2F;724&#x2F;2", "title": "Pinecone just wiped every free user account (by accident)", "updated_at": "2024-09-20T13:21:28Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "gk1"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "<em>Pinecone</em> 2.0: Vector Search from the Lab to <em>Production</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pinecone"], "value": "https://www.<em>pinecone</em>.io/learn/<em>pinecone</em>-v2/"}}, "_tags": ["story", "author_gk1", "story_28528474"], "author": "gk1", "created_at": "2021-09-14T18:08:33Z", "created_at_i": 1631642913, "num_comments": 0, "objectID": "28528474", "points": 4, "story_id": 28528474, "title": "Pinecone 2.0: Vector Search from the Lab to Production", "updated_at": "2024-09-20T09:24:49Z", "url": "https://www.pinecone.io/learn/pinecone-v2/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fazlerocks"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "I've been building AI applications using Next.js, GPT, and Langchain. As I'm approaching <em>production</em> scale, I'm curious how others are handling deployment infrastructure.<p>Current stack:\n- Next.js on Vercel\n- Serverless functions for AI/LLM endpoints\n- <em>Pinecone</em> for vector storage<p>Questions for those running AI in <em>production</em>:<p>1. What's your serverless infrastructure choice? (Vercel/Cloud Run/Lambda)<p>2. How are you handling state management for long-running agent tasks?<p>3. What's your approach to cost optimization with LLM API calls?<p>4. Are you self-hosting any components?<p>5. How are you handling vector store scaling?<p>Particularly interested in hearing from teams who've scaled beyond prototype stage. Have you hit any unexpected limitations with serverless for AI workloads?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: What's your serverless stack for AI/LLM apps in <em>production</em>?"}}, "_tags": ["story", "author_fazlerocks", "story_42659704", "ask_hn"], "author": "fazlerocks", "children": [42660081], "created_at": "2025-01-10T20:22:57Z", "created_at_i": 1736540577, "num_comments": 3, "objectID": "42659704", "points": 3, "story_id": 42659704, "story_text": "I&#x27;ve been building AI applications using Next.js, GPT, and Langchain. As I&#x27;m approaching production scale, I&#x27;m curious how others are handling deployment infrastructure.<p>Current stack:\n- Next.js on Vercel\n- Serverless functions for AI&#x2F;LLM endpoints\n- Pinecone for vector storage<p>Questions for those running AI in production:<p>1. What&#x27;s your serverless infrastructure choice? (Vercel&#x2F;Cloud Run&#x2F;Lambda)<p>2. How are you handling state management for long-running agent tasks?<p>3. What&#x27;s your approach to cost optimization with LLM API calls?<p>4. Are you self-hosting any components?<p>5. How are you handling vector store scaling?<p>Particularly interested in hearing from teams who&#x27;ve scaled beyond prototype stage. Have you hit any unexpected limitations with serverless for AI workloads?", "title": "Ask HN: What's your serverless stack for AI/LLM apps in production?", "updated_at": "2025-01-12T21:52:01Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "gk1"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pinecone"], "value": "Multi-Tenancy in <em>Pinecone</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "https://www.<em>pinecone</em>.io/learn/series/vector-databases-in-<em>production</em>-for-busy-engineers/vector-database-multi-tenancy/"}}, "_tags": ["story", "author_gk1", "story_40088786"], "author": "gk1", "created_at": "2024-04-19T16:28:29Z", "created_at_i": 1713544109, "num_comments": 0, "objectID": "40088786", "points": 1, "story_id": 40088786, "title": "Multi-Tenancy in Pinecone", "updated_at": "2024-09-20T16:53:25Z", "url": "https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/vector-database-multi-tenancy/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "waleedlatif1"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "Hey HN, Waleed here. We're building Sim (<a href=\"https://sim.ai/\">https://sim.ai/</a>), an open-source visual editor to build agentic workflows. Repo here: <a href=\"https://github.com/simstudioai/sim/\" rel=\"nofollow\">https://github.com/simstudioai/sim/</a>. Docs here: <a href=\"https://docs.sim.ai\">https://docs.sim.ai</a>.<p>You can run Sim locally using Docker, with no execution limits or other restrictions.<p>We started building Sim almost a year ago after repeatedly troubleshooting why our agents failed in <em>production</em>. Code-first frameworks felt hard to debug because of implicit control flow, and workflow platforms added more overhead than they removed. We wanted granular control and easy observability without piecing everything together ourselves.<p>We launched Sim [1][2] as a drag-and-drop canvas around 6 months ago. Since then, we've added:<p>- 138 blocks: Slack, GitHub, Linear, Notion, Supabase, SSH, TTS, SFTP, MongoDB, S3, <em>Pinecone</em>, ...<p>- Tool calling with granular control: forced, auto<p>- Agent memory: conversation memory with sliding window support (by last n messages or tokens)<p>- Trace spans: detailed logging and observability for nested workflows and tool calling<p>- Native RAG: upload documents, we chunk, embed with pgvector, and expose vector search to agents<p>- Workflow deployment versioning with rollbacks<p>- MCP support, Human-in-the-loop block<p>- Copilot to build workflows using natural language (just shipped a new version that also acts as a superagent and can call into any of your connected services directly, not just build workflows)<p>Under the hood, the workflow is a DAG with concurrent execution by default. Nodes run as soon as their dependencies (upstream blocks) are satisfied. Loops (for, forEach, while, do-while) and parallel fan-out/join are also first-class primitives.<p>Agent blocks are pass-through to the provider. You pick your model (OpenAI, Anthropic, Gemini, Ollama, vLLM), and and we pass through prompts, tools, and response format directly to the provider API. We normalize response shapes for block interoperability, but we're not adding layers that obscure what's happening.<p>We're currently working on our own MCP server and the ability to deploy workflows as MCP servers. Would love to hear your thoughts and where we should take it next :)<p>[1] <a href=\"https://news.ycombinator.com/item?id=43823096\">https://news.ycombinator.com/item?id=43823096</a><p>[2] <a href=\"https://news.ycombinator.com/item?id=44052766\">https://news.ycombinator.com/item?id=44052766</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Sim \u2013 Apache-2.0 n8n alternative"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/simstudioai/sim"}}, "_tags": ["story", "author_waleedlatif1", "story_46234186", "show_hn"], "author": "waleedlatif1", "children": [46235878, 46236104, 46236129, 46236422, 46237382, 46237417, 46238673, 46238813, 46239574, 46240560, 46241088, 46241089, 46241968, 46241970, 46244396, 46244712, 46245239, 46256736, 46321424], "created_at": "2025-12-11T17:20:11Z", "created_at_i": 1765473611, "num_comments": 61, "objectID": "46234186", "points": 240, "story_id": 46234186, "story_text": "Hey HN, Waleed here. We&#x27;re building Sim (<a href=\"https:&#x2F;&#x2F;sim.ai&#x2F;\">https:&#x2F;&#x2F;sim.ai&#x2F;</a>), an open-source visual editor to build agentic workflows. Repo here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;simstudioai&#x2F;sim&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;simstudioai&#x2F;sim&#x2F;</a>. Docs here: <a href=\"https:&#x2F;&#x2F;docs.sim.ai\">https:&#x2F;&#x2F;docs.sim.ai</a>.<p>You can run Sim locally using Docker, with no execution limits or other restrictions.<p>We started building Sim almost a year ago after repeatedly troubleshooting why our agents failed in production. Code-first frameworks felt hard to debug because of implicit control flow, and workflow platforms added more overhead than they removed. We wanted granular control and easy observability without piecing everything together ourselves.<p>We launched Sim [1][2] as a drag-and-drop canvas around 6 months ago. Since then, we&#x27;ve added:<p>- 138 blocks: Slack, GitHub, Linear, Notion, Supabase, SSH, TTS, SFTP, MongoDB, S3, Pinecone, ...<p>- Tool calling with granular control: forced, auto<p>- Agent memory: conversation memory with sliding window support (by last n messages or tokens)<p>- Trace spans: detailed logging and observability for nested workflows and tool calling<p>- Native RAG: upload documents, we chunk, embed with pgvector, and expose vector search to agents<p>- Workflow deployment versioning with rollbacks<p>- MCP support, Human-in-the-loop block<p>- Copilot to build workflows using natural language (just shipped a new version that also acts as a superagent and can call into any of your connected services directly, not just build workflows)<p>Under the hood, the workflow is a DAG with concurrent execution by default. Nodes run as soon as their dependencies (upstream blocks) are satisfied. Loops (for, forEach, while, do-while) and parallel fan-out&#x2F;join are also first-class primitives.<p>Agent blocks are pass-through to the provider. You pick your model (OpenAI, Anthropic, Gemini, Ollama, vLLM), and and we pass through prompts, tools, and response format directly to the provider API. We normalize response shapes for block interoperability, but we&#x27;re not adding layers that obscure what&#x27;s happening.<p>We&#x27;re currently working on our own MCP server and the ability to deploy workflows as MCP servers. Would love to hear your thoughts and where we should take it next :)<p>[1] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43823096\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43823096</a><p>[2] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44052766\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44052766</a>", "title": "Show HN: Sim \u2013 Apache-2.0 n8n alternative", "updated_at": "2026-01-13T19:45:40Z", "url": "https://github.com/simstudioai/sim"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "r_thambapillai"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "Hi Hacker News! We\u2019re Ravin and Jack, the founders of Credal.ai (<a href=\"https://www.credal.ai/\">https://www.credal.ai/</a>). We provide a Chat UI and APIs that enforce PII redaction, audit logging, and data access controls for companies that want to use LLMs with their corporate data from Google Docs, Slack, or Confluence. There\u2019s a demo video here: <a href=\"https://www.loom.com/share/2b5409fd64464dc9b5b6277f2be4e90f?sid=7a728d97-58ac-4355-9c87-75eaf12b0775\" rel=\"nofollow noreferrer\">https://www.loom.com/share/2b5409fd64464dc9b5b6277f2be4e90f?...</a>.<p>One big thing enterprises and businesses are worried about with LLMs is \u201cwhat\u2019s happening to my data\u201d? The way we see it, there are three big security and privacy barriers companies need to solve:<p>1. Controlling what data goes to whom: the basic stuff is just putting controls in place around customer and employee PII, but it can get trickier when you also want to be putting controls in place around business secrets, so companies can ensure the Coca Cola recipe doesn\u2019t accidentally leave the company.<p>2. Visibility: Enterprise IT wants to know exactly what data was shared by whom, when, at what time, and what the model responded with (not to mention how much the request cost!). Each provider gives you a piece of the puzzle in their dashboard, but getting all this visibility per request from either of the main providers currently requires writing code yourself.<p>3. Access Controls: Enterprises have lots of documents that for whatever reason cannot be shared internally to everyone. So how do I make sure employees can use AI with this stuff, without compromising the sensitivity of the data?<p>Typically this pain is something that is felt most acutely by Enterprise IT, but also of course by the developers and business people who get told not to build the great stuff they can envision.\nWe think it\u2019s critical to solve these issues since the more visibility and control we can give Enterprise IT about how data is used, the more we can actually build on top of these APIs and start applying some of the awesome capabilities of the foundation models across every business problem.<p>You can easily grab data from sources like Google Docs via their APIs, but for <em>production</em> use cases, you have to respect the permissions on each Google Doc, Confluence Page, Slack channel etc. This gets tricky when these systems combine some permissions defined totally inside their product, with permissions that are inherited from the company\u2019s SSO provider (often Okta or Azure AD). Respecting all these permissions becomes both hard and vital as the number of employees and tools accessing the data grows.<p>The current state of the art is to use a vector database like <em>Pinecone</em>, Milvus, or Chroma, integrate your internal data with those systems, and then when a user asks a question, dynamically figure out which bits are relevant to the user\u2019s question and send those to the AI as part of the prompt. We handle all this automatically for you (using Milvus for now, which we host ourselves), including the point and click connectors for your data (Google Docs/Sheets, Slack, Confluence with many more coming soon). You can use that data through our UI already and we\u2019re in the process of adding this search functionality to the API as well.<p>There\u2019s other schlep work that devs would rather not worry about: building out request level audit logs, staying on top of the rapidly changing API formats from these providers, implementing failover for when these heavily overburdened APIs go down etc,  We think  individual devs should not have to do these themselves, but the foundation model providers are unlikely to provide consistent, customer centric approaches for them. The PII detection piece in some ways is the easiest - there are a lot of good open source models for doing this, and companies using Azure OpenAI and AWS Bedrock seem less concerned with it anyway. We expect that the emphasis companies place on the redactions we provide may actually go down over time, while the emphasis on unified, consistent audit logging and data access controls will increase.<p>Right now we have three plans: a free tier (which is admittedly very limited but intended to give you a feel for the product), the business plan which starts at $500pm which gets you access to the data integration as well as the most powerful models like GPT 4 32k, Anthropic 100k etc, and an enterprise plan which starts at $5000pm, which is a scaled up version of the business tier and lets you go on-prem (more details on each plan are on the website). You can try the free tier self-serve, but we haven\u2019t yet built out fully self service onboarding for the paid plans so for now it is a \u201cbook a meeting\u201d button, apologies! (But it only takes 5 minutes and if you want it, we can fully onboard you in the meeting itself).<p>When Jack and I started Credal, we actually set out to solve a different problem: an \u2018AI Chief of Staff\u2019 that could read your documents and task trackers, and guide your strategic decision making. We knew that data security was going to be a critical problem for enterprises. Jack and I were both deep in the Enterprise Data Security + AI space before Credal, so we naturally took a security first approach to building out our AI Chief of Staff. But in reality, when we started showing the product to customers, we learned pretty fast that the \u2018Chief of Staff\u2019 features were at best nice to have, and the security features were what they were actually excited by. So we stripped the product back to basics, and built out the thing our customers actually needed. Since then we\u2019ve signed a bunch of customers and thousands of users, which has been really exciting.<p>Now that our product is concretely helping a bunch of people at work, is SOC 2 T1 Compliant, and is ready for anyone to just walk up and use, we\u2019re super excited to share it with the Hacker News community, which Jack and I have been avid readers of for a decade now. It\u2019s still a very early product (the private beta opened in March), but we can\u2019t wait to get your feedback and see how we can make it even better!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Credal.ai (YC W23) \u2013 Data Safety for Enterprise AI"}}, "_tags": ["story", "author_r_thambapillai", "story_36326525", "launch_hn"], "author": "r_thambapillai", "children": [36326917, 36326962, 36327564, 36327940, 36328429, 36328819, 36330020, 36330371, 36333611, 36357825], "created_at": "2023-06-14T14:26:55Z", "created_at_i": 1686752815, "num_comments": 24, "objectID": "36326525", "points": 114, "story_id": 36326525, "story_text": "Hi Hacker News! We\u2019re Ravin and Jack, the founders of Credal.ai (<a href=\"https:&#x2F;&#x2F;www.credal.ai&#x2F;\">https:&#x2F;&#x2F;www.credal.ai&#x2F;</a>). We provide a Chat UI and APIs that enforce PII redaction, audit logging, and data access controls for companies that want to use LLMs with their corporate data from Google Docs, Slack, or Confluence. There\u2019s a demo video here: <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;2b5409fd64464dc9b5b6277f2be4e90f?sid=7a728d97-58ac-4355-9c87-75eaf12b0775\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;2b5409fd64464dc9b5b6277f2be4e90f?...</a>.<p>One big thing enterprises and businesses are worried about with LLMs is \u201cwhat\u2019s happening to my data\u201d? The way we see it, there are three big security and privacy barriers companies need to solve:<p>1. Controlling what data goes to whom: the basic stuff is just putting controls in place around customer and employee PII, but it can get trickier when you also want to be putting controls in place around business secrets, so companies can ensure the Coca Cola recipe doesn\u2019t accidentally leave the company.<p>2. Visibility: Enterprise IT wants to know exactly what data was shared by whom, when, at what time, and what the model responded with (not to mention how much the request cost!). Each provider gives you a piece of the puzzle in their dashboard, but getting all this visibility per request from either of the main providers currently requires writing code yourself.<p>3. Access Controls: Enterprises have lots of documents that for whatever reason cannot be shared internally to everyone. So how do I make sure employees can use AI with this stuff, without compromising the sensitivity of the data?<p>Typically this pain is something that is felt most acutely by Enterprise IT, but also of course by the developers and business people who get told not to build the great stuff they can envision.\nWe think it\u2019s critical to solve these issues since the more visibility and control we can give Enterprise IT about how data is used, the more we can actually build on top of these APIs and start applying some of the awesome capabilities of the foundation models across every business problem.<p>You can easily grab data from sources like Google Docs via their APIs, but for production use cases, you have to respect the permissions on each Google Doc, Confluence Page, Slack channel etc. This gets tricky when these systems combine some permissions defined totally inside their product, with permissions that are inherited from the company\u2019s SSO provider (often Okta or Azure AD). Respecting all these permissions becomes both hard and vital as the number of employees and tools accessing the data grows.<p>The current state of the art is to use a vector database like Pinecone, Milvus, or Chroma, integrate your internal data with those systems, and then when a user asks a question, dynamically figure out which bits are relevant to the user\u2019s question and send those to the AI as part of the prompt. We handle all this automatically for you (using Milvus for now, which we host ourselves), including the point and click connectors for your data (Google Docs&#x2F;Sheets, Slack, Confluence with many more coming soon). You can use that data through our UI already and we\u2019re in the process of adding this search functionality to the API as well.<p>There\u2019s other schlep work that devs would rather not worry about: building out request level audit logs, staying on top of the rapidly changing API formats from these providers, implementing failover for when these heavily overburdened APIs go down etc,  We think  individual devs should not have to do these themselves, but the foundation model providers are unlikely to provide consistent, customer centric approaches for them. The PII detection piece in some ways is the easiest - there are a lot of good open source models for doing this, and companies using Azure OpenAI and AWS Bedrock seem less concerned with it anyway. We expect that the emphasis companies place on the redactions we provide may actually go down over time, while the emphasis on unified, consistent audit logging and data access controls will increase.<p>Right now we have three plans: a free tier (which is admittedly very limited but intended to give you a feel for the product), the business plan which starts at $500pm which gets you access to the data integration as well as the most powerful models like GPT 4 32k, Anthropic 100k etc, and an enterprise plan which starts at $5000pm, which is a scaled up version of the business tier and lets you go on-prem (more details on each plan are on the website). You can try the free tier self-serve, but we haven\u2019t yet built out fully self service onboarding for the paid plans so for now it is a \u201cbook a meeting\u201d button, apologies! (But it only takes 5 minutes and if you want it, we can fully onboard you in the meeting itself).<p>When Jack and I started Credal, we actually set out to solve a different problem: an \u2018AI Chief of Staff\u2019 that could read your documents and task trackers, and guide your strategic decision making. We knew that data security was going to be a critical problem for enterprises. Jack and I were both deep in the Enterprise Data Security + AI space before Credal, so we naturally took a security first approach to building out our AI Chief of Staff. But in reality, when we started showing the product to customers, we learned pretty fast that the \u2018Chief of Staff\u2019 features were at best nice to have, and the security features were what they were actually excited by. So we stripped the product back to basics, and built out the thing our customers actually needed. Since then we\u2019ve signed a bunch of customers and thousands of users, which has been really exciting.<p>Now that our product is concretely helping a bunch of people at work, is SOC 2 T1 Compliant, and is ready for anyone to just walk up and use, we\u2019re super excited to share it with the Hacker News community, which Jack and I have been avid readers of for a decade now. It\u2019s still a very early product (the private beta opened in March), but we can\u2019t wait to get your feedback and see how we can make it even better!", "title": "Launch HN: Credal.ai (YC W23) \u2013 Data Safety for Enterprise AI", "updated_at": "2024-09-20T14:21:48Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jmiran15"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "I am excited to announce a new tool for music producers and audio enthusiasts - a music audio search engine. With just a simple description of the groove you're looking for, our semantic search engine will output the most similar audio in seconds.<p>I used the Freesound.org API to upload over 3,000 grooves to MongoDB, and combined all the relevant data such as tags, title, description, BPM, etc. into OpenAI's Text-Davinci to generate a unique description of each sound. I then embedded these descriptions using the Ada Embeddings Model and inserted them into <em>Pinecone</em> DB vector database, making it easier to find the perfect sound for your next project.<p>This search engine is designed to save you time and make your music <em>production</em> process more efficient. Give it a try and see the difference it makes in your workflow. I would also appreciate any feedback on how I can improve the website and make it even more user-friendly.<p>EDIT: \n- Google Sign In removed\n- 25 total sounds now (only 5 before)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Music Audio Search Engine Using OpenAI's Embeddings on GPT Descriptions"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://muzic-sage.vercel.app/"}}, "_tags": ["story", "author_jmiran15", "story_34448334", "show_hn"], "author": "jmiran15", "children": [34448655, 34448754, 34448771, 34448952, 34448985, 34449364, 34449388, 34449637, 34450088, 34451418, 34451977, 34452301, 34453252, 34453709, 34455447, 34455684, 34455803, 34455837, 34456215], "created_at": "2023-01-20T02:07:59Z", "created_at_i": 1674180479, "num_comments": 38, "objectID": "34448334", "points": 92, "story_id": 34448334, "story_text": "I am excited to announce a new tool for music producers and audio enthusiasts - a music audio search engine. With just a simple description of the groove you&#x27;re looking for, our semantic search engine will output the most similar audio in seconds.<p>I used the Freesound.org API to upload over 3,000 grooves to MongoDB, and combined all the relevant data such as tags, title, description, BPM, etc. into OpenAI&#x27;s Text-Davinci to generate a unique description of each sound. I then embedded these descriptions using the Ada Embeddings Model and inserted them into Pinecone DB vector database, making it easier to find the perfect sound for your next project.<p>This search engine is designed to save you time and make your music production process more efficient. Give it a try and see the difference it makes in your workflow. I would also appreciate any feedback on how I can improve the website and make it even more user-friendly.<p>EDIT: \n- Google Sign In removed\n- 25 total sounds now (only 5 before)", "title": "Show HN: Music Audio Search Engine Using OpenAI's Embeddings on GPT Descriptions", "updated_at": "2024-09-20T13:03:20Z", "url": "https://muzic-sage.vercel.app/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "waleedlatif1"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "Hey HN! We're Emir and Waleed from Sim Studio (<a href=\"https://simstudio.ai\">https://simstudio.ai</a>). We did a Show HN a few weeks ago about our open-source project (<a href=\"https://news.ycombinator.com/item?id=43823096\">https://news.ycombinator.com/item?id=43823096</a>). Today, we\u2019re launching our hosted platform\u2014a collaborative interface to build and deploy agent workflows. We just removed the waitlist (with 5,000+ people) and you can sign up and access it at: <a href=\"https://simstudio.ai\">https://simstudio.ai</a><p>Sim Studio lets you build agents with different models, tools, and custom functions. In minutes, you can deploy a research agent. Demo here - <a href=\"https://youtu.be/F6MqNvnfxbI\" rel=\"nofollow\">https://youtu.be/F6MqNvnfxbI</a><p>We started working on Sim Studio because we were frustrated with agent frameworks that required excessive boilerplate code and unhelpful abstractions (like agent roles and backstories). We also tried other visual interfaces, but found them difficult to set up and too limited for <em>production</em> use cases. Specifically, we wanted the ability to write custom functions, force tool calls, and run parallel execution.<p>We\u2019re taking a different approach. Our visual canvas lets you explicitly define how agents interact, use external tools, and handle complex logic like branching, loops, and conditional execution. You can seamlessly integrate with popular tools like Supabase, <em>Pinecone</em>, Mem0, and Exa without obscure abstractions.<p>You can simulate workflow runs, deploy workflows as APIs or standalone chat interfaces, and trigger them via webhooks. We offer built-in observability, detailed trace spans, and logs that make debugging straightforward. Try searching yourself below:<p>Example message for <a href=\"https://hn.simstudio.ai\">https://hn.simstudio.ai</a>:<p>- Search [person] from [company] - &quot;Search Emir Karabeg from Sim Studio&quot;<p>Check out our Apache 2.0 licensed repo at <a href=\"https://github.com/simstudioai/sim\">https://github.com/simstudioai/sim</a>, explore our docs at <a href=\"https://docs.simstudio.ai\">https://docs.simstudio.ai</a>, or try out our deployed chat from the demo video here: <a href=\"https://hn.simstudio.ai\">https://hn.simstudio.ai</a>.<p>We\u2019d love to hear your feedback! Do you think our visual solution is a good approach to building agents?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Sim Studio (YC X25) \u2013 Figma-like canvas for agent workflows"}}, "_tags": ["story", "author_waleedlatif1", "story_44052766", "launch_hn"], "author": "waleedlatif1", "children": [44053060, 44053427, 44053452, 44053463, 44053534, 44053543, 44053544, 44054269, 44054400, 44055236, 44070798], "created_at": "2025-05-21T15:49:15Z", "created_at_i": 1747842555, "num_comments": 32, "objectID": "44052766", "points": 55, "story_id": 44052766, "story_text": "Hey HN! We&#x27;re Emir and Waleed from Sim Studio (<a href=\"https:&#x2F;&#x2F;simstudio.ai\">https:&#x2F;&#x2F;simstudio.ai</a>). We did a Show HN a few weeks ago about our open-source project (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43823096\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43823096</a>). Today, we\u2019re launching our hosted platform\u2014a collaborative interface to build and deploy agent workflows. We just removed the waitlist (with 5,000+ people) and you can sign up and access it at: <a href=\"https:&#x2F;&#x2F;simstudio.ai\">https:&#x2F;&#x2F;simstudio.ai</a><p>Sim Studio lets you build agents with different models, tools, and custom functions. In minutes, you can deploy a research agent. Demo here - <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;F6MqNvnfxbI\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;F6MqNvnfxbI</a><p>We started working on Sim Studio because we were frustrated with agent frameworks that required excessive boilerplate code and unhelpful abstractions (like agent roles and backstories). We also tried other visual interfaces, but found them difficult to set up and too limited for production use cases. Specifically, we wanted the ability to write custom functions, force tool calls, and run parallel execution.<p>We\u2019re taking a different approach. Our visual canvas lets you explicitly define how agents interact, use external tools, and handle complex logic like branching, loops, and conditional execution. You can seamlessly integrate with popular tools like Supabase, Pinecone, Mem0, and Exa without obscure abstractions.<p>You can simulate workflow runs, deploy workflows as APIs or standalone chat interfaces, and trigger them via webhooks. We offer built-in observability, detailed trace spans, and logs that make debugging straightforward. Try searching yourself below:<p>Example message for <a href=\"https:&#x2F;&#x2F;hn.simstudio.ai\">https:&#x2F;&#x2F;hn.simstudio.ai</a>:<p>- Search [person] from [company] - &quot;Search Emir Karabeg from Sim Studio&quot;<p>Check out our Apache 2.0 licensed repo at <a href=\"https:&#x2F;&#x2F;github.com&#x2F;simstudioai&#x2F;sim\">https:&#x2F;&#x2F;github.com&#x2F;simstudioai&#x2F;sim</a>, explore our docs at <a href=\"https:&#x2F;&#x2F;docs.simstudio.ai\">https:&#x2F;&#x2F;docs.simstudio.ai</a>, or try out our deployed chat from the demo video here: <a href=\"https:&#x2F;&#x2F;hn.simstudio.ai\">https:&#x2F;&#x2F;hn.simstudio.ai</a>.<p>We\u2019d love to hear your feedback! Do you think our visual solution is a good approach to building agents?", "title": "Launch HN: Sim Studio (YC X25) \u2013 Figma-like canvas for agent workflows", "updated_at": "2025-12-11T21:46:32Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tikkun"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "I have a list of ai tools that I've used or want to use in my notes and I'm cleaning it up so I can share it.<p>Here's what I have so far of tools for building ai products:<p>You'll probably start with an LLM API - OpenAI\n - Cohere and others aren't as good\n - Anthropic's isn't available<p>If you're using embeddings\n - If you're working with a lot of items, you'll want a vector database, like <em>Pinecone</em>, or Weaviate, or pgvector<p>If you're building Q&amp;A over a document\n - I'd suggest using GPT Index<p>If you need to be able to interact with external data sources, do google searches, database lookups, python REPL\n - I'd suggest using langchain<p>If you're doing chained prompts\n - Check out dust tt and langchain<p>If you want to deploy a little app quickly\n - Check out Streamlit<p>If you need to use something like stable diffusion or whisper in your product\n - banana dev, modal, replicate, tiyaro ai, beam cloud, inferrd, or pipeline ai<p>If you need something to optimize your prompts\n - Check out Humanloop and Everyprompt<p>If you're building models and need an ml framework\n - PyTorch, Keras, TensorFlow<p>If you're deploying models to <em>production</em>\n -  Check out MLOps tools like MLflow, Kubeflow, Metaflow, Airflow, Seldon Core, TFServing<p>If you need to check out example projects for inspiration\n - Check out the <em>pinecone</em> op stack, the langchain gallery, the gpt index showcase, and the openai cookbook<p>If you want to browse the latest research, check out arXiv, of course"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: What tools for building AI products do you wish you'd discovered sooner?"}}, "_tags": ["story", "author_tikkun", "story_34764554", "ask_hn"], "author": "tikkun", "children": [34764572, 34764832, 34764922, 34765018], "created_at": "2023-02-12T17:11:48Z", "created_at_i": 1676221908, "num_comments": 7, "objectID": "34764554", "points": 6, "story_id": 34764554, "story_text": "I have a list of ai tools that I&#x27;ve used or want to use in my notes and I&#x27;m cleaning it up so I can share it.<p>Here&#x27;s what I have so far of tools for building ai products:<p>You&#x27;ll probably start with an LLM API - OpenAI\n - Cohere and others aren&#x27;t as good\n - Anthropic&#x27;s isn&#x27;t available<p>If you&#x27;re using embeddings\n - If you&#x27;re working with a lot of items, you&#x27;ll want a vector database, like Pinecone, or Weaviate, or pgvector<p>If you&#x27;re building Q&amp;A over a document\n - I&#x27;d suggest using GPT Index<p>If you need to be able to interact with external data sources, do google searches, database lookups, python REPL\n - I&#x27;d suggest using langchain<p>If you&#x27;re doing chained prompts\n - Check out dust tt and langchain<p>If you want to deploy a little app quickly\n - Check out Streamlit<p>If you need to use something like stable diffusion or whisper in your product\n - banana dev, modal, replicate, tiyaro ai, beam cloud, inferrd, or pipeline ai<p>If you need something to optimize your prompts\n - Check out Humanloop and Everyprompt<p>If you&#x27;re building models and need an ml framework\n - PyTorch, Keras, TensorFlow<p>If you&#x27;re deploying models to production\n -  Check out MLOps tools like MLflow, Kubeflow, Metaflow, Airflow, Seldon Core, TFServing<p>If you need to check out example projects for inspiration\n - Check out the pinecone op stack, the langchain gallery, the gpt index showcase, and the openai cookbook<p>If you want to browse the latest research, check out arXiv, of course", "title": "Ask HN: What tools for building AI products do you wish you'd discovered sooner?", "updated_at": "2025-05-26T04:16:27Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tcarambat1010"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "Hey HN,<p>At Mintplex Labs are building developer tools for AI applications. One area we encountered frustration was the use of Vector Databases like <em>Pinecone</em>, Chroma, QDrant, or Weaviate to &quot;unlock&quot; long-term memory and contextual answers. It is nearly impossible to manage this data when in use for <em>production</em>.<p>The craziest thing was how you cannot atomically CRUD any vectors in most of these vector databases. Let alone easily copy, clone, or migrate data or entire indexes without paying for re-embedding - among other things.<p>With VectorAdmin you get a database level UI with the ability to easily search for embeddings and atomically manage them on top of being a general tool suite for those using vector databases with LLMs.<p>Some things we have unlocked with Vector Admin:\n- Upload data directly into the vector db via a text doc or PDF\n- One click sync of entire existing vector databases\n- Migrating entire db's to another provider to escape vendor lock in\n- Ability to duplicate collections/namespaces to create dev-environments off <em>production</em> data at no cost\n- Be able to finally reset a vector database (provider agnostic)<p>and soon, be able to detect &quot;drift&quot; in semantic search results and catch it early before your <em>production</em> system starts providing wild context snippets.<p>VectorAdmin is open-source or hosted and has a 3-day trial. We really want HN's feedback on the issues or problem you are having wrangling the actual data in a vector database while building any LLM application."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: VectorAdmin \u2013 An open-source vector database management system"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://vectoradmin.com/"}}, "_tags": ["story", "author_tcarambat1010", "story_37846029", "show_hn"], "author": "tcarambat1010", "children": [37855104], "created_at": "2023-10-11T15:48:42Z", "created_at_i": 1697039322, "num_comments": 1, "objectID": "37846029", "points": 6, "story_id": 37846029, "story_text": "Hey HN,<p>At Mintplex Labs are building developer tools for AI applications. One area we encountered frustration was the use of Vector Databases like Pinecone, Chroma, QDrant, or Weaviate to &quot;unlock&quot; long-term memory and contextual answers. It is nearly impossible to manage this data when in use for production.<p>The craziest thing was how you cannot atomically CRUD any vectors in most of these vector databases. Let alone easily copy, clone, or migrate data or entire indexes without paying for re-embedding - among other things.<p>With VectorAdmin you get a database level UI with the ability to easily search for embeddings and atomically manage them on top of being a general tool suite for those using vector databases with LLMs.<p>Some things we have unlocked with Vector Admin:\n- Upload data directly into the vector db via a text doc or PDF\n- One click sync of entire existing vector databases\n- Migrating entire db&#x27;s to another provider to escape vendor lock in\n- Ability to duplicate collections&#x2F;namespaces to create dev-environments off production data at no cost\n- Be able to finally reset a vector database (provider agnostic)<p>and soon, be able to detect &quot;drift&quot; in semantic search results and catch it early before your production system starts providing wild context snippets.<p>VectorAdmin is open-source or hosted and has a 3-day trial. We really want HN&#x27;s feedback on the issues or problem you are having wrangling the actual data in a vector database while building any LLM application.", "title": "Show HN: VectorAdmin \u2013 An open-source vector database management system", "updated_at": "2024-09-20T15:24:21Z", "url": "https://vectoradmin.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "behnamoh"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pinecone", "production"], "value": "There are many options out there:<p>- FAISS<p>- <em>Pinecone</em><p>- Milvus<p>- Qdrant<p>- Weaviate<p>- Elasticsearch<p>- Vespa<p>- pgvector<p>- ScaNN<p>- Vald<p>I wonder which one is <em>production</em>-ready and has better features. I'm trying to stay away from Langchain, so if there are no integrations with Langchain that's fine."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Which Vector Database do you recommend for LLM applications?"}}, "_tags": ["story", "author_behnamoh", "story_36413626", "ask_hn"], "author": "behnamoh", "children": [36414342, 36414352, 36414672, 36415115, 36417118, 36420088, 36520456], "created_at": "2023-06-21T02:25:45Z", "created_at_i": 1687314345, "num_comments": 9, "objectID": "36413626", "points": 5, "story_id": 36413626, "story_text": "There are many options out there:<p>- FAISS<p>- Pinecone<p>- Milvus<p>- Qdrant<p>- Weaviate<p>- Elasticsearch<p>- Vespa<p>- pgvector<p>- ScaNN<p>- Vald<p>I wonder which one is production-ready and has better features. I&#x27;m trying to stay away from Langchain, so if there are no integrations with Langchain that&#x27;s fine.", "title": "Ask HN: Which Vector Database do you recommend for LLM applications?", "updated_at": "2024-09-20T14:22:41Z"}], "hitsPerPage": 15, "nbHits": 21, "nbPages": 2, "page": 0, "params": "query=pinecone+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 12, "processingTimingsMS": {"_request": {"roundTrip": 28}, "afterFetch": {"format": {"highlighting": 1, "total": 2}}, "fetch": {"query": 9, "scanning": 1, "total": 11}, "total": 12}, "query": "pinecone production", "serverTimeMS": 14}}