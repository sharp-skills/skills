{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jeffchuber"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Hey HN - I\u2019m Jeff, co-founder of <em>Chroma</em>.<p>In December of 2022, I was scrolling Twitter in the wee-hours of the morning holding my then-newborn daughter. ChatGPT had launched,  and we were all figuring out what this technology was and how to make it useful. Developers were using retrieval to bring their data to the models - and so I DM\u2019d every person who had tweeted about \u201cembeddings\u201d in the entire month of December. (it was only 120 people!) I saw then how AI was going to need to search to all the world\u2019s information to build useful and reliable applications.<p>Anton Troynikov and I started <em>Chroma</em> with the beliefs that:<p>1. AI-based systems were way too difficult to <em>production</em>ize<p>2. Latent space was incredibly important to improving AI-based systems (no one understood this at the time)<p>On Valentines Day 2023, we launched first version of <em>Chroma</em> and it immediately took off. <em>Chroma</em> made retrieval just work. <em>Chroma</em> is now a large open-source project with 21k+ stars and 5M monthly downloads, used at companies like Apple, Amazon, Salesforce, and Microsoft.<p>Today we\u2019re excited to launch <em>Chroma</em> Cloud - our fully-managed offering backed by an Apache 2.0 serverless database called <em>Chroma</em> Distributed.  <em>Chroma</em> Distributed is written in Rust and uses object-storage for extreme scalability and reliability. <em>Chroma</em> Cloud is fast and cheap. Leading AI companies such as Factory, Weights &amp; Biases, Propel, and Foam already use <em>Chroma</em> Cloud in <em>production</em> to power their agents. It brings the \u201cit just works\u201d developer experience developers have come to know <em>Chroma</em> for - to the Cloud.<p>Try it out and let me know what you think!<p>\u2014 Jeff"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["chroma"], "value": "Show HN: <em>Chroma</em> Cloud \u2013 serverless search database for AI"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://trychroma.com/cloud"}}, "_tags": ["story", "author_jeffchuber", "story_44944241", "show_hn"], "author": "jeffchuber", "children": [44954123, 44954504, 44954540, 44954547, 44955088, 44956068, 44956240, 44956372, 44956589, 44957763, 44957979, 44958085, 44958408, 44958868, 44960398, 44961178], "created_at": "2025-08-18T19:20:01Z", "created_at_i": 1755544801, "num_comments": 40, "objectID": "44944241", "points": 93, "story_id": 44944241, "story_text": "Hey HN - I\u2019m Jeff, co-founder of Chroma.<p>In December of 2022, I was scrolling Twitter in the wee-hours of the morning holding my then-newborn daughter. ChatGPT had launched,  and we were all figuring out what this technology was and how to make it useful. Developers were using retrieval to bring their data to the models - and so I DM\u2019d every person who had tweeted about \u201cembeddings\u201d in the entire month of December. (it was only 120 people!) I saw then how AI was going to need to search to all the world\u2019s information to build useful and reliable applications.<p>Anton Troynikov and I started Chroma with the beliefs that:<p>1. AI-based systems were way too difficult to productionize<p>2. Latent space was incredibly important to improving AI-based systems (no one understood this at the time)<p>On Valentines Day 2023, we launched first version of Chroma and it immediately took off. Chroma made retrieval just work. Chroma is now a large open-source project with 21k+ stars and 5M monthly downloads, used at companies like Apple, Amazon, Salesforce, and Microsoft.<p>Today we\u2019re excited to launch Chroma Cloud - our fully-managed offering backed by an Apache 2.0 serverless database called Chroma Distributed.  Chroma Distributed is written in Rust and uses object-storage for extreme scalability and reliability. Chroma Cloud is fast and cheap. Leading AI companies such as Factory, Weights &amp; Biases, Propel, and Foam already use Chroma Cloud in production to power their agents. It brings the \u201cit just works\u201d developer experience developers have come to know Chroma for - to the Cloud.<p>Try it out and let me know what you think!<p>\u2014 Jeff", "title": "Show HN: Chroma Cloud \u2013 serverless search database for AI", "updated_at": "2026-02-03T13:15:37Z", "url": "https://trychroma.com/cloud"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kmassimilian"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["chroma"], "value": "Been working on a robust Q&amp;A app for enterprise. I used llamaindex (+langchain) as a pipeline. Started using <em>Chroma</em> for my vector db, which worked pretty well, but I realized that my app runs faster when I store the indices in an S3 bucket rather than use <em>Chroma</em> to store my embeddings and generate the index from these embeddings at query time. Are there tradeoffs I'm making in using a pre-built index in S3 rather than a vector db to stash embeddings? Has anyone come across this kind of consideration? I've looked at Weaviate (offers hybrid search) but haven't decided to retool code based around it. Basically, I'm just looking for whichever implementation will result in the fastest response times (knowledge base size is 'large' ~40GB).<p>RE Weaviate, this looks interesting: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb<p>Further and related, has anyone tried to embed a larger amount of data before? I estimated total time using CPU ~29 hours. With GPU I've seen demos reducing this to minutes. https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray<p>TY"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Design considerations for RAG application in <em>production</em> mode"}}, "_tags": ["story", "author_kmassimilian", "story_37123940", "ask_hn"], "author": "kmassimilian", "created_at": "2023-08-14T17:34:27Z", "created_at_i": 1692034467, "num_comments": 0, "objectID": "37123940", "points": 3, "story_id": 37123940, "story_text": "Been working on a robust Q&amp;A app for enterprise. I used llamaindex (+langchain) as a pipeline. Started using Chroma for my vector db, which worked pretty well, but I realized that my app runs faster when I store the indices in an S3 bucket rather than use Chroma to store my embeddings and generate the index from these embeddings at query time. Are there tradeoffs I&#x27;m making in using a pre-built index in S3 rather than a vector db to stash embeddings? Has anyone come across this kind of consideration? I&#x27;ve looked at Weaviate (offers hybrid search) but haven&#x27;t decided to retool code based around it. Basically, I&#x27;m just looking for whichever implementation will result in the fastest response times (knowledge base size is &#x27;large&#x27; ~40GB).<p>RE Weaviate, this looks interesting: https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-cookbook&#x2F;blob&#x2F;main&#x2F;examples&#x2F;vector_databases&#x2F;weaviate&#x2F;hybrid-search-with-weaviate-and-openai.ipynb<p>Further and related, has anyone tried to embed a larger amount of data before? I estimated total time using CPU ~29 hours. With GPU I&#x27;ve seen demos reducing this to minutes. https:&#x2F;&#x2F;www.anyscale.com&#x2F;blog&#x2F;build-and-scale-a-powerful-query-engine-with-llamaindex-ray<p>TY", "title": "Design considerations for RAG application in production mode", "updated_at": "2024-09-20T14:47:35Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "r_thambapillai"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Hi Hacker News! We\u2019re Ravin and Jack, the founders of Credal.ai (<a href=\"https://www.credal.ai/\">https://www.credal.ai/</a>). We provide a Chat UI and APIs that enforce PII redaction, audit logging, and data access controls for companies that want to use LLMs with their corporate data from Google Docs, Slack, or Confluence. There\u2019s a demo video here: <a href=\"https://www.loom.com/share/2b5409fd64464dc9b5b6277f2be4e90f?sid=7a728d97-58ac-4355-9c87-75eaf12b0775\" rel=\"nofollow noreferrer\">https://www.loom.com/share/2b5409fd64464dc9b5b6277f2be4e90f?...</a>.<p>One big thing enterprises and businesses are worried about with LLMs is \u201cwhat\u2019s happening to my data\u201d? The way we see it, there are three big security and privacy barriers companies need to solve:<p>1. Controlling what data goes to whom: the basic stuff is just putting controls in place around customer and employee PII, but it can get trickier when you also want to be putting controls in place around business secrets, so companies can ensure the Coca Cola recipe doesn\u2019t accidentally leave the company.<p>2. Visibility: Enterprise IT wants to know exactly what data was shared by whom, when, at what time, and what the model responded with (not to mention how much the request cost!). Each provider gives you a piece of the puzzle in their dashboard, but getting all this visibility per request from either of the main providers currently requires writing code yourself.<p>3. Access Controls: Enterprises have lots of documents that for whatever reason cannot be shared internally to everyone. So how do I make sure employees can use AI with this stuff, without compromising the sensitivity of the data?<p>Typically this pain is something that is felt most acutely by Enterprise IT, but also of course by the developers and business people who get told not to build the great stuff they can envision.\nWe think it\u2019s critical to solve these issues since the more visibility and control we can give Enterprise IT about how data is used, the more we can actually build on top of these APIs and start applying some of the awesome capabilities of the foundation models across every business problem.<p>You can easily grab data from sources like Google Docs via their APIs, but for <em>production</em> use cases, you have to respect the permissions on each Google Doc, Confluence Page, Slack channel etc. This gets tricky when these systems combine some permissions defined totally inside their product, with permissions that are inherited from the company\u2019s SSO provider (often Okta or Azure AD). Respecting all these permissions becomes both hard and vital as the number of employees and tools accessing the data grows.<p>The current state of the art is to use a vector database like Pinecone, Milvus, or <em>Chroma</em>, integrate your internal data with those systems, and then when a user asks a question, dynamically figure out which bits are relevant to the user\u2019s question and send those to the AI as part of the prompt. We handle all this automatically for you (using Milvus for now, which we host ourselves), including the point and click connectors for your data (Google Docs/Sheets, Slack, Confluence with many more coming soon). You can use that data through our UI already and we\u2019re in the process of adding this search functionality to the API as well.<p>There\u2019s other schlep work that devs would rather not worry about: building out request level audit logs, staying on top of the rapidly changing API formats from these providers, implementing failover for when these heavily overburdened APIs go down etc,  We think  individual devs should not have to do these themselves, but the foundation model providers are unlikely to provide consistent, customer centric approaches for them. The PII detection piece in some ways is the easiest - there are a lot of good open source models for doing this, and companies using Azure OpenAI and AWS Bedrock seem less concerned with it anyway. We expect that the emphasis companies place on the redactions we provide may actually go down over time, while the emphasis on unified, consistent audit logging and data access controls will increase.<p>Right now we have three plans: a free tier (which is admittedly very limited but intended to give you a feel for the product), the business plan which starts at $500pm which gets you access to the data integration as well as the most powerful models like GPT 4 32k, Anthropic 100k etc, and an enterprise plan which starts at $5000pm, which is a scaled up version of the business tier and lets you go on-prem (more details on each plan are on the website). You can try the free tier self-serve, but we haven\u2019t yet built out fully self service onboarding for the paid plans so for now it is a \u201cbook a meeting\u201d button, apologies! (But it only takes 5 minutes and if you want it, we can fully onboard you in the meeting itself).<p>When Jack and I started Credal, we actually set out to solve a different problem: an \u2018AI Chief of Staff\u2019 that could read your documents and task trackers, and guide your strategic decision making. We knew that data security was going to be a critical problem for enterprises. Jack and I were both deep in the Enterprise Data Security + AI space before Credal, so we naturally took a security first approach to building out our AI Chief of Staff. But in reality, when we started showing the product to customers, we learned pretty fast that the \u2018Chief of Staff\u2019 features were at best nice to have, and the security features were what they were actually excited by. So we stripped the product back to basics, and built out the thing our customers actually needed. Since then we\u2019ve signed a bunch of customers and thousands of users, which has been really exciting.<p>Now that our product is concretely helping a bunch of people at work, is SOC 2 T1 Compliant, and is ready for anyone to just walk up and use, we\u2019re super excited to share it with the Hacker News community, which Jack and I have been avid readers of for a decade now. It\u2019s still a very early product (the private beta opened in March), but we can\u2019t wait to get your feedback and see how we can make it even better!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Credal.ai (YC W23) \u2013 Data Safety for Enterprise AI"}}, "_tags": ["story", "author_r_thambapillai", "story_36326525", "launch_hn"], "author": "r_thambapillai", "children": [36326917, 36326962, 36327564, 36327940, 36328429, 36328819, 36330020, 36330371, 36333611, 36357825], "created_at": "2023-06-14T14:26:55Z", "created_at_i": 1686752815, "num_comments": 24, "objectID": "36326525", "points": 114, "story_id": 36326525, "story_text": "Hi Hacker News! We\u2019re Ravin and Jack, the founders of Credal.ai (<a href=\"https:&#x2F;&#x2F;www.credal.ai&#x2F;\">https:&#x2F;&#x2F;www.credal.ai&#x2F;</a>). We provide a Chat UI and APIs that enforce PII redaction, audit logging, and data access controls for companies that want to use LLMs with their corporate data from Google Docs, Slack, or Confluence. There\u2019s a demo video here: <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;2b5409fd64464dc9b5b6277f2be4e90f?sid=7a728d97-58ac-4355-9c87-75eaf12b0775\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;2b5409fd64464dc9b5b6277f2be4e90f?...</a>.<p>One big thing enterprises and businesses are worried about with LLMs is \u201cwhat\u2019s happening to my data\u201d? The way we see it, there are three big security and privacy barriers companies need to solve:<p>1. Controlling what data goes to whom: the basic stuff is just putting controls in place around customer and employee PII, but it can get trickier when you also want to be putting controls in place around business secrets, so companies can ensure the Coca Cola recipe doesn\u2019t accidentally leave the company.<p>2. Visibility: Enterprise IT wants to know exactly what data was shared by whom, when, at what time, and what the model responded with (not to mention how much the request cost!). Each provider gives you a piece of the puzzle in their dashboard, but getting all this visibility per request from either of the main providers currently requires writing code yourself.<p>3. Access Controls: Enterprises have lots of documents that for whatever reason cannot be shared internally to everyone. So how do I make sure employees can use AI with this stuff, without compromising the sensitivity of the data?<p>Typically this pain is something that is felt most acutely by Enterprise IT, but also of course by the developers and business people who get told not to build the great stuff they can envision.\nWe think it\u2019s critical to solve these issues since the more visibility and control we can give Enterprise IT about how data is used, the more we can actually build on top of these APIs and start applying some of the awesome capabilities of the foundation models across every business problem.<p>You can easily grab data from sources like Google Docs via their APIs, but for production use cases, you have to respect the permissions on each Google Doc, Confluence Page, Slack channel etc. This gets tricky when these systems combine some permissions defined totally inside their product, with permissions that are inherited from the company\u2019s SSO provider (often Okta or Azure AD). Respecting all these permissions becomes both hard and vital as the number of employees and tools accessing the data grows.<p>The current state of the art is to use a vector database like Pinecone, Milvus, or Chroma, integrate your internal data with those systems, and then when a user asks a question, dynamically figure out which bits are relevant to the user\u2019s question and send those to the AI as part of the prompt. We handle all this automatically for you (using Milvus for now, which we host ourselves), including the point and click connectors for your data (Google Docs&#x2F;Sheets, Slack, Confluence with many more coming soon). You can use that data through our UI already and we\u2019re in the process of adding this search functionality to the API as well.<p>There\u2019s other schlep work that devs would rather not worry about: building out request level audit logs, staying on top of the rapidly changing API formats from these providers, implementing failover for when these heavily overburdened APIs go down etc,  We think  individual devs should not have to do these themselves, but the foundation model providers are unlikely to provide consistent, customer centric approaches for them. The PII detection piece in some ways is the easiest - there are a lot of good open source models for doing this, and companies using Azure OpenAI and AWS Bedrock seem less concerned with it anyway. We expect that the emphasis companies place on the redactions we provide may actually go down over time, while the emphasis on unified, consistent audit logging and data access controls will increase.<p>Right now we have three plans: a free tier (which is admittedly very limited but intended to give you a feel for the product), the business plan which starts at $500pm which gets you access to the data integration as well as the most powerful models like GPT 4 32k, Anthropic 100k etc, and an enterprise plan which starts at $5000pm, which is a scaled up version of the business tier and lets you go on-prem (more details on each plan are on the website). You can try the free tier self-serve, but we haven\u2019t yet built out fully self service onboarding for the paid plans so for now it is a \u201cbook a meeting\u201d button, apologies! (But it only takes 5 minutes and if you want it, we can fully onboard you in the meeting itself).<p>When Jack and I started Credal, we actually set out to solve a different problem: an \u2018AI Chief of Staff\u2019 that could read your documents and task trackers, and guide your strategic decision making. We knew that data security was going to be a critical problem for enterprises. Jack and I were both deep in the Enterprise Data Security + AI space before Credal, so we naturally took a security first approach to building out our AI Chief of Staff. But in reality, when we started showing the product to customers, we learned pretty fast that the \u2018Chief of Staff\u2019 features were at best nice to have, and the security features were what they were actually excited by. So we stripped the product back to basics, and built out the thing our customers actually needed. Since then we\u2019ve signed a bunch of customers and thousands of users, which has been really exciting.<p>Now that our product is concretely helping a bunch of people at work, is SOC 2 T1 Compliant, and is ready for anyone to just walk up and use, we\u2019re super excited to share it with the Hacker News community, which Jack and I have been avid readers of for a decade now. It\u2019s still a very early product (the private beta opened in March), but we can\u2019t wait to get your feedback and see how we can make it even better!", "title": "Launch HN: Credal.ai (YC W23) \u2013 Data Safety for Enterprise AI", "updated_at": "2024-09-20T14:21:48Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "lokahdev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "I built [VectorLiteDB (<a href=\"https://github.com/vectorlitedb/vectorlitedb\" rel=\"nofollow\">https://github.com/vectorlitedb/vectorlitedb</a>)<p>\u2014 a simple, embedded vector database that stores everything in a single file, just like SQLite.<p>The problem:<p>If you\u2019re a developer building AI apps, you usually have two choices for vector search<p>- Set up a server (e.g. <em>Chroma</em>, Weaviate)  \n- Use a cloud service (e.g. Pinecone)<p>That works for <em>production</em>, but it\u2019s overkill when you just want to:<p>- Quickly prototype with embeddings  \n- Run offline without cloud dependencies  \n- Keep your data portable in a single file<p>The inspiration was *SQLite* during development \u2014 simple, local, and reliable.<p>The solution:<p>So I built VectorLiteDB<p>- Single-file, embedded, no server  \n- Stores vectors + metadata, persists to disk  \n- Supports cosine / L2 / dot similarity  \n- Works offline, ~100ms for 10K vectors  \n- Perfect for local RAG, prototyping or personal AI memory<p>Feedback on both the tool and the approach would be really helpful.<p>- Is this something that would be useful\n- Use cases you\u2019d try this for"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: VectorLiteDB \u2013 a vector DB for local dev, like SQLite but for vectors"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/vectorlitedb/vectorlitedb"}}, "_tags": ["story", "author_lokahdev", "story_45319922", "show_hn"], "author": "lokahdev", "children": [45322272, 45336263], "created_at": "2025-09-21T03:54:25Z", "created_at_i": 1758426865, "num_comments": 4, "objectID": "45319922", "points": 13, "story_id": 45319922, "story_text": "I built [VectorLiteDB (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;vectorlitedb&#x2F;vectorlitedb\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vectorlitedb&#x2F;vectorlitedb</a>)<p>\u2014 a simple, embedded vector database that stores everything in a single file, just like SQLite.<p>The problem:<p>If you\u2019re a developer building AI apps, you usually have two choices for vector search<p>- Set up a server (e.g. Chroma, Weaviate)  \n- Use a cloud service (e.g. Pinecone)<p>That works for production, but it\u2019s overkill when you just want to:<p>- Quickly prototype with embeddings  \n- Run offline without cloud dependencies  \n- Keep your data portable in a single file<p>The inspiration was *SQLite* during development \u2014 simple, local, and reliable.<p>The solution:<p>So I built VectorLiteDB<p>- Single-file, embedded, no server  \n- Stores vectors + metadata, persists to disk  \n- Supports cosine &#x2F; L2 &#x2F; dot similarity  \n- Works offline, ~100ms for 10K vectors  \n- Perfect for local RAG, prototyping or personal AI memory<p>Feedback on both the tool and the approach would be really helpful.<p>- Is this something that would be useful\n- Use cases you\u2019d try this for", "title": "Show HN: VectorLiteDB \u2013 a vector DB for local dev, like SQLite but for vectors", "updated_at": "2025-09-26T11:39:07Z", "url": "https://github.com/vectorlitedb/vectorlitedb"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tcarambat1010"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Hey HN,<p>At Mintplex Labs are building developer tools for AI applications. One area we encountered frustration was the use of Vector Databases like Pinecone, <em>Chroma</em>, QDrant, or Weaviate to &quot;unlock&quot; long-term memory and contextual answers. It is nearly impossible to manage this data when in use for <em>production</em>.<p>The craziest thing was how you cannot atomically CRUD any vectors in most of these vector databases. Let alone easily copy, clone, or migrate data or entire indexes without paying for re-embedding - among other things.<p>With VectorAdmin you get a database level UI with the ability to easily search for embeddings and atomically manage them on top of being a general tool suite for those using vector databases with LLMs.<p>Some things we have unlocked with Vector Admin:\n- Upload data directly into the vector db via a text doc or PDF\n- One click sync of entire existing vector databases\n- Migrating entire db's to another provider to escape vendor lock in\n- Ability to duplicate collections/namespaces to create dev-environments off <em>production</em> data at no cost\n- Be able to finally reset a vector database (provider agnostic)<p>and soon, be able to detect &quot;drift&quot; in semantic search results and catch it early before your <em>production</em> system starts providing wild context snippets.<p>VectorAdmin is open-source or hosted and has a 3-day trial. We really want HN's feedback on the issues or problem you are having wrangling the actual data in a vector database while building any LLM application."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: VectorAdmin \u2013 An open-source vector database management system"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://vectoradmin.com/"}}, "_tags": ["story", "author_tcarambat1010", "story_37846029", "show_hn"], "author": "tcarambat1010", "children": [37855104], "created_at": "2023-10-11T15:48:42Z", "created_at_i": 1697039322, "num_comments": 1, "objectID": "37846029", "points": 6, "story_id": 37846029, "story_text": "Hey HN,<p>At Mintplex Labs are building developer tools for AI applications. One area we encountered frustration was the use of Vector Databases like Pinecone, Chroma, QDrant, or Weaviate to &quot;unlock&quot; long-term memory and contextual answers. It is nearly impossible to manage this data when in use for production.<p>The craziest thing was how you cannot atomically CRUD any vectors in most of these vector databases. Let alone easily copy, clone, or migrate data or entire indexes without paying for re-embedding - among other things.<p>With VectorAdmin you get a database level UI with the ability to easily search for embeddings and atomically manage them on top of being a general tool suite for those using vector databases with LLMs.<p>Some things we have unlocked with Vector Admin:\n- Upload data directly into the vector db via a text doc or PDF\n- One click sync of entire existing vector databases\n- Migrating entire db&#x27;s to another provider to escape vendor lock in\n- Ability to duplicate collections&#x2F;namespaces to create dev-environments off production data at no cost\n- Be able to finally reset a vector database (provider agnostic)<p>and soon, be able to detect &quot;drift&quot; in semantic search results and catch it early before your production system starts providing wild context snippets.<p>VectorAdmin is open-source or hosted and has a 3-day trial. We really want HN&#x27;s feedback on the issues or problem you are having wrangling the actual data in a vector database while building any LLM application.", "title": "Show HN: VectorAdmin \u2013 An open-source vector database management system", "updated_at": "2024-09-20T15:24:21Z", "url": "https://vectoradmin.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yxchen1994"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "I built BPM Finder, a comprehensive audio analysis toolkit that provides three professional-grade tools: BPM detection, musical key identification, and tempo adjustment \u0013 all running entirely in your browser.<p>## What makes it different:<p>*Privacy-first approach*: Your audio files never leave your device. All processing happens client-side using Web Audio API, rubberband-wasm, and advanced algorithms.<p>*Three powerful tools in one*:\n- *BPM Finder*: Dual-algorithm analysis with 99.5% accuracy and confidence scoring\n- *Key Finder*: <em>Chroma</em> analysis with correlation matrices for precise key detection  \n- *Tempo Change*: Adjust audio speed without pitch changes using professional rubberband algorithm<p>*Multiple analysis modes*:\n- Single file analysis with detailed results\n- Batch processing (up to 50 files simultaneously)\n- Real-time tap tempo for live BPM detection<p>## Technical highlights:<p>- Built with Next.js 15, TypeScript, and Web Workers for intensive processing\n- Supports MP3, WAV, FLAC, AAC, OGG, M4A (up to 50MB each)\n- No registration required, completely free to use\n- Export results to CSV or integrate with DAW/DJ software<p>## Real-world use cases:<p>Professional DJs use it for beatmatching in live sets, music producers organize sample libraries by BPM, fitness instructors create tempo-matched workout playlists, and video editors sync background music to cuts.<p>The project started when I needed reliable BPM detection for my own music <em>production</em> work but couldn't find tools that were both accurate and privacy-respecting. After months of research into audio analysis algorithms and Web Audio API optimization, I built this comprehensive solution.<p>*Try it*: <a href=\"https://bpm-finder.net/\" rel=\"nofollow\">https://bpm-finder.net/</a><p>Would love feedback from the HN community, especially from fellow musicians, developers working with audio processing, or anyone who's tackled similar browser-based analysis challenges.<p>*Tech stack*: Next.js, TypeScript, Web Audio API, rubberband-wasm, web-audio-beat-detector, Tailwind CSS"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: BPM Finder: Advanced Audio Analysis Toolkit"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://bpm-finder.net"}}, "_tags": ["story", "author_yxchen1994", "story_45182222", "show_hn"], "author": "yxchen1994", "children": [45184881], "created_at": "2025-09-09T14:12:49Z", "created_at_i": 1757427169, "num_comments": 1, "objectID": "45182222", "points": 3, "story_id": 45182222, "story_text": "I built BPM Finder, a comprehensive audio analysis toolkit that provides three professional-grade tools: BPM detection, musical key identification, and tempo adjustment \u0013 all running entirely in your browser.<p>## What makes it different:<p>*Privacy-first approach*: Your audio files never leave your device. All processing happens client-side using Web Audio API, rubberband-wasm, and advanced algorithms.<p>*Three powerful tools in one*:\n- *BPM Finder*: Dual-algorithm analysis with 99.5% accuracy and confidence scoring\n- *Key Finder*: Chroma analysis with correlation matrices for precise key detection  \n- *Tempo Change*: Adjust audio speed without pitch changes using professional rubberband algorithm<p>*Multiple analysis modes*:\n- Single file analysis with detailed results\n- Batch processing (up to 50 files simultaneously)\n- Real-time tap tempo for live BPM detection<p>## Technical highlights:<p>- Built with Next.js 15, TypeScript, and Web Workers for intensive processing\n- Supports MP3, WAV, FLAC, AAC, OGG, M4A (up to 50MB each)\n- No registration required, completely free to use\n- Export results to CSV or integrate with DAW&#x2F;DJ software<p>## Real-world use cases:<p>Professional DJs use it for beatmatching in live sets, music producers organize sample libraries by BPM, fitness instructors create tempo-matched workout playlists, and video editors sync background music to cuts.<p>The project started when I needed reliable BPM detection for my own music production work but couldn&#x27;t find tools that were both accurate and privacy-respecting. After months of research into audio analysis algorithms and Web Audio API optimization, I built this comprehensive solution.<p>*Try it*: <a href=\"https:&#x2F;&#x2F;bpm-finder.net&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;bpm-finder.net&#x2F;</a><p>Would love feedback from the HN community, especially from fellow musicians, developers working with audio processing, or anyone who&#x27;s tackled similar browser-based analysis challenges.<p>*Tech stack*: Next.js, TypeScript, Web Audio API, rubberband-wasm, web-audio-beat-detector, Tailwind CSS", "title": "Show HN: BPM Finder: Advanced Audio Analysis Toolkit", "updated_at": "2025-11-02T07:01:50Z", "url": "https://bpm-finder.net"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "david1542"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Hey all,<p>I\u2019ve been working on an AI agent system over the past year that connects to internal company tools like Slack, GitHub, Notion, etc, to help investigate <em>production</em> incidents. The agent needs context, so we built a system that ingests this data, processes it, and builds a structured knowledge graph (kind of a mix of RAG and GraphRAG).<p>What we didn\u2019t expect was just how much infra work that would require.<p>We ended up:<p>- Using LlamaIndex's OS abstractions for chunking, embedding and retrieval.<p>- Adopting <em>Chroma</em> as the vector store.<p>- Writing custom integrations for Slack/GitHub/Notion. We used LlamaHub here for the actual querying, although some parts were a bit unmaintained and we had to fork + fix. We could\u2019ve used Nango or Airbyte tbh but eventually didn't do that.<p>- Building an auto-refresh pipeline to sync data every few hours and do diffs based on timestamps. This was pretty hard as well.<p>- Handling security and privacy (most customers needed to keep data in their own environments).<p>- Handling scale - some orgs had hundreds of thousands of documents across different tools.<p>It became clear we were spending a lot more time on data infrastructure than on the actual agent logic. I think it might be ok for a company that interacts with customers' data, but definitely we felt like we were dealing with a lot of non-core work.<p>So I\u2019m curious: for folks building LLM apps that connect to company systems, how are you approaching this? Are you building it all from scratch too? Using open-source tools? Is there something obvious we\u2019re missing?<p>Would really appreciate hearing how others are tackling this part of the stack."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: How do you build per-user RAG/GraphRAG"}}, "_tags": ["story", "author_david1542", "story_43772702", "ask_hn"], "author": "david1542", "children": [43772828, 43772937], "created_at": "2025-04-23T14:34:23Z", "created_at_i": 1745418863, "num_comments": 4, "objectID": "43772702", "points": 2, "story_id": 43772702, "story_text": "Hey all,<p>I\u2019ve been working on an AI agent system over the past year that connects to internal company tools like Slack, GitHub, Notion, etc, to help investigate production incidents. The agent needs context, so we built a system that ingests this data, processes it, and builds a structured knowledge graph (kind of a mix of RAG and GraphRAG).<p>What we didn\u2019t expect was just how much infra work that would require.<p>We ended up:<p>- Using LlamaIndex&#x27;s OS abstractions for chunking, embedding and retrieval.<p>- Adopting Chroma as the vector store.<p>- Writing custom integrations for Slack&#x2F;GitHub&#x2F;Notion. We used LlamaHub here for the actual querying, although some parts were a bit unmaintained and we had to fork + fix. We could\u2019ve used Nango or Airbyte tbh but eventually didn&#x27;t do that.<p>- Building an auto-refresh pipeline to sync data every few hours and do diffs based on timestamps. This was pretty hard as well.<p>- Handling security and privacy (most customers needed to keep data in their own environments).<p>- Handling scale - some orgs had hundreds of thousands of documents across different tools.<p>It became clear we were spending a lot more time on data infrastructure than on the actual agent logic. I think it might be ok for a company that interacts with customers&#x27; data, but definitely we felt like we were dealing with a lot of non-core work.<p>So I\u2019m curious: for folks building LLM apps that connect to company systems, how are you approaching this? Are you building it all from scratch too? Using open-source tools? Is there something obvious we\u2019re missing?<p>Would really appreciate hearing how others are tackling this part of the stack.", "title": "Ask HN: How do you build per-user RAG/GraphRAG", "updated_at": "2025-04-23T18:47:25Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mimchak"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "I launched Embex two weeks ago and hit 9,000 downloads (7K PyPI, 2K npm).<p>Embex is a universal ORM for vector databases. One API that works across LanceDB, Qdrant, Pinecone, <em>Chroma</em>, PgVector, Milvus, and Weaviate.<p>The problem: Every vector database has a different API. Switching from Pinecone to Qdrant means rewriting your data layer.<p>Example:\n```python\n# Works with ANY provider\nclient = await EmbexClient.new_async(provider=&quot;lancedb&quot;, url=&quot;./data&quot;)\nawait client.insert(&quot;products&quot;, vectors)\nresults = await client.search(&quot;products&quot;, vector=query, top_k=5)<p># Switch to Qdrant? Change one line:\nclient = await EmbexClient.new_async(provider=&quot;qdrant&quot;, url=&quot;http://localhost:6333&quot;)\n```<p>Built with Rust core + SIMD acceleration (4x faster than pure Python/JS). Available for Python and Node.js.<p>What happened after launch:\n- Published to PyPI and npm\n- Downloads started coming in organically\n- 9K downloads in 2 weeks<p>I'm honestly not sure where most of the traffic came from. PyPI/npm search probably, but I haven't dug into the analytics deeply. I just made one LinkedIn post which didn't get any likes/comments.<p>Start local with LanceDB (embedded, zero Docker), then switch to <em>production</em> databases (Qdrant, Pinecone, Milvus) without changing code.<p>GitHub: <a href=\"https://github.com/bridgerust/bridgerust\" rel=\"nofollow\">https://github.com/bridgerust/bridgerust</a>\nDocs: <a href=\"https://bridgerust.dev/embex/introduction\" rel=\"nofollow\">https://bridgerust.dev/embex/introduction</a><p>Would love feedback on:\n- API design decisions\n- Which databases to support next\n- Performance optimizations<p>Built using BridgeRust, a framework for creating cross-language Rust libraries (also open source in this repo)."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Embex \u2013 9K organic downloads in 2 weeks with zero marketing"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.bridgerust.dev/embex/introduction/"}}, "_tags": ["story", "author_mimchak", "story_46571103", "show_hn"], "author": "mimchak", "created_at": "2026-01-10T23:37:26Z", "created_at_i": 1768088246, "num_comments": 0, "objectID": "46571103", "points": 2, "story_id": 46571103, "story_text": "I launched Embex two weeks ago and hit 9,000 downloads (7K PyPI, 2K npm).<p>Embex is a universal ORM for vector databases. One API that works across LanceDB, Qdrant, Pinecone, Chroma, PgVector, Milvus, and Weaviate.<p>The problem: Every vector database has a different API. Switching from Pinecone to Qdrant means rewriting your data layer.<p>Example:\n```python\n# Works with ANY provider\nclient = await EmbexClient.new_async(provider=&quot;lancedb&quot;, url=&quot;.&#x2F;data&quot;)\nawait client.insert(&quot;products&quot;, vectors)\nresults = await client.search(&quot;products&quot;, vector=query, top_k=5)<p># Switch to Qdrant? Change one line:\nclient = await EmbexClient.new_async(provider=&quot;qdrant&quot;, url=&quot;http:&#x2F;&#x2F;localhost:6333&quot;)\n```<p>Built with Rust core + SIMD acceleration (4x faster than pure Python&#x2F;JS). Available for Python and Node.js.<p>What happened after launch:\n- Published to PyPI and npm\n- Downloads started coming in organically\n- 9K downloads in 2 weeks<p>I&#x27;m honestly not sure where most of the traffic came from. PyPI&#x2F;npm search probably, but I haven&#x27;t dug into the analytics deeply. I just made one LinkedIn post which didn&#x27;t get any likes&#x2F;comments.<p>Start local with LanceDB (embedded, zero Docker), then switch to production databases (Qdrant, Pinecone, Milvus) without changing code.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;bridgerust&#x2F;bridgerust\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;bridgerust&#x2F;bridgerust</a>\nDocs: <a href=\"https:&#x2F;&#x2F;bridgerust.dev&#x2F;embex&#x2F;introduction\" rel=\"nofollow\">https:&#x2F;&#x2F;bridgerust.dev&#x2F;embex&#x2F;introduction</a><p>Would love feedback on:\n- API design decisions\n- Which databases to support next\n- Performance optimizations<p>Built using BridgeRust, a framework for creating cross-language Rust libraries (also open source in this repo).", "title": "Show HN: Embex \u2013 9K organic downloads in 2 weeks with zero marketing", "updated_at": "2026-01-11T08:55:11Z", "url": "https://www.bridgerust.dev/embex/introduction/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Tuisto"}, "story_text": {"matchLevel": "none", "matchedWords": [], "value": ""}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Using clang for <em>Chrome</em> <em>production</em> builds on Linux"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "http://blog.llvm.org/2015/01/using-clang-for-<em>chrome</em>-<em>production</em>.html"}}, "_tags": ["story", "author_Tuisto", "story_8843652"], "author": "Tuisto", "created_at": "2015-01-06T10:14:03Z", "created_at_i": 1420539243, "num_comments": 0, "objectID": "8843652", "points": 5, "story_id": 8843652, "story_text": "", "title": "Using clang for Chrome production builds on Linux", "updated_at": "2024-09-19T21:27:25Z", "url": "http://blog.llvm.org/2015/01/using-clang-for-chrome-production.html"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mrskitch"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Phantom pain: the first year running headless <em>Chrome</em> in <em>production</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://docs.browserless.io/blog/2019/01/07/headless-browser-practices.html"}}, "_tags": ["story", "author_mrskitch", "story_18848908"], "author": "mrskitch", "created_at": "2019-01-07T19:18:17Z", "created_at_i": 1546888697, "num_comments": 0, "objectID": "18848908", "points": 4, "story_id": 18848908, "title": "Phantom pain: the first year running headless Chrome in production", "updated_at": "2024-09-20T03:32:01Z", "url": "https://docs.browserless.io/blog/2019/01/07/headless-browser-practices.html"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "gokulsiva"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Curious what broke first \u2014 pagination, tables, fonts, or maintainability.<p>Context: I\u2019m asking because I\u2019m building an open-source tool around\nserver-side HTML \u2192 PDF and keep seeing teams try \u201clighter\u201d libraries\nfirst, then fall back to headless <em>Chrome</em> in <em>production</em>.<p>Curious where that tipping point was for you."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: What made you move back to HTML-to-PDF in <em>production</em>?"}}, "_tags": ["story", "author_gokulsiva", "story_46602288", "ask_hn"], "author": "gokulsiva", "children": [46602529, 46602551, 46615783], "created_at": "2026-01-13T15:32:24Z", "created_at_i": 1768318344, "num_comments": 7, "objectID": "46602288", "points": 6, "story_id": 46602288, "story_text": "Curious what broke first \u2014 pagination, tables, fonts, or maintainability.<p>Context: I\u2019m asking because I\u2019m building an open-source tool around\nserver-side HTML \u2192 PDF and keep seeing teams try \u201clighter\u201d libraries\nfirst, then fall back to headless Chrome in production.<p>Curious where that tipping point was for you.", "title": "Ask HN: What made you move back to HTML-to-PDF in production?", "updated_at": "2026-01-15T17:07:00Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "smharris65"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Google pushed a one-character typo to <em>production</em>, bricking <em>Chrome</em> OS devices"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "https://arstechnica.com/gadgets/2021/07/google-pushed-a-one-character-typo-to-<em>production</em>-bricking-<em>chrome</em>-os-devices/?comments=1"}}, "_tags": ["story", "author_smharris65", "story_27924469"], "author": "smharris65", "children": [27924576], "created_at": "2021-07-22T21:17:30Z", "created_at_i": 1626988650, "num_comments": 1, "objectID": "27924469", "points": 4, "story_id": 27924469, "title": "Google pushed a one-character typo to production, bricking Chrome OS devices", "updated_at": "2024-09-20T09:02:11Z", "url": "https://arstechnica.com/gadgets/2021/07/google-pushed-a-one-character-typo-to-production-bricking-chrome-os-devices/?comments=1"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "FridayoLeary"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Google pushed a one-character typo to <em>production</em>, bricking <em>Chrome</em> OS devices"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "https://arstechnica.com/gadgets/2021/07/google-pushed-a-one-character-typo-to-<em>production</em>-bricking-<em>chrome</em>-os-devices/Googlepushedaone-charactertypotoproduction,brickingChromeOSdevices"}}, "_tags": ["story", "author_FridayoLeary", "story_27923113"], "author": "FridayoLeary", "created_at": "2021-07-22T19:28:03Z", "created_at_i": 1626982083, "num_comments": 0, "objectID": "27923113", "points": 2, "story_id": 27923113, "title": "Google pushed a one-character typo to production, bricking Chrome OS devices", "updated_at": "2024-09-20T09:02:05Z", "url": "https://arstechnica.com/gadgets/2021/07/google-pushed-a-one-character-typo-to-production-bricking-chrome-os-devices/Googlepushedaone-charactertypotoproduction,brickingChromeOSdevices"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jpindar"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "Google pushed a one-character typo to <em>production</em>, bricking <em>Chrome</em> OS devices"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "https://arstechnica.com/gadgets/2021/07/google-pushed-a-one-character-typo-to-<em>production</em>-bricking-<em>chrome</em>-os-devices/"}}, "_tags": ["story", "author_jpindar", "story_27923236"], "author": "jpindar", "created_at": "2021-07-22T19:38:21Z", "created_at_i": 1626982701, "num_comments": 0, "objectID": "27923236", "points": 1, "story_id": 27923236, "title": "Google pushed a one-character typo to production, bricking Chrome OS devices", "updated_at": "2024-09-20T09:02:05Z", "url": "https://arstechnica.com/gadgets/2021/07/google-pushed-a-one-character-typo-to-production-bricking-chrome-os-devices/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "stalfosknight"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["chroma"], "value": "Google broke a conditional statement that verifies passwords on <em>Chrome</em> OS"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["chroma", "production"], "value": "https://arstechnica.com/gadgets/2021/07/google-pushed-a-one-character-typo-to-<em>production</em>-bricking-<em>chrome</em>-os-devices/"}}, "_tags": ["story", "author_stalfosknight", "story_27922545"], "author": "stalfosknight", "children": [27922746, 27922917, 27923054, 27923291, 27923673, 27923680, 27923708, 27923742, 27923836, 27923985, 27924049, 27924050, 27924056, 27924111, 27924369, 27925034, 27925171, 27925173, 27925326, 27925495, 27925943, 27926062, 27927469, 27928633, 27929327, 27930297, 27931819], "created_at": "2021-07-22T18:32:58Z", "created_at_i": 1626978778, "num_comments": 269, "objectID": "27922545", "points": 292, "story_id": 27922545, "title": "Google broke a conditional statement that verifies passwords on Chrome OS", "updated_at": "2024-09-20T09:01:59Z", "url": "https://arstechnica.com/gadgets/2021/07/google-pushed-a-one-character-typo-to-production-bricking-chrome-os-devices/"}], "hitsPerPage": 15, "nbHits": 64, "nbPages": 5, "page": 0, "params": "query=chroma+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 8, "processingTimingsMS": {"_request": {"roundTrip": 25}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 4, "scanning": 2, "total": 7}, "total": 8}, "query": "chroma production", "serverTimeMS": 9}}