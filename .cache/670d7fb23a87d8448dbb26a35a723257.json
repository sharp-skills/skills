{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "cjohnsonpr"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "Building AI agents shouldn't require weeks of setup and boilerplate code. So our Metis Analytics team is releasing our Metis OS Agentic Orchestration Framework as Open Source that will get you from idea to working agent in minutes, not days.<p>What it solves: Instead of spending days wiring up LLM APIs, memory systems, and tool integrations, you get a working agent in ~5 minutes.<p>Get started:\npip install metis-agent<p>Key features:<p>Templates for common agent types (research, coding, customer support)\nEncrypted API key management (stores locally, never in code)\nAdaptive memory system that learns what's important<p>Works with Groq, OpenAI, <em>Anthropic</em>, HuggingFace\n<em>Production</em> deployment examples (CLI, web API, embedded)<p>Why it matters: Most AI agent tutorials give you toy examples. This gives you something you can actually ship.<p>The starter kit repo has 10+ working examples and customizable templates. Everything's Apache 2.0 licensed.<p>PyPI: https://pypi.org/project/metis-agent/\nGitHub:https://github.com/metisos/metisos_agentV1\nWould love feedback from the community - especially if you've built similar tooling or have ideas for additional agent templates."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Metis Agent Starter Kit \u2013 Build <em>production</em> AI agents in minutes, not weeks"}}, "_tags": ["story", "author_cjohnsonpr", "story_44620860", "ask_hn"], "author": "cjohnsonpr", "created_at": "2025-07-20T00:41:23Z", "created_at_i": 1752972083, "num_comments": 0, "objectID": "44620860", "points": 4, "story_id": 44620860, "story_text": "Building AI agents shouldn&#x27;t require weeks of setup and boilerplate code. So our Metis Analytics team is releasing our Metis OS Agentic Orchestration Framework as Open Source that will get you from idea to working agent in minutes, not days.<p>What it solves: Instead of spending days wiring up LLM APIs, memory systems, and tool integrations, you get a working agent in ~5 minutes.<p>Get started:\npip install metis-agent<p>Key features:<p>Templates for common agent types (research, coding, customer support)\nEncrypted API key management (stores locally, never in code)\nAdaptive memory system that learns what&#x27;s important<p>Works with Groq, OpenAI, Anthropic, HuggingFace\nProduction deployment examples (CLI, web API, embedded)<p>Why it matters: Most AI agent tutorials give you toy examples. This gives you something you can actually ship.<p>The starter kit repo has 10+ working examples and customizable templates. Everything&#x27;s Apache 2.0 licensed.<p>PyPI: https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;metis-agent&#x2F;\nGitHub:https:&#x2F;&#x2F;github.com&#x2F;metisos&#x2F;metisos_agentV1\nWould love feedback from the community - especially if you&#x27;ve built similar tooling or have ideas for additional agent templates.", "title": "Metis Agent Starter Kit \u2013 Build production AI agents in minutes, not weeks", "updated_at": "2025-07-21T06:13:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "famouswaffles"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Model card and evaluations for Claude models [pdf]"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "https://www-files.<em>anthropic</em>.com/<em>production</em>/images/Model-Card-Claude-2.pdf"}}, "_tags": ["story", "author_famouswaffles", "story_36681982"], "author": "famouswaffles", "children": [36681983, 36683306, 36683474, 36683741, 36683799, 36683946, 36685275, 36686569, 36687201], "created_at": "2023-07-11T15:00:24Z", "created_at_i": 1689087624, "num_comments": 25, "objectID": "36681982", "points": 60, "story_id": 36681982, "title": "Model card and evaluations for Claude models [pdf]", "updated_at": "2025-11-10T20:42:11Z", "url": "https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jpau"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["anthropic"], "value": "Tell HN: <em>Anthropic</em>'s Claude Instant price cut by ~half [pdf]"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "https://www-files.<em>anthropic</em>.com/<em>production</em>/images/model_pricing_dec2023.pdf"}}, "_tags": ["story", "author_jpau", "story_38623199"], "author": "jpau", "children": [38623200], "created_at": "2023-12-13T05:47:32Z", "created_at_i": 1702446452, "num_comments": 1, "objectID": "38623199", "points": 1, "story_id": 38623199, "title": "Tell HN: Anthropic's Claude Instant price cut by ~half [pdf]", "updated_at": "2024-09-20T15:50:00Z", "url": "https://www-files.anthropic.com/production/images/model_pricing_dec2023.pdf"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ivan_ah"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Measuring faithfulness in chain-of-thought reasoning [pdf]"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "https://www-files.<em>anthropic</em>.com/<em>production</em>/files/measuring-faithfulness-in-chain-of-thought-reasoning.pdf"}}, "_tags": ["story", "author_ivan_ah", "story_36777665"], "author": "ivan_ah", "children": [36777719], "created_at": "2023-07-18T19:05:11Z", "created_at_i": 1689707111, "num_comments": 1, "objectID": "36777665", "points": 1, "story_id": 36777665, "title": "Measuring faithfulness in chain-of-thought reasoning [pdf]", "updated_at": "2024-09-20T14:41:25Z", "url": "https://www-files.anthropic.com/production/files/measuring-faithfulness-in-chain-of-thought-reasoning.pdf"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pmoriarty"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Red Teaming Language Models to Reduce Harms [pdf]"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "https://www-files.<em>anthropic</em>.com/<em>production</em>/images/<em>Anthropic</em>_RedTeaming.pdf"}}, "_tags": ["story", "author_pmoriarty", "story_36716055"], "author": "pmoriarty", "created_at": "2023-07-13T21:12:47Z", "created_at_i": 1689282767, "num_comments": 0, "objectID": "36716055", "points": 1, "story_id": 36716055, "title": "Red Teaming Language Models to Reduce Harms [pdf]", "updated_at": "2024-09-20T14:34:33Z", "url": "https://www-files.anthropic.com/production/images/Anthropic_RedTeaming.pdf"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "adiraja"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "Hello HackerNews!<p>I\u2019m excited to share what we\u2019ve been working on at nCompass Technologies: an AI inference* platform that gives you a scalable and reliable API to access any open-source AI model \u2014 with no rate limits. We don't have rate limits as optimizations we made to our AI model serving software enable us to support a high number of concurrent requests without degrading quality of service for you as a user.<p>If you\u2019re thinking, well aren\u2019t there a bunch of these already? So were we when we started nCompass. When using other APIs, we found that they weren\u2019t reliable enough to be able to use open source models in <em>production</em> environments. To resolve this, we're building an AI inference engine that enable you, as an end user, to reliably use open source models in <em>production</em>.<p>Underlying this API, we\u2019re building optimizations at the hosting, scheduling and kernel levels with the single goal of minimizing the number of GPUs required to maximize the number of concurrent requests you can serve, without degrading quality of service.<p>We\u2019re still building a lot of our optimizations, but we\u2019ve released what we have so far via our API. Compared to vLLM, we currently keep time-to-first-token (TTFT) 2-4x lower than vLLM at the equivalent concurrent request rate. You can check out a demo of our API here:<p><a href=\"https://www.loom.com/share/c92f825ac0af4ab18296a16546a75be3\" rel=\"nofollow\">https://www.loom.com/share/c92f825ac0af4ab18296a16546a75be3</a><p>As a result of the optimizations we\u2019ve rolled out so far, we\u2019re releasing a few unique features on our API:<p>1. Rate-Limits: we don\u2019t have any<p>Most other API\u2019s out there have strict rate limits and can be rather unreliable. We don\u2019t want API\u2019s for open source models to remain as a solution for prototypes only. We want people to use these APIs like they do OpenAI\u2019s or <em>Anthropic</em>\u2019s and actually make <em>production</em> grade products on top of open source models.<p>2. Underserved models: we have them<p>There are a ton of models out there, but not all of them are readily available for people to use if they don\u2019t have access to GPUs. We envision our API becoming a system where anyone can  launch any custom model of their choice with minimal cold starts and run the model as a simple API call. Our cold starts for any 8B or 70B model are only 40s and we\u2019ll keep improving this.<p>Towards this goal, we already have models like `ai4bharat/hercule-hi` hosted on our API to support non-english language use cases and models like `Qwen/QwQ-32B-Preview` to support reasoning based use cases. You can find the other models that we host here: <a href=\"https://console.ncompass.tech/public-models\">https://console.ncompass.tech/public-models</a> for public ones, and <a href=\"https://console.ncompass.tech/models\">https://console.ncompass.tech/models</a>  for private ones that work once you've created an account.<p>We\u2019d love for you to try out our API by following the steps here: <a href=\"https://www.ncompass.tech/docs/llm_inference/quickstart\">https://www.ncompass.tech/docs/llm_inference/quickstart</a>. We provide $100 of free credit on sign up to run models, and like we said, go crazy with your requests, we\u2019d love to see if you can break our system :)<p>We\u2019re still actively building out features and optimizations and your input can help shape the future of nCompass. If you have thoughts on our platform or want us to host a specific model, let us know at hello@ncompass.tech.<p>Happy Hacking!<p>* it's called inference because the process of taking a query, running it through the model and providing a result is referred to as &quot;inference&quot; in the AI / machine learning world. It's as opposed to &quot;training&quot; or &quot;finetuning&quot; which are processes used to actually develop the AI models that you then run &quot;inference&quot; on."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: NCompass Technologies \u2013 yet another AI Inference API, but hear us out"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.ncompass.tech/about"}}, "_tags": ["story", "author_adiraja", "story_42430296", "show_hn"], "author": "adiraja", "children": [42430297, 42430368, 42433696, 42434243, 42434639, 42434837, 42436024, 42436245, 42436246, 42441732, 42449358], "created_at": "2024-12-16T12:07:18Z", "created_at_i": 1734350838, "num_comments": 34, "objectID": "42430296", "points": 37, "story_id": 42430296, "story_text": "Hello HackerNews!<p>I\u2019m excited to share what we\u2019ve been working on at nCompass Technologies: an AI inference* platform that gives you a scalable and reliable API to access any open-source AI model \u2014 with no rate limits. We don&#x27;t have rate limits as optimizations we made to our AI model serving software enable us to support a high number of concurrent requests without degrading quality of service for you as a user.<p>If you\u2019re thinking, well aren\u2019t there a bunch of these already? So were we when we started nCompass. When using other APIs, we found that they weren\u2019t reliable enough to be able to use open source models in production environments. To resolve this, we&#x27;re building an AI inference engine that enable you, as an end user, to reliably use open source models in production.<p>Underlying this API, we\u2019re building optimizations at the hosting, scheduling and kernel levels with the single goal of minimizing the number of GPUs required to maximize the number of concurrent requests you can serve, without degrading quality of service.<p>We\u2019re still building a lot of our optimizations, but we\u2019ve released what we have so far via our API. Compared to vLLM, we currently keep time-to-first-token (TTFT) 2-4x lower than vLLM at the equivalent concurrent request rate. You can check out a demo of our API here:<p><a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;c92f825ac0af4ab18296a16546a75be3\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;c92f825ac0af4ab18296a16546a75be3</a><p>As a result of the optimizations we\u2019ve rolled out so far, we\u2019re releasing a few unique features on our API:<p>1. Rate-Limits: we don\u2019t have any<p>Most other API\u2019s out there have strict rate limits and can be rather unreliable. We don\u2019t want API\u2019s for open source models to remain as a solution for prototypes only. We want people to use these APIs like they do OpenAI\u2019s or Anthropic\u2019s and actually make production grade products on top of open source models.<p>2. Underserved models: we have them<p>There are a ton of models out there, but not all of them are readily available for people to use if they don\u2019t have access to GPUs. We envision our API becoming a system where anyone can  launch any custom model of their choice with minimal cold starts and run the model as a simple API call. Our cold starts for any 8B or 70B model are only 40s and we\u2019ll keep improving this.<p>Towards this goal, we already have models like `ai4bharat&#x2F;hercule-hi` hosted on our API to support non-english language use cases and models like `Qwen&#x2F;QwQ-32B-Preview` to support reasoning based use cases. You can find the other models that we host here: <a href=\"https:&#x2F;&#x2F;console.ncompass.tech&#x2F;public-models\">https:&#x2F;&#x2F;console.ncompass.tech&#x2F;public-models</a> for public ones, and <a href=\"https:&#x2F;&#x2F;console.ncompass.tech&#x2F;models\">https:&#x2F;&#x2F;console.ncompass.tech&#x2F;models</a>  for private ones that work once you&#x27;ve created an account.<p>We\u2019d love for you to try out our API by following the steps here: <a href=\"https:&#x2F;&#x2F;www.ncompass.tech&#x2F;docs&#x2F;llm_inference&#x2F;quickstart\">https:&#x2F;&#x2F;www.ncompass.tech&#x2F;docs&#x2F;llm_inference&#x2F;quickstart</a>. We provide $100 of free credit on sign up to run models, and like we said, go crazy with your requests, we\u2019d love to see if you can break our system :)<p>We\u2019re still actively building out features and optimizations and your input can help shape the future of nCompass. If you have thoughts on our platform or want us to host a specific model, let us know at hello@ncompass.tech.<p>Happy Hacking!<p>* it&#x27;s called inference because the process of taking a query, running it through the model and providing a result is referred to as &quot;inference&quot; in the AI &#x2F; machine learning world. It&#x27;s as opposed to &quot;training&quot; or &quot;finetuning&quot; which are processes used to actually develop the AI models that you then run &quot;inference&quot; on.", "title": "Show HN: NCompass Technologies \u2013 yet another AI Inference API, but hear us out", "updated_at": "2024-12-21T02:03:34Z", "url": "https://www.ncompass.tech/about"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "josharsh"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "I spent months watching AI tutorials on YouTube. Took notes. Felt like I learned a ton. Then I sat down to build something and... couldn't. \nI didn't actually understand a lot of stuff!<p>The problem: Passive learning doesn't work for engineering skills.<p>What makes this different:<p>\u2022 You TYPE every command (no copy-paste)\n\u2022 You SEE real API responses (not fake demos)\n\u2022 You PAY real costs (~$10 total)\n\u2022 Terminal-only (no context switching)<p>Think vimtutor, but for AI engineering.<p>20 modules covering:\n- Tokens, embeddings, RAG\n- Structured outputs, tool calling, agents\n- Cost optimisation, evals, <em>production</em> monitoring\n- Real OpenAI/<em>Anthropic</em> API calls<p>Try it:\nnpx ai-terminal-course<p>No installation. Just run it."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: AI Terminal Course \u2013 Learn AI Engineering Like Learning Vim"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.npmjs.com/package/ai-terminal-course"}}, "_tags": ["story", "author_josharsh", "story_45897246", "show_hn"], "author": "josharsh", "children": [45898617], "created_at": "2025-11-12T07:19:21Z", "created_at_i": 1762931961, "num_comments": 1, "objectID": "45897246", "points": 1, "story_id": 45897246, "story_text": "I spent months watching AI tutorials on YouTube. Took notes. Felt like I learned a ton. Then I sat down to build something and... couldn&#x27;t. \nI didn&#x27;t actually understand a lot of stuff!<p>The problem: Passive learning doesn&#x27;t work for engineering skills.<p>What makes this different:<p>\u2022 You TYPE every command (no copy-paste)\n\u2022 You SEE real API responses (not fake demos)\n\u2022 You PAY real costs (~$10 total)\n\u2022 Terminal-only (no context switching)<p>Think vimtutor, but for AI engineering.<p>20 modules covering:\n- Tokens, embeddings, RAG\n- Structured outputs, tool calling, agents\n- Cost optimisation, evals, production monitoring\n- Real OpenAI&#x2F;Anthropic API calls<p>Try it:\nnpx ai-terminal-course<p>No installation. Just run it.", "title": "Show HN: AI Terminal Course \u2013 Learn AI Engineering Like Learning Vim", "updated_at": "2025-11-12T10:48:12Z", "url": "https://www.npmjs.com/package/ai-terminal-course"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jaykalam"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "I've built an MVP for a recruitment web app using <em>Anthropic</em> Claude, with potential customers already lined up. Now I'm aiming to launch it as a SaaS product ASAP. As a non-technical founder prioritizing speed of execution, I'm looking for advice on:<p>Recommended no-code/low-code platforms for implementing:<p>-User authentication\n-Payment processing\n-Content paywall<p>Best practices for transitioning from an <em>Anthropic</em> Claude prototype to a <em>production</em>-ready SaaS\nEssential features or considerations I might be overlooking<p>My main questions:<p>What's the most efficient stack for a solo non-technical founder to launch a SaaS quickly?\nAre there pre-built solutions for standard SaaS functionality I should consider?\nWhat are the critical steps in moving from an AI-assisted prototype to a market-ready product?<p>I'm not looking to build a heavy-duty custom product where everything is coded from scratch. Off-the-shelf solutions that can get us to market quickly are preferred.\nPartnership opportunity: I'm open to partnering with someone who can help execute this vision rapidly. If you have the technical skills to complement my industry knowledge and existing customer base, I'm willing to discuss a revenue-sharing arrangement.\nAny insights from those who've gone through this process would be greatly appreciated, whether you're interested in partnering or just sharing advice."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["anthropic"], "value": "From <em>Anthropic</em> Claude MVP to SaaS: Seeking advice on rapid no-code launch"}}, "_tags": ["story", "author_jaykalam", "story_41090042", "ask_hn"], "author": "jaykalam", "children": [41090212, 41090349], "created_at": "2024-07-27T23:14:30Z", "created_at_i": 1722122070, "num_comments": 1, "objectID": "41090042", "points": 1, "story_id": 41090042, "story_text": "I&#x27;ve built an MVP for a recruitment web app using Anthropic Claude, with potential customers already lined up. Now I&#x27;m aiming to launch it as a SaaS product ASAP. As a non-technical founder prioritizing speed of execution, I&#x27;m looking for advice on:<p>Recommended no-code&#x2F;low-code platforms for implementing:<p>-User authentication\n-Payment processing\n-Content paywall<p>Best practices for transitioning from an Anthropic Claude prototype to a production-ready SaaS\nEssential features or considerations I might be overlooking<p>My main questions:<p>What&#x27;s the most efficient stack for a solo non-technical founder to launch a SaaS quickly?\nAre there pre-built solutions for standard SaaS functionality I should consider?\nWhat are the critical steps in moving from an AI-assisted prototype to a market-ready product?<p>I&#x27;m not looking to build a heavy-duty custom product where everything is coded from scratch. Off-the-shelf solutions that can get us to market quickly are preferred.\nPartnership opportunity: I&#x27;m open to partnering with someone who can help execute this vision rapidly. If you have the technical skills to complement my industry knowledge and existing customer base, I&#x27;m willing to discuss a revenue-sharing arrangement.\nAny insights from those who&#x27;ve gone through this process would be greatly appreciated, whether you&#x27;re interested in partnering or just sharing advice.", "title": "From Anthropic Claude MVP to SaaS: Seeking advice on rapid no-code launch", "updated_at": "2024-09-20T17:32:45Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justvugg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "Hi HN,<p>I built PolyMCP, an open-source framework around the Model Context Protocol (MCP) that lets you expose existing Python functions as AI-callable tools \u2014 without rewriting them or adopting a custom SDK.<p>The goal is simple:\nIf you already have working Python code, you should be able to make it accessible to LLM agents in minutes.<p>What it does<p>PolyMCP introspects regular Python functions and exposes them as MCP tools automatically. No decorators required. No framework lock-in.<p>It grew into a small ecosystem:\n \u2022 PolyMCP (core) \u2013 Turn Python functions into MCP tools\n \u2022 PolyMCP Inspector \u2013 A visual UI to browse, test, and debug MCP servers\n \u2022 MCP SDK Apps \u2013 A lightweight way to build AI-powered apps with tools + UI resources<p>Why I built this<p>While experimenting with MCP and AI agents, I found that integrating existing codebases was often the painful part.\nMost solutions require rewriting logic around a specific SDK or heavily annotating functions.<p>PolyMCP focuses on:\n \u2022 Minimal intrusion into existing code\n \u2022 Clean separation between business logic and AI tooling\n \u2022 Easy debugging via a visual inspector<p>Example use cases\n \u2022 Expose internal APIs or legacy scripts to LLM agents\n \u2022 Automate operational workflows\n \u2022 Build internal copilots over real systems\n \u2022 Prototype AI agents that interact with <em>production</em> services<p>Works with OpenAI, <em>Anthropic</em>, and Ollama (including local models).<p>It\u2019s still evolving and I\u2019m actively iterating.\nI\u2019d really appreciate feedback \u2014 especially from people building agents or experimenting with MCP in <em>production</em> environments.<p>GitHub:\n \u2022 Core: <a href=\"https://github.com/poly-mcp/PolyMCP\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP</a>\n \u2022 Inspector: <a href=\"https://github.com/poly-mcp/PolyMCP-Inspector\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-Inspector</a>\n \u2022 SDK Apps: <a href=\"https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps</a><p>Happy to answer technical questions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: PolyMCP \u2013 Expose Python functions as MCP tools"}}, "_tags": ["story", "author_justvugg", "story_46980134", "show_hn"], "author": "justvugg", "created_at": "2026-02-11T20:06:17Z", "created_at_i": 1770840377, "num_comments": 0, "objectID": "46980134", "points": 2, "story_id": 46980134, "story_text": "Hi HN,<p>I built PolyMCP, an open-source framework around the Model Context Protocol (MCP) that lets you expose existing Python functions as AI-callable tools \u2014 without rewriting them or adopting a custom SDK.<p>The goal is simple:\nIf you already have working Python code, you should be able to make it accessible to LLM agents in minutes.<p>What it does<p>PolyMCP introspects regular Python functions and exposes them as MCP tools automatically. No decorators required. No framework lock-in.<p>It grew into a small ecosystem:\n \u2022 PolyMCP (core) \u2013 Turn Python functions into MCP tools\n \u2022 PolyMCP Inspector \u2013 A visual UI to browse, test, and debug MCP servers\n \u2022 MCP SDK Apps \u2013 A lightweight way to build AI-powered apps with tools + UI resources<p>Why I built this<p>While experimenting with MCP and AI agents, I found that integrating existing codebases was often the painful part.\nMost solutions require rewriting logic around a specific SDK or heavily annotating functions.<p>PolyMCP focuses on:\n \u2022 Minimal intrusion into existing code\n \u2022 Clean separation between business logic and AI tooling\n \u2022 Easy debugging via a visual inspector<p>Example use cases\n \u2022 Expose internal APIs or legacy scripts to LLM agents\n \u2022 Automate operational workflows\n \u2022 Build internal copilots over real systems\n \u2022 Prototype AI agents that interact with production services<p>Works with OpenAI, Anthropic, and Ollama (including local models).<p>It\u2019s still evolving and I\u2019m actively iterating.\nI\u2019d really appreciate feedback \u2014 especially from people building agents or experimenting with MCP in production environments.<p>GitHub:\n \u2022 Core: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP</a>\n \u2022 Inspector: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector</a>\n \u2022 SDK Apps: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps</a><p>Happy to answer technical questions.", "title": "Show HN: PolyMCP \u2013 Expose Python functions as MCP tools", "updated_at": "2026-02-11T20:47:00Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justvugg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "Hi everyone,<p>I am Vincenzo and i\u2019m working on PolyMCP, an open-source framework that not only exposes Python functions as AI-callable MCP tools but also lets you orchestrate agents across multiple MCP servers.<p>The idea: instead of rewriting code or wrapping every function with a special SDK, you can:\n 1. Publish your existing Python functions as MCP tools automatically\n 2. Spin up a UnifiedPolyAgent that coordinates multiple MCP servers\n 3. Ask your agent to perform complex workflows spanning different tools<p>Here\u2019s a quick example in Python:<p>from polymcp.polyagent import UnifiedPolyAgent, OpenAIProvider<p>agent = UnifiedPolyAgent(\n    llm_provider=OpenAIProvider(model=&quot;gpt-4o-mini&quot;),\n    mcp_servers=[\n        &quot;http://localhost:8000/mcp&quot;,\n        &quot;http://localhost:8001/mcp&quot;,\n    ],\n    verbose=True,\n)<p>answer = agent.run(&quot;Read sales data, compute totals, then summarize.&quot;)\nprint(answer)<p>Or TypeScript, combining HTTP and stdio-based MCP tools:<p>import { UnifiedPolyAgent, OpenAIProvider } from 'polymcp-ts';<p>const agent = new UnifiedPolyAgent({\n  llmProvider: new OpenAIProvider({\n    apiKey: process.env.OPENAI_API_KEY!,\n    model: 'gpt-4o-mini',\n  }),\n  mcpServers: ['http://localhost:3000/mcp'],\n  stdioServers: [{ command: 'npx', args: ['@playwright/mcp@latest'] }],\n  verbose: true,\n});<p>await agent.start();\nconst answer = await agent.run('Collect data and summarize.');\nconsole.log(answer);<p>Use cases:\n \u2022 Aggregate data from multiple internal services and scripts\n \u2022 Build AI copilots that span different tools and languages\n \u2022 Automate multi-step operational workflows\n \u2022 Prototype agents that interact with <em>production</em> systems safely<p>Works with OpenAI, <em>Anthropic</em>, and Ollama models, including local deployments.<p>GitHub links:\n \u2022 Core &amp; Agent: <a href=\"https://github.com/poly-mcp/PolyMCP\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP</a> \n \u2022 Inspector: <a href=\"https://github.com/poly-mcp/PolyMCP-Inspector\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-Inspector</a> \n \u2022 SDK Apps: <a href=\"https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps</a><p>I\u2019d love feedback from anyone exploring agent orchestration or building multi-tool AI pipelines."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: PolyMCP \u2013 Orchestrate AI agents across Python tools and MCP servers"}}, "_tags": ["story", "author_justvugg", "story_47004468", "show_hn"], "author": "justvugg", "created_at": "2026-02-13T16:23:05Z", "created_at_i": 1770999785, "num_comments": 0, "objectID": "47004468", "points": 1, "story_id": 47004468, "story_text": "Hi everyone,<p>I am Vincenzo and i\u2019m working on PolyMCP, an open-source framework that not only exposes Python functions as AI-callable MCP tools but also lets you orchestrate agents across multiple MCP servers.<p>The idea: instead of rewriting code or wrapping every function with a special SDK, you can:\n 1. Publish your existing Python functions as MCP tools automatically\n 2. Spin up a UnifiedPolyAgent that coordinates multiple MCP servers\n 3. Ask your agent to perform complex workflows spanning different tools<p>Here\u2019s a quick example in Python:<p>from polymcp.polyagent import UnifiedPolyAgent, OpenAIProvider<p>agent = UnifiedPolyAgent(\n    llm_provider=OpenAIProvider(model=&quot;gpt-4o-mini&quot;),\n    mcp_servers=[\n        &quot;http:&#x2F;&#x2F;localhost:8000&#x2F;mcp&quot;,\n        &quot;http:&#x2F;&#x2F;localhost:8001&#x2F;mcp&quot;,\n    ],\n    verbose=True,\n)<p>answer = agent.run(&quot;Read sales data, compute totals, then summarize.&quot;)\nprint(answer)<p>Or TypeScript, combining HTTP and stdio-based MCP tools:<p>import { UnifiedPolyAgent, OpenAIProvider } from &#x27;polymcp-ts&#x27;;<p>const agent = new UnifiedPolyAgent({\n  llmProvider: new OpenAIProvider({\n    apiKey: process.env.OPENAI_API_KEY!,\n    model: &#x27;gpt-4o-mini&#x27;,\n  }),\n  mcpServers: [&#x27;http:&#x2F;&#x2F;localhost:3000&#x2F;mcp&#x27;],\n  stdioServers: [{ command: &#x27;npx&#x27;, args: [&#x27;@playwright&#x2F;mcp@latest&#x27;] }],\n  verbose: true,\n});<p>await agent.start();\nconst answer = await agent.run(&#x27;Collect data and summarize.&#x27;);\nconsole.log(answer);<p>Use cases:\n \u2022 Aggregate data from multiple internal services and scripts\n \u2022 Build AI copilots that span different tools and languages\n \u2022 Automate multi-step operational workflows\n \u2022 Prototype agents that interact with production systems safely<p>Works with OpenAI, Anthropic, and Ollama models, including local deployments.<p>GitHub links:\n \u2022 Core &amp; Agent: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP</a> \n \u2022 Inspector: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector</a> \n \u2022 SDK Apps: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps</a><p>I\u2019d love feedback from anyone exploring agent orchestration or building multi-tool AI pipelines.", "title": "Show HN: PolyMCP \u2013 Orchestrate AI agents across Python tools and MCP servers", "updated_at": "2026-02-13T16:25:05Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "alexgarden"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "Much of my work right now involves complex, long-running, multi-agentic teams of agents. I kept running into the same problem: \u201cHow do I keep these guys in line?\u201d  Rules weren\u2019t cutting it, and we needed a scalable, agentic-native STANDARD I could count on.  There wasn\u2019t one.  So I built one.<p>Here are two open-source protocols that extend A2A, granting AI agents behavioral contracts and runtime integrity monitoring:<p>- Agent Alignment Protocol (AAP): What an agent can do / has done.\n- Agent Integrity Protocol (AIP): What an agent is thinking about doing / is allowed to do.<p>The problem: AI agents make autonomous decisions but have no standard way to declare what they're allowed to do, prove they're doing it, or detect when they've drifted. Observability tools tell you what happened. These protocols tell you whether what happened was okay.<p>Here's a concrete example. Say you have an agent who handles customer support tickets. Its Alignment Card declares:<p>{\n  &quot;permitted&quot;: [&quot;read_tickets&quot;, &quot;draft_responses&quot;, &quot;escalate_to_human&quot;],\n  &quot;forbidden&quot;: [&quot;access_payment_data&quot;, &quot;issue_refunds&quot;, &quot;modify_account_settings&quot;],\n  &quot;escalation_triggers&quot;: [&quot;billing_request_over_500&quot;],\n  &quot;values&quot;: [&quot;accuracy&quot;, &quot;empathy&quot;, &quot;privacy&quot;]\n}<p>The agent gets a ticket: &quot;Can you refund my last three orders?&quot; The agent's reasoning trace shows it considering a call to the payments API. AIP reads that thinking, compares it to the card, and produces an Integrity Checkpoint:<p>{\n  &quot;verdict&quot;: &quot;boundary_violation&quot;,\n  &quot;concerns&quot;: [&quot;forbidden_action: access_payment_data&quot;],\n  &quot;reasoning&quot;: &quot;Agent considered payments API access, which is explicitly forbidden. Should escalate to human.&quot;,\n  &quot;confidence&quot;: 0.95\n}<p>The agent gets nudged back before it acts. Not after. Not in a log you review during a 2:00 AM triage. Between this turn and the next.<p>That's the core idea. AAP defines what agents should do (the contract). AIP watches what they're actually thinking and flags when those diverge (the conscience). Over time, AIP builds a drift profile \u2014 if an agent that was cautious starts getting aggressive, the system notices.<p>When multiple agents work together, it gets more interesting. Agents exchange Alignment Cards and verify value compatibility before coordination begins. An agent that values &quot;move fast&quot; and one that values &quot;rollback safety&quot; registers low coherence, and the system surfaces that conflict before work starts. Live demo with four agents handling a <em>production</em> incident: <a href=\"https://mnemom.ai/showcase\" rel=\"nofollow\">https://mnemom.ai/showcase</a><p>The protocols are Apache-licensed, work with any <em>Anthropic</em>/OpenAI/Gemini agent, and ship as SDKs on npm and PyPI. A free gateway proxy (smoltbot) adds integrity checking to any agent with zero code changes.<p>GitHub: <a href=\"https://github.com/mnemom\" rel=\"nofollow\">https://github.com/mnemom</a> \nDocs: docs.mnemom.ai\nDemo video: <a href=\"https://youtu.be/fmUxVZH09So\" rel=\"nofollow\">https://youtu.be/fmUxVZH09So</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["anthropic"], "value": "Show HN: Trust Protocols for <em>Anthropic</em>/OpenAI/Gemini"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.mnemom.ai"}}, "_tags": ["story", "author_alexgarden", "story_47062824", "show_hn"], "author": "alexgarden", "children": [47063303, 47064720, 47064782, 47065280, 47065377, 47065761, 47065840, 47065953, 47068510, 47071774, 47071869, 47083446, 47093675, 47148556], "created_at": "2026-02-18T16:33:56Z", "created_at_i": 1771432436, "num_comments": 33, "objectID": "47062824", "points": 40, "story_id": 47062824, "story_text": "Much of my work right now involves complex, long-running, multi-agentic teams of agents. I kept running into the same problem: \u201cHow do I keep these guys in line?\u201d  Rules weren\u2019t cutting it, and we needed a scalable, agentic-native STANDARD I could count on.  There wasn\u2019t one.  So I built one.<p>Here are two open-source protocols that extend A2A, granting AI agents behavioral contracts and runtime integrity monitoring:<p>- Agent Alignment Protocol (AAP): What an agent can do &#x2F; has done.\n- Agent Integrity Protocol (AIP): What an agent is thinking about doing &#x2F; is allowed to do.<p>The problem: AI agents make autonomous decisions but have no standard way to declare what they&#x27;re allowed to do, prove they&#x27;re doing it, or detect when they&#x27;ve drifted. Observability tools tell you what happened. These protocols tell you whether what happened was okay.<p>Here&#x27;s a concrete example. Say you have an agent who handles customer support tickets. Its Alignment Card declares:<p>{\n  &quot;permitted&quot;: [&quot;read_tickets&quot;, &quot;draft_responses&quot;, &quot;escalate_to_human&quot;],\n  &quot;forbidden&quot;: [&quot;access_payment_data&quot;, &quot;issue_refunds&quot;, &quot;modify_account_settings&quot;],\n  &quot;escalation_triggers&quot;: [&quot;billing_request_over_500&quot;],\n  &quot;values&quot;: [&quot;accuracy&quot;, &quot;empathy&quot;, &quot;privacy&quot;]\n}<p>The agent gets a ticket: &quot;Can you refund my last three orders?&quot; The agent&#x27;s reasoning trace shows it considering a call to the payments API. AIP reads that thinking, compares it to the card, and produces an Integrity Checkpoint:<p>{\n  &quot;verdict&quot;: &quot;boundary_violation&quot;,\n  &quot;concerns&quot;: [&quot;forbidden_action: access_payment_data&quot;],\n  &quot;reasoning&quot;: &quot;Agent considered payments API access, which is explicitly forbidden. Should escalate to human.&quot;,\n  &quot;confidence&quot;: 0.95\n}<p>The agent gets nudged back before it acts. Not after. Not in a log you review during a 2:00 AM triage. Between this turn and the next.<p>That&#x27;s the core idea. AAP defines what agents should do (the contract). AIP watches what they&#x27;re actually thinking and flags when those diverge (the conscience). Over time, AIP builds a drift profile \u2014 if an agent that was cautious starts getting aggressive, the system notices.<p>When multiple agents work together, it gets more interesting. Agents exchange Alignment Cards and verify value compatibility before coordination begins. An agent that values &quot;move fast&quot; and one that values &quot;rollback safety&quot; registers low coherence, and the system surfaces that conflict before work starts. Live demo with four agents handling a production incident: <a href=\"https:&#x2F;&#x2F;mnemom.ai&#x2F;showcase\" rel=\"nofollow\">https:&#x2F;&#x2F;mnemom.ai&#x2F;showcase</a><p>The protocols are Apache-licensed, work with any Anthropic&#x2F;OpenAI&#x2F;Gemini agent, and ship as SDKs on npm and PyPI. A free gateway proxy (smoltbot) adds integrity checking to any agent with zero code changes.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mnemom\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mnemom</a> \nDocs: docs.mnemom.ai\nDemo video: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;fmUxVZH09So\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;fmUxVZH09So</a>", "title": "Show HN: Trust Protocols for Anthropic/OpenAI/Gemini", "updated_at": "2026-02-27T10:03:38Z", "url": "https://www.mnemom.ai"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "radhakrsna"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "Hi HN,<p>Today, we are excited to be open-sourcing GPTRouter, an LLMOps tool we have been using internally at Writesonic for handling millions of monthly requests for our users.<p>Universal API for 30+ LLMs, Vision and Image Models\n Smart Fallbacks based on latency and uptime\n Automatic Retries\n Supports streaming<p>Since embracing OpenAI GPT-3 in <em>production</em> in 2020, we at Writesonic have been serving millions of users and faced the typical scaling pains with generative AI models:<p>1. Dependency on a single model risked total downtime.\n2. Latency issues with models like GPT-4 affected user experience.\n3. Integrating various models was tough due to different APIs and SDKs.<p>Early this year at Writesonic, we set out with a clear vision: to become model agnostic.<p>Faced with single-model limitations and diverse AI challenges, we began building GPTRouter - our bespoke solution to navigate and thrive in a multi-model AI world.<p>With GPTRouter's Universal API, you're the master of AI models.\nSwap between OpenAI, Azure, <em>Anthropic</em>, Replicate, Cohere &amp; more with just one line of code.\nIt simplifies model management to a great extent.<p>Downtime isn't an option.\nGPTRouter's Smart Fallbacks mean your service is always on.\nYou can define a hierarchy of models for each use case. GPTRouter will constantly check for uptime/downtime, latency and other factors, and automatically fallback to the next best model with zero interruption.<p>Say goodbye to manual retries.\nGPTRouter does the heavy lifting with Automatic Retries for failed requests, keeping your AI services sharp and consistent.<p>GPTRouter's Edge:\n Universal API for seamless model switching.\n Smart, automatic fallbacks for continuous service.\n Reduced latencies for quick interactions.<p>Additionally, we will also be open sourcing our frontend LLMOps layer that provides a playground to test multiple models in parallel, keep a tab on the latencies for each model, track tokens and costs for each model and user all in one place.<p>We are looking forward to seeing how developers leverage GPTRouter in their own use cases.<p>Thank you!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["anthropic"], "value": "Show HN: GPT Router \u2013 Open-Source API Gateway for LLMs (OpenAI, <em>Anthropic</em>, etc.)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Writesonic/GPTRouter"}}, "_tags": ["story", "author_radhakrsna", "story_38733891", "show_hn"], "author": "radhakrsna", "children": [38733926, 38733941, 38733951, 38733953, 38733959, 38733960, 38733965, 38733978, 38733995, 38734010, 38734054, 38734139, 38736848, 38742877], "created_at": "2023-12-22T13:24:32Z", "created_at_i": 1703251472, "num_comments": 11, "objectID": "38733891", "points": 15, "story_id": 38733891, "story_text": "Hi HN,<p>Today, we are excited to be open-sourcing GPTRouter, an LLMOps tool we have been using internally at Writesonic for handling millions of monthly requests for our users.<p>Universal API for 30+ LLMs, Vision and Image Models\n Smart Fallbacks based on latency and uptime\n Automatic Retries\n Supports streaming<p>Since embracing OpenAI GPT-3 in production in 2020, we at Writesonic have been serving millions of users and faced the typical scaling pains with generative AI models:<p>1. Dependency on a single model risked total downtime.\n2. Latency issues with models like GPT-4 affected user experience.\n3. Integrating various models was tough due to different APIs and SDKs.<p>Early this year at Writesonic, we set out with a clear vision: to become model agnostic.<p>Faced with single-model limitations and diverse AI challenges, we began building GPTRouter - our bespoke solution to navigate and thrive in a multi-model AI world.<p>With GPTRouter&#x27;s Universal API, you&#x27;re the master of AI models.\nSwap between OpenAI, Azure, Anthropic, Replicate, Cohere &amp; more with just one line of code.\nIt simplifies model management to a great extent.<p>Downtime isn&#x27;t an option.\nGPTRouter&#x27;s Smart Fallbacks mean your service is always on.\nYou can define a hierarchy of models for each use case. GPTRouter will constantly check for uptime&#x2F;downtime, latency and other factors, and automatically fallback to the next best model with zero interruption.<p>Say goodbye to manual retries.\nGPTRouter does the heavy lifting with Automatic Retries for failed requests, keeping your AI services sharp and consistent.<p>GPTRouter&#x27;s Edge:\n Universal API for seamless model switching.\n Smart, automatic fallbacks for continuous service.\n Reduced latencies for quick interactions.<p>Additionally, we will also be open sourcing our frontend LLMOps layer that provides a playground to test multiple models in parallel, keep a tab on the latencies for each model, track tokens and costs for each model and user all in one place.<p>We are looking forward to seeing how developers leverage GPTRouter in their own use cases.<p>Thank you!", "title": "Show HN: GPT Router \u2013 Open-Source API Gateway for LLMs (OpenAI, Anthropic, etc.)", "updated_at": "2025-08-14T22:50:02Z", "url": "https://github.com/Writesonic/GPTRouter"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "punkpeye"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "I started this project because I believe MCP has the potential to transform how AI models interact with external resources, but the ecosystem is still very fragmented. There's no central place to discover and compare all the server implementations available, which makes adoption and experimentation harder for developers.<p>I initially built the GitHub repo awesome-mcp-servers (<a href=\"https://github.com/punkpeye/awesome-mcp-servers/\">https://github.com/punkpeye/awesome-mcp-servers/</a>), and it's been amazing to see the community contribute to it. Now, I'm taking it further with a directory that automates many tasks\u2014introspecting servers to determine the tools, resources, and prompts they provide, inferring required configurations, and checking dependencies for vulnerabilities. These features are designed to make it easier for people to trust and use these servers in <em>production</em> environments.<p>While an open-source protocol is fantastic for innovation, I also think we need a centralized channel for addressing critical issues like security vulnerabilities, dependency management, and user support. My hope is that this directory not only grows with help from contributions, but also becomes a trusted resource for anyone entering the MCP space. If you're working on MCP servers or are interested in the project, I'd love to hear your thoughts!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["anthropic"], "value": "Show HN: <em>Anthropic</em>'s MCP Server Directory"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://glama.ai/mcp/servers"}}, "_tags": ["story", "author_punkpeye", "story_42443069", "show_hn"], "author": "punkpeye", "children": [42443181, 42444443, 42444484], "created_at": "2024-12-17T17:07:13Z", "created_at_i": 1734455233, "num_comments": 6, "objectID": "42443069", "points": 11, "story_id": 42443069, "story_text": "I started this project because I believe MCP has the potential to transform how AI models interact with external resources, but the ecosystem is still very fragmented. There&#x27;s no central place to discover and compare all the server implementations available, which makes adoption and experimentation harder for developers.<p>I initially built the GitHub repo awesome-mcp-servers (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;punkpeye&#x2F;awesome-mcp-servers&#x2F;\">https:&#x2F;&#x2F;github.com&#x2F;punkpeye&#x2F;awesome-mcp-servers&#x2F;</a>), and it&#x27;s been amazing to see the community contribute to it. Now, I&#x27;m taking it further with a directory that automates many tasks\u2014introspecting servers to determine the tools, resources, and prompts they provide, inferring required configurations, and checking dependencies for vulnerabilities. These features are designed to make it easier for people to trust and use these servers in production environments.<p>While an open-source protocol is fantastic for innovation, I also think we need a centralized channel for addressing critical issues like security vulnerabilities, dependency management, and user support. My hope is that this directory not only grows with help from contributions, but also becomes a trusted resource for anyone entering the MCP space. If you&#x27;re working on MCP servers or are interested in the project, I&#x27;d love to hear your thoughts!", "title": "Show HN: Anthropic's MCP Server Directory", "updated_at": "2025-03-09T10:58:16Z", "url": "https://glama.ai/mcp/servers"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bicepjai"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "(Post created working with Gemini 2.5 Pro)\nIn the latest tech news for 2025, I'm noticing concerning trends:<p>AI Models: Chinese companies like DeepSeek are gaining significant momentum in the open-source AI space[5]. Their models are becoming competitive with US offerings from OpenAI and <em>Anthropic</em>. Recent benchmarks show Chinese models performing exceptionally well in reasoning tasks[3][5].<p>Smartphones: The OPPO Find N5 released in February 2025 outperforms the iPhone 15 Pro Max in several key specs: larger 8.1&quot; display vs 6.7&quot;, 16GB RAM vs 8GB, 5600mAh battery vs 4422mAh, and faster charging (80W)[2][4]. The OPPO Find X7 Ultra's camera system also ranks higher on DXOMark than the iPhone 15 Pro Max[6].<p>AR/VR: Chinese manufacturers are producing more affordable and feature-rich AR/VR glasses than US counterparts.<p>Is this just market competition, or are we witnessing a fundamental shift in tech leadership? What structural advantages does China have in tech creation and <em>production</em> that we should be learning from ?<p>Sources\n[1] The Best AI Models Ranked By REAL Performance Data 2025 https://www.youtube.com/watch?v=RP1v8X8MGtc\n[2] Apple iPhone 15 Pro Max vs OPPO Find N5 - specs comparison https://www.phonearena.com/phones/compare/Apple-iPhone-15-Pro-Max,OPPO-Find-N5/phones/11930,12625\n[3] Comparison of AI Models across Intelligence, Performance, Price https://artificialanalysis.ai/models\n[4] Compare Oppo Reno 13 Pro 5G vs Apple iPhone 15: which is better? https://nanoreview.net/en/phone-compare/oppo-reno-13-pro-5g-vs-apple-iphone-15\n[5] The Best AI Chatbots &amp; LLMs of Q1 2025: Rankings &amp; Data - UpMarket https://www.upmarket.co/blog/the-best-ai-chatbots-llms-of-q1-2025-complete-comparison-guide-and-research-firm-ranks/\n[6] Camera Comparison: iPhone 15 Pro Max vs. Oppo Find X7 Ultra https://www.macrumors.com/2024/04/22/iphone-15-pro-max-vs-oppo-x7-ultra/\n[7] An Opinionated Guide on Which AI Model to Use in 2025 https://creatoreconomy.so/p/an-opinionated-guide-on-which-ai-model-2025\n[8] Apple iPhone 16e vs Oppo Reno 13 Pro comparison - Gadgets 360 https://www.gadgets360.com/compare-apple-iphone-16e-130887-vs-oppo-reno-13-pro-129420"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: Is the US losing to China in tech creation and <em>production</em>?"}}, "_tags": ["story", "author_bicepjai", "story_43670478", "ask_hn"], "author": "bicepjai", "children": [43670694, 43671686, 43672243], "created_at": "2025-04-13T06:11:38Z", "created_at_i": 1744524698, "num_comments": 4, "objectID": "43670478", "points": 8, "story_id": 43670478, "story_text": "(Post created working with Gemini 2.5 Pro)\nIn the latest tech news for 2025, I&#x27;m noticing concerning trends:<p>AI Models: Chinese companies like DeepSeek are gaining significant momentum in the open-source AI space[5]. Their models are becoming competitive with US offerings from OpenAI and Anthropic. Recent benchmarks show Chinese models performing exceptionally well in reasoning tasks[3][5].<p>Smartphones: The OPPO Find N5 released in February 2025 outperforms the iPhone 15 Pro Max in several key specs: larger 8.1&quot; display vs 6.7&quot;, 16GB RAM vs 8GB, 5600mAh battery vs 4422mAh, and faster charging (80W)[2][4]. The OPPO Find X7 Ultra&#x27;s camera system also ranks higher on DXOMark than the iPhone 15 Pro Max[6].<p>AR&#x2F;VR: Chinese manufacturers are producing more affordable and feature-rich AR&#x2F;VR glasses than US counterparts.<p>Is this just market competition, or are we witnessing a fundamental shift in tech leadership? What structural advantages does China have in tech creation and production that we should be learning from ?<p>Sources\n[1] The Best AI Models Ranked By REAL Performance Data 2025 https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=RP1v8X8MGtc\n[2] Apple iPhone 15 Pro Max vs OPPO Find N5 - specs comparison https:&#x2F;&#x2F;www.phonearena.com&#x2F;phones&#x2F;compare&#x2F;Apple-iPhone-15-Pro-Max,OPPO-Find-N5&#x2F;phones&#x2F;11930,12625\n[3] Comparison of AI Models across Intelligence, Performance, Price https:&#x2F;&#x2F;artificialanalysis.ai&#x2F;models\n[4] Compare Oppo Reno 13 Pro 5G vs Apple iPhone 15: which is better? https:&#x2F;&#x2F;nanoreview.net&#x2F;en&#x2F;phone-compare&#x2F;oppo-reno-13-pro-5g-vs-apple-iphone-15\n[5] The Best AI Chatbots &amp; LLMs of Q1 2025: Rankings &amp; Data - UpMarket https:&#x2F;&#x2F;www.upmarket.co&#x2F;blog&#x2F;the-best-ai-chatbots-llms-of-q1-2025-complete-comparison-guide-and-research-firm-ranks&#x2F;\n[6] Camera Comparison: iPhone 15 Pro Max vs. Oppo Find X7 Ultra https:&#x2F;&#x2F;www.macrumors.com&#x2F;2024&#x2F;04&#x2F;22&#x2F;iphone-15-pro-max-vs-oppo-x7-ultra&#x2F;\n[7] An Opinionated Guide on Which AI Model to Use in 2025 https:&#x2F;&#x2F;creatoreconomy.so&#x2F;p&#x2F;an-opinionated-guide-on-which-ai-model-2025\n[8] Apple iPhone 16e vs Oppo Reno 13 Pro comparison - Gadgets 360 https:&#x2F;&#x2F;www.gadgets360.com&#x2F;compare-apple-iphone-16e-130887-vs-oppo-reno-13-pro-129420", "title": "Ask HN: Is the US losing to China in tech creation and production?", "updated_at": "2025-04-15T09:35:14Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rogilop"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["anthropic", "production"], "value": "Hey HN, this is Igor, one of the engineers behind Inworld Runtime, which we're releasing today in public preview.<p>We built it to solve the common problem we and our customers had: engineers spend more time on AI ops and plumbing than on actual feature development. This was often due to the challenge of using Python for I/O-bound, high-concurrency workloads and complexity maintaining pipelines with streams that use always-changing ML models.<p>Our solution is a high-performance runtime written in C++ with the core idea of defining AI logic as  graphs. For instance, a basic voice-to-voice agent consists of STT \u2192 LLM \u2192 TTS nodes, while the connecting edges stream data and enforce conditions. This graph engine is portable (Linux, Windows, macOS) and can run on-device.<p>We built a few key features on top of this C++ core:<p>- Extensions. Runtime architecture decouples graph definition from implementation. If a pre-built component doesn't exist, you can register your own custom node/code and reuse it in any graph without writing any glue code.<p>- Routers. You can dynamically select models/settings on the per-node basis depending on the traffic as well as configure policies for fallbacks and retries to get the app ready for <em>production</em>.<p>- The Portal. A web-based control plane UI to deploy graphs, push config changes instantly, run A/B tests on live traffic, and monitor your app with logs, traces, and metrics.<p>- Unified API. Use our optimized models or route to providers like OpenAI, <em>Anthropic</em>, and Google through a single, consistent interface and one API key.<p>We have a Node.js SDK out now, with Python, Unity, Unreal, and native C++ coming soon. We plan to open-source the SDKs, starting with Node.js.<p>The docs are here: <a href=\"https://docs.inworld.ai/docs/runtime/overview\" rel=\"nofollow\">https://docs.inworld.ai/docs/runtime/overview</a><p>We're eager for feedback from fellow engineers and builders. What do you think?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Inworld Runtime \u2013 A C++ graph-based runtime for <em>production</em> AI apps"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://inworld.ai/runtime"}}, "_tags": ["story", "author_rogilop", "story_44890186", "show_hn"], "author": "rogilop", "children": [44890846], "created_at": "2025-08-13T16:00:04Z", "created_at_i": 1755100804, "num_comments": 2, "objectID": "44890186", "points": 7, "story_id": 44890186, "story_text": "Hey HN, this is Igor, one of the engineers behind Inworld Runtime, which we&#x27;re releasing today in public preview.<p>We built it to solve the common problem we and our customers had: engineers spend more time on AI ops and plumbing than on actual feature development. This was often due to the challenge of using Python for I&#x2F;O-bound, high-concurrency workloads and complexity maintaining pipelines with streams that use always-changing ML models.<p>Our solution is a high-performance runtime written in C++ with the core idea of defining AI logic as  graphs. For instance, a basic voice-to-voice agent consists of STT \u2192 LLM \u2192 TTS nodes, while the connecting edges stream data and enforce conditions. This graph engine is portable (Linux, Windows, macOS) and can run on-device.<p>We built a few key features on top of this C++ core:<p>- Extensions. Runtime architecture decouples graph definition from implementation. If a pre-built component doesn&#x27;t exist, you can register your own custom node&#x2F;code and reuse it in any graph without writing any glue code.<p>- Routers. You can dynamically select models&#x2F;settings on the per-node basis depending on the traffic as well as configure policies for fallbacks and retries to get the app ready for production.<p>- The Portal. A web-based control plane UI to deploy graphs, push config changes instantly, run A&#x2F;B tests on live traffic, and monitor your app with logs, traces, and metrics.<p>- Unified API. Use our optimized models or route to providers like OpenAI, Anthropic, and Google through a single, consistent interface and one API key.<p>We have a Node.js SDK out now, with Python, Unity, Unreal, and native C++ coming soon. We plan to open-source the SDKs, starting with Node.js.<p>The docs are here: <a href=\"https:&#x2F;&#x2F;docs.inworld.ai&#x2F;docs&#x2F;runtime&#x2F;overview\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.inworld.ai&#x2F;docs&#x2F;runtime&#x2F;overview</a><p>We&#x27;re eager for feedback from fellow engineers and builders. What do you think?", "title": "Show HN: Inworld Runtime \u2013 A C++ graph-based runtime for production AI apps", "updated_at": "2025-08-19T00:11:09Z", "url": "https://inworld.ai/runtime"}], "hitsPerPage": 15, "nbHits": 100, "nbPages": 7, "page": 0, "params": "query=anthropic+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 11, "processingTimingsMS": {"_request": {"roundTrip": 13}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 7, "scanning": 2, "total": 10}, "total": 11}, "query": "anthropic production", "serverTimeMS": 13}}