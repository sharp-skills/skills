{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "art049"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "Hi HN! We\u2019re Arthur and Adrien from CodSpeed. We\u2019re building a tool measuring software performance before any <em>production</em> deployment, catching performance regressions before they hit <em>production</em> environments and reporting performance changes directly in Pull Request comments. It\u2019s kind of like Codecov but for performance measurement.<p>Today, the go to solution to measure performance is probably to use an APM(<em>DataDog</em>, Sentry, \u2026), continuously analyzing your <em>production</em> environment. However, since those solutions are operating on real environments they need real users to experience poor performance in order to report issues and unfortunately, performance remains an afterthought appearing only at the end of the development cycle.<p>Another possibility to measure performance is to create benchmarks while developing and to run them on a regular basis to have an idea of the performance trend of your project. However, with this approach, the variance in the results creates a lot of noise and it\u2019s rarely possible to compare your results with the ones from a co-worker or a <em>production</em> environment.<p>To make consistent performance measurement as easy as unit testing and fully integrated in CI workflows, we chose a benchmark based solution. And, to eliminate the usual variance associated with running them, we measure the number of instructions and memory/cache accesses through CPU instrumentation performed with Valgrind. This approach gives repeatable and consistent results that couldn\u2019t be obtained with a time based statistical approach, especially in extremely noisy CI and cloud environments.<p>We have been in closed beta for a few months, already being used by popular open-source projects such as Prisma and Pydantic. Notably, CodSpeed helped Pydantic through their Rust migration, empowering them to make the library 17x faster: <a href=\"https://docs.pydantic.dev/latest/blog/pydantic-v2/#performance\" rel=\"nofollow noreferrer\">https://docs.pydantic.dev/latest/blog/pydantic-v2/#performan...</a><p>Today, we\u2019re super excited to finally make the product available to everyone. We currently support Python, Node.js and Rust and are looking forward to integrate with more languages soon.<p>The product is and will be free forever for open-source projects. Also, we have a per-seat pricing for private repository usage.\nWe have a lot of exciting features planned regarding additional integrations, such as Database and GPU integrations that should come in upcoming months.<p>Don\u2019t hesitate to try out the product and give your honest feedback. We\u2019re looking forward to your comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: CodSpeed \u2013 Continuous Performance Measurement"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://codspeed.io/"}}, "_tags": ["story", "author_art049", "story_36682012", "show_hn"], "author": "art049", "children": [36682013, 36682668, 36683693, 36715126], "created_at": "2023-07-11T15:02:20Z", "created_at_i": 1689087740, "num_comments": 3, "objectID": "36682012", "points": 32, "story_id": 36682012, "story_text": "Hi HN! We\u2019re Arthur and Adrien from CodSpeed. We\u2019re building a tool measuring software performance before any production deployment, catching performance regressions before they hit production environments and reporting performance changes directly in Pull Request comments. It\u2019s kind of like Codecov but for performance measurement.<p>Today, the go to solution to measure performance is probably to use an APM(DataDog, Sentry, \u2026), continuously analyzing your production environment. However, since those solutions are operating on real environments they need real users to experience poor performance in order to report issues and unfortunately, performance remains an afterthought appearing only at the end of the development cycle.<p>Another possibility to measure performance is to create benchmarks while developing and to run them on a regular basis to have an idea of the performance trend of your project. However, with this approach, the variance in the results creates a lot of noise and it\u2019s rarely possible to compare your results with the ones from a co-worker or a production environment.<p>To make consistent performance measurement as easy as unit testing and fully integrated in CI workflows, we chose a benchmark based solution. And, to eliminate the usual variance associated with running them, we measure the number of instructions and memory&#x2F;cache accesses through CPU instrumentation performed with Valgrind. This approach gives repeatable and consistent results that couldn\u2019t be obtained with a time based statistical approach, especially in extremely noisy CI and cloud environments.<p>We have been in closed beta for a few months, already being used by popular open-source projects such as Prisma and Pydantic. Notably, CodSpeed helped Pydantic through their Rust migration, empowering them to make the library 17x faster: <a href=\"https:&#x2F;&#x2F;docs.pydantic.dev&#x2F;latest&#x2F;blog&#x2F;pydantic-v2&#x2F;#performance\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;docs.pydantic.dev&#x2F;latest&#x2F;blog&#x2F;pydantic-v2&#x2F;#performan...</a><p>Today, we\u2019re super excited to finally make the product available to everyone. We currently support Python, Node.js and Rust and are looking forward to integrate with more languages soon.<p>The product is and will be free forever for open-source projects. Also, we have a per-seat pricing for private repository usage.\nWe have a lot of exciting features planned regarding additional integrations, such as Database and GPU integrations that should come in upcoming months.<p>Don\u2019t hesitate to try out the product and give your honest feedback. We\u2019re looking forward to your comments!", "title": "Show HN: CodSpeed \u2013 Continuous Performance Measurement", "updated_at": "2024-09-20T14:39:18Z", "url": "https://codspeed.io/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "revscat"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "I believe my former employer may be criminally negligent in their data security and I'm seeking advice on how to proceed, or if anything can be done.<p>InsCo suffered a ransomware attack in 2023 that affected some 9 million customers, mostly Medicaid recipients. Data that was compromised included the usual litany of PII: SSNs, DL numbers, addresses, DOBs, etc. Their IT infra is composed of a combination of Ruby on Rails and .NET applications. After the breach, the agency that built the Rails application was fired. In May 2024 I was brought in to lead the team responsible for upgrading the Rails app. All of us were contractors. This is what I learned about the processes they had in place, and the Rails application itself.<p>As of August 2024:<p>1. The application ran on Ruby 1.9.3, which reached EOL in 2013.<p>2. It used Ruby on Rails 3.1, which reached EOL in 2016.<p>3. The OS was CentOS7, which reached EOL in July 2024.<p>4. There was no containerization of applications; development and <em>production</em> were on bespoke AWS sandboxes. When we managed to get the application running in Docker, Docker Scout reported around 650 vulnerabilities.<p>5. There were no processes around secure coding practices, either automated or manual.<p>6. Unit tests were discouraged and flaky. I required unit tests for all PRs, but Director X reversed this decision, stating, &quot;tests are acceptable but not encouraged.&quot; Existing tests were unreliable due to a shared test database causing data integrity issues.<p>7. Code reviews were discouraged. The team started using PRs, but Director X banned them, stating it made auditing harder. He preferred using text files attached to Jira issues and became angry when I requested a template.<p>8. Management rarely documented requests/orders and ignored explicit requests for written feedback. Issues with open questions were often left unanswered.<p>9. All code changes had to be approved by management before being merged to master, and all merges to master were done by management.<p>10. Library changes, whether minor version upgrades or changes to different libraries, also required management approval.<p>11. There was no <em>production</em> monitoring in place such as <em>DataDog</em> or New Relic.<p>12. Medicaid data was received from the State of CriticallyImportantToTheElection in an unencrypted zip of XML data and uploaded using FTP. Not even SFTP, just raw FTP.<p>13. The entire upgrade team was fired in early August 2024 without explanation. We were starting to make good progress, then... bam. Our GitHub access was revoked and we no longer had access to Jira or Teams. No communication was given regarding the reasons.<p>14. An effort to upgrade from CentOS7 to RHEL9 also failed; when I left, they were still using the outdated CentOS7.<p>This situation seems to reflect severe negligence, if not intentional disregard. What advice does HN have on this matter? Are there any law enforcement or regulatory agencies that might be interested? The personal data of millions remains exposed, and this data is crucial for state and federal protection efforts. At the very least, this appears to be a result of incompetence, if not malicious intent.<p>I\u2019m looking to take action if possible."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Security negligence by former employer, a health insurance co.?"}}, "_tags": ["story", "author_revscat", "story_41329682", "ask_hn"], "author": "revscat", "children": [41330259, 41330726], "created_at": "2024-08-23T15:11:46Z", "created_at_i": 1724425906, "num_comments": 2, "objectID": "41329682", "points": 2, "story_id": 41329682, "story_text": "I believe my former employer may be criminally negligent in their data security and I&#x27;m seeking advice on how to proceed, or if anything can be done.<p>InsCo suffered a ransomware attack in 2023 that affected some 9 million customers, mostly Medicaid recipients. Data that was compromised included the usual litany of PII: SSNs, DL numbers, addresses, DOBs, etc. Their IT infra is composed of a combination of Ruby on Rails and .NET applications. After the breach, the agency that built the Rails application was fired. In May 2024 I was brought in to lead the team responsible for upgrading the Rails app. All of us were contractors. This is what I learned about the processes they had in place, and the Rails application itself.<p>As of August 2024:<p>1. The application ran on Ruby 1.9.3, which reached EOL in 2013.<p>2. It used Ruby on Rails 3.1, which reached EOL in 2016.<p>3. The OS was CentOS7, which reached EOL in July 2024.<p>4. There was no containerization of applications; development and production were on bespoke AWS sandboxes. When we managed to get the application running in Docker, Docker Scout reported around 650 vulnerabilities.<p>5. There were no processes around secure coding practices, either automated or manual.<p>6. Unit tests were discouraged and flaky. I required unit tests for all PRs, but Director X reversed this decision, stating, &quot;tests are acceptable but not encouraged.&quot; Existing tests were unreliable due to a shared test database causing data integrity issues.<p>7. Code reviews were discouraged. The team started using PRs, but Director X banned them, stating it made auditing harder. He preferred using text files attached to Jira issues and became angry when I requested a template.<p>8. Management rarely documented requests&#x2F;orders and ignored explicit requests for written feedback. Issues with open questions were often left unanswered.<p>9. All code changes had to be approved by management before being merged to master, and all merges to master were done by management.<p>10. Library changes, whether minor version upgrades or changes to different libraries, also required management approval.<p>11. There was no production monitoring in place such as DataDog or New Relic.<p>12. Medicaid data was received from the State of CriticallyImportantToTheElection in an unencrypted zip of XML data and uploaded using FTP. Not even SFTP, just raw FTP.<p>13. The entire upgrade team was fired in early August 2024 without explanation. We were starting to make good progress, then... bam. Our GitHub access was revoked and we no longer had access to Jira or Teams. No communication was given regarding the reasons.<p>14. An effort to upgrade from CentOS7 to RHEL9 also failed; when I left, they were still using the outdated CentOS7.<p>This situation seems to reflect severe negligence, if not intentional disregard. What advice does HN have on this matter? Are there any law enforcement or regulatory agencies that might be interested? The personal data of millions remains exposed, and this data is crucial for state and federal protection efforts. At the very least, this appears to be a result of incompetence, if not malicious intent.<p>I\u2019m looking to take action if possible.", "title": "Ask HN: Security negligence by former employer, a health insurance co.?", "updated_at": "2024-09-20T17:37:37Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "skull8888888"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Hey HN, we\u2019re Robert, Din and Temirlan from Laminar (<a href=\"https://www.lmnr.ai\">https://www.lmnr.ai</a>), an open-source observability and analytics platform for complex LLM apps. It\u2019s designed to be fast, reliable, and scalable. The stack is RabbitMQ for message queues, Postgres for storage, Clickhouse for analytics, Qdrant for semantic search - all powered by Rust.<p>How is Laminar different from the swarm of other \u201cLLM observability\u201d platforms?<p>On the observability part, we\u2019re focused on handling full execution traces, not just LLM calls. We built a Rust ingestor for OpenTelemetry (Otel) spans with GenAI semantic conventions. As LLM apps get more complex (think Agents with hundreds of LLM and function calls, or complex RAG pipelines), full tracing is critical. With Otel spans, we can: 1. Cover the entire execution trace. 2. Keep the platform future-proof 3. Leverage an amazing OpenLLMetry (<a href=\"https://github.com/traceloop/openllmetry\">https://github.com/traceloop/openllmetry</a>), open-source package for span <em>production</em>.<p>The key difference is that we tie text analytics directly to execution traces. Rich text data makes LLM traces unique, so we let you track \u201csemantic metrics\u201d (like what your AI agent is actually saying) and connect those metrics to where they happen in the trace. If you want to know if your AI drive-through agent made an upsell, you can design an LLM extraction pipeline in our builder (more on it later), host it on Laminar, and handle everything from event requests to output logging. Processing requests simply come as events in the Otel span.<p>We think it\u2019s a win to separate core app logic from LLM event processing. Most devs don\u2019t want to manage background queues for LLM analytics processing but still want insights into how their Agents or RAGs are working.<p>Our Pipeline Builder uses graph UI where nodes are LLM and util functions, and edges showing data flow. We built a custom task execution engine with support of parallel branch executions, cycles and branches (it\u2019s overkill for simple pipelines, but it\u2019s extremely cool and we\u2019ve spent a lot of time designing a robust engine). You can also call pipelines directly as API endpoints. We found them to be extremely useful for iterating on and separating LLM logic. Laminar also traces pipeline directly, which removes the overhead of sending large outputs over the network.<p>One thing missing from all LLM observability platforms right now is an adequate search over traces. We\u2019re attacking this problem by indexing each span in a vector DB and performing hybrid search at query time. This feature is still in beta, but we think it\u2019s gonna be crucial part of our platform going forward.<p>We also support evaluations. We loved the \u201crun everything locally, send results to a server\u201d approach from Braintrust and Weights &amp; Biases, so we did that too: a simple SDK and nice dashboards to track everything. Evals are still early, but we\u2019re pushing hard on them.<p>Our goal is to make Laminar the Supabase for LLMOps - the go-to open-source comprehensive platform for all things LLMs / GenAI. In it\u2019s current shape, Laminar is just few weeks old and developing rapidly, we\u2019d love any feedback or for you to give Laminar a try in your LLM projects!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["datadog"], "value": "Show HN: Laminar \u2013 Open-Source <em>DataDog</em> + PostHog for LLM Apps, Built in Rust"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/lmnr-ai/lmnr"}}, "_tags": ["story", "author_skull8888888", "story_41451698", "show_hn"], "author": "skull8888888", "children": [41452424, 41452994, 41453001, 41453058, 41453087, 41453241, 41453566, 41453832, 41455101, 41455103, 41456518, 41459224, 41459548, 41459620, 41462581], "created_at": "2024-09-04T22:52:19Z", "created_at_i": 1725490339, "num_comments": 45, "objectID": "41451698", "points": 203, "story_id": 41451698, "story_text": "Hey HN, we\u2019re Robert, Din and Temirlan from Laminar (<a href=\"https:&#x2F;&#x2F;www.lmnr.ai\">https:&#x2F;&#x2F;www.lmnr.ai</a>), an open-source observability and analytics platform for complex LLM apps. It\u2019s designed to be fast, reliable, and scalable. The stack is RabbitMQ for message queues, Postgres for storage, Clickhouse for analytics, Qdrant for semantic search - all powered by Rust.<p>How is Laminar different from the swarm of other \u201cLLM observability\u201d platforms?<p>On the observability part, we\u2019re focused on handling full execution traces, not just LLM calls. We built a Rust ingestor for OpenTelemetry (Otel) spans with GenAI semantic conventions. As LLM apps get more complex (think Agents with hundreds of LLM and function calls, or complex RAG pipelines), full tracing is critical. With Otel spans, we can: 1. Cover the entire execution trace. 2. Keep the platform future-proof 3. Leverage an amazing OpenLLMetry (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry\">https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry</a>), open-source package for span production.<p>The key difference is that we tie text analytics directly to execution traces. Rich text data makes LLM traces unique, so we let you track \u201csemantic metrics\u201d (like what your AI agent is actually saying) and connect those metrics to where they happen in the trace. If you want to know if your AI drive-through agent made an upsell, you can design an LLM extraction pipeline in our builder (more on it later), host it on Laminar, and handle everything from event requests to output logging. Processing requests simply come as events in the Otel span.<p>We think it\u2019s a win to separate core app logic from LLM event processing. Most devs don\u2019t want to manage background queues for LLM analytics processing but still want insights into how their Agents or RAGs are working.<p>Our Pipeline Builder uses graph UI where nodes are LLM and util functions, and edges showing data flow. We built a custom task execution engine with support of parallel branch executions, cycles and branches (it\u2019s overkill for simple pipelines, but it\u2019s extremely cool and we\u2019ve spent a lot of time designing a robust engine). You can also call pipelines directly as API endpoints. We found them to be extremely useful for iterating on and separating LLM logic. Laminar also traces pipeline directly, which removes the overhead of sending large outputs over the network.<p>One thing missing from all LLM observability platforms right now is an adequate search over traces. We\u2019re attacking this problem by indexing each span in a vector DB and performing hybrid search at query time. This feature is still in beta, but we think it\u2019s gonna be crucial part of our platform going forward.<p>We also support evaluations. We loved the \u201crun everything locally, send results to a server\u201d approach from Braintrust and Weights &amp; Biases, so we did that too: a simple SDK and nice dashboards to track everything. Evals are still early, but we\u2019re pushing hard on them.<p>Our goal is to make Laminar the Supabase for LLMOps - the go-to open-source comprehensive platform for all things LLMs &#x2F; GenAI. In it\u2019s current shape, Laminar is just few weeks old and developing rapidly, we\u2019d love any feedback or for you to give Laminar a try in your LLM projects!", "title": "Show HN: Laminar \u2013 Open-Source DataDog + PostHog for LLM Apps, Built in Rust", "updated_at": "2026-02-18T07:06:12Z", "url": "https://github.com/lmnr-ai/lmnr"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yasmind"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "Hi Hacker News! We launched an autonomous agent that helps debug <em>production</em> issues, and we\u2019re curious to get your feedback.<p>Today's GenAI devtools, such as Copilot, are limited: they are great for writing code, but we all know that programming is only 20% coding, and 80% debugging.<p>So how can GenAI be used for debugging? As opposed to code completion or test automation, <em>production</em> debugging is not about generating text. Debugging is mostly about root-cause analysis. We realized two things:<p>1) Generative AI is drastically changing the way we work with data, thanks to its ability to not only generate queries, but also run code and analyze unstructured data. This enables building better data-exploration experiences with far more intuitive interfaces.<p>2) RCA is all about exploring different types of data. When you don\u2019t know why a transaction was dropped or which customers are affected \u2013 you explore metrics, logs, your code, other people\u2019s code, old slack messages, and whatnot, to figure out what\u2019s broken.<p>Putting those two together, we built an autonomous agent that helps debug <em>production</em> issues. Our LLM &quot;moose&quot; (ok, it's corny but we like it) connects to your logs, metrics, and other observability data, and lets you extract and analyze them by conversing with it. Once it gets a task, it will start reasoning, invoking APIs, and running code, until it comes back with an answer.<p>For example, when requested to \u201cshow me IDs of transactions that took over 1 minute today\u201d, it will fetch those transactions from <em>Datadog</em> for you. You might then ask it if long-running transactions correlate with a metric such as DB CPU load. It will fetch the metric values, visualize them on a graph alongside the long transaction frequency, and give you the answer.<p>Our software both runs code and invokes API calls; the interplay between these is nontrivial to design and a fertile ground for innovation. There are \u201ctextbook\u201d solutions to let agents write and run code (open sourced by, for example, Open Interpreter), and also to invoke tools/APIs (for example, Gorilla). But doing both together is harder, and yet required. We welcome your thoughts on this!<p>Try our tool with your <em>Datadog</em>\u2019s logs and metrics &gt;&gt; <a href=\"https://app.wildmoose.ai/slack/install\">https://app.wildmoose.ai/slack/install</a><p>Setup demo &gt;&gt; <a href=\"https://www.loom.com/share/9a4adc39806742c48d14cdd39da6e560?sid=3d518e11-64b3-4139-82fd-451a3f055940\" rel=\"nofollow noreferrer\">https://www.loom.com/share/9a4adc39806742c48d14cdd39da6e560?...</a><p>If you want to see other integrations, or have ideas for features, or you\u2019ve spotted behaviors that seem off - we\u2019d love to hear. Hit us up in the comments!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Wild Moose \u2013 Autonomous agent for <em>production</em> debugging"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.wildmoose.ai/"}}, "_tags": ["story", "author_yasmind", "story_37753791", "show_hn"], "author": "yasmind", "children": [37757663, 37758210, 37758537, 37758552, 37759020, 37763047, 37769148, 37772967], "created_at": "2023-10-03T16:08:50Z", "created_at_i": 1696349330, "num_comments": 19, "objectID": "37753791", "points": 55, "story_id": 37753791, "story_text": "Hi Hacker News! We launched an autonomous agent that helps debug production issues, and we\u2019re curious to get your feedback.<p>Today&#x27;s GenAI devtools, such as Copilot, are limited: they are great for writing code, but we all know that programming is only 20% coding, and 80% debugging.<p>So how can GenAI be used for debugging? As opposed to code completion or test automation, production debugging is not about generating text. Debugging is mostly about root-cause analysis. We realized two things:<p>1) Generative AI is drastically changing the way we work with data, thanks to its ability to not only generate queries, but also run code and analyze unstructured data. This enables building better data-exploration experiences with far more intuitive interfaces.<p>2) RCA is all about exploring different types of data. When you don\u2019t know why a transaction was dropped or which customers are affected \u2013 you explore metrics, logs, your code, other people\u2019s code, old slack messages, and whatnot, to figure out what\u2019s broken.<p>Putting those two together, we built an autonomous agent that helps debug production issues. Our LLM &quot;moose&quot; (ok, it&#x27;s corny but we like it) connects to your logs, metrics, and other observability data, and lets you extract and analyze them by conversing with it. Once it gets a task, it will start reasoning, invoking APIs, and running code, until it comes back with an answer.<p>For example, when requested to \u201cshow me IDs of transactions that took over 1 minute today\u201d, it will fetch those transactions from Datadog for you. You might then ask it if long-running transactions correlate with a metric such as DB CPU load. It will fetch the metric values, visualize them on a graph alongside the long transaction frequency, and give you the answer.<p>Our software both runs code and invokes API calls; the interplay between these is nontrivial to design and a fertile ground for innovation. There are \u201ctextbook\u201d solutions to let agents write and run code (open sourced by, for example, Open Interpreter), and also to invoke tools&#x2F;APIs (for example, Gorilla). But doing both together is harder, and yet required. We welcome your thoughts on this!<p>Try our tool with your Datadog\u2019s logs and metrics &gt;&gt; <a href=\"https:&#x2F;&#x2F;app.wildmoose.ai&#x2F;slack&#x2F;install\">https:&#x2F;&#x2F;app.wildmoose.ai&#x2F;slack&#x2F;install</a><p>Setup demo &gt;&gt; <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;9a4adc39806742c48d14cdd39da6e560?sid=3d518e11-64b3-4139-82fd-451a3f055940\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;9a4adc39806742c48d14cdd39da6e560?...</a><p>If you want to see other integrations, or have ideas for features, or you\u2019ve spotted behaviors that seem off - we\u2019d love to hear. Hit us up in the comments!", "title": "Show HN: Wild Moose \u2013 Autonomous agent for production debugging", "updated_at": "2024-09-20T15:16:10Z", "url": "https://www.wildmoose.ai/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "talboren"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "Hi Hacker News! Shahar and Tal from Keep Here.<p>We were tired of creating alerts for our applications, so we've built an open-source GitHub Bot that lets you write application alerts using plain English. The code is open-sourced: <a href=\"https://github.com/keephq/keep\">https://github.com/keephq/keep</a> so you can review it yourself.<p>Every developer and DevOps professional is familiar with the fact that in order to ensure your application works in <em>production</em>, you need to access your observability tool's user interface (such as Grafana, <em>Datadog</em>, New Relic, etc.) and carefully determine how to create alerts that effectively monitor your application.<p>Instead, by installing Keep, every time you open a PR, the bot combines the alert description (alerts under the .keep directory) with the tool context (mostly the configuration of the alerts you already have) to generate (GPT) new alerts that keep you monitored.<p>So, for example, if you create a .keep/db-timeout.yaml and open a PR, the bot will comment on the PR with the actual alert you can deploy to your tool.<p># The alert text in plain English\nalert: |\n   Alert when the connections to the database are slower than 5 seconds for more than 5 minutes\nprovider: grafana<p>You can Install the bot and connect your providers via <a href=\"https://platform.keephq.dev\">https://platform.keephq.dev</a> (after login, you'll start the installation flow) or just clone the repository and use docker-compose to start the web app and the installation flow.<p>Demo Video - <a href=\"https://www.loom.com/share/23541a03944c4dca99b0504a1753d1b4\" rel=\"nofollow\">https://www.loom.com/share/23541a03944c4dca99b0504a1753d1b4</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Keep \u2013 Create <em>production</em> alerts from plain English"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/keephq/keep"}}, "_tags": ["story", "author_talboren", "story_36223543", "show_hn"], "author": "talboren", "children": [36223682, 36224517, 36224539, 36224729, 36226454, 36236962, 36238023, 36238796, 36298001], "created_at": "2023-06-07T06:25:48Z", "created_at_i": 1686119148, "num_comments": 13, "objectID": "36223543", "points": 46, "story_id": 36223543, "story_text": "Hi Hacker News! Shahar and Tal from Keep Here.<p>We were tired of creating alerts for our applications, so we&#x27;ve built an open-source GitHub Bot that lets you write application alerts using plain English. The code is open-sourced: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;keephq&#x2F;keep\">https:&#x2F;&#x2F;github.com&#x2F;keephq&#x2F;keep</a> so you can review it yourself.<p>Every developer and DevOps professional is familiar with the fact that in order to ensure your application works in production, you need to access your observability tool&#x27;s user interface (such as Grafana, Datadog, New Relic, etc.) and carefully determine how to create alerts that effectively monitor your application.<p>Instead, by installing Keep, every time you open a PR, the bot combines the alert description (alerts under the .keep directory) with the tool context (mostly the configuration of the alerts you already have) to generate (GPT) new alerts that keep you monitored.<p>So, for example, if you create a .keep&#x2F;db-timeout.yaml and open a PR, the bot will comment on the PR with the actual alert you can deploy to your tool.<p># The alert text in plain English\nalert: |\n   Alert when the connections to the database are slower than 5 seconds for more than 5 minutes\nprovider: grafana<p>You can Install the bot and connect your providers via <a href=\"https:&#x2F;&#x2F;platform.keephq.dev\">https:&#x2F;&#x2F;platform.keephq.dev</a> (after login, you&#x27;ll start the installation flow) or just clone the repository and use docker-compose to start the web app and the installation flow.<p>Demo Video - <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;23541a03944c4dca99b0504a1753d1b4\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;23541a03944c4dca99b0504a1753d1b4</a>", "title": "Show HN: Keep \u2013 Create production alerts from plain English", "updated_at": "2024-09-20T14:11:58Z", "url": "https://github.com/keephq/keep"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Dimittri"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https://sonarly.com\">https://sonarly.com</a>), an AI engineer for <em>production</em>. It connects to your observability tools like Sentry, <em>Datadog</em>, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here's a demo: <a href=\"https://www.youtube.com/watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https://www.youtube.com/watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from <em>production</em> alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it's a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in <em>production</em>, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can't catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don't need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don't want to add a new tracker or change their monitoring stack, as these platforms do the job they're supposed to do. So we decided to build above them. Now we connect to tools like Sentry, <em>Datadog</em>, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on <em>Datadog</em>/Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the <em>production</em> system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts/day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don't look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180/day to 50/day (by grouping issues) and giving a severity based on the impact on the user/infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https://sonarly.com\">https://sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), <em>Datadog</em> (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I'll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything's constructive!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your <em>production</em> alerts"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://sonarly.com/"}}, "_tags": ["story", "author_Dimittri", "story_47049776", "launch_hn"], "author": "Dimittri", "children": [47052409, 47054509, 47055028, 47055342, 47061697], "created_at": "2026-02-17T17:03:09Z", "created_at_i": 1771347789, "num_comments": 17, "objectID": "47049776", "points": 30, "story_id": 47049776, "story_text": "Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>), an AI engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here&#x27;s a demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it&#x27;s a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in production, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can&#x27;t catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don&#x27;t need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don&#x27;t want to add a new tracker or change their monitoring stack, as these platforms do the job they&#x27;re supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog&#x2F;Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts&#x2F;day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don&#x27;t look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180&#x2F;day to 50&#x2F;day (by grouping issues) and giving a severity based on the impact on the user&#x2F;infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I&#x27;ll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything&#x27;s constructive!", "title": "Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your production alerts", "updated_at": "2026-02-24T03:55:09Z", "url": "https://sonarly.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mikeshi42"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "Hi HN, Mike and Warren here! We\u2019ve been building an open source local-dev-friendly mode for HyperDX (hyperdx.io). It's a single Docker container that lets you view logs, metrics, and traces for local development in a similar way you can use them for <em>production</em> (live tail, filter/correlate logs &amp; spans, build charts, flamegraph, etc.) Basically, imagine you can run <em>Datadog</em> in a single container for local development.  We does this by spinning up an OpenTelemetry collector, Clickhouse DB and HyperDX UI all in a single container - it only takes a few seconds to start and lets you start live tailing your local logs and traces immediately (and graph metrics of course).<p>README (w/ demo gif): <a href=\"https://github.com/hyperdxio/hyperdx/blob/main/LOCAL.md\">https://github.com/hyperdxio/hyperdx/blob/main/LOCAL.md</a><p>We started building local mode as it became one of our most loved [1] community issue after sharing HyperDX. It made sense - as we constantly use HyperDX to debug our own issues locally, and have found it to be a huge productivity boost for things like\u2026<p>1. Tailing multiple container logs (and grepping/isolating them) without a ton of different terminal splits open at the same time.<p>2. Be able to sensibly view structured logs and correlated traces to debug local issues (because you\u2019re using structured logging and tracing right?) instead of adding ad-hoc console statements that print out exactly what I could\u2019ve gotten from tracing.<p>3. Testing telemetry are actually emitting/correlating as expected, before shipping it all the way out to prod and realizing I accidentally created a very expensive high-cardinality metric or a span that has the wrong properties.<p>We spent some time packaging &amp; tuning our existing OSS stack to run in a single container with less memory/space requirements by staring at `dive` to slim down the image and applied incantations from documentation until memory usage improved. Additionally we removed a few non-local-friendly things like authentication requirements and extraneous services.<p>It still has all the goodness you\u2019d want - so you can full text search your logs/traces, live tail all your events, view spans correlated with logs (and vice versa), create dashboards based on logs, metrics, traces, and is fully OpenTelemetry compatible - just point your Otel SDK/collector to <a href=\"http://localhost:4318\" rel=\"nofollow\">http://localhost:4318</a> (or 4317 for the grpc folks) and you\u2019re already good to go.<p>I\u2019m excited to share what we\u2019ve been working on and would love to hear your feedback and opinions!<p>Spin up the container yourself to try it out:<p>docker run -p 8000:8000 -p 4318:4318 -p 4317:4317 -p 8080:8080 -p 8002:8002 hyperdx/hyperdx-local<p>Main Open Source Repo: <a href=\"https://github.com/hyperdxio/hyperdx\">https://github.com/hyperdxio/hyperdx</a><p>Hosted Demo (in case you want to play around in a cloud sandbox instead): <a href=\"https://api.hyperdx.io/login/demo\">https://api.hyperdx.io/login/demo</a><p>HyperDX Landing Page: <a href=\"https://hyperdx.io\">https://hyperdx.io</a><p>[1]: <a href=\"https://github.com/hyperdxio/hyperdx/issues/7\">https://github.com/hyperdxio/hyperdx/issues/7</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["datadog"], "value": "Show HN: HyperDX Local \u2013 Open-source <em>Datadog</em> alternative for local debugging/dev"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/hyperdxio/hyperdx/blob/main/LOCAL.md"}}, "_tags": ["story", "author_mikeshi42", "story_39907137", "show_hn"], "author": "mikeshi42", "children": [39907383], "created_at": "2024-04-02T15:49:24Z", "created_at_i": 1712072964, "num_comments": 2, "objectID": "39907137", "points": 29, "story_id": 39907137, "story_text": "Hi HN, Mike and Warren here! We\u2019ve been building an open source local-dev-friendly mode for HyperDX (hyperdx.io). It&#x27;s a single Docker container that lets you view logs, metrics, and traces for local development in a similar way you can use them for production (live tail, filter&#x2F;correlate logs &amp; spans, build charts, flamegraph, etc.) Basically, imagine you can run Datadog in a single container for local development.  We does this by spinning up an OpenTelemetry collector, Clickhouse DB and HyperDX UI all in a single container - it only takes a few seconds to start and lets you start live tailing your local logs and traces immediately (and graph metrics of course).<p>README (w&#x2F; demo gif): <a href=\"https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdx&#x2F;blob&#x2F;main&#x2F;LOCAL.md\">https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdx&#x2F;blob&#x2F;main&#x2F;LOCAL.md</a><p>We started building local mode as it became one of our most loved [1] community issue after sharing HyperDX. It made sense - as we constantly use HyperDX to debug our own issues locally, and have found it to be a huge productivity boost for things like\u2026<p>1. Tailing multiple container logs (and grepping&#x2F;isolating them) without a ton of different terminal splits open at the same time.<p>2. Be able to sensibly view structured logs and correlated traces to debug local issues (because you\u2019re using structured logging and tracing right?) instead of adding ad-hoc console statements that print out exactly what I could\u2019ve gotten from tracing.<p>3. Testing telemetry are actually emitting&#x2F;correlating as expected, before shipping it all the way out to prod and realizing I accidentally created a very expensive high-cardinality metric or a span that has the wrong properties.<p>We spent some time packaging &amp; tuning our existing OSS stack to run in a single container with less memory&#x2F;space requirements by staring at `dive` to slim down the image and applied incantations from documentation until memory usage improved. Additionally we removed a few non-local-friendly things like authentication requirements and extraneous services.<p>It still has all the goodness you\u2019d want - so you can full text search your logs&#x2F;traces, live tail all your events, view spans correlated with logs (and vice versa), create dashboards based on logs, metrics, traces, and is fully OpenTelemetry compatible - just point your Otel SDK&#x2F;collector to <a href=\"http:&#x2F;&#x2F;localhost:4318\" rel=\"nofollow\">http:&#x2F;&#x2F;localhost:4318</a> (or 4317 for the grpc folks) and you\u2019re already good to go.<p>I\u2019m excited to share what we\u2019ve been working on and would love to hear your feedback and opinions!<p>Spin up the container yourself to try it out:<p>docker run -p 8000:8000 -p 4318:4318 -p 4317:4317 -p 8080:8080 -p 8002:8002 hyperdx&#x2F;hyperdx-local<p>Main Open Source Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdx\">https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdx</a><p>Hosted Demo (in case you want to play around in a cloud sandbox instead): <a href=\"https:&#x2F;&#x2F;api.hyperdx.io&#x2F;login&#x2F;demo\">https:&#x2F;&#x2F;api.hyperdx.io&#x2F;login&#x2F;demo</a><p>HyperDX Landing Page: <a href=\"https:&#x2F;&#x2F;hyperdx.io\">https:&#x2F;&#x2F;hyperdx.io</a><p>[1]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdx&#x2F;issues&#x2F;7\">https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdx&#x2F;issues&#x2F;7</a>", "title": "Show HN: HyperDX Local \u2013 Open-source Datadog alternative for local debugging/dev", "updated_at": "2024-09-20T16:44:28Z", "url": "https://github.com/hyperdxio/hyperdx/blob/main/LOCAL.md"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "devneelpatel"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "ABOUT ONEUPTIME: OneUptime (<a href=\"https://github.com/oneuptime/oneuptime\">https://github.com/oneuptime/oneuptime</a>) is the open-source alternative to <em>DataDog</em> + StausPage.io + UptimeRobot + Loggly + PagerDuty. It's 100% free and you can self-host it on your VM / server.<p>OneUptime has Uptime Monitoring, Logs Management, Status Pages, Tracing, On Call Software, Incident Management and more all under one platform.<p>Updates:<p>Several new monitor options launched - You can now monitor your SSL Certificates and Servers (Processes running, Mem, CPU, Disk, etc) Evaluate monitor metrics over time. You can set up alerts for things like - &quot;Create an incident when my website response time is &gt;5 seconds for 5 minutes&quot;. This wasn't possible before.<p>Added Logs ingestion with fluentd and OpenTelemetry. Traces and Metrics ingestion with OpenTelemetry.<p>Roadmap to end of Q2:<p>New Monitors: We will be working on new monitors options, specifically &quot;Log Monitor&quot;, &quot;Traces Monitor&quot;, &quot;Metrics Monitor&quot; where you can set up alerts for things like - if there are logs of error logs, create an incident and alert the team.<p><em>Datadog</em> like Dashboards coming soon.<p>Roadmap to end of Q3:<p>We're working on a reliability co-pilot. All you need to do is run a GitHub actions job / CI job where it scans your codebase, queries OneUptime API to get all the error's your software has seen in <em>production</em>. We then try to fix those errors and create PR's automatically. Making your software reliable and better every since day. None of your code will be sent to us. It'll stay on GitHub action runner. We will do this via a local LLM on the runner. Needless to say this will be beta and will getb better over time.<p>REQUEST FOR FEEDBACK &amp; FEATURES: This community has been kind to us. Thank you so much for all the feedback you've given us. This has helped make the software better. We're looking for more feedback as always. If you do have something in mind, please feel free to comment, talk to us, contribute. All of this goes a long way to make this software better for all of us to use.<p>OPEN SOURCE COMMITMENT: OneUptime is open source and free under Apache 2 license and always will be. We're 100% open-source, and not open-core (like others in our industry)"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["datadog"], "value": "Show HN: OneUptime (New Update) \u2013 Open-Source <em>Datadog</em> Alternative"}}, "_tags": ["story", "author_devneelpatel", "story_40285343", "show_hn"], "author": "devneelpatel", "children": [40287286, 40294114, 40306855, 40387405], "created_at": "2024-05-07T13:42:00Z", "created_at_i": 1715089320, "num_comments": 6, "objectID": "40285343", "points": 8, "story_id": 40285343, "story_text": "ABOUT ONEUPTIME: OneUptime (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;oneuptime&#x2F;oneuptime\">https:&#x2F;&#x2F;github.com&#x2F;oneuptime&#x2F;oneuptime</a>) is the open-source alternative to DataDog + StausPage.io + UptimeRobot + Loggly + PagerDuty. It&#x27;s 100% free and you can self-host it on your VM &#x2F; server.<p>OneUptime has Uptime Monitoring, Logs Management, Status Pages, Tracing, On Call Software, Incident Management and more all under one platform.<p>Updates:<p>Several new monitor options launched - You can now monitor your SSL Certificates and Servers (Processes running, Mem, CPU, Disk, etc) Evaluate monitor metrics over time. You can set up alerts for things like - &quot;Create an incident when my website response time is &gt;5 seconds for 5 minutes&quot;. This wasn&#x27;t possible before.<p>Added Logs ingestion with fluentd and OpenTelemetry. Traces and Metrics ingestion with OpenTelemetry.<p>Roadmap to end of Q2:<p>New Monitors: We will be working on new monitors options, specifically &quot;Log Monitor&quot;, &quot;Traces Monitor&quot;, &quot;Metrics Monitor&quot; where you can set up alerts for things like - if there are logs of error logs, create an incident and alert the team.<p>Datadog like Dashboards coming soon.<p>Roadmap to end of Q3:<p>We&#x27;re working on a reliability co-pilot. All you need to do is run a GitHub actions job &#x2F; CI job where it scans your codebase, queries OneUptime API to get all the error&#x27;s your software has seen in production. We then try to fix those errors and create PR&#x27;s automatically. Making your software reliable and better every since day. None of your code will be sent to us. It&#x27;ll stay on GitHub action runner. We will do this via a local LLM on the runner. Needless to say this will be beta and will getb better over time.<p>REQUEST FOR FEEDBACK &amp; FEATURES: This community has been kind to us. Thank you so much for all the feedback you&#x27;ve given us. This has helped make the software better. We&#x27;re looking for more feedback as always. If you do have something in mind, please feel free to comment, talk to us, contribute. All of this goes a long way to make this software better for all of us to use.<p>OPEN SOURCE COMMITMENT: OneUptime is open source and free under Apache 2 license and always will be. We&#x27;re 100% open-source, and not open-core (like others in our industry)", "title": "Show HN: OneUptime (New Update) \u2013 Open-Source Datadog Alternative", "updated_at": "2024-09-20T17:04:25Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "prasenjit_pro"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "Background<p>Proxies API is a rotating proxies API that developers use to fetch hard to scrape data at scale and consistently. We auto-rotate millions of proxy servers, and also handle auto retries, rotate user agent strings, handle cookies, CAPTCHAs behind the scenes.<p>The Constraints<p>The service has to scale to millions of URL fetches a day without lagging on speed and should work out of a single API like this.<p>curl \u201chttp://api.proxiesapi.com/?key=API_KEY&amp;url=https://example.com&quot;<p>The Architecture<p>The architecture looks somewhat like this.<p>A more in-depth look at this architecture is provided in this article.<p>Load balancing &amp; Serving<p>The whole infrastructure is on Amazon EC2. The load balancer plays a significant role in calling the right service based on just the variables passed and the API KEY that holds some secret information like what pool of servers are assigned for them etc.<p>Our plans have higher concurrency as customers pay more, so we need to pass them to more reliable, dedicated servers as they have higher concurrency needs.<p>Luckily amazon elastic load balancer can make this using rules you can set, which look at the URL patterns and direct them to different pools of servers intelligently.<p>Without this exact combination of facilities, we would not have been able to create it all using a single API call without compromising speed and performance somewhere.<p>We use Apache if there is a need for HTTP servers. Many of these are super optimized to be able to crawl data concurrently using multiple cores. We will soon write an article on how we can get the most amount of crawling ability out of Apache instances.<p>Proxy Components<p>For CGI, we use a combination of Python and even PHP where required.<p>Memcache is used to monitor and throttle concurrencies gracefully, so some rouge code by one of our clients doesn\u2019t bring down the entire setup.<p>As I described earlier, most of this throttling happens in the way we route out requests from the load balancer.<p>We use the Node Js/Puppeteer library extensively as an interface to connect to Browserless docker instances located on multiple servers. Browserless is optimized to run as many ready-to-go instances of Chromium as possible, so there is no lag in loading any of them for our clients.<p>Database<p>We use good old MySQL in a Master/Slave configuration to hold user info, cache info, millions of proxy info, quality metrics for each, etc.<p>Maintenance tools<p>We have a pool pruner that\u2019s written in Python running as a Cronjob. Uses Scrapy and Scrapyd extensively in making calls, monitoring, benchmarking, and pruning proxies from our database.<p>We use the Beanstalk library for Queues. We use <em>Datadog</em> for monitoring and PagerDuty for alerts.<p>The author is the founder of Proxies API, a proxy rotation API service.<p>This article originally appeared here:https://www.proxiesapi.com/blog/The-Stack-Behind-A-<em>Production</em>-Level-Rotating-Proxy-Service.php"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "The Stack Behind a <em>Production</em> Level Rotating Proxy Service"}}, "_tags": ["story", "author_prasenjit_pro", "story_33187751", "ask_hn"], "author": "prasenjit_pro", "children": [33187915, 33189247], "created_at": "2022-10-13T05:59:07Z", "created_at_i": 1665640747, "num_comments": 2, "objectID": "33187751", "points": 2, "story_id": 33187751, "story_text": "Background<p>Proxies API is a rotating proxies API that developers use to fetch hard to scrape data at scale and consistently. We auto-rotate millions of proxy servers, and also handle auto retries, rotate user agent strings, handle cookies, CAPTCHAs behind the scenes.<p>The Constraints<p>The service has to scale to millions of URL fetches a day without lagging on speed and should work out of a single API like this.<p>curl \u201chttp:&#x2F;&#x2F;api.proxiesapi.com&#x2F;?key=API_KEY&amp;url=https:&#x2F;&#x2F;example.com&quot;<p>The Architecture<p>The architecture looks somewhat like this.<p>A more in-depth look at this architecture is provided in this article.<p>Load balancing &amp; Serving<p>The whole infrastructure is on Amazon EC2. The load balancer plays a significant role in calling the right service based on just the variables passed and the API KEY that holds some secret information like what pool of servers are assigned for them etc.<p>Our plans have higher concurrency as customers pay more, so we need to pass them to more reliable, dedicated servers as they have higher concurrency needs.<p>Luckily amazon elastic load balancer can make this using rules you can set, which look at the URL patterns and direct them to different pools of servers intelligently.<p>Without this exact combination of facilities, we would not have been able to create it all using a single API call without compromising speed and performance somewhere.<p>We use Apache if there is a need for HTTP servers. Many of these are super optimized to be able to crawl data concurrently using multiple cores. We will soon write an article on how we can get the most amount of crawling ability out of Apache instances.<p>Proxy Components<p>For CGI, we use a combination of Python and even PHP where required.<p>Memcache is used to monitor and throttle concurrencies gracefully, so some rouge code by one of our clients doesn\u2019t bring down the entire setup.<p>As I described earlier, most of this throttling happens in the way we route out requests from the load balancer.<p>We use the Node Js&#x2F;Puppeteer library extensively as an interface to connect to Browserless docker instances located on multiple servers. Browserless is optimized to run as many ready-to-go instances of Chromium as possible, so there is no lag in loading any of them for our clients.<p>Database<p>We use good old MySQL in a Master&#x2F;Slave configuration to hold user info, cache info, millions of proxy info, quality metrics for each, etc.<p>Maintenance tools<p>We have a pool pruner that\u2019s written in Python running as a Cronjob. Uses Scrapy and Scrapyd extensively in making calls, monitoring, benchmarking, and pruning proxies from our database.<p>We use the Beanstalk library for Queues. We use Datadog for monitoring and PagerDuty for alerts.<p>The author is the founder of Proxies API, a proxy rotation API service.<p>This article originally appeared here:https:&#x2F;&#x2F;www.proxiesapi.com&#x2F;blog&#x2F;The-Stack-Behind-A-Production-Level-Rotating-Proxy-Service.php", "title": "The Stack Behind a Production Level Rotating Proxy Service", "updated_at": "2024-09-20T12:18:04Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "brntsllvn"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "<em>Datadog</em> browser synthetics: $12/1,000 runs. AWS synthetics is the cost of running a lambda plus whatever logs/metrics you want to retrieve/persist. Worth it? Why?<p>Main goals are cross-browser latency/Lighthouse and <em>production</em> testing on a cron schedule. Plus option to run during CICD and fail for latency threshold.<p>Better alternatives for synthetic testing? Cheaper, faster, hand rolled?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["datadog"], "value": "Ask HN: <em>Datadog</em> and AWS synthetics are expensive. Worth it? Why?"}}, "_tags": ["story", "author_brntsllvn", "story_32963050", "ask_hn"], "author": "brntsllvn", "created_at": "2022-09-24T14:42:58Z", "created_at_i": 1664030578, "num_comments": 0, "objectID": "32963050", "points": 2, "story_id": 32963050, "story_text": "Datadog browser synthetics: $12&#x2F;1,000 runs. AWS synthetics is the cost of running a lambda plus whatever logs&#x2F;metrics you want to retrieve&#x2F;persist. Worth it? Why?<p>Main goals are cross-browser latency&#x2F;Lighthouse and production testing on a cron schedule. Plus option to run during CICD and fail for latency threshold.<p>Better alternatives for synthetic testing? Cheaper, faster, hand rolled?", "title": "Ask HN: Datadog and AWS synthetics are expensive. Worth it? Why?", "updated_at": "2024-09-20T12:07:00Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "devneelpatel"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "ABOUT ONEUPTIME: OneUptime (<a href=\"https://github.com/oneuptime/oneuptime\">https://github.com/oneuptime/oneuptime</a>) is the open-source alternative to <em>DataDog</em> + StausPage.io + UptimeRobot + Loggly + PagerDuty. It's 100% free and you can self-host it on your VM / server.<p>OneUptime has Uptime Monitoring, Logs Management, Status Pages, Tracing, On Call Software, Incident Management and more all under one platform.<p>Updates:<p>Several new monitor options launched - You can now monitor your SSL Certificates and Servers (Processes running, Mem, CPU, Disk, etc) Evaluate monitor metrics over time. You can set up alerts for things like - &quot;Create an incident when my website response time is &gt;5 seconds for 5 minutes&quot;. This wasn't possible before.<p>Added Logs ingestion with fluentd and OpenTelemetry. Traces and Metrics ingestion with OpenTelemetry.<p>Roadmap to end of Q2:<p>New Monitors: We will be working on new monitors options, specifically &quot;Log Monitor&quot;, &quot;Traces Monitor&quot;, &quot;Metrics Monitor&quot; where you can set up alerts for things like - if there are logs of error logs, create an incident and alert the team.<p><em>Datadog</em> like Dashboards coming soon.<p>Roadmap to end of Q3:<p>We're working on a reliability co-pilot. All you need to do is run a GitHub actions job / CI job where it scans your codebase, queries OneUptime API to get all the error's your software has seen in <em>production</em>. We then try to fix those errors and create PR's automatically. Making your software reliable and better every since day. None of your code will be sent to us. It'll stay on GitHub action runner. We will do this via a local LLM on the runner. Needless to say this will be beta and will getb better over time.<p>REQUEST FOR FEEDBACK &amp; FEATURES: This community has been kind to us. Thank you so much for all the feedback you've given us. This has helped make the software better. We're looking for more feedback as always. If you do have something in mind, please feel free to comment, talk to us, contribute. All of this goes a long way to make this software better for all of us to use.<p>OPEN SOURCE COMMITMENT: OneUptime is open source and free under Apache 2 license and always will be. We're 100% open-source, and not open-core (like others in our industry)"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["datadog"], "value": "Show HN: OneUptime (New Update) \u2013 Open-Source <em>Datadog</em> Alternative"}}, "_tags": ["story", "author_devneelpatel", "story_40284070", "show_hn"], "author": "devneelpatel", "created_at": "2024-05-07T10:36:40Z", "created_at_i": 1715078200, "num_comments": 0, "objectID": "40284070", "points": 1, "story_id": 40284070, "story_text": "ABOUT ONEUPTIME: OneUptime (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;oneuptime&#x2F;oneuptime\">https:&#x2F;&#x2F;github.com&#x2F;oneuptime&#x2F;oneuptime</a>) is the open-source alternative to DataDog + StausPage.io + UptimeRobot + Loggly + PagerDuty. It&#x27;s 100% free and you can self-host it on your VM &#x2F; server.<p>OneUptime has Uptime Monitoring, Logs Management, Status Pages, Tracing, On Call Software, Incident Management and more all under one platform.<p>Updates:<p>Several new monitor options launched - You can now monitor your SSL Certificates and Servers (Processes running, Mem, CPU, Disk, etc) Evaluate monitor metrics over time. You can set up alerts for things like - &quot;Create an incident when my website response time is &gt;5 seconds for 5 minutes&quot;. This wasn&#x27;t possible before.<p>Added Logs ingestion with fluentd and OpenTelemetry. Traces and Metrics ingestion with OpenTelemetry.<p>Roadmap to end of Q2:<p>New Monitors: We will be working on new monitors options, specifically &quot;Log Monitor&quot;, &quot;Traces Monitor&quot;, &quot;Metrics Monitor&quot; where you can set up alerts for things like - if there are logs of error logs, create an incident and alert the team.<p>Datadog like Dashboards coming soon.<p>Roadmap to end of Q3:<p>We&#x27;re working on a reliability co-pilot. All you need to do is run a GitHub actions job &#x2F; CI job where it scans your codebase, queries OneUptime API to get all the error&#x27;s your software has seen in production. We then try to fix those errors and create PR&#x27;s automatically. Making your software reliable and better every since day. None of your code will be sent to us. It&#x27;ll stay on GitHub action runner. We will do this via a local LLM on the runner. Needless to say this will be beta and will getb better over time.<p>REQUEST FOR FEEDBACK &amp; FEATURES: This community has been kind to us. Thank you so much for all the feedback you&#x27;ve given us. This has helped make the software better. We&#x27;re looking for more feedback as always. If you do have something in mind, please feel free to comment, talk to us, contribute. All of this goes a long way to make this software better for all of us to use.<p>OPEN SOURCE COMMITMENT: OneUptime is open source and free under Apache 2 license and always will be. We&#x27;re 100% open-source, and not open-core (like others in our industry)", "title": "Show HN: OneUptime (New Update) \u2013 Open-Source Datadog Alternative", "updated_at": "2024-09-20T17:04:20Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pm3310"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "Let's say that I have developed a restful service using FAST API. Is there any tool or library that can provide an estimate of log costs for <em>Datadog</em> (or any other observability tool like Grafana or New Relic) for this service before pushing it to <em>production</em>? I assume I need to provide some input on expected traffic and where the logs are saved"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Get log cost estimate before pushing to <em>production</em>"}}, "_tags": ["story", "author_pm3310", "story_37118558", "ask_hn"], "author": "pm3310", "children": [37118695, 37118770], "created_at": "2023-08-14T07:59:50Z", "created_at_i": 1691999990, "num_comments": 0, "objectID": "37118558", "points": 1, "story_id": 37118558, "story_text": "Let&#x27;s say that I have developed a restful service using FAST API. Is there any tool or library that can provide an estimate of log costs for Datadog (or any other observability tool like Grafana or New Relic) for this service before pushing it to production? I assume I need to provide some input on expected traffic and where the logs are saved", "title": "Get log cost estimate before pushing to production", "updated_at": "2024-12-22T04:08:15Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "shardullavekar"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "Hi All, this is Shardul here - I am the co-founder of Videobug.<p>https://bug.video<p>We are super excited to share Videobug with you. Videobug records run time code execution so that developers can watch it line by line as frequently as they want, right in their IDE. It takes away the pain of recreating exact conditions that led to a bug and saves developer time in every bug squash.<p>Parth (my co-founder) and I have worked on multiple <em>production</em> grade applications in startups and enterprises.<p>We used Logrocket, Sentry, and <em>Datadog</em> for logging and found ourselves adding more logs after a bug is found.<p>Adding accurate logs required disciplined engineering and collaboration across teams. Time to connect these logs to what\u2019s wrong in the code, took quite sometime.<p>Videobug logs everything automatically. We are super psyched to launch our offline version. We plan to launch a fully self hosted version in 4 weeks from now, which will allow you to record and replay code executions in your staging and <em>production</em> environments.<p>We are looking for product feedback.<p>Here is a 2 min demo explaining how to use Videobug.<p>https://www.youtube.com/watch?v=U53IQifMt54<p>Join our discord channel to share your feedback or in case you need support.<p>https://discord.gg/Hhwvay8uTa<p>P.S. Please note that we collect the following analytics data from your IDE: Your computer hostname, debugging events such as \u201cstarted debugger\u201d, \u201cfetched exceptions\u201d and \u201csearched for code execution\u201d."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Videobug \u2013 The time travel debugger for JVM"}}, "_tags": ["story", "author_shardullavekar", "story_31286126", "show_hn"], "author": "shardullavekar", "children": [31286189, 31286242, 31286712, 31286967, 31287373, 31292311, 31292723], "created_at": "2022-05-06T15:13:41Z", "created_at_i": 1651850021, "num_comments": 17, "objectID": "31286126", "points": 63, "story_id": 31286126, "story_text": "Hi All, this is Shardul here - I am the co-founder of Videobug.<p>https:&#x2F;&#x2F;bug.video<p>We are super excited to share Videobug with you. Videobug records run time code execution so that developers can watch it line by line as frequently as they want, right in their IDE. It takes away the pain of recreating exact conditions that led to a bug and saves developer time in every bug squash.<p>Parth (my co-founder) and I have worked on multiple production grade applications in startups and enterprises.<p>We used Logrocket, Sentry, and Datadog for logging and found ourselves adding more logs after a bug is found.<p>Adding accurate logs required disciplined engineering and collaboration across teams. Time to connect these logs to what\u2019s wrong in the code, took quite sometime.<p>Videobug logs everything automatically. We are super psyched to launch our offline version. We plan to launch a fully self hosted version in 4 weeks from now, which will allow you to record and replay code executions in your staging and production environments.<p>We are looking for product feedback.<p>Here is a 2 min demo explaining how to use Videobug.<p>https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=U53IQifMt54<p>Join our discord channel to share your feedback or in case you need support.<p>https:&#x2F;&#x2F;discord.gg&#x2F;Hhwvay8uTa<p>P.S. Please note that we collect the following analytics data from your IDE: Your computer hostname, debugging events such as \u201cstarted debugger\u201d, \u201cfetched exceptions\u201d and \u201csearched for code execution\u201d.", "title": "Show HN: Videobug \u2013 The time travel debugger for JVM", "updated_at": "2024-09-20T11:08:17Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "theogravity"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "As a longtime TypeScript/Node.js developer, I've often faced challenges with logging\u2014choosing, using, and maintaining the right logger for various projects. While most loggers offer the usual methods like &quot;info&quot;, &quot;warn&quot;, and &quot;error&quot;, they vary significantly in how they handle structured metadata or Error objects. This can lead to ad-hoc solutions, like serializing errors or writing custom pipelines, just to get logs formatted correctly.<p>I built LogLayer to address these pain points by introducing a fluid, expressive API. With methods like &quot;withMetadata&quot; and &quot;withError&quot;, LogLayer separates object injection from the log message itself, making your logging code both cleaner and more maintainable.<p>Logs are processed through a LogLayer Transport, which acts as an adapter for your preferred logging library. This design offers several key advantages:<p>- Multi-Transport Support: Send logs to multiple destinations (e.g., <em>DataDog</em> and New Relic) simultaneously. I've personally used this feature to ship logs directly to <em>DataDog</em> without relying on their APM package or sidecars.<p>- Easy Logger Swapping: If you\u2019ve ever used Pino with Next.js, you might have encountered issues where it doesn\u2019t work out of the box after a <em>production</em> build without webpack hacks. With LogLayer, you can swap in a better-suited library without touching your logging code.<p>I spent a good few months on and off and used my winter break to launch version 5 of LogLayer, and also created the documentation using Vitepress.<p>LogLayer has been battle-tested in <em>production</em> at Airtop (<a href=\"https://airtop.ai\" rel=\"nofollow\">https://airtop.ai</a>), where it\u2019s been an integral part of our systems for years (we were running as Switchboard for almost four years and pivoted late last year).<p>(Disclaimer: I work at Airtop, but LogLayer is not sponsored / affiliated with them.)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LogLayer \u2013 Unified logger that routes logs to various logging libraries"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://loglayer.dev/"}}, "_tags": ["story", "author_theogravity", "story_42606454", "show_hn"], "author": "theogravity", "children": [42606937, 42607220, 42607255, 42608049, 42610137], "created_at": "2025-01-06T00:52:24Z", "created_at_i": 1736124744, "num_comments": 26, "objectID": "42606454", "points": 56, "story_id": 42606454, "story_text": "As a longtime TypeScript&#x2F;Node.js developer, I&#x27;ve often faced challenges with logging\u2014choosing, using, and maintaining the right logger for various projects. While most loggers offer the usual methods like &quot;info&quot;, &quot;warn&quot;, and &quot;error&quot;, they vary significantly in how they handle structured metadata or Error objects. This can lead to ad-hoc solutions, like serializing errors or writing custom pipelines, just to get logs formatted correctly.<p>I built LogLayer to address these pain points by introducing a fluid, expressive API. With methods like &quot;withMetadata&quot; and &quot;withError&quot;, LogLayer separates object injection from the log message itself, making your logging code both cleaner and more maintainable.<p>Logs are processed through a LogLayer Transport, which acts as an adapter for your preferred logging library. This design offers several key advantages:<p>- Multi-Transport Support: Send logs to multiple destinations (e.g., DataDog and New Relic) simultaneously. I&#x27;ve personally used this feature to ship logs directly to DataDog without relying on their APM package or sidecars.<p>- Easy Logger Swapping: If you\u2019ve ever used Pino with Next.js, you might have encountered issues where it doesn\u2019t work out of the box after a production build without webpack hacks. With LogLayer, you can swap in a better-suited library without touching your logging code.<p>I spent a good few months on and off and used my winter break to launch version 5 of LogLayer, and also created the documentation using Vitepress.<p>LogLayer has been battle-tested in production at Airtop (<a href=\"https:&#x2F;&#x2F;airtop.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;airtop.ai</a>), where it\u2019s been an integral part of our systems for years (we were running as Switchboard for almost four years and pivoted late last year).<p>(Disclaimer: I work at Airtop, but LogLayer is not sponsored &#x2F; affiliated with them.)", "title": "Show HN: LogLayer \u2013 Unified logger that routes logs to various logging libraries", "updated_at": "2025-09-22T20:27:10Z", "url": "https://loglayer.dev/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ivan_tsarynny"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["datadog", "production"], "value": "Hi HN! I'm Ivan, the co-founder of Feroot Security (YC W21) (<a href=\"https://www.feroot.com/\" rel=\"nofollow\">https://www.feroot.com/</a>). Feroot Inspector is a security scanner for the client-side javascript code of web apps made for app sec teams.<p>If you're not testing the security of the client-side code of your web app, there\u2019s a good chance you could be exposed to Magecart skimmers, malware and spyware loaded with third-party scripts - css, pixels, tags, trackers, and more. We use synthetic users (i.e. bots\u2014good ones!) to detect keyloggers, spyware, security misconfigurations, vulnerabilities, anomalies in the client-side code of web applications. Simulating activities that real users do, our scanner triggers all code activities first. And then it performs security testing and assessments of actual JavaScript code and everything else that is loaded into the browser when your users are using your web app. Pretty much what security scanners (like Qualys and Acunetix) are doing to test the application side code of web apps, but we do it for client-side code.<p>So why did we build Feroot? First, nobody knows what actually happens on the client-side of web apps. Client-side code is a mystery and nobody knows when keyloggers are stealing users\u2019 keystrokes or doing anything else sketchy. \nSecond, existing web app security testing tools don\u2019t perform data asset discovery. They don\u2019t tell you what web forms exist throughout the user journeys and what information is ingested by the web app through each and every web form. All that is missing. Third, client-side code of web apps is highly variable and dynamic. As web developers are moving logic to the client-side a lot more externally controlled JavaScript code is included into users\u2019 web browsers. Meaning, that every script, third-party and open source library can open a backdoor for hackers to exploit. \nWe saw a need for a simple self-serve solution that brings security, developers, marketing and compliance teams together to help them secure the client-side of web apps.<p>Feroot Inspector uses synthetic users and headless Chrome, which use algorithmic and heuristic approaches, to do activities that real users do -- type input into forms, submit forms to trigger potential keyloggers, skimmers, and all other  client-side script activities. It also monitors all incoming and outgoing network traffic from the browser and uses data traps to terminate outbound network requests, to avoid any impact during the scan.<p>Tech specs: 1) Support single-page/multiple-page web apps, and auto-discovery pm multi-page websites; 2) Resolves captchas, undetected by bot detection systems; 3) Tracks script changes, stores scripts content, detection of unauthorized scripts; 4) Audits page and frame security matrix, permission model for main frame of the page and all child-frames; 5) Detects data input and data ingestion points and report on data transfer, active data read (keystroke read), data access model; 6) Form-based authentication for scanning password-protected websites and custom scenario based authentication; 7) Detects data transfers from browser of user sessions to third-party hosts and domains; 8) Geo-decoding in real time of the destination country of data transfers; 8) Report export to: JSON (using API), CSV, Excel, and PDF; 9) Native Integrations: Slack, Jira, <em>Datadog</em>, PagerDuty, Splunk, JupiterOne, Sumo Logic, AWS Cloudwatch Events/logs, Opsgenie, ServiceNow, and webhooks; 10) Inspector performs non-intrusive, outside-in scanning of <em>production</em> live web apps.<p>We would love to hear your feedback about Feroot scanner, as well as answer questions you might have!<p>Thanks, Ivan &amp; Vitaliy"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Feroot (YC W21) \u2013 security scanner for front-end JavaScript code"}}, "_tags": ["story", "author_ivan_tsarynny", "story_26024912", "launch_hn"], "author": "ivan_tsarynny", "children": [26024913, 26025272, 26025662, 26025753, 26028330, 26028428, 26028764, 26028905, 26029152, 26029533, 26031600, 26032128, 26032565, 26047505], "created_at": "2021-02-04T12:55:30Z", "created_at_i": 1612443330, "num_comments": 10, "objectID": "26024912", "points": 47, "story_id": 26024912, "story_text": "Hi HN! I&#x27;m Ivan, the co-founder of Feroot Security (YC W21) (<a href=\"https:&#x2F;&#x2F;www.feroot.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.feroot.com&#x2F;</a>). Feroot Inspector is a security scanner for the client-side javascript code of web apps made for app sec teams.<p>If you&#x27;re not testing the security of the client-side code of your web app, there\u2019s a good chance you could be exposed to Magecart skimmers, malware and spyware loaded with third-party scripts - css, pixels, tags, trackers, and more. We use synthetic users (i.e. bots\u2014good ones!) to detect keyloggers, spyware, security misconfigurations, vulnerabilities, anomalies in the client-side code of web applications. Simulating activities that real users do, our scanner triggers all code activities first. And then it performs security testing and assessments of actual JavaScript code and everything else that is loaded into the browser when your users are using your web app. Pretty much what security scanners (like Qualys and Acunetix) are doing to test the application side code of web apps, but we do it for client-side code.<p>So why did we build Feroot? First, nobody knows what actually happens on the client-side of web apps. Client-side code is a mystery and nobody knows when keyloggers are stealing users\u2019 keystrokes or doing anything else sketchy. \nSecond, existing web app security testing tools don\u2019t perform data asset discovery. They don\u2019t tell you what web forms exist throughout the user journeys and what information is ingested by the web app through each and every web form. All that is missing. Third, client-side code of web apps is highly variable and dynamic. As web developers are moving logic to the client-side a lot more externally controlled JavaScript code is included into users\u2019 web browsers. Meaning, that every script, third-party and open source library can open a backdoor for hackers to exploit. \nWe saw a need for a simple self-serve solution that brings security, developers, marketing and compliance teams together to help them secure the client-side of web apps.<p>Feroot Inspector uses synthetic users and headless Chrome, which use algorithmic and heuristic approaches, to do activities that real users do -- type input into forms, submit forms to trigger potential keyloggers, skimmers, and all other  client-side script activities. It also monitors all incoming and outgoing network traffic from the browser and uses data traps to terminate outbound network requests, to avoid any impact during the scan.<p>Tech specs: 1) Support single-page&#x2F;multiple-page web apps, and auto-discovery pm multi-page websites; 2) Resolves captchas, undetected by bot detection systems; 3) Tracks script changes, stores scripts content, detection of unauthorized scripts; 4) Audits page and frame security matrix, permission model for main frame of the page and all child-frames; 5) Detects data input and data ingestion points and report on data transfer, active data read (keystroke read), data access model; 6) Form-based authentication for scanning password-protected websites and custom scenario based authentication; 7) Detects data transfers from browser of user sessions to third-party hosts and domains; 8) Geo-decoding in real time of the destination country of data transfers; 8) Report export to: JSON (using API), CSV, Excel, and PDF; 9) Native Integrations: Slack, Jira, Datadog, PagerDuty, Splunk, JupiterOne, Sumo Logic, AWS Cloudwatch Events&#x2F;logs, Opsgenie, ServiceNow, and webhooks; 10) Inspector performs non-intrusive, outside-in scanning of production live web apps.<p>We would love to hear your feedback about Feroot scanner, as well as answer questions you might have!<p>Thanks, Ivan &amp; Vitaliy", "title": "Launch HN: Feroot (YC W21) \u2013 security scanner for front-end JavaScript code", "updated_at": "2024-09-20T07:49:54Z"}], "hitsPerPage": 15, "nbHits": 37, "nbPages": 3, "page": 0, "params": "query=datadog+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 13, "processingTimingsMS": {"_request": {"roundTrip": 18}, "afterFetch": {"format": {"highlighting": 2, "total": 2}, "merge": {"mergeLoop": {"total": 1}, "total": 1}, "total": 1}, "fetch": {"query": 8, "scanning": 2, "total": 11}, "total": 13}, "query": "datadog production", "serverTimeMS": 17}}