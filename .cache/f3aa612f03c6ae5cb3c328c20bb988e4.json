{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "richrichie"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Non-<em>Linear</em> Effect of Temperature on Economic <em>Production</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "https://econjwatch.org/articles/global-non-<em>linear</em>-effect-of-temperature-on-economic-<em>production</em>-comment-on-burke-hsiang-and-miguel"}}, "_tags": ["story", "author_richrichie", "story_39936851"], "author": "richrichie", "children": [39936852, 39936938], "created_at": "2024-04-04T22:57:25Z", "created_at_i": 1712271445, "num_comments": 2, "objectID": "39936851", "points": 5, "story_id": 39936851, "title": "Non-Linear Effect of Temperature on Economic Production", "updated_at": "2024-09-20T16:48:37Z", "url": "https://econjwatch.org/articles/global-non-linear-effect-of-temperature-on-economic-production-comment-on-burke-hsiang-and-miguel"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "cryoshon"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Global non-<em>linear</em> effect of temperature on economic <em>production</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "http://www.nature.com/nature/journal/vaop/ncurrent/full/nature15725.html"}}, "_tags": ["story", "author_cryoshon", "story_10465339"], "author": "cryoshon", "children": [10465363], "created_at": "2015-10-28T16:21:42Z", "created_at_i": 1446049302, "num_comments": 1, "objectID": "10465339", "points": 1, "story_id": 10465339, "title": "Global non-linear effect of temperature on economic production", "updated_at": "2024-09-19T22:25:16Z", "url": "http://www.nature.com/nature/journal/vaop/ncurrent/full/nature15725.html"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "buildinext"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Hey HN! I've been building Quell, an AI QA testing agent designed to run tests directly triggered from issue trackers (Jira, <em>Linear</em>) or designs (Figma) or CI/CD platforms (Vercel, Netlify, GitHub), ensuring rapid and accurate acceptance testing without manual effort.<p>What problem does it solve?\nAs a product manager and founder myself, I constantly faced issues releasing a new feature only to discover critical bugs or that the build doesn't fully meet acceptance criteria. Bottlenecks in our QA and release process\u2014tickets and issues stuck in manual testing, slow deployments due to delayed verification, and missed acceptance criteria leading to bugs slipping into <em>production</em>. I built Quell to automate these tedious steps, freeing up teams to focus on actual feature development and faster iterations.<p>How is it different?<p>Integrates with existing dev workflows\u2014triggers tests via Jira/<em>Linear</em> issue status automatically testing Vercel, or Netlify or other URL deployment builds.<p>Tests against explicit acceptance criteria pulled directly from your issue tracker.<p>Current Capabilities (free to test):<p>Automatically trigger QA runs from <em>Linear</em>/Jira issue state transitions or Vercel/Netlify deploy previews.<p>Generate immediate test reports and tickets for issues spotted.<p>Quell is ready to test right now\u2014email only required to try out demo functionality directly:<p>Try out [Quellit.ai](<a href=\"http://quellit.ai/\" rel=\"nofollow\">http://quellit.ai/</a>) for free<p>I'm actively iterating based on user feedback\u2014would love to hear your thoughts, suggestions, or even criticisms on the idea, implementation, integrations, or anything else.<p>Thanks for checking it out!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["linear"], "value": "Show HN: Quell \u2013 AI QA Agent Working Across <em>Linear</em>, Vercel, Jira, Netlify, Figma"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.quellit.ai/"}}, "_tags": ["story", "author_buildinext", "story_44083596", "show_hn"], "author": "buildinext", "children": [44187584, 44193749], "created_at": "2025-05-24T20:25:34Z", "created_at_i": 1748118334, "num_comments": 2, "objectID": "44083596", "points": 7, "story_id": 44083596, "story_text": "Hey HN! I&#x27;ve been building Quell, an AI QA testing agent designed to run tests directly triggered from issue trackers (Jira, Linear) or designs (Figma) or CI&#x2F;CD platforms (Vercel, Netlify, GitHub), ensuring rapid and accurate acceptance testing without manual effort.<p>What problem does it solve?\nAs a product manager and founder myself, I constantly faced issues releasing a new feature only to discover critical bugs or that the build doesn&#x27;t fully meet acceptance criteria. Bottlenecks in our QA and release process\u2014tickets and issues stuck in manual testing, slow deployments due to delayed verification, and missed acceptance criteria leading to bugs slipping into production. I built Quell to automate these tedious steps, freeing up teams to focus on actual feature development and faster iterations.<p>How is it different?<p>Integrates with existing dev workflows\u2014triggers tests via Jira&#x2F;Linear issue status automatically testing Vercel, or Netlify or other URL deployment builds.<p>Tests against explicit acceptance criteria pulled directly from your issue tracker.<p>Current Capabilities (free to test):<p>Automatically trigger QA runs from Linear&#x2F;Jira issue state transitions or Vercel&#x2F;Netlify deploy previews.<p>Generate immediate test reports and tickets for issues spotted.<p>Quell is ready to test right now\u2014email only required to try out demo functionality directly:<p>Try out [Quellit.ai](<a href=\"http:&#x2F;&#x2F;quellit.ai&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;quellit.ai&#x2F;</a>) for free<p>I&#x27;m actively iterating based on user feedback\u2014would love to hear your thoughts, suggestions, or even criticisms on the idea, implementation, integrations, or anything else.<p>Thanks for checking it out!", "title": "Show HN: Quell \u2013 AI QA Agent Working Across Linear, Vercel, Jira, Netlify, Figma", "updated_at": "2025-06-06T04:40:09Z", "url": "https://www.quellit.ai/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kmelve"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Hey HN! We forked styled-components after it entered maintenance mode because \nour <em>production</em> apps (and many others) can't migrate overnight.<p>Backstory: We submitted PR #4332 (<a href=\"https://github.com/styled-components/styled-components/pull/4332\" rel=\"nofollow\">https://github.com/styled-components/styled-components/pull/...</a>) to styled-components in July 2024 with React 18 \noptimizations. When maintenance mode was announced, we turned that PR into this fork.<p>What we fixed:\n- Added React 18's useInsertionEffect\n- Rewrote streaming SSR for React 19 \n- Replaced ES5 output with modern JS\n- Optimized array operations with native flatMap\n- Fixed Next.js App Router to work without 50+ lines of boilerplate<p><em>Linear</em> tested it and saw 40% faster initial renders with zero code changes.<p>How to try it:\n  npm install @sanity/styled-components@npm:styled-components<p>Or for React 19:\n  npm install @sanity/css-in-js@npm:styled-components<p>Benchmark tool to test yourself: <a href=\"https://css-in-js-benchmarks.sanity.dev/\" rel=\"nofollow\">https://css-in-js-benchmarks.sanity.dev/</a><p>We named it &quot;last-resort&quot; because that's what it is. We're not trying to maintain \nstyled-components long-term - we're actually migrating to vanilla-extract ourselves. \nThis is just a performance bridge while teams migrate properly.<p>The React team recommends moving away from runtime CSS injection. We agree. But \nmigrations take time, and <em>production</em> apps need to ship today."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["linear"], "value": "Show HN: Forked styled-components with optimizations (40% faster for <em>Linear</em>)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/sanity-io/styled-components-last-resort"}}, "_tags": ["story", "author_kmelve", "story_45213836", "show_hn"], "author": "kmelve", "children": [45215183], "created_at": "2025-09-11T17:07:03Z", "created_at_i": 1757610423, "num_comments": 1, "objectID": "45213836", "points": 4, "story_id": 45213836, "story_text": "Hey HN! We forked styled-components after it entered maintenance mode because \nour production apps (and many others) can&#x27;t migrate overnight.<p>Backstory: We submitted PR #4332 (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;styled-components&#x2F;styled-components&#x2F;pull&#x2F;4332\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;styled-components&#x2F;styled-components&#x2F;pull&#x2F;...</a>) to styled-components in July 2024 with React 18 \noptimizations. When maintenance mode was announced, we turned that PR into this fork.<p>What we fixed:\n- Added React 18&#x27;s useInsertionEffect\n- Rewrote streaming SSR for React 19 \n- Replaced ES5 output with modern JS\n- Optimized array operations with native flatMap\n- Fixed Next.js App Router to work without 50+ lines of boilerplate<p>Linear tested it and saw 40% faster initial renders with zero code changes.<p>How to try it:\n  npm install @sanity&#x2F;styled-components@npm:styled-components<p>Or for React 19:\n  npm install @sanity&#x2F;css-in-js@npm:styled-components<p>Benchmark tool to test yourself: <a href=\"https:&#x2F;&#x2F;css-in-js-benchmarks.sanity.dev&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;css-in-js-benchmarks.sanity.dev&#x2F;</a><p>We named it &quot;last-resort&quot; because that&#x27;s what it is. We&#x27;re not trying to maintain \nstyled-components long-term - we&#x27;re actually migrating to vanilla-extract ourselves. \nThis is just a performance bridge while teams migrate properly.<p>The React team recommends moving away from runtime CSS injection. We agree. But \nmigrations take time, and production apps need to ship today.", "title": "Show HN: Forked styled-components with optimizations (40% faster for Linear)", "updated_at": "2025-09-11T19:21:46Z", "url": "https://github.com/sanity-io/styled-components-last-resort"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "gb_la"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "My goal is to transition out of a film/commercial <em>production</em> coordinator position and (eventually) into a project management role within a startup/tech firm. I've spoken with a few friends/colleagues who have made similar transitions and most think it wise to start in an account management role to build up a core competency. I am open to anything that would get me in the door and moving in the right direction. Any recommendations for good starting points?<p>I hope/believe much of my skill set is transferrable as a film/commercial <em>production</em> shares many (oversimplified, of course) similarities in terms of workflow: development of concept/idea, establishing plan/objectives, resource allocation, facilitating information flow, ensuring project stays on budget/schedule and removing roadblocks/problem solving. Though, I do understand this line of thinking requires a significant leap of faith on behalf of any potential employer, so I am working on a way to market myself effectively. May be tough but I am dying for something with a more <em>linear</em> career path offering stability and more challenging/exciting work.<p>If anyone has any suggestions or advice I'd really appreciate it. I realize this won't be a direct or even simple transition so I could use all the help I can get. Thank you in advance!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: How to transition from film <em>production</em> to project management?"}, "url": {"matchLevel": "none", "matchedWords": [], "value": ""}}, "_tags": ["story", "author_gb_la", "story_5822812", "ask_hn"], "author": "gb_la", "created_at": "2013-06-04T23:45:07Z", "created_at_i": 1370389507, "num_comments": 0, "objectID": "5822812", "points": 1, "story_id": 5822812, "story_text": "My goal is to transition out of a film/commercial production coordinator position and (eventually) into a project management role within a startup/tech firm. I've spoken with a few friends/colleagues who have made similar transitions and most think it wise to start in an account management role to build up a core competency. I am open to anything that would get me in the door and moving in the right direction. Any recommendations for good starting points?<p>I hope/believe much of my skill set is transferrable as a film/commercial production shares many (oversimplified, of course) similarities in terms of workflow: development of concept/idea, establishing plan/objectives, resource allocation, facilitating information flow, ensuring project stays on budget/schedule and removing roadblocks/problem solving. Though, I do understand this line of thinking requires a significant leap of faith on behalf of any potential employer, so I am working on a way to market myself effectively. May be tough but I am dying for something with a more linear career path offering stability and more challenging/exciting work.<p>If anyone has any suggestions or advice I'd really appreciate it. I realize this won't be a direct or even simple transition so I could use all the help I can get. Thank you in advance!", "title": "Ask HN: How to transition from film production to project management?", "updated_at": "2023-09-06T21:40:22Z", "url": ""}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Winipedia"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "I built pyrig because I did the same setup everytime I started a proper python project. I genuinely believe this can be a great tool, for me it has already genuinely simplified my python project development. I would love feedback on this of any kind. Below is a description of what pyrig is and does.<p>pyrig generates and maintains a complete, <em>production</em>-ready Python project from a single command. It creates all the files you need \u2014 source structure, tests, CI/CD, documentation, configs \u2014 and keeps them in sync as your project evolves.<p>uv init<p>uv add pyrig<p>uv run pyrig init<p>You get: package structure with a CLI entry point, test framework with 90% coverage enforcement, GitHub Actions (CI/CD, releases, docs deployment), pre-commit hooks, MkDocs documentation site, and container support.<p>The key difference from templates like cookiecutter is that pyrig isn't a one-shot scaffold. It's idempotent \u2014 rerun it anytime to update configs, add missing files, or sync with the latest defaults. It merges missing values without removing your customizations.<p>Under the hood, each config file (pyproject.toml, GitHub workflows, .gitignore, etc.) is a Python class that generates the file with sensible defaults, validates it, and merges in anything missing. You can create custom configs by subclassing \u2014 pyrig discovers them automatically.<p>It also supports multi-package inheritance, so you can build a multiproject-wide base on top of pyrig and have all your projects inherit the same configs, CLI commands, test fixtures, and build steps:<p>pyrig \u2192 service-base \n\u2192 auth-service<p>\u2192 payment-service<p>\u2192 notification-service<p>Opinionated: Python 3.12+, all ruff rules enabled, strict type checking with ty, 90% test coverage minimum, <em>linear</em> git history with branch protection.<p>GitHub: <a href=\"https://github.com/Winipedia/pyrig\" rel=\"nofollow\">https://github.com/Winipedia/pyrig</a><p>Docs: <a href=\"https://winipedia.github.io/pyrig\" rel=\"nofollow\">https://winipedia.github.io/pyrig</a><p>Code-Wiki AI Docs: <a href=\"https://codewiki.google/github.com/winipedia/pyrig\" rel=\"nofollow\">https://codewiki.google/github.com/winipedia/pyrig</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Pyrig \u2013 One command to set up a <em>production</em>-ready Python project"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Winipedia/pyrig"}}, "_tags": ["story", "author_Winipedia", "story_46926941", "show_hn"], "author": "Winipedia", "created_at": "2026-02-07T19:39:57Z", "created_at_i": 1770493197, "num_comments": 0, "objectID": "46926941", "points": 1, "story_id": 46926941, "story_text": "I built pyrig because I did the same setup everytime I started a proper python project. I genuinely believe this can be a great tool, for me it has already genuinely simplified my python project development. I would love feedback on this of any kind. Below is a description of what pyrig is and does.<p>pyrig generates and maintains a complete, production-ready Python project from a single command. It creates all the files you need \u2014 source structure, tests, CI&#x2F;CD, documentation, configs \u2014 and keeps them in sync as your project evolves.<p>uv init<p>uv add pyrig<p>uv run pyrig init<p>You get: package structure with a CLI entry point, test framework with 90% coverage enforcement, GitHub Actions (CI&#x2F;CD, releases, docs deployment), pre-commit hooks, MkDocs documentation site, and container support.<p>The key difference from templates like cookiecutter is that pyrig isn&#x27;t a one-shot scaffold. It&#x27;s idempotent \u2014 rerun it anytime to update configs, add missing files, or sync with the latest defaults. It merges missing values without removing your customizations.<p>Under the hood, each config file (pyproject.toml, GitHub workflows, .gitignore, etc.) is a Python class that generates the file with sensible defaults, validates it, and merges in anything missing. You can create custom configs by subclassing \u2014 pyrig discovers them automatically.<p>It also supports multi-package inheritance, so you can build a multiproject-wide base on top of pyrig and have all your projects inherit the same configs, CLI commands, test fixtures, and build steps:<p>pyrig \u2192 service-base \n\u2192 auth-service<p>\u2192 payment-service<p>\u2192 notification-service<p>Opinionated: Python 3.12+, all ruff rules enabled, strict type checking with ty, 90% test coverage minimum, linear git history with branch protection.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Winipedia&#x2F;pyrig\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Winipedia&#x2F;pyrig</a><p>Docs: <a href=\"https:&#x2F;&#x2F;winipedia.github.io&#x2F;pyrig\" rel=\"nofollow\">https:&#x2F;&#x2F;winipedia.github.io&#x2F;pyrig</a><p>Code-Wiki AI Docs: <a href=\"https:&#x2F;&#x2F;codewiki.google&#x2F;github.com&#x2F;winipedia&#x2F;pyrig\" rel=\"nofollow\">https:&#x2F;&#x2F;codewiki.google&#x2F;github.com&#x2F;winipedia&#x2F;pyrig</a>", "title": "Show HN: Pyrig \u2013 One command to set up a production-ready Python project", "updated_at": "2026-02-07T19:45:46Z", "url": "https://github.com/Winipedia/pyrig"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "marcuslima"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Hi HN, I\u2019m Marcus, I\u2019m the co-founder of Heimdal together with Erik (<a href=\"https://www.heimdalccu.com/\" rel=\"nofollow\">https://www.heimdalccu.com/</a>). We remove atmospheric carbon dioxide and trap it in materials that are used to make cement. More CO2 is trapped in our process than is re-emitted in cement <em>production</em>.<p>Concrete is responsible for 8% of global CO2 emissions. Cement is usually made from mined limestone, which is one of the largest natural stores of carbon dioxide. Using that to make cement is a bit like burning oil. The world is addicted to concrete, so this problem is not going away. We make synthetic limestone using atmospheric CO2, such that when it is used to make cement, the process is carbon neutral.<p>We were both master's students in engineering at Oxford University in the UK. I decided to write my dissertation on direct air capture of CO2. While looking through existing solutions it struck me that none were sufficient. They all operated a circular process that left them with gaseous CO2 that needed to be stored somewhere. A circular process is one that uses a sorbent to trap atmospheric CO2 but then re-releases the trapped CO2 as a pure gas stream to regenerate the sorbent for re-use. We don't have enough high-quality cheap stores of CO2 to justify such an approach. Storage must be permanent and safe. We realized that by taking a <em>linear</em> approach, we both make the process of capturing CO2 profitable and avoid the problem of where to store the CO2. We make sorbents for trapping CO2 in the form of mineral carbonates, these compounds are inert and trap CO2 for millions of years. They can also be commercialized as raw materials for making building materials including glass and concrete. In one step we solve three key problems of carbon capture: 1. How to trap CO2 energy efficiently 2. How to store the CO2 3. How to make money while doing all this.<p>Specifically, we use renewable electricity to extract dissolved oceanic CO2 as mineral carbonates of calcium and magnesium by contacting seawater with our proprietary alkaline sorbent. These mineral carbonates are important ingredients in cement as well as other building materials. The undersaturated ocean then re-absorbs an amount of atmospheric CO2 equivalent to the amount we removed when reacting with our sorbent. Effectively, the world\u2019s oceans become our air contactor.<p>There are other companies addressing emissions from concrete <em>production</em>, but they don\u2019t address the unavoidable process emission from the raw materials used in concrete. Start-ups in this space have so far focused on curing concrete with CO2 at the end of the <em>production</em> process. These are great solutions that can create low-carbon cement, however they\u2019ll never get to carbon neutral cement that the world needs. The 70% of emissions from <em>production</em> are not being tackled by anyone on the market today. Until now concrete producers have favoured capturing emissions at the point where they\u2019re released as their \u201c2050-solution\u201d, ie. in the distant future. Point source carbon capture can expensively capture 80-90% of emissions. This solution has the same problem as circular DAC solutions where a method of permanent CO2 storage is needed. There is a trial $3B (!) project in Norway to pump CO2 into empty gas fields at a cost of ~$1000/tCO2. This is expensive and complicated engineering. On the other hand, all we need is renewable electricity and seawater.<p>We make money from selling synthetic limestone to cement producers and commercializing parallel byproducts including green hydrogen and desalinated water. We also generate carbon credits from our process. We are currently negotiating with concrete producers to decarbonize their limestone supply. Response has so far been very positive with multiple LOIs signed with producers across Europe. We are also working with a construction company to build the world\u2019s first carbon neutral houses this decade. We are currently building a demo plant just outside Oxford. It has the capacity to remove and store 1 tonne of CO2 per year. We will use this plant to make enough product that we can deliver to our commercial partners to confirm compatibility with their manufacturing set-up. Following successful testing, we will scale this up to replace all of global limestone mining; currently &gt;2 billion tonnes of limestone per year.<p>We're excited to hear any thoughts, insights, questions, encouragement and concerns in the comments below! Erik and I will be monitoring the thread over the course of today to answer any questions. Also feel free to reach out to me by email at Marcus.lima@heimdalccu.com."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Heimdal (YC S21) \u2013 Carbon neutral cement"}}, "_tags": ["story", "author_marcuslima", "story_28036927", "launch_hn"], "author": "marcuslima", "children": [28037225, 28037339, 28037451, 28037489, 28037570, 28037760, 28037810, 28037841, 28037910, 28038041, 28038215, 28038287, 28038309, 28038445, 28038926, 28039068, 28039156, 28039248, 28039512, 28039595, 28039780, 28039927, 28039929, 28040187, 28040345, 28040619, 28041044, 28041166, 28041203, 28041261, 28041328, 28041348, 28041627, 28041927, 28042325, 28042338, 28042734, 28043106, 28043158, 28043266, 28043673, 28043837, 28044179, 28044266, 28044346, 28044453, 28044506, 28045166, 28045252, 28045257, 28045259, 28045450, 28045513, 28045525, 28045540, 28045581, 28045595, 28046262, 28046965, 28047050, 28047735, 28047757, 28047865, 28052887, 28054109, 28065742, 28111242, 28124661], "created_at": "2021-08-02T14:26:09Z", "created_at_i": 1627914369, "num_comments": 220, "objectID": "28036927", "points": 617, "story_id": 28036927, "story_text": "Hi HN, I\u2019m Marcus, I\u2019m the co-founder of Heimdal together with Erik (<a href=\"https:&#x2F;&#x2F;www.heimdalccu.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.heimdalccu.com&#x2F;</a>). We remove atmospheric carbon dioxide and trap it in materials that are used to make cement. More CO2 is trapped in our process than is re-emitted in cement production.<p>Concrete is responsible for 8% of global CO2 emissions. Cement is usually made from mined limestone, which is one of the largest natural stores of carbon dioxide. Using that to make cement is a bit like burning oil. The world is addicted to concrete, so this problem is not going away. We make synthetic limestone using atmospheric CO2, such that when it is used to make cement, the process is carbon neutral.<p>We were both master&#x27;s students in engineering at Oxford University in the UK. I decided to write my dissertation on direct air capture of CO2. While looking through existing solutions it struck me that none were sufficient. They all operated a circular process that left them with gaseous CO2 that needed to be stored somewhere. A circular process is one that uses a sorbent to trap atmospheric CO2 but then re-releases the trapped CO2 as a pure gas stream to regenerate the sorbent for re-use. We don&#x27;t have enough high-quality cheap stores of CO2 to justify such an approach. Storage must be permanent and safe. We realized that by taking a linear approach, we both make the process of capturing CO2 profitable and avoid the problem of where to store the CO2. We make sorbents for trapping CO2 in the form of mineral carbonates, these compounds are inert and trap CO2 for millions of years. They can also be commercialized as raw materials for making building materials including glass and concrete. In one step we solve three key problems of carbon capture: 1. How to trap CO2 energy efficiently 2. How to store the CO2 3. How to make money while doing all this.<p>Specifically, we use renewable electricity to extract dissolved oceanic CO2 as mineral carbonates of calcium and magnesium by contacting seawater with our proprietary alkaline sorbent. These mineral carbonates are important ingredients in cement as well as other building materials. The undersaturated ocean then re-absorbs an amount of atmospheric CO2 equivalent to the amount we removed when reacting with our sorbent. Effectively, the world\u2019s oceans become our air contactor.<p>There are other companies addressing emissions from concrete production, but they don\u2019t address the unavoidable process emission from the raw materials used in concrete. Start-ups in this space have so far focused on curing concrete with CO2 at the end of the production process. These are great solutions that can create low-carbon cement, however they\u2019ll never get to carbon neutral cement that the world needs. The 70% of emissions from production are not being tackled by anyone on the market today. Until now concrete producers have favoured capturing emissions at the point where they\u2019re released as their \u201c2050-solution\u201d, ie. in the distant future. Point source carbon capture can expensively capture 80-90% of emissions. This solution has the same problem as circular DAC solutions where a method of permanent CO2 storage is needed. There is a trial $3B (!) project in Norway to pump CO2 into empty gas fields at a cost of ~$1000&#x2F;tCO2. This is expensive and complicated engineering. On the other hand, all we need is renewable electricity and seawater.<p>We make money from selling synthetic limestone to cement producers and commercializing parallel byproducts including green hydrogen and desalinated water. We also generate carbon credits from our process. We are currently negotiating with concrete producers to decarbonize their limestone supply. Response has so far been very positive with multiple LOIs signed with producers across Europe. We are also working with a construction company to build the world\u2019s first carbon neutral houses this decade. We are currently building a demo plant just outside Oxford. It has the capacity to remove and store 1 tonne of CO2 per year. We will use this plant to make enough product that we can deliver to our commercial partners to confirm compatibility with their manufacturing set-up. Following successful testing, we will scale this up to replace all of global limestone mining; currently &gt;2 billion tonnes of limestone per year.<p>We&#x27;re excited to hear any thoughts, insights, questions, encouragement and concerns in the comments below! Erik and I will be monitoring the thread over the course of today to answer any questions. Also feel free to reach out to me by email at Marcus.lima@heimdalccu.com.", "title": "Launch HN: Heimdal (YC S21) \u2013 Carbon neutral cement", "updated_at": "2024-09-20T09:04:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "waleedlatif1"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Hey HN, Waleed here. We're building Sim (<a href=\"https://sim.ai/\">https://sim.ai/</a>), an open-source visual editor to build agentic workflows. Repo here: <a href=\"https://github.com/simstudioai/sim/\" rel=\"nofollow\">https://github.com/simstudioai/sim/</a>. Docs here: <a href=\"https://docs.sim.ai\">https://docs.sim.ai</a>.<p>You can run Sim locally using Docker, with no execution limits or other restrictions.<p>We started building Sim almost a year ago after repeatedly troubleshooting why our agents failed in <em>production</em>. Code-first frameworks felt hard to debug because of implicit control flow, and workflow platforms added more overhead than they removed. We wanted granular control and easy observability without piecing everything together ourselves.<p>We launched Sim [1][2] as a drag-and-drop canvas around 6 months ago. Since then, we've added:<p>- 138 blocks: Slack, GitHub, <em>Linear</em>, Notion, Supabase, SSH, TTS, SFTP, MongoDB, S3, Pinecone, ...<p>- Tool calling with granular control: forced, auto<p>- Agent memory: conversation memory with sliding window support (by last n messages or tokens)<p>- Trace spans: detailed logging and observability for nested workflows and tool calling<p>- Native RAG: upload documents, we chunk, embed with pgvector, and expose vector search to agents<p>- Workflow deployment versioning with rollbacks<p>- MCP support, Human-in-the-loop block<p>- Copilot to build workflows using natural language (just shipped a new version that also acts as a superagent and can call into any of your connected services directly, not just build workflows)<p>Under the hood, the workflow is a DAG with concurrent execution by default. Nodes run as soon as their dependencies (upstream blocks) are satisfied. Loops (for, forEach, while, do-while) and parallel fan-out/join are also first-class primitives.<p>Agent blocks are pass-through to the provider. You pick your model (OpenAI, Anthropic, Gemini, Ollama, vLLM), and and we pass through prompts, tools, and response format directly to the provider API. We normalize response shapes for block interoperability, but we're not adding layers that obscure what's happening.<p>We're currently working on our own MCP server and the ability to deploy workflows as MCP servers. Would love to hear your thoughts and where we should take it next :)<p>[1] <a href=\"https://news.ycombinator.com/item?id=43823096\">https://news.ycombinator.com/item?id=43823096</a><p>[2] <a href=\"https://news.ycombinator.com/item?id=44052766\">https://news.ycombinator.com/item?id=44052766</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Sim \u2013 Apache-2.0 n8n alternative"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/simstudioai/sim"}}, "_tags": ["story", "author_waleedlatif1", "story_46234186", "show_hn"], "author": "waleedlatif1", "children": [46235878, 46236104, 46236129, 46236422, 46237382, 46237417, 46238673, 46238813, 46239574, 46240560, 46241088, 46241089, 46241968, 46241970, 46244396, 46244712, 46245239, 46256736, 46321424], "created_at": "2025-12-11T17:20:11Z", "created_at_i": 1765473611, "num_comments": 61, "objectID": "46234186", "points": 240, "story_id": 46234186, "story_text": "Hey HN, Waleed here. We&#x27;re building Sim (<a href=\"https:&#x2F;&#x2F;sim.ai&#x2F;\">https:&#x2F;&#x2F;sim.ai&#x2F;</a>), an open-source visual editor to build agentic workflows. Repo here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;simstudioai&#x2F;sim&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;simstudioai&#x2F;sim&#x2F;</a>. Docs here: <a href=\"https:&#x2F;&#x2F;docs.sim.ai\">https:&#x2F;&#x2F;docs.sim.ai</a>.<p>You can run Sim locally using Docker, with no execution limits or other restrictions.<p>We started building Sim almost a year ago after repeatedly troubleshooting why our agents failed in production. Code-first frameworks felt hard to debug because of implicit control flow, and workflow platforms added more overhead than they removed. We wanted granular control and easy observability without piecing everything together ourselves.<p>We launched Sim [1][2] as a drag-and-drop canvas around 6 months ago. Since then, we&#x27;ve added:<p>- 138 blocks: Slack, GitHub, Linear, Notion, Supabase, SSH, TTS, SFTP, MongoDB, S3, Pinecone, ...<p>- Tool calling with granular control: forced, auto<p>- Agent memory: conversation memory with sliding window support (by last n messages or tokens)<p>- Trace spans: detailed logging and observability for nested workflows and tool calling<p>- Native RAG: upload documents, we chunk, embed with pgvector, and expose vector search to agents<p>- Workflow deployment versioning with rollbacks<p>- MCP support, Human-in-the-loop block<p>- Copilot to build workflows using natural language (just shipped a new version that also acts as a superagent and can call into any of your connected services directly, not just build workflows)<p>Under the hood, the workflow is a DAG with concurrent execution by default. Nodes run as soon as their dependencies (upstream blocks) are satisfied. Loops (for, forEach, while, do-while) and parallel fan-out&#x2F;join are also first-class primitives.<p>Agent blocks are pass-through to the provider. You pick your model (OpenAI, Anthropic, Gemini, Ollama, vLLM), and and we pass through prompts, tools, and response format directly to the provider API. We normalize response shapes for block interoperability, but we&#x27;re not adding layers that obscure what&#x27;s happening.<p>We&#x27;re currently working on our own MCP server and the ability to deploy workflows as MCP servers. Would love to hear your thoughts and where we should take it next :)<p>[1] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43823096\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43823096</a><p>[2] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44052766\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44052766</a>", "title": "Show HN: Sim \u2013 Apache-2.0 n8n alternative", "updated_at": "2026-01-13T19:45:40Z", "url": "https://github.com/simstudioai/sim"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rubenfiszel"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Ruben here, software engineer, long-time lurker of Hacker News and founder of Windmill. Windmill is a fully open-source self-hostable platform and runtime to build complex workflows, internal apps and integrations using any scripts in Python or Typescript-deno. I am back after having been revealed a bit too soon on HN and miraculously getting into YC (<a href=\"https://news.ycombinator.com/item?id=31272793\" rel=\"nofollow\">https://news.ycombinator.com/item?id=31272793</a>).<p>To build internal apps for ops, integrations between services that cannot talk to each other directly, or to run background jobs that run your business logic and analytics, the two main options today are no-code solutions and old-fashioned, roll-your-own scripting. Both have problems, and our goal with Windmill is to find a new sweet spot between the two. No-code solutions are productive <i>if</i> your problem matches the tool exactly - but it not, they are rigid, hard to extend and quickly become tech debt, annihilating their initial time advantage. Indeed, no-code is just code but made by an opinionated someone else and hidden as a blackbox with an UI.<p>The alternative is to do it the old-fashioned way, writing everything from scratch, both backend and frontend, perhaps deploying it on the latest flavor of serverless, and pray to never have to touch it again because that took way too much time and it has now became a burden that the ops and business team might poke you about regularly.<p>Furthermore, the landscape of SaaS is specialized tools for everything\u2014alerting, data analytics, administration panels, support management, integration between services\u2014when it feels like a few scripts would have been as good or even better and spared you the need of depending on one yet another tool. This could be even further facilitated if there was a way to import the right bunch of scripts from a fellow community of engineers, tweak it and deploy it like you can do in communities where automation can be shared as simple JSON files, for instance in the node-red or home assistant community.. That\u2019s the idea of Windmill: to bring back the power of scripting in an easy way.<p>With Windmill, you write normal scripts, or reuse ones made by others, and we make them <em>production</em>-grade and composable. You shouldn\u2019t have to worry about things like http requests or scheduling jobs. We abstract much of that away, making your scripts be both more focused and more composable. You end up doing things the right way but much quicker.<p>We reduce the complexity of workflows, integrations and internal apps by uniting them all under one banner. At the heart, they mostly have the same needs: workflows with a UI or a schedule. One tool that does it all out-of-the-box offers greater consistency and allows you to grow the complexity of your toolset at your own pace.<p>I have an academic background in compilers and industry experience in distributed systems. My compiler work made me wary of solving every problem with a domain-specific-language (DSL) or complex frameworks. We can just do more with the well-crafted existing languages like Python or Typescript.  Rolling up your own DSL is nice in theory, you can make it very ergonomic and focused on the task at hand, but then you start adding features and either reinvent existing \u2013 albeit worse \u2013 programming language or decide to stop there. In the very large majority of cases, a well crafted library is vastly superior to any DSL. By being able to use any library of Python and Typescript, we stand on the shoulders of giants.<p>I have also observed that the best distributed systems are often the most simple as they are more predictable and have invariants that are easier to reason with and scale horizontally. This is why for Windmill, we rely solely on Postgres + our native workers + our http REST api layer. Later on, we plan to build adapters to host the workers on AWS lambda or Cloudflare workers, and the queue on Kafka if your needs are exceptionally high.<p>At the heart of what we have built is a queue implemented in Postgres and workers implemented in Rust that create a sandbox (using nsjail), fetch dependencies, and execute  scripts. Every script can be triggered through its name  with an HTTP POST by passing a JSON payload in which every field corresponds 1:1 to an argument of the script\u2019s main parameters. Most primitive types in Python or Typescript have a natural corresponding type in JSON so the conversion is always what you would expect.  We then execute the script inside a new sandbox and then store the results in the same Postgres DB at the end of the job execution.<p>The HTTP payload can be sent from your own frontend or you can use our automatically generated UI. Indeed, we do a simple, yet effective analysis of the parameters of your script, and from it, generate the jsonschema corresponding to your parameters. That schema is what enables us to convert any script into a no-code like module for flows, or a standalone internal app with its auto-generated UI. In the case of Python, we also look at the imports to deduce the Pypi dependencies without you having to declare them.<p>For flows, we defined an open spec for building them out of those scripts we call OpenFlow: <a href=\"https://docs.windmill.dev/docs/openflow\" rel=\"nofollow\">https://docs.windmill.dev/docs/openflow</a>. It  is essentially a json format for describing a sequence of steps with for loops and soon branching. The most interesting bit here is that each input of each step can define its input as a javascript expression that refers to and transforms the output of any previous step. We make it fast by leveraging native v8 integration in Rust (thanks to the deno team) for executing those expressions. This makes this apparently <em>linear</em> sequence a flexible DAG in which one can express complex workflows.<p>Then on top of that we have an UI builder for flows that hides most of the complexity to give an experience that is similar to a low-code platform where every step is treated as a blackbox. The platform itself offers all the features that you would expect: a variable and object store for storing states, plain values and credentials; a cron scheduler, tight permissioning for the sensitive credentials, groups, a webeditor with smart assistant to edit the scripts directly in the platform etc. Finally, we made a hub (<a href=\"https://hub.windmill.dev\" rel=\"nofollow\">https://hub.windmill.dev</a>) to share flows and scripts with everyone. The goal is to grow over time an exhaustive library of pre-made modules and flows to tweak from so that you can focus on what is actually custom to you.<p>Windmill is open-source and self-hostable. You can think of it as a superset of both Pipedream and Airplane.dev. Compared to Temporal, the scripts themselves are agnostic of the flow in which they are embedded, which has the benefit of making it easier to build a hub of reusable modules. We are the only ones as far as we know to convert script parameters to UI automatically. We see ourselves as complementary to UI builder solutions like Retool or Tooljet as we do not want to focus too much on the auto-generated UI and could be used solely as the backend part of the two aforementioned tools.<p>We are now a team of 3 senior engineers and the product is progressing faster than ever with a public roadmap: <a href=\"https://github.com/orgs/windmill-labs/projects/2\" rel=\"nofollow\">https://github.com/orgs/windmill-labs/projects/2</a><p>We make money from commercial licenses, support and team plans on the hosted solution.<p>You can self-host it or try it  <a href=\"https://app.windmill.dev\" rel=\"nofollow\">https://app.windmill.dev</a>, the free tier is generous (and the paid one is not enforced yet). Our landing page is: <a href=\"https://windmill.dev\" rel=\"nofollow\">https://windmill.dev</a>. We would appreciate your feedback and ideas and look forward to all your comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Windmill (YC S22) \u2013 Turn scripts into internal apps and workflows"}}, "_tags": ["story", "author_rubenfiszel", "story_32400849", "launch_hn"], "author": "rubenfiszel", "children": [32401024, 32401130, 32401179, 32401382, 32401671, 32401979, 32402106, 32402484, 32402797, 32403674, 32405249, 32405271, 32405426, 32405497, 32407247, 32407428, 32408428, 32408482, 32409571, 32411171, 32411397, 32411697, 32413950, 32429873], "created_at": "2022-08-09T17:19:18Z", "created_at_i": 1660065558, "num_comments": 79, "objectID": "32400849", "points": 212, "story_id": 32400849, "story_text": "Ruben here, software engineer, long-time lurker of Hacker News and founder of Windmill. Windmill is a fully open-source self-hostable platform and runtime to build complex workflows, internal apps and integrations using any scripts in Python or Typescript-deno. I am back after having been revealed a bit too soon on HN and miraculously getting into YC (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31272793\" rel=\"nofollow\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31272793</a>).<p>To build internal apps for ops, integrations between services that cannot talk to each other directly, or to run background jobs that run your business logic and analytics, the two main options today are no-code solutions and old-fashioned, roll-your-own scripting. Both have problems, and our goal with Windmill is to find a new sweet spot between the two. No-code solutions are productive <i>if</i> your problem matches the tool exactly - but it not, they are rigid, hard to extend and quickly become tech debt, annihilating their initial time advantage. Indeed, no-code is just code but made by an opinionated someone else and hidden as a blackbox with an UI.<p>The alternative is to do it the old-fashioned way, writing everything from scratch, both backend and frontend, perhaps deploying it on the latest flavor of serverless, and pray to never have to touch it again because that took way too much time and it has now became a burden that the ops and business team might poke you about regularly.<p>Furthermore, the landscape of SaaS is specialized tools for everything\u2014alerting, data analytics, administration panels, support management, integration between services\u2014when it feels like a few scripts would have been as good or even better and spared you the need of depending on one yet another tool. This could be even further facilitated if there was a way to import the right bunch of scripts from a fellow community of engineers, tweak it and deploy it like you can do in communities where automation can be shared as simple JSON files, for instance in the node-red or home assistant community.. That\u2019s the idea of Windmill: to bring back the power of scripting in an easy way.<p>With Windmill, you write normal scripts, or reuse ones made by others, and we make them production-grade and composable. You shouldn\u2019t have to worry about things like http requests or scheduling jobs. We abstract much of that away, making your scripts be both more focused and more composable. You end up doing things the right way but much quicker.<p>We reduce the complexity of workflows, integrations and internal apps by uniting them all under one banner. At the heart, they mostly have the same needs: workflows with a UI or a schedule. One tool that does it all out-of-the-box offers greater consistency and allows you to grow the complexity of your toolset at your own pace.<p>I have an academic background in compilers and industry experience in distributed systems. My compiler work made me wary of solving every problem with a domain-specific-language (DSL) or complex frameworks. We can just do more with the well-crafted existing languages like Python or Typescript.  Rolling up your own DSL is nice in theory, you can make it very ergonomic and focused on the task at hand, but then you start adding features and either reinvent existing \u2013 albeit worse \u2013 programming language or decide to stop there. In the very large majority of cases, a well crafted library is vastly superior to any DSL. By being able to use any library of Python and Typescript, we stand on the shoulders of giants.<p>I have also observed that the best distributed systems are often the most simple as they are more predictable and have invariants that are easier to reason with and scale horizontally. This is why for Windmill, we rely solely on Postgres + our native workers + our http REST api layer. Later on, we plan to build adapters to host the workers on AWS lambda or Cloudflare workers, and the queue on Kafka if your needs are exceptionally high.<p>At the heart of what we have built is a queue implemented in Postgres and workers implemented in Rust that create a sandbox (using nsjail), fetch dependencies, and execute  scripts. Every script can be triggered through its name  with an HTTP POST by passing a JSON payload in which every field corresponds 1:1 to an argument of the script\u2019s main parameters. Most primitive types in Python or Typescript have a natural corresponding type in JSON so the conversion is always what you would expect.  We then execute the script inside a new sandbox and then store the results in the same Postgres DB at the end of the job execution.<p>The HTTP payload can be sent from your own frontend or you can use our automatically generated UI. Indeed, we do a simple, yet effective analysis of the parameters of your script, and from it, generate the jsonschema corresponding to your parameters. That schema is what enables us to convert any script into a no-code like module for flows, or a standalone internal app with its auto-generated UI. In the case of Python, we also look at the imports to deduce the Pypi dependencies without you having to declare them.<p>For flows, we defined an open spec for building them out of those scripts we call OpenFlow: <a href=\"https:&#x2F;&#x2F;docs.windmill.dev&#x2F;docs&#x2F;openflow\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.windmill.dev&#x2F;docs&#x2F;openflow</a>. It  is essentially a json format for describing a sequence of steps with for loops and soon branching. The most interesting bit here is that each input of each step can define its input as a javascript expression that refers to and transforms the output of any previous step. We make it fast by leveraging native v8 integration in Rust (thanks to the deno team) for executing those expressions. This makes this apparently linear sequence a flexible DAG in which one can express complex workflows.<p>Then on top of that we have an UI builder for flows that hides most of the complexity to give an experience that is similar to a low-code platform where every step is treated as a blackbox. The platform itself offers all the features that you would expect: a variable and object store for storing states, plain values and credentials; a cron scheduler, tight permissioning for the sensitive credentials, groups, a webeditor with smart assistant to edit the scripts directly in the platform etc. Finally, we made a hub (<a href=\"https:&#x2F;&#x2F;hub.windmill.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;hub.windmill.dev</a>) to share flows and scripts with everyone. The goal is to grow over time an exhaustive library of pre-made modules and flows to tweak from so that you can focus on what is actually custom to you.<p>Windmill is open-source and self-hostable. You can think of it as a superset of both Pipedream and Airplane.dev. Compared to Temporal, the scripts themselves are agnostic of the flow in which they are embedded, which has the benefit of making it easier to build a hub of reusable modules. We are the only ones as far as we know to convert script parameters to UI automatically. We see ourselves as complementary to UI builder solutions like Retool or Tooljet as we do not want to focus too much on the auto-generated UI and could be used solely as the backend part of the two aforementioned tools.<p>We are now a team of 3 senior engineers and the product is progressing faster than ever with a public roadmap: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;windmill-labs&#x2F;projects&#x2F;2\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;windmill-labs&#x2F;projects&#x2F;2</a><p>We make money from commercial licenses, support and team plans on the hosted solution.<p>You can self-host it or try it  <a href=\"https:&#x2F;&#x2F;app.windmill.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;app.windmill.dev</a>, the free tier is generous (and the paid one is not enforced yet). Our landing page is: <a href=\"https:&#x2F;&#x2F;windmill.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;windmill.dev</a>. We would appreciate your feedback and ideas and look forward to all your comments!", "title": "Launch HN: Windmill (YC S22) \u2013 Turn scripts into internal apps and workflows", "updated_at": "2024-09-20T11:44:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "areddyyt"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Hey Hacker News! We\u2019re Abhi and Alex from deepsilicon (<a href=\"https://deepsilicon.com\" rel=\"nofollow\">https://deepsilicon.com</a>). We are building software and hardware for training and inferencing ternary transformer models. Here's a video of the software: <a href=\"https://www.youtube.com/watch?v=VqBn-I5D6pk\" rel=\"nofollow\">https://www.youtube.com/watch?v=VqBn-I5D6pk</a>.<p>Transformer-based models are getting bigger every generation, making the inference hardware requirements more and more expensive. Running large transformer models on device is even more challenging. Usually, they require trillions of FLOPs to run at decent speeds and use too much energy and space.<p>Our solution is to train ternary transformer models. There are two advantages to using ternary values. The first is that the weights can now be stored in two bits (or even less) from 16 bits. This represents an almost 8x compression ratio for every weight matrix in the transformer model (slightly less because of the float16 scaling value and extra norm, but that\u2019s negligible). The second advantage is a reduction in the arithmetic intensity. If we do a dot product between ternary values and INT8 values, we either add the INT8 if the ternary value is 1, subtract the INT8 if the ternary values is -1, or do nothing if the ternary value is 0. There are numerous ways to take advantage of this change in arithmetic, from look up tables to bit mask reductions. As for why ternary and not quaternary/binary, ternary hits a sweet spot of compression and (symmetric) representational value for weights in our experiments.<p>Currently, hardware is not really optimized for extreme low bit-width matrix operations (whether multiplication or otherwise). We\u2019ve tried various implementations of kernels on both CPUs/GPUs (really only NVIDIA GPUs). We don\u2019t even come close to the theoretical maximum speed for our kernels, and a large part of the failure is because the architecture of existing hardware isn\u2019t optimized for the operations we want them to do. Creating custom silicon for ternary LLMs can accelerate inference by implementing and designing algorithms/circuits that only work for ternary LLMs. Unlike most hardware companies, which need silicon to show improvements, we can already show improvements to active VRAM usage and throughput with our custom kernels on existing hardware. This sets pretty impressive lower bounds for custom silicon.<p>We originally started working on this after reading the BitNet paper from Microsoft, and were disenchanted that we couldn't run SOTA models on our consumer hardware (3090 and 3070M). Both Alex and I worked on research at Dartmouth, I worked more on the ML/model architecture side, while Alex worked on randNLA CUDA kernels to accelerate training. The research experience, and opportunity to talk to professors, made us realize that if we could pull off ternary transformers, it could solve the large scale inference problem on the edge and cloud.<p>First, we must either retrain or pretrain a model with our custom <em>linear</em> layers based on the Bitnet 1.58 layers (we\u2019re working on open sourcing our framework for training, data labelling, and synthetic data generation here: <a href=\"https://github.com/deepsilicon/Sila\">https://github.com/deepsilicon/Sila</a>). The model is trained with FP16 weights, but the weights are quantized and the quantization function is detached from the computational graph to allow gradients to flow, and the loss is measured w.r.t. the quantized weights. Once the model converges, we can inference the model with our custom kernels written for CPUs or GPUs (we are working on Inferentia and TPU support). The end goal is to create purpose-built custom silicon to work with the ternary weights, where we can have better compression, throughput, latency, and energy improvements compared to our kernels on existing hardware.<p>We know this is a highly challenging problem due to technical and market difficulties. Plenty of hardware companies have tried to accelerate inference, but most are not profitable. The biggest problem in the ML hardware market, perplexingly, is software. It's challenging to convince companies to switch to some new hardware when their entire infrastructure and software stack has been configured for some other hardware. On the technical side, we must support various deployment options and model architectures to make large-scale custom silicon <em>production</em> worthwhile. This is compounded by the fact we want to have a single line of code handle everything, abstracting what we're doing away from the ML engineers. So, we need to handle everything on the technical side: compiling the right kernels for your platform, generating the right bindings for ONNX/TensorRT, tuning the kernels, setting the mode to training or inference, etc.<p>We\u2019d love to hear your opinions about ASICs for transformer inference - and if you know anyone who might be interested in deploying these models, my email is abhi@deepsilicon.com. We can\u2019t wait to hear what you all think!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Deepsilicon (YC S24) \u2013 Software and hardware for ternary transformers"}}, "_tags": ["story", "author_areddyyt", "story_41490196", "launch_hn"], "author": "areddyyt", "children": [41490376, 41490446, 41490544, 41490812, 41490975, 41491044, 41491418, 41491455, 41491500, 41492546, 41492962, 41493075, 41493186, 41493915, 41494719, 41495050, 41496279, 41497730, 41497746, 41498048, 41498577, 41499284, 41499930, 41499982, 41501462, 41503008, 41504586, 41508963], "created_at": "2024-09-09T16:30:46Z", "created_at_i": 1725899446, "num_comments": 79, "objectID": "41490196", "points": 189, "story_id": 41490196, "story_text": "Hey Hacker News! We\u2019re Abhi and Alex from deepsilicon (<a href=\"https:&#x2F;&#x2F;deepsilicon.com\" rel=\"nofollow\">https:&#x2F;&#x2F;deepsilicon.com</a>). We are building software and hardware for training and inferencing ternary transformer models. Here&#x27;s a video of the software: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=VqBn-I5D6pk\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=VqBn-I5D6pk</a>.<p>Transformer-based models are getting bigger every generation, making the inference hardware requirements more and more expensive. Running large transformer models on device is even more challenging. Usually, they require trillions of FLOPs to run at decent speeds and use too much energy and space.<p>Our solution is to train ternary transformer models. There are two advantages to using ternary values. The first is that the weights can now be stored in two bits (or even less) from 16 bits. This represents an almost 8x compression ratio for every weight matrix in the transformer model (slightly less because of the float16 scaling value and extra norm, but that\u2019s negligible). The second advantage is a reduction in the arithmetic intensity. If we do a dot product between ternary values and INT8 values, we either add the INT8 if the ternary value is 1, subtract the INT8 if the ternary values is -1, or do nothing if the ternary value is 0. There are numerous ways to take advantage of this change in arithmetic, from look up tables to bit mask reductions. As for why ternary and not quaternary&#x2F;binary, ternary hits a sweet spot of compression and (symmetric) representational value for weights in our experiments.<p>Currently, hardware is not really optimized for extreme low bit-width matrix operations (whether multiplication or otherwise). We\u2019ve tried various implementations of kernels on both CPUs&#x2F;GPUs (really only NVIDIA GPUs). We don\u2019t even come close to the theoretical maximum speed for our kernels, and a large part of the failure is because the architecture of existing hardware isn\u2019t optimized for the operations we want them to do. Creating custom silicon for ternary LLMs can accelerate inference by implementing and designing algorithms&#x2F;circuits that only work for ternary LLMs. Unlike most hardware companies, which need silicon to show improvements, we can already show improvements to active VRAM usage and throughput with our custom kernels on existing hardware. This sets pretty impressive lower bounds for custom silicon.<p>We originally started working on this after reading the BitNet paper from Microsoft, and were disenchanted that we couldn&#x27;t run SOTA models on our consumer hardware (3090 and 3070M). Both Alex and I worked on research at Dartmouth, I worked more on the ML&#x2F;model architecture side, while Alex worked on randNLA CUDA kernels to accelerate training. The research experience, and opportunity to talk to professors, made us realize that if we could pull off ternary transformers, it could solve the large scale inference problem on the edge and cloud.<p>First, we must either retrain or pretrain a model with our custom linear layers based on the Bitnet 1.58 layers (we\u2019re working on open sourcing our framework for training, data labelling, and synthetic data generation here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;deepsilicon&#x2F;Sila\">https:&#x2F;&#x2F;github.com&#x2F;deepsilicon&#x2F;Sila</a>). The model is trained with FP16 weights, but the weights are quantized and the quantization function is detached from the computational graph to allow gradients to flow, and the loss is measured w.r.t. the quantized weights. Once the model converges, we can inference the model with our custom kernels written for CPUs or GPUs (we are working on Inferentia and TPU support). The end goal is to create purpose-built custom silicon to work with the ternary weights, where we can have better compression, throughput, latency, and energy improvements compared to our kernels on existing hardware.<p>We know this is a highly challenging problem due to technical and market difficulties. Plenty of hardware companies have tried to accelerate inference, but most are not profitable. The biggest problem in the ML hardware market, perplexingly, is software. It&#x27;s challenging to convince companies to switch to some new hardware when their entire infrastructure and software stack has been configured for some other hardware. On the technical side, we must support various deployment options and model architectures to make large-scale custom silicon production worthwhile. This is compounded by the fact we want to have a single line of code handle everything, abstracting what we&#x27;re doing away from the ML engineers. So, we need to handle everything on the technical side: compiling the right kernels for your platform, generating the right bindings for ONNX&#x2F;TensorRT, tuning the kernels, setting the mode to training or inference, etc.<p>We\u2019d love to hear your opinions about ASICs for transformer inference - and if you know anyone who might be interested in deploying these models, my email is abhi@deepsilicon.com. We can\u2019t wait to hear what you all think!", "title": "Launch HN: Deepsilicon (YC S24) \u2013 Software and hardware for ternary transformers", "updated_at": "2025-08-14T22:03:30Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "arsalanb"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Hi HN, I'm Arsalan, founder of LiveDocs (<a href=\"https://livedocs.com\">https://livedocs.com</a>). We're building an AI-native data workspace that lets teams ask questions of their real data and have the system plan, execute, and maintain the analysis end-to-end.<p>We previously posted about LiveDocs four years ago (<a href=\"https://news.ycombinator.com/item?id=30735058\">https://news.ycombinator.com/item?id=30735058</a>). Back then, LiveDocs was a no-code analytics tool for stitching together metrics from tools like Stripe and Google Analytics. It worked for basic reporting, but over time we ran into the same ceiling our users did. Dashboards are fine until the questions get messy, and notebooks slowly turn into hard-to-maintain piles of glue.<p>Over the last few years, we rebuilt LiveDocs almost entirely around a different idea. Data work should behave like a living system, not a static document or a chat transcript.<p>Today, LiveDocs is a reactive notebook environment backed by real execution engines. Notebooks are not <em>linear</em>. Each cell participates in a dependency graph, so when data or logic changes, only the affected parts recompute. You can freely mix SQL, Python, charts, tables, and text in the same document and everything stays in sync. Locally we run on DuckDB and Polars, and when you connect a warehouse like Snowflake, BigQuery, or Postgres, queries are pushed down instead of copying data out. Every result is inspectable and reproducible.<p>On top of this environment sits an AI agent, but it is not &quot;chat with your data.&quot; The agent works inside the notebook itself. It can plan multi-step analyses, write and debug SQL or Python, spawn specialized sub-agents for different tasks, run code in a terminal, and browse documentation or the web when it lacks context. Because it operates inside the same execution graph as humans, you can see exactly what it ran, edit it, or take over at any point.<p>We also support a canvas mode where the agent can build custom UI for your analysis, not just charts. This includes tables with controls, comparisons, and derived views that stay wired to the underlying data. When a notebook is not the right interface, you can publish parts of it as an interactive app. These behave more like lightweight internal tools, similar in spirit to Retool, but backed by the same analysis logic.<p>Everything in LiveDocs is fully real-time collaborative. Multiple people can edit the same notebook, see results update live, comment inline, and share documents or apps without exposing raw code unless they want to.<p>Teams use LiveDocs to investigate questions that do not fit cleanly into dashboards, build analyses that evolve over time without constant rewrites, and automate recurring questions without turning them into brittle pipelines.<p>Pricing is pay-as-you-go, starting at $15 per month, with a free tier so people can try it without talking to us. You'll have to sign up, as it requires us to provision a sandbox for your to run your notebook. Here's a video demo: <a href=\"https://youtu.be/Hl12su9Jn_I\" rel=\"nofollow\">https://youtu.be/Hl12su9Jn_I</a><p>We are still learning where this breaks. Long-running agent workflows on <em>production</em> data surface a lot of sharp edges. We would love feedback from people who have built or lived with analytics systems, notebooks, or &quot;chat with your data&quot; tools and felt their limits. Happy to go deep on technical details and trade notes."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Livedocs (YC W22) \u2013 An AI-native notebook for data analysis"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://livedocs.com"}}, "_tags": ["story", "author_arsalanb", "story_46964162", "launch_hn"], "author": "arsalanb", "children": [46964462, 46965208, 46965378, 46965384, 46965743, 46966242, 46966512, 46969689], "created_at": "2026-02-10T18:09:14Z", "created_at_i": 1770746954, "num_comments": 19, "objectID": "46964162", "points": 48, "story_id": 46964162, "story_text": "Hi HN, I&#x27;m Arsalan, founder of LiveDocs (<a href=\"https:&#x2F;&#x2F;livedocs.com\">https:&#x2F;&#x2F;livedocs.com</a>). We&#x27;re building an AI-native data workspace that lets teams ask questions of their real data and have the system plan, execute, and maintain the analysis end-to-end.<p>We previously posted about LiveDocs four years ago (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30735058\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30735058</a>). Back then, LiveDocs was a no-code analytics tool for stitching together metrics from tools like Stripe and Google Analytics. It worked for basic reporting, but over time we ran into the same ceiling our users did. Dashboards are fine until the questions get messy, and notebooks slowly turn into hard-to-maintain piles of glue.<p>Over the last few years, we rebuilt LiveDocs almost entirely around a different idea. Data work should behave like a living system, not a static document or a chat transcript.<p>Today, LiveDocs is a reactive notebook environment backed by real execution engines. Notebooks are not linear. Each cell participates in a dependency graph, so when data or logic changes, only the affected parts recompute. You can freely mix SQL, Python, charts, tables, and text in the same document and everything stays in sync. Locally we run on DuckDB and Polars, and when you connect a warehouse like Snowflake, BigQuery, or Postgres, queries are pushed down instead of copying data out. Every result is inspectable and reproducible.<p>On top of this environment sits an AI agent, but it is not &quot;chat with your data.&quot; The agent works inside the notebook itself. It can plan multi-step analyses, write and debug SQL or Python, spawn specialized sub-agents for different tasks, run code in a terminal, and browse documentation or the web when it lacks context. Because it operates inside the same execution graph as humans, you can see exactly what it ran, edit it, or take over at any point.<p>We also support a canvas mode where the agent can build custom UI for your analysis, not just charts. This includes tables with controls, comparisons, and derived views that stay wired to the underlying data. When a notebook is not the right interface, you can publish parts of it as an interactive app. These behave more like lightweight internal tools, similar in spirit to Retool, but backed by the same analysis logic.<p>Everything in LiveDocs is fully real-time collaborative. Multiple people can edit the same notebook, see results update live, comment inline, and share documents or apps without exposing raw code unless they want to.<p>Teams use LiveDocs to investigate questions that do not fit cleanly into dashboards, build analyses that evolve over time without constant rewrites, and automate recurring questions without turning them into brittle pipelines.<p>Pricing is pay-as-you-go, starting at $15 per month, with a free tier so people can try it without talking to us. You&#x27;ll have to sign up, as it requires us to provision a sandbox for your to run your notebook. Here&#x27;s a video demo: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;Hl12su9Jn_I\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;Hl12su9Jn_I</a><p>We are still learning where this breaks. Long-running agent workflows on production data surface a lot of sharp edges. We would love feedback from people who have built or lived with analytics systems, notebooks, or &quot;chat with your data&quot; tools and felt their limits. Happy to go deep on technical details and trade notes.", "title": "Launch HN: Livedocs (YC W22) \u2013 An AI-native notebook for data analysis", "updated_at": "2026-02-15T13:00:40Z", "url": "https://livedocs.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fasten"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Hello Hacker News! We're Roxane, Julien, Pierre, Mawen and Stephane from Anyshift.io. We are building a GitHub app (and platform) that detects Terraform complex dependencies (hardcoded values, intricated-modules, shadow IT\u2026), flags potential breakages, and provides a Terraform \u2018Superplan\u2019 for your changes. \nTo do that we create and maintain a digital twin of your infrastructure using Neo4j.<p>- 2 min demo : <a href=\"https://app.guideflow.com/player/dkd2en3t9r\" rel=\"nofollow\">https://app.guideflow.com/player/dkd2en3t9r</a>\n- try it now: <a href=\"https://app.anyshift.io/\" rel=\"nofollow\">https://app.anyshift.io/</a> (5min setup).<p>We experienced how dealing with IaC/Terraform is complex and opaque. Terraform \u2018plans\u2019 are hard to navigate and intertwined dependencies are error prone: one simple change in a security group, firewall rules, subnet CIDR range... can lead to a cascading effect of breaking changes.<p>I\u2019ve dealt in <em>production</em> with those issues since Terraform\u2019s early days. In 2016, I wrote a book about Infrastructure-as-code and created driftctl based on those experiences (open source tool to manage drifts which was acquired by Snyk).<p>Our team is building Anyshift because we believe this problem of complex dependencies is unresolved and is going to explode with AI-generated code (more legacy, weaker sense of ownership). Unlike existing tools (Terraform Cloud/Stacks, Terragrunt, etc...), Anyshift uses a graph-based approach that references the real environment to uncover hidden, interlinked changes.<p>For instance, changing a subnet can force an ENI to switch IP addresses, triggering an EC2 reconfiguration and breaking DNS referenced records. Our GitHub app identifies these hidden issues, while our platform uncovers unmanaged \u201cshadow IT\u201d and lets you search any cloud resource to find exactly where it\u2019s defined in your Terraform code.<p>To do so, one of our key challenges was to achieve a frictionless setup, so we created an event-driven reconciliation system that unifies AWS resources, Terraform states, and code in a Neo4j graph database. This \u201ctime machine\u201d of your infra updates automatically, and for each PR, we query it (via Cypher) to see what might break.<p>Thanks to that, the onboarding is super fast (5 min): \n1. Install the Github app\n2. Grant AWS read only access to the app<p>The choice of a graph database was a way for us to avoid scale limitations compared to relational databases. We already have a handful of enterprise customers running it in prod and can query hundreds of thousands of relationships with <em>linear</em> search times. We'd love you to try our free plan to see it in action<p>We're excited to share this with you, thanks for reading! Let us know your thoughts or questions here or in our future Slack discussions.\nRoxane, Julien, Pierre, Mawen and Stephane!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Anyshift.io \u2013 Terraform \"Superplan\""}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://app.anyshift.io/"}}, "_tags": ["story", "author_fasten", "story_42712522", "show_hn"], "author": "fasten", "children": [42712697, 42712773, 42712821, 42712848, 42712970, 42712971, 42713038, 42713039, 42713050, 42713210, 42713229, 42713378, 42713563, 42713565, 42713954, 42714247, 42714419, 42714541, 42714544, 42715961, 42716703, 42722834, 42723019, 42726549, 42738587, 42797865], "created_at": "2025-01-15T16:00:49Z", "created_at_i": 1736956849, "num_comments": 42, "objectID": "42712522", "points": 35, "story_id": 42712522, "story_text": "Hello Hacker News! We&#x27;re Roxane, Julien, Pierre, Mawen and Stephane from Anyshift.io. We are building a GitHub app (and platform) that detects Terraform complex dependencies (hardcoded values, intricated-modules, shadow IT\u2026), flags potential breakages, and provides a Terraform \u2018Superplan\u2019 for your changes. \nTo do that we create and maintain a digital twin of your infrastructure using Neo4j.<p>- 2 min demo : <a href=\"https:&#x2F;&#x2F;app.guideflow.com&#x2F;player&#x2F;dkd2en3t9r\" rel=\"nofollow\">https:&#x2F;&#x2F;app.guideflow.com&#x2F;player&#x2F;dkd2en3t9r</a>\n- try it now: <a href=\"https:&#x2F;&#x2F;app.anyshift.io&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;app.anyshift.io&#x2F;</a> (5min setup).<p>We experienced how dealing with IaC&#x2F;Terraform is complex and opaque. Terraform \u2018plans\u2019 are hard to navigate and intertwined dependencies are error prone: one simple change in a security group, firewall rules, subnet CIDR range... can lead to a cascading effect of breaking changes.<p>I\u2019ve dealt in production with those issues since Terraform\u2019s early days. In 2016, I wrote a book about Infrastructure-as-code and created driftctl based on those experiences (open source tool to manage drifts which was acquired by Snyk).<p>Our team is building Anyshift because we believe this problem of complex dependencies is unresolved and is going to explode with AI-generated code (more legacy, weaker sense of ownership). Unlike existing tools (Terraform Cloud&#x2F;Stacks, Terragrunt, etc...), Anyshift uses a graph-based approach that references the real environment to uncover hidden, interlinked changes.<p>For instance, changing a subnet can force an ENI to switch IP addresses, triggering an EC2 reconfiguration and breaking DNS referenced records. Our GitHub app identifies these hidden issues, while our platform uncovers unmanaged \u201cshadow IT\u201d and lets you search any cloud resource to find exactly where it\u2019s defined in your Terraform code.<p>To do so, one of our key challenges was to achieve a frictionless setup, so we created an event-driven reconciliation system that unifies AWS resources, Terraform states, and code in a Neo4j graph database. This \u201ctime machine\u201d of your infra updates automatically, and for each PR, we query it (via Cypher) to see what might break.<p>Thanks to that, the onboarding is super fast (5 min): \n1. Install the Github app\n2. Grant AWS read only access to the app<p>The choice of a graph database was a way for us to avoid scale limitations compared to relational databases. We already have a handful of enterprise customers running it in prod and can query hundreds of thousands of relationships with linear search times. We&#x27;d love you to try our free plan to see it in action<p>We&#x27;re excited to share this with you, thanks for reading! Let us know your thoughts or questions here or in our future Slack discussions.\nRoxane, Julien, Pierre, Mawen and Stephane!", "title": "Show HN: Anyshift.io \u2013 Terraform \"Superplan\"", "updated_at": "2025-02-14T07:45:15Z", "url": "https://app.anyshift.io/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "gmwhitebox_dev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "I\u2019ve spent the last few months building a deep learning engine completely from scratch in Python (using only math and random).<p>What started as a basic <em>linear</em> algebra calculator project grew into a symbolic tensor system with autodiff, custom matrix ops, attention mechanisms, LayerNorm, GELU, and even a text generation demo trained on the Brown corpus.<p>I'm still an undergrad, so my main goal is to deeply understand how deep learning actually works under the hood - gradients, attention, backpropagation, optimizers - by building it step-by-step with full visibility into everything, and without relying on big frameworks or libraries.<p>It\u2019s not fast or <em>production</em>-ready, but that\u2019s not the point. As of now, it\u2019s more so aimed at exploration and understanding. I mainly wanted to explore how deep learning works by building it through first principles.<p>It\u2019s still a work in progress (lots to learn and improve in terms of structure, docs, and performance), but I figured it was worth sharing.<p>I\u2019d love any feedback, questions, ideas, or even just thoughts about what you\u2019d add, change, or do differently.  \nThanks for reading!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: I built a deep learning engine from scratch in Python"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/whitegra/dolphin"}}, "_tags": ["story", "author_gmwhitebox_dev", "story_43711339", "show_hn"], "author": "gmwhitebox_dev", "children": [43723158, 43800274], "created_at": "2025-04-16T23:15:24Z", "created_at_i": 1744845324, "num_comments": 3, "objectID": "43711339", "points": 30, "story_id": 43711339, "story_text": "I\u2019ve spent the last few months building a deep learning engine completely from scratch in Python (using only math and random).<p>What started as a basic linear algebra calculator project grew into a symbolic tensor system with autodiff, custom matrix ops, attention mechanisms, LayerNorm, GELU, and even a text generation demo trained on the Brown corpus.<p>I&#x27;m still an undergrad, so my main goal is to deeply understand how deep learning actually works under the hood - gradients, attention, backpropagation, optimizers - by building it step-by-step with full visibility into everything, and without relying on big frameworks or libraries.<p>It\u2019s not fast or production-ready, but that\u2019s not the point. As of now, it\u2019s more so aimed at exploration and understanding. I mainly wanted to explore how deep learning works by building it through first principles.<p>It\u2019s still a work in progress (lots to learn and improve in terms of structure, docs, and performance), but I figured it was worth sharing.<p>I\u2019d love any feedback, questions, ideas, or even just thoughts about what you\u2019d add, change, or do differently.  \nThanks for reading!", "title": "Show HN: I built a deep learning engine from scratch in Python", "updated_at": "2025-04-26T02:04:22Z", "url": "https://github.com/whitegra/dolphin"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "thesssaism"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "We recently did a deep dive into our engineering time allocation for a standard 5-person team building a B2B SaaS application. The results were pretty depressing: we spent roughly 960 hours (annualized) on &quot;setup&quot; tasks\u2014environment config, auth flows, RBAC, CI/CD pipelines, and database scaffolding\u2014before we built a single unique feature that actually differentiated the product.<p>I\u2019m sharing this because I think we\u2019ve become numb to the &quot;Setup Tax&quot; in web development. We assume it's just the cost of doing business, but when you look at the economics, it\u2019s a disaster.<p>The problem isn't just &quot;writing boilerplate.&quot; It's the decision fatigue and integration cost that comes with it. Even with modern frameworks, we found that for a standard CRUD app, about 80% of our engineering effort went into the &quot;commodity layer&quot;\u2014the stuff that every SaaS has, but no customer pays for. Only 20% went into the unique business logic.<p>We tried to fix this by throwing more bodies at it, but that just increased coordination overhead. So we tried something different: instead of using AI to write code snippets (Copilot style), we used it to generate the entire architectural foundation at once. I'm talking about the full repo structure, the Docker configs, the auth integration, the API gateways\u2014the whole boring 80%.<p>The goal was to invert that ratio. To get to a point where 70% of our time is on features and only 30% on glue code.<p>The results from our initial runs suggest it works, but the math is what\u2019s interesting. Moving from 20% feature focus to 70% feature focus isn't just a <em>linear</em> improvement. It\u2019s a 3.5x multiplier on <i>feature velocity</i>. The total lines of code produced might be similar, but the amount of <i>valuable</i> code shipping to <em>production</em> skyrockets.<p>Obviously, there are massive trade-offs here.<p>First, you end up with a very generic architecture initially. If you need something novel or specialized (like high-frequency trading or deep tech), this approach is useless. It only works for the &quot;standard web app&quot; pattern.<p>Second, there's a real risk of &quot;black box&quot; infrastructure. If the team doesn't understand the generated auth flow, they can't debug it when it inevitably breaks. We have to enforce strict governance to stop this from becoming generated spaghetti.<p>Third, I'm not sure if this efficiency holds up long-term. Maintenance is always the real killer, not day-one setup. We haven't been doing this long enough to see if the generated foundations rot faster than bespoke ones.<p>I'm curious what others are seeing:<p>- Does anyone else track &quot;time to first feature&quot;?\n- What is your ratio of infrastructure/boilerplate vs. actual business logic?\n- Have internal developer platforms (IDPs) actually solved this for you, or did they just hide the maintenance cost elsewhere?<p>It feels like we're at a weird inflection point where &quot;starting from scratch&quot; is becoming economically irresponsible for standard software, but the alternative feels like cheating."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Tell HN: We analyzed our dev time.80% is still infrastructure'setup',notfeatures"}}, "_tags": ["story", "author_thesssaism", "story_47060234", "ask_hn"], "author": "thesssaism", "children": [47060449, 47061600, 47062105, 47063628], "created_at": "2026-02-18T12:20:13Z", "created_at_i": 1771417213, "num_comments": 4, "objectID": "47060234", "points": 13, "story_id": 47060234, "story_text": "We recently did a deep dive into our engineering time allocation for a standard 5-person team building a B2B SaaS application. The results were pretty depressing: we spent roughly 960 hours (annualized) on &quot;setup&quot; tasks\u2014environment config, auth flows, RBAC, CI&#x2F;CD pipelines, and database scaffolding\u2014before we built a single unique feature that actually differentiated the product.<p>I\u2019m sharing this because I think we\u2019ve become numb to the &quot;Setup Tax&quot; in web development. We assume it&#x27;s just the cost of doing business, but when you look at the economics, it\u2019s a disaster.<p>The problem isn&#x27;t just &quot;writing boilerplate.&quot; It&#x27;s the decision fatigue and integration cost that comes with it. Even with modern frameworks, we found that for a standard CRUD app, about 80% of our engineering effort went into the &quot;commodity layer&quot;\u2014the stuff that every SaaS has, but no customer pays for. Only 20% went into the unique business logic.<p>We tried to fix this by throwing more bodies at it, but that just increased coordination overhead. So we tried something different: instead of using AI to write code snippets (Copilot style), we used it to generate the entire architectural foundation at once. I&#x27;m talking about the full repo structure, the Docker configs, the auth integration, the API gateways\u2014the whole boring 80%.<p>The goal was to invert that ratio. To get to a point where 70% of our time is on features and only 30% on glue code.<p>The results from our initial runs suggest it works, but the math is what\u2019s interesting. Moving from 20% feature focus to 70% feature focus isn&#x27;t just a linear improvement. It\u2019s a 3.5x multiplier on <i>feature velocity</i>. The total lines of code produced might be similar, but the amount of <i>valuable</i> code shipping to production skyrockets.<p>Obviously, there are massive trade-offs here.<p>First, you end up with a very generic architecture initially. If you need something novel or specialized (like high-frequency trading or deep tech), this approach is useless. It only works for the &quot;standard web app&quot; pattern.<p>Second, there&#x27;s a real risk of &quot;black box&quot; infrastructure. If the team doesn&#x27;t understand the generated auth flow, they can&#x27;t debug it when it inevitably breaks. We have to enforce strict governance to stop this from becoming generated spaghetti.<p>Third, I&#x27;m not sure if this efficiency holds up long-term. Maintenance is always the real killer, not day-one setup. We haven&#x27;t been doing this long enough to see if the generated foundations rot faster than bespoke ones.<p>I&#x27;m curious what others are seeing:<p>- Does anyone else track &quot;time to first feature&quot;?\n- What is your ratio of infrastructure&#x2F;boilerplate vs. actual business logic?\n- Have internal developer platforms (IDPs) actually solved this for you, or did they just hide the maintenance cost elsewhere?<p>It feels like we&#x27;re at a weird inflection point where &quot;starting from scratch&quot; is becoming economically irresponsible for standard software, but the alternative feels like cheating.", "title": "Tell HN: We analyzed our dev time.80% is still infrastructure'setup',notfeatures", "updated_at": "2026-02-20T06:47:59Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jordandearsley"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["linear", "production"], "value": "Hey HN, I\u2019m Jordan, founder of Vapi. For the past 1.5 years, we've been building voice infrastructure designed to help developers easily create, test, and deploy voice agents. We\u2019re everything between raw models (like the realtime API) and end-users interacting with voice agents over telephony or within applications.<p>We\u2019ve been around for about 1.5yrs. Vapi was born out of a personal frustration\u2014I wanted to build my own voice agent but found existing solutions extremely limited. Back then, there were only a handful of voice AI startups. Since March 2024, we've witnessed a Cambrian explosion of new entrants.<p>In the early days, the biggest challenge was always latency. But as models have rapidly improved and funding has flooded into the space, voice agents are now trusted for business-critical applications like prior authorizations, payment processing, and customer support.<p>Today\u2019s challenge is reliability: can these voice agents consistently perform in <em>production</em>? \nWhat we\u2019ve learned deploying 1000s of them: Yes, but it's really hard.<p>Why? Real-time is unweildy\u2014coordinating 10+ third party real-time models, handling diverse, unconstrained inputs and outputs, with no latency budget for reasoning and reflection. Voice interactions don\u2019t follow a <em>linear</em> flow like text, making determinism a challenge. Getting it right takes a ton of tuning and iteration.<p>Our approach does not simplify, it assumes it will be hard. We\u2019re built for developers. We expose extensive configuration options through an API-native platform, including custom models, 100+ integrations, and detailed call testing and monitoring features.<p>--<p>Tldr; it works!<p>- We achieve performance equivalent to humans for use cases with clear user intent (appointment bookings, support, etc.).<p>- We've handled over 44M voice calls across support automation, healthcare intake, and logistics.<p>- We reliably manage long-running jobs (10\u201360+ minutes) with 99.95% uptime.<p>Questions we\u2019re still exploring:<p>- When you have a complex product with 1000s of nobs, how do you help developers wield them, without heavy consulting?<p>- How do you detect and eliminate bad actors when the line can be blurry / not enough context to make the call on what is fraud and what isn\u2019t?<p>- How do you guarantee reliability when we are built on services that are less reliable than industry standard (e.g. OpenAI posts &lt; 99.9% uptime)<p>--<p>You can try a live agent by calling: 1-844-HEY-VAPI. Yes, that\u2019s my voice.<p>Docs: <a href=\"https://docs.vapi.ai/introduction\">https://docs.vapi.ai/introduction</a><p>Let me know if you have any questions about voice; the market or the tech. Happy to poke the eng team to chat on architecture, etc. if it\u2019s of interest.<p>PS- we are also launching on product hunt today. we got an awesome team working really hard to make that happen, so pls support, thanks!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: 1-844-HEY-VAPI \u2013 voice AI platform for developers"}}, "_tags": ["story", "author_jordandearsley", "story_43556789", "show_hn"], "author": "jordandearsley", "children": [43560178, 43560230, 43560369, 43560558, 43564162], "created_at": "2025-04-02T14:00:53Z", "created_at_i": 1743602453, "num_comments": 5, "objectID": "43556789", "points": 12, "story_id": 43556789, "story_text": "Hey HN, I\u2019m Jordan, founder of Vapi. For the past 1.5 years, we&#x27;ve been building voice infrastructure designed to help developers easily create, test, and deploy voice agents. We\u2019re everything between raw models (like the realtime API) and end-users interacting with voice agents over telephony or within applications.<p>We\u2019ve been around for about 1.5yrs. Vapi was born out of a personal frustration\u2014I wanted to build my own voice agent but found existing solutions extremely limited. Back then, there were only a handful of voice AI startups. Since March 2024, we&#x27;ve witnessed a Cambrian explosion of new entrants.<p>In the early days, the biggest challenge was always latency. But as models have rapidly improved and funding has flooded into the space, voice agents are now trusted for business-critical applications like prior authorizations, payment processing, and customer support.<p>Today\u2019s challenge is reliability: can these voice agents consistently perform in production? \nWhat we\u2019ve learned deploying 1000s of them: Yes, but it&#x27;s really hard.<p>Why? Real-time is unweildy\u2014coordinating 10+ third party real-time models, handling diverse, unconstrained inputs and outputs, with no latency budget for reasoning and reflection. Voice interactions don\u2019t follow a linear flow like text, making determinism a challenge. Getting it right takes a ton of tuning and iteration.<p>Our approach does not simplify, it assumes it will be hard. We\u2019re built for developers. We expose extensive configuration options through an API-native platform, including custom models, 100+ integrations, and detailed call testing and monitoring features.<p>--<p>Tldr; it works!<p>- We achieve performance equivalent to humans for use cases with clear user intent (appointment bookings, support, etc.).<p>- We&#x27;ve handled over 44M voice calls across support automation, healthcare intake, and logistics.<p>- We reliably manage long-running jobs (10\u201360+ minutes) with 99.95% uptime.<p>Questions we\u2019re still exploring:<p>- When you have a complex product with 1000s of nobs, how do you help developers wield them, without heavy consulting?<p>- How do you detect and eliminate bad actors when the line can be blurry &#x2F; not enough context to make the call on what is fraud and what isn\u2019t?<p>- How do you guarantee reliability when we are built on services that are less reliable than industry standard (e.g. OpenAI posts &lt; 99.9% uptime)<p>--<p>You can try a live agent by calling: 1-844-HEY-VAPI. Yes, that\u2019s my voice.<p>Docs: <a href=\"https:&#x2F;&#x2F;docs.vapi.ai&#x2F;introduction\">https:&#x2F;&#x2F;docs.vapi.ai&#x2F;introduction</a><p>Let me know if you have any questions about voice; the market or the tech. Happy to poke the eng team to chat on architecture, etc. if it\u2019s of interest.<p>PS- we are also launching on product hunt today. we got an awesome team working really hard to make that happen, so pls support, thanks!", "title": "Show HN: 1-844-HEY-VAPI \u2013 voice AI platform for developers", "updated_at": "2025-10-06T11:13:31Z"}], "hitsPerPage": 15, "nbHits": 48, "nbPages": 4, "page": 0, "params": "query=linear+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 9, "processingTimingsMS": {"_request": {"roundTrip": 16}, "afterFetch": {"format": {"highlighting": 3, "total": 3}}, "fetch": {"query": 5, "scanning": 2, "total": 8}, "total": 9}, "query": "linear production", "serverTimeMS": 13}}