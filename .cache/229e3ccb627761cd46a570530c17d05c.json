{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jeffbarr"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "sdk", "production"], "value": "<em>AWS SDK</em> for C++ Now Ready for <em>Production</em> Use"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "sdk", "production"], "value": "https://aws.amazon.com/blogs/aws/<em>aws-sdk</em>-for-c-now-ready-for-<em>production</em>-use/"}}, "_tags": ["story", "author_jeffbarr", "story_12439348"], "author": "jeffbarr", "created_at": "2016-09-06T21:16:56Z", "created_at_i": 1473196616, "num_comments": 0, "objectID": "12439348", "points": 3, "story_id": 12439348, "title": "AWS SDK for C++ Now Ready for Production Use", "updated_at": "2024-09-19T23:39:53Z", "url": "https://aws.amazon.com/blogs/aws/aws-sdk-for-c-now-ready-for-production-use/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "alexgarden"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "sdk", "production"], "value": "Much of my work right now involves complex, long-running, multi-agentic teams of agents. I kept running into the same problem: \u201cHow do I keep these guys in line?\u201d  Rules weren\u2019t cutting it, and we needed a scalable, agentic-native STANDARD I could count on.  There wasn\u2019t one.  So I built one.<p>Here are two open-source protocols that extend A2A, granting AI agents behavioral contracts and runtime integrity monitoring:<p>- Agent Alignment Protocol (AAP): What an agent can do / has done.\n- Agent Integrity Protocol (AIP): What an agent is thinking about doing / is allowed to do.<p>The problem: AI agents make autonomous decisions but have no standard way to declare what they're allowed to do, prove they're doing it, or detect when they've drifted. Observability tools tell you what happened. These protocols tell you whether what happened was okay.<p>Here's a concrete example. Say you have an agent who handles customer support tickets. Its Alignment Card declares:<p>{\n  &quot;permitted&quot;: [&quot;read_tickets&quot;, &quot;draft_responses&quot;, &quot;escalate_to_human&quot;],\n  &quot;forbidden&quot;: [&quot;access_payment_data&quot;, &quot;issue_refunds&quot;, &quot;modify_account_settings&quot;],\n  &quot;escalation_triggers&quot;: [&quot;billing_request_over_500&quot;],\n  &quot;values&quot;: [&quot;accuracy&quot;, &quot;empathy&quot;, &quot;privacy&quot;]\n}<p>The agent gets a ticket: &quot;Can you refund my last three orders?&quot; The agent's reasoning trace shows it considering a call to the payments API. AIP reads that thinking, compares it to the card, and produces an Integrity Checkpoint:<p>{\n  &quot;verdict&quot;: &quot;boundary_violation&quot;,\n  &quot;concerns&quot;: [&quot;forbidden_action: access_payment_data&quot;],\n  &quot;reasoning&quot;: &quot;Agent considered payments API access, which is explicitly forbidden. Should escalate to human.&quot;,\n  &quot;confidence&quot;: 0.95\n}<p>The agent gets nudged back before it acts. Not after. Not in a log you review during a 2:00 AM triage. Between this turn and the next.<p>That's the core idea. AAP defines what agents should do (the contract). AIP watches what they're actually thinking and flags when those diverge (the conscience). Over time, AIP builds a drift profile \u2014 if an agent that was cautious starts getting aggressive, the system notices.<p>When multiple agents work together, it gets more interesting. Agents exchange Alignment Cards and verify value compatibility before coordination begins. An agent that values &quot;move fast&quot; and one that values &quot;rollback safety&quot; registers low coherence, and the system surfaces that conflict before work starts. Live demo with four agents handling a <em>production</em> incident: <a href=\"https://mnemom.ai/showcase\" rel=\"nofollow\">https://mnemom.ai/showcase</a><p>The protocols are Apache-licensed, work with any Anthropic/OpenAI/Gemini agent, and ship <em>as SDKs</em> on npm and PyPI. A free gateway proxy (smoltbot) adds integrity checking to any agent with zero code changes.<p>GitHub: <a href=\"https://github.com/mnemom\" rel=\"nofollow\">https://github.com/mnemom</a> \nDocs: docs.mnemom.ai\nDemo video: <a href=\"https://youtu.be/fmUxVZH09So\" rel=\"nofollow\">https://youtu.be/fmUxVZH09So</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Trust Protocols for Anthropic/OpenAI/Gemini"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.mnemom.ai"}}, "_tags": ["story", "author_alexgarden", "story_47062824", "show_hn"], "author": "alexgarden", "children": [47063303, 47064720, 47064782, 47065280, 47065377, 47065761, 47065840, 47065953, 47068510, 47071774, 47071869, 47083446, 47093675, 47148556], "created_at": "2026-02-18T16:33:56Z", "created_at_i": 1771432436, "num_comments": 33, "objectID": "47062824", "points": 40, "story_id": 47062824, "story_text": "Much of my work right now involves complex, long-running, multi-agentic teams of agents. I kept running into the same problem: \u201cHow do I keep these guys in line?\u201d  Rules weren\u2019t cutting it, and we needed a scalable, agentic-native STANDARD I could count on.  There wasn\u2019t one.  So I built one.<p>Here are two open-source protocols that extend A2A, granting AI agents behavioral contracts and runtime integrity monitoring:<p>- Agent Alignment Protocol (AAP): What an agent can do &#x2F; has done.\n- Agent Integrity Protocol (AIP): What an agent is thinking about doing &#x2F; is allowed to do.<p>The problem: AI agents make autonomous decisions but have no standard way to declare what they&#x27;re allowed to do, prove they&#x27;re doing it, or detect when they&#x27;ve drifted. Observability tools tell you what happened. These protocols tell you whether what happened was okay.<p>Here&#x27;s a concrete example. Say you have an agent who handles customer support tickets. Its Alignment Card declares:<p>{\n  &quot;permitted&quot;: [&quot;read_tickets&quot;, &quot;draft_responses&quot;, &quot;escalate_to_human&quot;],\n  &quot;forbidden&quot;: [&quot;access_payment_data&quot;, &quot;issue_refunds&quot;, &quot;modify_account_settings&quot;],\n  &quot;escalation_triggers&quot;: [&quot;billing_request_over_500&quot;],\n  &quot;values&quot;: [&quot;accuracy&quot;, &quot;empathy&quot;, &quot;privacy&quot;]\n}<p>The agent gets a ticket: &quot;Can you refund my last three orders?&quot; The agent&#x27;s reasoning trace shows it considering a call to the payments API. AIP reads that thinking, compares it to the card, and produces an Integrity Checkpoint:<p>{\n  &quot;verdict&quot;: &quot;boundary_violation&quot;,\n  &quot;concerns&quot;: [&quot;forbidden_action: access_payment_data&quot;],\n  &quot;reasoning&quot;: &quot;Agent considered payments API access, which is explicitly forbidden. Should escalate to human.&quot;,\n  &quot;confidence&quot;: 0.95\n}<p>The agent gets nudged back before it acts. Not after. Not in a log you review during a 2:00 AM triage. Between this turn and the next.<p>That&#x27;s the core idea. AAP defines what agents should do (the contract). AIP watches what they&#x27;re actually thinking and flags when those diverge (the conscience). Over time, AIP builds a drift profile \u2014 if an agent that was cautious starts getting aggressive, the system notices.<p>When multiple agents work together, it gets more interesting. Agents exchange Alignment Cards and verify value compatibility before coordination begins. An agent that values &quot;move fast&quot; and one that values &quot;rollback safety&quot; registers low coherence, and the system surfaces that conflict before work starts. Live demo with four agents handling a production incident: <a href=\"https:&#x2F;&#x2F;mnemom.ai&#x2F;showcase\" rel=\"nofollow\">https:&#x2F;&#x2F;mnemom.ai&#x2F;showcase</a><p>The protocols are Apache-licensed, work with any Anthropic&#x2F;OpenAI&#x2F;Gemini agent, and ship as SDKs on npm and PyPI. A free gateway proxy (smoltbot) adds integrity checking to any agent with zero code changes.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mnemom\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mnemom</a> \nDocs: docs.mnemom.ai\nDemo video: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;fmUxVZH09So\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;fmUxVZH09So</a>", "title": "Show HN: Trust Protocols for Anthropic/OpenAI/Gemini", "updated_at": "2026-02-27T10:03:38Z", "url": "https://www.mnemom.ai"}], "hitsPerPage": 15, "nbHits": 2, "nbPages": 1, "page": 0, "params": "query=aws-sdk+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 13, "processingTimingsMS": {"_request": {"queue": 1, "roundTrip": 21}, "fetch": {"query": 7, "scanning": 5, "total": 13}, "total": 13}, "query": "aws-sdk production", "serverTimeMS": 15}}