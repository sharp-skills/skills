{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "williamzeng0"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "Hi HN! We\u2019re William and Kevin from Sweep AI. We\u2019re building an AI coding assistant for JetBrains IDEs.<p>We previously tried to build an AI junior developer that writes GitHub PRs (<a href=\"https://news.ycombinator.com/item?id=36987454\">https://news.ycombinator.com/item?id=36987454</a>). It was fun, but we ultimately decided to pivot. Here are a couple reasons it didn\u2019t work:<p>1. Our agent really needed a well defined spec to have a &gt;90% success rate on tasks. Developers are lazy (myself included) when describing tasks and agents weren\u2019t good enough to make up for it. We found developers don\u2019t want to write a spec, they want to see the agent try and iterate with it. Fixing this is hard! The flow needs to be <i>fast</i> otherwise people get distracted and go scroll HN or check slack.<p>2. Executing code is challenging, especially for <em>production</em> apps. <em>Github actions</em> were too slow to use as a code  execution sandbox, and emulating the developer\u2019s environment in Docker or what-have-you was not feasible. Agents weren't ready for real codebases because their CI wasn't built with agents in mind.<p>We looked around for a better UX than GitHub issues, and we noticed that JetBrains developers were consistently unhappy with GitHub Copilot. Cursor and Windsurf (the current market leaders) only supported VSCode.<p>There were other good options but none were Cursor-quality. We asked ourselves \u201cwhy not?\u201d and decided to investigate. I spoke with an ex-JetBrains employee who said: \u201cThe best AI developers don\u2019t really use Java, and the best Java developers tend to work in enterprise companies rather than startups.\u201d<p>So we decided to take our experience in building an AI agent and go for JetBrains. Here\u2019s what we\u2019ve learned so far:<p>- The latest open-source models like Qwen are really good. Some use cases like applying code to a file work decently with these models out of the box, so we don\u2019t have to do as much 0 \u2192 1 R&amp;D to build a great product. This doesn\u2019t make it easy, but it does mean a small team that really cares can deliver a great product.<p>- Most agents are still while-loop wrappers. We tried that with the last generation of models and found it to slow. Instead we\u2019re trying a different approach that relies more on the code graph to see neighboring files. It really decreases the latency because we can rely on the user\u2019s current file and skip the \u201cgrep in a loop\u201d section. It also costs a bit more, but this hasn\u2019t been a problem yet.<p>- Big company solutions for JetBrains feel like box-ticking and are behind their VSCode equivalents. I\u2019ve seen many complaints that GH Copilot uses a couple of gigs of RAM (which really sucks when your IDE already takes 3+ gigs of RAM.<p>Let me know what you think! I\u2019m also curious how/if you\u2019re using agents. I usually get impatient waiting for the LLM."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: AI in JetBrains that doesn't suck"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://docs.sweep.dev"}}, "_tags": ["story", "author_williamzeng0", "story_43490121", "show_hn"], "author": "williamzeng0", "children": [43555606, 43566291], "created_at": "2025-03-27T03:28:12Z", "created_at_i": 1743046092, "num_comments": 4, "objectID": "43490121", "points": 9, "story_id": 43490121, "story_text": "Hi HN! We\u2019re William and Kevin from Sweep AI. We\u2019re building an AI coding assistant for JetBrains IDEs.<p>We previously tried to build an AI junior developer that writes GitHub PRs (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36987454\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36987454</a>). It was fun, but we ultimately decided to pivot. Here are a couple reasons it didn\u2019t work:<p>1. Our agent really needed a well defined spec to have a &gt;90% success rate on tasks. Developers are lazy (myself included) when describing tasks and agents weren\u2019t good enough to make up for it. We found developers don\u2019t want to write a spec, they want to see the agent try and iterate with it. Fixing this is hard! The flow needs to be <i>fast</i> otherwise people get distracted and go scroll HN or check slack.<p>2. Executing code is challenging, especially for production apps. Github actions were too slow to use as a code  execution sandbox, and emulating the developer\u2019s environment in Docker or what-have-you was not feasible. Agents weren&#x27;t ready for real codebases because their CI wasn&#x27;t built with agents in mind.<p>We looked around for a better UX than GitHub issues, and we noticed that JetBrains developers were consistently unhappy with GitHub Copilot. Cursor and Windsurf (the current market leaders) only supported VSCode.<p>There were other good options but none were Cursor-quality. We asked ourselves \u201cwhy not?\u201d and decided to investigate. I spoke with an ex-JetBrains employee who said: \u201cThe best AI developers don\u2019t really use Java, and the best Java developers tend to work in enterprise companies rather than startups.\u201d<p>So we decided to take our experience in building an AI agent and go for JetBrains. Here\u2019s what we\u2019ve learned so far:<p>- The latest open-source models like Qwen are really good. Some use cases like applying code to a file work decently with these models out of the box, so we don\u2019t have to do as much 0 \u2192 1 R&amp;D to build a great product. This doesn\u2019t make it easy, but it does mean a small team that really cares can deliver a great product.<p>- Most agents are still while-loop wrappers. We tried that with the last generation of models and found it to slow. Instead we\u2019re trying a different approach that relies more on the code graph to see neighboring files. It really decreases the latency because we can rely on the user\u2019s current file and skip the \u201cgrep in a loop\u201d section. It also costs a bit more, but this hasn\u2019t been a problem yet.<p>- Big company solutions for JetBrains feel like box-ticking and are behind their VSCode equivalents. I\u2019ve seen many complaints that GH Copilot uses a couple of gigs of RAM (which really sucks when your IDE already takes 3+ gigs of RAM.<p>Let me know what you think! I\u2019m also curious how&#x2F;if you\u2019re using agents. I usually get impatient waiting for the LLM.", "title": "Show HN: AI in JetBrains that doesn't suck", "updated_at": "2025-04-10T18:13:19Z", "url": "https://docs.sweep.dev"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "iamsamwood"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "I\u2019m one of the developers behind Luther. We make it easy to operate \u201cmega-workflows\u201d \u2013 the long, cross-team, multi-system processes (insurance claims, mortgages, etc.) that today are usually held together with bespoke glue code and mismatched workflow tools, which are always about to be upgraded \u201cany day now\u201d.<p>Our platform replaces that glue code with a Common Operating Script, that runs reliably and consistently across teams and systems. Systems are connected to the platform rather than each other, so it\u2019s easy to add and remove systems as your tools change, and changes must be approved by all teams before they go live, so everyone\u2019s operating the same process, all the time.<p>To show how this works in <em>production</em>, we open-sourced a full Claims Settlement case study (75k loc in <em>production</em>, 12 systems and 5 teams), and you can check it out here:\nRepo: <a href=\"https://github.com/luthersystems/cross-department-claims-settlement\" rel=\"nofollow\">https://github.com/luthersystems/cross-department-claims-set...</a>\nVideo walkthrough: <a href=\"https://vimeo.com/1141432607?fl=pl&amp;fe=cmt\" rel=\"nofollow\">https://vimeo.com/1141432607?fl=pl&amp;fe=cmt</a><p>How it works<p>\u2022 Logic: Write process-operations logic in a transactional Common Operating Script. Nodes execute this logic to process events across the various teams, while maintaining consistency.<p>\u2022 Infra: We provide a drop-in Kubernetes cluster with Prometheus/Grafana pre-wired. Use our managed version, or deploy to your cloud via Terraform.<p>\u2022 DevEx: Our connector hub provides 100s of ready-to-go connectors (S3, Postgres, etc.) and a prebuilt <em>GitHub Actions</em> pipeline. We provide a <em>production</em>-ready repo for you.\nWe\u2019d love feedback from the HN crowd, especially on the pain points you\u2019ve seen when stitching together complex workflows in enterprise environments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Luther Enterprise: Dev platform for operating end-to-end mega workflows"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://enterprise.luthersystems.com/"}}, "_tags": ["story", "author_iamsamwood", "story_46528939", "show_hn"], "author": "iamsamwood", "children": [46530220, 46530845, 46531293], "created_at": "2026-01-07T17:02:32Z", "created_at_i": 1767805352, "num_comments": 6, "objectID": "46528939", "points": 8, "story_id": 46528939, "story_text": "I\u2019m one of the developers behind Luther. We make it easy to operate \u201cmega-workflows\u201d \u2013 the long, cross-team, multi-system processes (insurance claims, mortgages, etc.) that today are usually held together with bespoke glue code and mismatched workflow tools, which are always about to be upgraded \u201cany day now\u201d.<p>Our platform replaces that glue code with a Common Operating Script, that runs reliably and consistently across teams and systems. Systems are connected to the platform rather than each other, so it\u2019s easy to add and remove systems as your tools change, and changes must be approved by all teams before they go live, so everyone\u2019s operating the same process, all the time.<p>To show how this works in production, we open-sourced a full Claims Settlement case study (75k loc in production, 12 systems and 5 teams), and you can check it out here:\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;luthersystems&#x2F;cross-department-claims-settlement\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;luthersystems&#x2F;cross-department-claims-set...</a>\nVideo walkthrough: <a href=\"https:&#x2F;&#x2F;vimeo.com&#x2F;1141432607?fl=pl&amp;fe=cmt\" rel=\"nofollow\">https:&#x2F;&#x2F;vimeo.com&#x2F;1141432607?fl=pl&amp;fe=cmt</a><p>How it works<p>\u2022 Logic: Write process-operations logic in a transactional Common Operating Script. Nodes execute this logic to process events across the various teams, while maintaining consistency.<p>\u2022 Infra: We provide a drop-in Kubernetes cluster with Prometheus&#x2F;Grafana pre-wired. Use our managed version, or deploy to your cloud via Terraform.<p>\u2022 DevEx: Our connector hub provides 100s of ready-to-go connectors (S3, Postgres, etc.) and a prebuilt GitHub Actions pipeline. We provide a production-ready repo for you.\nWe\u2019d love feedback from the HN crowd, especially on the pain points you\u2019ve seen when stitching together complex workflows in enterprise environments.", "title": "Show HN: Luther Enterprise: Dev platform for operating end-to-end mega workflows", "updated_at": "2026-02-06T05:25:17Z", "url": "https://enterprise.luthersystems.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "andriosr"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "Hello HN! I'm Andrios, from Runops.io - we're building a proxy to commands you run in the terminal that adds Git, code reviews in Slack, and removes sensitive data from results. It's like the Cloud Shells from GCP/AWS, but with more features and using your local zsh/bash terminal.<p>You run an AWS CLI command in the terminal and it goes to Runops instead of AWS. Runops adds the command to Git and gets peer reviews (when required) in Slack before sending it to AWS. After it runs, we deliver the results back in the terminal, but with all sensitive data masked. It works for AWS, Kubernetes, databases, and others.<p>I was leading the Infra team at a Fintech (pismo.io/en), and we wanted to give autonomy to all developers in <em>production</em>. But we couldn\u2019t give them direct access due to compliance requirements. The solution was to have a small number of people (my team) with &quot;full access&quot; to <em>production</em> systems. Engineers would ask us when they needed to run one-off scripts in <em>production</em>. Our goal was to deliver automations so that other teams wouldn't need to ask us to do things. We would build a way for them to do it with compliance, security, and reliability.<p>It didn't work. We were spending 80% of the time processing the queue of requests, and 20% building automations. The backlog was always increasing, and the team was burning out. Engineers were not happy as their requests took a long time to process and clients were angry at them.<p>But some nice automations came out of that. For instance: we needed to review ad-hoc prod database reads to avoid bad queries. So we built a Jenkins pipeline that ran SQL queries from Git after code review using Flyway. Any engineer could run queries in prod, leaving traces on who did it, reviews, when it happened, and why, for every query.<p>When talking to friends at similar companies, I saw the problem was even worse. Some of them weren't trying to automate, they already had dedicated people for running these scripts, i.e., an ops team. I knew there was a better way, so I set out to build it. I quit this job mid last year, with about 8 months' worth of savings to make this work before I'd need to find a job again. It was tough in the beginning, as I\u2019m an engineer and had to learn sales, marketing and product management on the job, but after getting the first few customers things started improving.<p>The goal for Runops is to let any engineer run anything in <em>production</em> as if they had full access, automating as much as possible of security and compliance. When human interaction is needed, we make it synchronous using Slack. Now, instead of having a single team as a bottleneck, you can have everyone do things in <em>production</em>. Centralizing teams with most of the access to AWS, Kubernetes, and databases is bad. It makes for slow Change Management processes using Jira or other tools with manual executions at the end. Runops let\u2019s you add quick reviews from experts (Infra, DBA, security, etc), and automates executions.<p>The primary interface is a CLI, where you run scripts that goes from SQL queries to kubectl exec and AWS CLI commands. We don't create new abstractions, you use the same commands and docs available, we just proxy them. A nice benefit is replacing VPNs and the 10 client tools/credentials you would need today. We also support templates for custom actions in a bunch of languages.<p>We built it using <em>Github Actions</em> for executing commands. We store configurations and credentials as Actions Secrets and they get injected when a command requires them. It's nice because we can run anything that goes in a Docker container in &lt;15 seconds. We have plans to improve it beyond Actions by creating a real-time proxy. That will enable a REPL-like experience.\nRunops doesn't have a web interface, this is on purpose, we don't want to be one more tool engineers have to learn. Most interactions happen with our CLI or Slack. We have a simple admin UI in Retool.<p>We do everything using Lisp. The CLI uses Clojurescript; the REST API uses Clojure. It's great to have the same language everywhere, and Lisp is also a fantastic advantage.<p>Today we have big Fintechs using Runops. They use it to let developers run commands inside Kubernetes pods, like Rails Runner and Elixir IEx, SQL queries, DynamoDB queries, and making internal API calls in private networks using cURL. One of the best parts of building this has been seeing developers doing more <em>production</em> work. Regulated companies that never considered giving this level of autonomy to all developers are changing their minds. It's great to see a tool impacting the culture, increasing trust.<p>We're really happy we get to show this to you all, thank you for reading about it! Please let us know your thoughts and questions."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Launch HN: Runops (YC W21) \u2013 A better cloud shell for <em>production</em> apps"}}, "_tags": ["story", "author_andriosr", "story_26385434", "launch_hn"], "author": "andriosr", "children": [26386156, 26386190, 26386400, 26386598, 26386627, 26386950, 26387509, 26392287, 26396005], "created_at": "2021-03-08T13:26:18Z", "created_at_i": 1615209978, "num_comments": 23, "objectID": "26385434", "points": 97, "story_id": 26385434, "story_text": "Hello HN! I&#x27;m Andrios, from Runops.io - we&#x27;re building a proxy to commands you run in the terminal that adds Git, code reviews in Slack, and removes sensitive data from results. It&#x27;s like the Cloud Shells from GCP&#x2F;AWS, but with more features and using your local zsh&#x2F;bash terminal.<p>You run an AWS CLI command in the terminal and it goes to Runops instead of AWS. Runops adds the command to Git and gets peer reviews (when required) in Slack before sending it to AWS. After it runs, we deliver the results back in the terminal, but with all sensitive data masked. It works for AWS, Kubernetes, databases, and others.<p>I was leading the Infra team at a Fintech (pismo.io&#x2F;en), and we wanted to give autonomy to all developers in production. But we couldn\u2019t give them direct access due to compliance requirements. The solution was to have a small number of people (my team) with &quot;full access&quot; to production systems. Engineers would ask us when they needed to run one-off scripts in production. Our goal was to deliver automations so that other teams wouldn&#x27;t need to ask us to do things. We would build a way for them to do it with compliance, security, and reliability.<p>It didn&#x27;t work. We were spending 80% of the time processing the queue of requests, and 20% building automations. The backlog was always increasing, and the team was burning out. Engineers were not happy as their requests took a long time to process and clients were angry at them.<p>But some nice automations came out of that. For instance: we needed to review ad-hoc prod database reads to avoid bad queries. So we built a Jenkins pipeline that ran SQL queries from Git after code review using Flyway. Any engineer could run queries in prod, leaving traces on who did it, reviews, when it happened, and why, for every query.<p>When talking to friends at similar companies, I saw the problem was even worse. Some of them weren&#x27;t trying to automate, they already had dedicated people for running these scripts, i.e., an ops team. I knew there was a better way, so I set out to build it. I quit this job mid last year, with about 8 months&#x27; worth of savings to make this work before I&#x27;d need to find a job again. It was tough in the beginning, as I\u2019m an engineer and had to learn sales, marketing and product management on the job, but after getting the first few customers things started improving.<p>The goal for Runops is to let any engineer run anything in production as if they had full access, automating as much as possible of security and compliance. When human interaction is needed, we make it synchronous using Slack. Now, instead of having a single team as a bottleneck, you can have everyone do things in production. Centralizing teams with most of the access to AWS, Kubernetes, and databases is bad. It makes for slow Change Management processes using Jira or other tools with manual executions at the end. Runops let\u2019s you add quick reviews from experts (Infra, DBA, security, etc), and automates executions.<p>The primary interface is a CLI, where you run scripts that goes from SQL queries to kubectl exec and AWS CLI commands. We don&#x27;t create new abstractions, you use the same commands and docs available, we just proxy them. A nice benefit is replacing VPNs and the 10 client tools&#x2F;credentials you would need today. We also support templates for custom actions in a bunch of languages.<p>We built it using Github Actions for executing commands. We store configurations and credentials as Actions Secrets and they get injected when a command requires them. It&#x27;s nice because we can run anything that goes in a Docker container in &lt;15 seconds. We have plans to improve it beyond Actions by creating a real-time proxy. That will enable a REPL-like experience.\nRunops doesn&#x27;t have a web interface, this is on purpose, we don&#x27;t want to be one more tool engineers have to learn. Most interactions happen with our CLI or Slack. We have a simple admin UI in Retool.<p>We do everything using Lisp. The CLI uses Clojurescript; the REST API uses Clojure. It&#x27;s great to have the same language everywhere, and Lisp is also a fantastic advantage.<p>Today we have big Fintechs using Runops. They use it to let developers run commands inside Kubernetes pods, like Rails Runner and Elixir IEx, SQL queries, DynamoDB queries, and making internal API calls in private networks using cURL. One of the best parts of building this has been seeing developers doing more production work. Regulated companies that never considered giving this level of autonomy to all developers are changing their minds. It&#x27;s great to see a tool impacting the culture, increasing trust.<p>We&#x27;re really happy we get to show this to you all, thank you for reading about it! Please let us know your thoughts and questions.", "title": "Launch HN: Runops (YC W21) \u2013 A better cloud shell for production apps", "updated_at": "2024-09-20T08:07:46Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "hugorut"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "Hey folks, I wanted to share an open source project I've been working on that might interest some of you.<p>Lacquer is a lightweight AI workflow engine that turns repeatable engineering tasks into reliable YAML workflows that never skip a step. Think <em>GitHub Actions</em>, but for AI-powered internal tools.<p>I built this because I needed something quick and practical for my day-to-day engineering work. The market is saturated with no-code automation GUIs, but they don't fit my workflow as an engineer. With Lacquer, I wanted:<p>- Local First: Prototype workflows directly in your terminal and editor\n- Version Control: Plain YAML files you can commit to GitHub and share with your team\n- Familiar DSL: <em>GitHub Actions</em>-like syntax that leverages what you already know\n- Single Binary: Ships as one Go binary (~40MB) - no complex dependencies or container orchestration\n- Easy Deployment: Run locally with `laq run` or deploy to <em>production</em> with `laq serve`<p>It's early days, but I'd love to hear if this scratches an itch for you, or if you have ideas for what would make it more useful for your workflows.<p>GitHub: <a href=\"https://github.com/lacquerai/lacquer\" rel=\"nofollow\">https://github.com/lacquerai/lacquer</a> | Website: <a href=\"https://lacquer.ai\" rel=\"nofollow\">https://lacquer.ai</a> | Docs: <a href=\"https://lacquer.ai/docs\" rel=\"nofollow\">https://lacquer.ai/docs</a><p>Thanks for checking it out!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["github", "actions"], "value": "Show HN: Lacquer \u2013 <em>GitHub Actions</em> for AI workflows in a single Go binary"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/lacquerai/lacquer"}}, "_tags": ["story", "author_hugorut", "story_44985058", "show_hn"], "author": "hugorut", "children": [44985138], "created_at": "2025-08-22T14:22:45Z", "created_at_i": 1755872565, "num_comments": 1, "objectID": "44985058", "points": 11, "story_id": 44985058, "story_text": "Hey folks, I wanted to share an open source project I&#x27;ve been working on that might interest some of you.<p>Lacquer is a lightweight AI workflow engine that turns repeatable engineering tasks into reliable YAML workflows that never skip a step. Think GitHub Actions, but for AI-powered internal tools.<p>I built this because I needed something quick and practical for my day-to-day engineering work. The market is saturated with no-code automation GUIs, but they don&#x27;t fit my workflow as an engineer. With Lacquer, I wanted:<p>- Local First: Prototype workflows directly in your terminal and editor\n- Version Control: Plain YAML files you can commit to GitHub and share with your team\n- Familiar DSL: GitHub Actions-like syntax that leverages what you already know\n- Single Binary: Ships as one Go binary (~40MB) - no complex dependencies or container orchestration\n- Easy Deployment: Run locally with `laq run` or deploy to production with `laq serve`<p>It&#x27;s early days, but I&#x27;d love to hear if this scratches an itch for you, or if you have ideas for what would make it more useful for your workflows.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;lacquerai&#x2F;lacquer\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;lacquerai&#x2F;lacquer</a> | Website: <a href=\"https:&#x2F;&#x2F;lacquer.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;lacquer.ai</a> | Docs: <a href=\"https:&#x2F;&#x2F;lacquer.ai&#x2F;docs\" rel=\"nofollow\">https:&#x2F;&#x2F;lacquer.ai&#x2F;docs</a><p>Thanks for checking it out!", "title": "Show HN: Lacquer \u2013 GitHub Actions for AI workflows in a single Go binary", "updated_at": "2025-08-26T14:56:06Z", "url": "https://github.com/lacquerai/lacquer"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "thesssaism"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "I recently ran an audit on our latest full-stack repo to figure out why &quot;spinning up a new project&quot; felt like such a heavy lift. I counted every file required just to reach a &quot;<em>production</em>-ready&quot; baseline\u2014before writing a single line of unique feature code.<p>The count was roughly 600 files.<p>To be clear, I'm not talking about a `create-react-app` sandbox. I mean a compliant, scalable SaaS foundation: Next.js frontend, Node.js/NestJS backend, mobile wrapper, CI/CD pipelines, and enough security config to pass a SOC2 audit.<p>It sounds ridiculous (and honestly, it feels ridiculous), but when I broke it down, I couldn't find many files I was willing to delete.<p>Here is where the bloat comes from:<p>First, the &quot;Configuration Hell&quot; accounts for about 40-50 files alone. We aren't just dealing with `package.json` anymore. It's `tsconfig.json` (base), `tsconfig.build.json`, `tsconfig.spec.json`... multiplied across frontend, backend, and shared libraries. Then add `.eslintrc.js`, `.prettierrc`, `jest.config.js`, `vitest.config.ts`, `nodemon.json`, and the Docker-compose variants for dev, test, and prod.<p>Then there\u2019s the DevOps and Quality layer. We have roughly 20-30 files for <em>GitHub Actions</em> workflows (lint, test, build, deploy, semantic release), Husky hooks (pre-commit, commit-msg), and pull request templates.<p>But the real multiplier is the separation of concerns. In a modern monorepo, a &quot;Hello World&quot; isn't just `console.log`. It\u2019s:\n- A NestJS module (Controller, Service, Module, DTO, Entity, Unit Test, E2E Test).\n- A Next.js slice (Page, Component, Type definition, API client wrapper).\n- A shared library entry.<p>We found that adding a single &quot;minimal&quot; API endpoint usually touches 5-7 files just to maintain architectural standards.<p>The trade-off is painful. On one hand, this setup handles the things we used to forget: security headers, proper logging, consistent error handling, and type safety across boundaries. It prevents the &quot;spaghetti code&quot; distinct to startups that scale too fast.<p>On the other hand, the cognitive load of managing a 600-file &quot;empty&quot; project is massive. Updating dependencies becomes a chore because a major version bump in one tool (like ESLint) cascades through forty config files.<p>I\u2019m curious how others are handling this &quot;starting line&quot; complexity.<p>Are you accepting the boilerplate as the cost of doing business? Or have you found a way to strip this down without sacrificing the compliance/safety guardrails that enterprise clients demand?<p>It feels like we've over-engineered the entry point of software development, but I\u2019m not sure what the alternative is for a serious project. We tried going &quot;lean&quot; initially, but spent weeks retrofitting auth and testing harnesses later\u2014which was worse.<p>Is there a middle ground I'm missing, or is ~600 files just the new normal?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Tell HN: A <em>production</em>-ready \"Hello World\" is now ~600 files"}}, "_tags": ["story", "author_thesssaism", "story_47074571", "ask_hn"], "author": "thesssaism", "children": [47074794, 47075001, 47075012, 47076406], "created_at": "2026-02-19T15:07:03Z", "created_at_i": 1771513623, "num_comments": 4, "objectID": "47074571", "points": 4, "story_id": 47074571, "story_text": "I recently ran an audit on our latest full-stack repo to figure out why &quot;spinning up a new project&quot; felt like such a heavy lift. I counted every file required just to reach a &quot;production-ready&quot; baseline\u2014before writing a single line of unique feature code.<p>The count was roughly 600 files.<p>To be clear, I&#x27;m not talking about a `create-react-app` sandbox. I mean a compliant, scalable SaaS foundation: Next.js frontend, Node.js&#x2F;NestJS backend, mobile wrapper, CI&#x2F;CD pipelines, and enough security config to pass a SOC2 audit.<p>It sounds ridiculous (and honestly, it feels ridiculous), but when I broke it down, I couldn&#x27;t find many files I was willing to delete.<p>Here is where the bloat comes from:<p>First, the &quot;Configuration Hell&quot; accounts for about 40-50 files alone. We aren&#x27;t just dealing with `package.json` anymore. It&#x27;s `tsconfig.json` (base), `tsconfig.build.json`, `tsconfig.spec.json`... multiplied across frontend, backend, and shared libraries. Then add `.eslintrc.js`, `.prettierrc`, `jest.config.js`, `vitest.config.ts`, `nodemon.json`, and the Docker-compose variants for dev, test, and prod.<p>Then there\u2019s the DevOps and Quality layer. We have roughly 20-30 files for GitHub Actions workflows (lint, test, build, deploy, semantic release), Husky hooks (pre-commit, commit-msg), and pull request templates.<p>But the real multiplier is the separation of concerns. In a modern monorepo, a &quot;Hello World&quot; isn&#x27;t just `console.log`. It\u2019s:\n- A NestJS module (Controller, Service, Module, DTO, Entity, Unit Test, E2E Test).\n- A Next.js slice (Page, Component, Type definition, API client wrapper).\n- A shared library entry.<p>We found that adding a single &quot;minimal&quot; API endpoint usually touches 5-7 files just to maintain architectural standards.<p>The trade-off is painful. On one hand, this setup handles the things we used to forget: security headers, proper logging, consistent error handling, and type safety across boundaries. It prevents the &quot;spaghetti code&quot; distinct to startups that scale too fast.<p>On the other hand, the cognitive load of managing a 600-file &quot;empty&quot; project is massive. Updating dependencies becomes a chore because a major version bump in one tool (like ESLint) cascades through forty config files.<p>I\u2019m curious how others are handling this &quot;starting line&quot; complexity.<p>Are you accepting the boilerplate as the cost of doing business? Or have you found a way to strip this down without sacrificing the compliance&#x2F;safety guardrails that enterprise clients demand?<p>It feels like we&#x27;ve over-engineered the entry point of software development, but I\u2019m not sure what the alternative is for a serious project. We tried going &quot;lean&quot; initially, but spent weeks retrofitting auth and testing harnesses later\u2014which was worse.<p>Is there a middle ground I&#x27;m missing, or is ~600 files just the new normal?", "title": "Tell HN: A production-ready \"Hello World\" is now ~600 files", "updated_at": "2026-02-20T00:09:42Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "geminimir"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "We kept breaking <em>production</em> with small prompt edits \u2014 suddenly outputs weren\u2019t valid JSON, fields disappeared, or formats changed silently.<p>So we built Promptproof, a <em>GitHub Action</em> that runs in CI and blocks PRs when prompts produce invalid outputs.<p>Features:<p>- Validates JSON output<p>- Enforces required keys &amp; schemas<p>- Runs fast in CI (no external infra)<p>- Works with OpenAI, Anthropic, and local models<p>- Adds PR comments so reviewers see failures immediately<p>We\u2019d love feedback: which rules or integrations would make this most useful for you?<p>Repo: <a href=\"https://github.com/geminimir/promptproof-action\" rel=\"nofollow\">https://github.com/geminimir/promptproof-action</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["github", "actions"], "value": "Show HN: Promptproof \u2013 <em>GitHub Action</em> to test LLM prompts, catch bad JSON schemas"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/geminimir/promptproof-action"}}, "_tags": ["story", "author_geminimir", "story_44935792", "show_hn"], "author": "geminimir", "created_at": "2025-08-17T23:20:59Z", "created_at_i": 1755472859, "num_comments": 0, "objectID": "44935792", "points": 3, "story_id": 44935792, "story_text": "We kept breaking production with small prompt edits \u2014 suddenly outputs weren\u2019t valid JSON, fields disappeared, or formats changed silently.<p>So we built Promptproof, a GitHub Action that runs in CI and blocks PRs when prompts produce invalid outputs.<p>Features:<p>- Validates JSON output<p>- Enforces required keys &amp; schemas<p>- Runs fast in CI (no external infra)<p>- Works with OpenAI, Anthropic, and local models<p>- Adds PR comments so reviewers see failures immediately<p>We\u2019d love feedback: which rules or integrations would make this most useful for you?<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;geminimir&#x2F;promptproof-action\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;geminimir&#x2F;promptproof-action</a>", "title": "Show HN: Promptproof \u2013 GitHub Action to test LLM prompts, catch bad JSON schemas", "updated_at": "2025-08-19T00:05:02Z", "url": "https://github.com/geminimir/promptproof-action"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "hjaveed"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "Hey devs!  Check out Docker Compose Anywhere <a href=\"https://github.com/hadijaveed/docker-compose-anywhere\">https://github.com/hadijaveed/docker-compose-anywhere</a>, a project I've been working on to streamline app deployments with docker-compose<p>Perfect for apps that can run on a single server without complex K8s or cloud setups.<p>Docker compose is great for local development, but running docker compose in <em>production</em> is challenging due to downtime, this template addresses zero downtime deployment and the setup with <em>github actions</em><p>## Key Features:<p>- One-click server setup<p>- Zero-downtime continuous deployment<p>- Easy secrets management<p>- Automated SSL setup<p>I would love any feedback, questions, etc."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Docker Compose Anywhere \u2013 Deploy Docker apps to <em>production</em> with ease"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/hadijaveed/docker-compose-anywhere"}}, "_tags": ["story", "author_hjaveed", "story_41462868", "show_hn"], "author": "hjaveed", "created_at": "2024-09-06T04:24:00Z", "created_at_i": 1725596640, "num_comments": 0, "objectID": "41462868", "points": 3, "story_id": 41462868, "story_text": "Hey devs!  Check out Docker Compose Anywhere <a href=\"https:&#x2F;&#x2F;github.com&#x2F;hadijaveed&#x2F;docker-compose-anywhere\">https:&#x2F;&#x2F;github.com&#x2F;hadijaveed&#x2F;docker-compose-anywhere</a>, a project I&#x27;ve been working on to streamline app deployments with docker-compose<p>Perfect for apps that can run on a single server without complex K8s or cloud setups.<p>Docker compose is great for local development, but running docker compose in production is challenging due to downtime, this template addresses zero downtime deployment and the setup with github actions<p>## Key Features:<p>- One-click server setup<p>- Zero-downtime continuous deployment<p>- Easy secrets management<p>- Automated SSL setup<p>I would love any feedback, questions, etc.", "title": "Show HN: Docker Compose Anywhere \u2013 Deploy Docker apps to production with ease", "updated_at": "2024-09-20T17:46:43Z", "url": "https://github.com/hadijaveed/docker-compose-anywhere"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "willsmith72"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "For organizational reasons, has to be AWS. How would you deploy an SPA in a <em>production</em>-ready way today?<p>Nice-to-haves would be:\n- scale-to-0 or serverless because of irregular traffic to save costs\n- easy to configure CI (add unit tests, e2e tests sometime, run lint and unit test on PRs)\n- multiple environments (dev, qa, prod)\n- multiple pages (react router but could change that)\n- still can be seo-crawled<p>So far looked at amplify, ecs with <em>github actions</em>, moving to one of the ssr frameworks"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: What would you use to deploy an SPA to <em>production</em> on AWS today?"}}, "_tags": ["story", "author_willsmith72", "story_35621318", "ask_hn"], "author": "willsmith72", "children": [35621587, 35624507], "created_at": "2023-04-18T22:29:20Z", "created_at_i": 1681856960, "num_comments": 5, "objectID": "35621318", "points": 2, "story_id": 35621318, "story_text": "For organizational reasons, has to be AWS. How would you deploy an SPA in a production-ready way today?<p>Nice-to-haves would be:\n- scale-to-0 or serverless because of irregular traffic to save costs\n- easy to configure CI (add unit tests, e2e tests sometime, run lint and unit test on PRs)\n- multiple environments (dev, qa, prod)\n- multiple pages (react router but could change that)\n- still can be seo-crawled<p>So far looked at amplify, ecs with github actions, moving to one of the ssr frameworks", "title": "Ask HN: What would you use to deploy an SPA to production on AWS today?", "updated_at": "2024-09-20T13:47:33Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Winipedia"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "pyrig \u2013 <em>Production</em>-ready Python project infrastructure in three commands<p>I built pyrig to stop spending hours setting up the same project infrastructure repeatedly.<p>uv init\nuv add pyrig\nuv run pyrig init<p>You get: source structure with a Typer CLI, pytest with 90% coverage enforcement, <em>GitHub Actions</em> (CI, release, deploy), MkDocs site, git hooks, Containerfile, and all the config files \u2014 pyproject.toml, .gitignore, branch protection, issue templates, everything for a full Python project.<p>Ships with all of Astral's tools (uv, ruff with all rules enabled, ty), plus pytest-cov, bandit, pip-audit, rumdl, prek, MkDocs Material, and Podman. Everything is pre-configured and wired into CI/CD and git hooks from the start.<p>The interesting part is what happens after scaffolding.<p>pyrig isn't a one-shot template generator. Every config is a Python class. Running &quot;pyrig mkroot&quot; regenerates and validates all configs \u2014 merging missing values without removing your customizations. Change your project description in pyproject.toml, rerun, and it propagates to your README and docs. Fully idempotent.<p>pytest enforces project correctness. 11 autouse session fixtures run before your tests: they verify every source module has a corresponding test file (auto-generating skeletons if missing), that no unittest usage exists, that src/ doesn't import from dev/, that there are no namespace packages, and that configs are up to date. You can't get a green test suite with a broken project structure.<p>Zero-boilerplate CLIs. Any public function in subcommands.py becomes a CLI command automatically \u2014 no decorators, no registration:<p>my_project/dev/cli/subcommands.py\ndef greet(name: str) -&gt; None:\n&quot;&quot;&quot;Say hello.&quot;&quot;&quot;\nprint(f&quot;Hello, {name}!&quot;)<p>$ uv run my-project greet --name World\nHello, World!<p>Automatic test generation. Add a new file my_project/src/utils.py, run pytest, and tests/test_my_project/test_src/test_utils.py appears with a NotImplementedError stub so you know what still needs writing. Customizable via subclassing.<p>Config subclassing. Want a custom git hook? Subclass PrekConfigFile, call super(), append your hook. pyrig discovers it automatically \u2014 the leaf class in the dependency chain always wins.<p>Multi-package inheritance. Build a base package on top of pyrig with shared configs, fixtures, and CLI commands. Every downstream project inherits everything:<p>pyrig -&gt; service-base -&gt; auth-service\n-&gt; payment-service\n-&gt; notification-service<p>All three services get the same standards, hooks, and CI/CD \u2014 defined once in service-base.<p>Everything is adjustable. Every tool and config can be customized or replaced through subclassing. Tools like ruff, ty, and pytest are wrapped in Tool classes \u2014 subclass one and pyrig uses yours instead. Want black instead of ruff? Subclass it. Config files work the same way. Standard Python inheritance, no patching.<p>Source: <a href=\"https://github.com/Winipedia/pyrig\" rel=\"nofollow\">https://github.com/Winipedia/pyrig</a>\nDocs: <a href=\"https://winipedia.github.io/pyrig/\" rel=\"nofollow\">https://winipedia.github.io/pyrig/</a>\nPyPI: <a href=\"https://pypi.org/project/pyrig/\" rel=\"nofollow\">https://pypi.org/project/pyrig/</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Pyrig \u2013 One command to set up a <em>production</em>-ready Python project"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Winipedia/pyrig"}}, "_tags": ["story", "author_Winipedia", "story_46947598", "show_hn"], "author": "Winipedia", "created_at": "2026-02-09T16:55:25Z", "created_at_i": 1770656125, "num_comments": 0, "objectID": "46947598", "points": 2, "story_id": 46947598, "story_text": "pyrig \u2013 Production-ready Python project infrastructure in three commands<p>I built pyrig to stop spending hours setting up the same project infrastructure repeatedly.<p>uv init\nuv add pyrig\nuv run pyrig init<p>You get: source structure with a Typer CLI, pytest with 90% coverage enforcement, GitHub Actions (CI, release, deploy), MkDocs site, git hooks, Containerfile, and all the config files \u2014 pyproject.toml, .gitignore, branch protection, issue templates, everything for a full Python project.<p>Ships with all of Astral&#x27;s tools (uv, ruff with all rules enabled, ty), plus pytest-cov, bandit, pip-audit, rumdl, prek, MkDocs Material, and Podman. Everything is pre-configured and wired into CI&#x2F;CD and git hooks from the start.<p>The interesting part is what happens after scaffolding.<p>pyrig isn&#x27;t a one-shot template generator. Every config is a Python class. Running &quot;pyrig mkroot&quot; regenerates and validates all configs \u2014 merging missing values without removing your customizations. Change your project description in pyproject.toml, rerun, and it propagates to your README and docs. Fully idempotent.<p>pytest enforces project correctness. 11 autouse session fixtures run before your tests: they verify every source module has a corresponding test file (auto-generating skeletons if missing), that no unittest usage exists, that src&#x2F; doesn&#x27;t import from dev&#x2F;, that there are no namespace packages, and that configs are up to date. You can&#x27;t get a green test suite with a broken project structure.<p>Zero-boilerplate CLIs. Any public function in subcommands.py becomes a CLI command automatically \u2014 no decorators, no registration:<p>my_project&#x2F;dev&#x2F;cli&#x2F;subcommands.py\ndef greet(name: str) -&gt; None:\n&quot;&quot;&quot;Say hello.&quot;&quot;&quot;\nprint(f&quot;Hello, {name}!&quot;)<p>$ uv run my-project greet --name World\nHello, World!<p>Automatic test generation. Add a new file my_project&#x2F;src&#x2F;utils.py, run pytest, and tests&#x2F;test_my_project&#x2F;test_src&#x2F;test_utils.py appears with a NotImplementedError stub so you know what still needs writing. Customizable via subclassing.<p>Config subclassing. Want a custom git hook? Subclass PrekConfigFile, call super(), append your hook. pyrig discovers it automatically \u2014 the leaf class in the dependency chain always wins.<p>Multi-package inheritance. Build a base package on top of pyrig with shared configs, fixtures, and CLI commands. Every downstream project inherits everything:<p>pyrig -&gt; service-base -&gt; auth-service\n-&gt; payment-service\n-&gt; notification-service<p>All three services get the same standards, hooks, and CI&#x2F;CD \u2014 defined once in service-base.<p>Everything is adjustable. Every tool and config can be customized or replaced through subclassing. Tools like ruff, ty, and pytest are wrapped in Tool classes \u2014 subclass one and pyrig uses yours instead. Want black instead of ruff? Subclass it. Config files work the same way. Standard Python inheritance, no patching.<p>Source: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Winipedia&#x2F;pyrig\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Winipedia&#x2F;pyrig</a>\nDocs: <a href=\"https:&#x2F;&#x2F;winipedia.github.io&#x2F;pyrig&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;winipedia.github.io&#x2F;pyrig&#x2F;</a>\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;pyrig&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;pyrig&#x2F;</a>", "title": "Show HN: Pyrig \u2013 One command to set up a production-ready Python project", "updated_at": "2026-02-09T17:29:49Z", "url": "https://github.com/Winipedia/pyrig"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "muyenlee"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "I built a <em>production</em>-ready full-stack template optimized for Claude Code and AI-assisted development.<p>The problem: You can vibe code features in hours. But shipping to <em>production</em> takes weeks of CI/CD, security scanning, infrastructure, testing, and deployment pipelines.<p>This template gives you the entire <em>production</em> stack from day one:\n- Backend: Go + Echo\n- Frontend: Next.js, Swift/iOS, Kotlin/Android\n- Infra: Google Cloud Run + Pulumi IaC\n- CI/CD: <em>GitHub Actions</em> with security scanning\n- Database: Firebase (Auth, Firestore, Storage)<p>AI-native features include Claude Code integration with MCP servers, pre-built AI commands, and quality gate hooks.<p>MIT licensed. Feedback welcome!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Vibe to Prod \u2013 <em>Production</em>-ready template for AI-assisted development"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/muyen/vibe-to-prod"}}, "_tags": ["story", "author_muyenlee", "story_46481023", "show_hn"], "author": "muyenlee", "created_at": "2026-01-03T20:10:48Z", "created_at_i": 1767471048, "num_comments": 0, "objectID": "46481023", "points": 2, "story_id": 46481023, "story_text": "I built a production-ready full-stack template optimized for Claude Code and AI-assisted development.<p>The problem: You can vibe code features in hours. But shipping to production takes weeks of CI&#x2F;CD, security scanning, infrastructure, testing, and deployment pipelines.<p>This template gives you the entire production stack from day one:\n- Backend: Go + Echo\n- Frontend: Next.js, Swift&#x2F;iOS, Kotlin&#x2F;Android\n- Infra: Google Cloud Run + Pulumi IaC\n- CI&#x2F;CD: GitHub Actions with security scanning\n- Database: Firebase (Auth, Firestore, Storage)<p>AI-native features include Claude Code integration with MCP servers, pre-built AI commands, and quality gate hooks.<p>MIT licensed. Feedback welcome!", "title": "Show HN: Vibe to Prod \u2013 Production-ready template for AI-assisted development", "updated_at": "2026-01-04T05:02:17Z", "url": "https://github.com/muyen/vibe-to-prod"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Parbhat-Kapila"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "I'm an Engineer with a track record of turning ideas into <em>production</em>-ready tools. Over the past year, I've architected and launched multiple developer tools from concept to <em>production</em>, focusing on solving real pain points in the development workflow.<p>Recent projects/products I've built:<p>- Vector Mail: An enterprise-grade, privacy-first email system engineered for speed, scalability, and security.<p>- Repo Docs: A developer-first platform that transforms codebases into living, auto-updating documentation.<p>- Visura: A high-performance data visualization suite delivering instant, actionable insights.<p>Tech stack:<p>- Frontend: React, Next.js, TypeScript, Tailwind CSS<p>- Backend: Node.js, Python, PostgreSQL, MongoDB, Redis<p>- DevOps: Docker, AWS/Vercel, <em>GitHub Actions</em>, CI/CD<p>- Currently exploring: PyTorch, LangChain, and building RAG applications<p>What I bring:<p>- Self-directed learner who can own features end-to-end<p>- Experience across the entire product lifecycle from ideation to deployment<p>- Strong focus on developer experience and clean, maintainable code<p>- Remote-first mindset with excellent async communication skills<p>Seeking a remote full-stack role where I can contribute to products that developers love. Excited about startups where I can wear multiple hats and grow with the team.<p>Email: parbhatkapila4@gmail.com<p>Portfolio: www.parbhat.dev"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: Engineer Who Ships \u2013 From Idea \u2192 <em>Production</em> (Remote Full-Stack Roles)"}}, "_tags": ["story", "author_Parbhat-Kapila", "story_45369810", "ask_hn"], "author": "Parbhat-Kapila", "created_at": "2025-09-25T06:23:52Z", "created_at_i": 1758781432, "num_comments": 0, "objectID": "45369810", "points": 2, "story_id": 45369810, "story_text": "I&#x27;m an Engineer with a track record of turning ideas into production-ready tools. Over the past year, I&#x27;ve architected and launched multiple developer tools from concept to production, focusing on solving real pain points in the development workflow.<p>Recent projects&#x2F;products I&#x27;ve built:<p>- Vector Mail: An enterprise-grade, privacy-first email system engineered for speed, scalability, and security.<p>- Repo Docs: A developer-first platform that transforms codebases into living, auto-updating documentation.<p>- Visura: A high-performance data visualization suite delivering instant, actionable insights.<p>Tech stack:<p>- Frontend: React, Next.js, TypeScript, Tailwind CSS<p>- Backend: Node.js, Python, PostgreSQL, MongoDB, Redis<p>- DevOps: Docker, AWS&#x2F;Vercel, GitHub Actions, CI&#x2F;CD<p>- Currently exploring: PyTorch, LangChain, and building RAG applications<p>What I bring:<p>- Self-directed learner who can own features end-to-end<p>- Experience across the entire product lifecycle from ideation to deployment<p>- Strong focus on developer experience and clean, maintainable code<p>- Remote-first mindset with excellent async communication skills<p>Seeking a remote full-stack role where I can contribute to products that developers love. Excited about startups where I can wear multiple hats and grow with the team.<p>Email: parbhatkapila4@gmail.com<p>Portfolio: www.parbhat.dev", "title": "Ask HN: Engineer Who Ships \u2013 From Idea \u2192 Production (Remote Full-Stack Roles)", "updated_at": "2025-09-25T06:28:18Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "geminimir"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "We kept breaking <em>production</em> with small prompt edits \u2014 suddenly outputs weren\u2019t valid JSON, fields disappeared, or formats changed silently.\nSo we built Promptproof, a <em>GitHub Action</em> that runs in CI and blocks PRs when prompts produce invalid outputs.<p>Features:<p>- Validates JSON output<p>- Enforces required keys &amp; schemas<p>- Runs fast in CI (no external infra)<p>- Works with OpenAI, Anthropic, and local models<p>- Adds PR comments so reviewers see failures immediately<p>We\u2019d love feedback: which rules or integrations would make this most useful for you?<p>Repo: <a href=\"https://github.com/geminimir/promptproof-action\" rel=\"nofollow\">https://github.com/geminimir/promptproof-action</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["github", "actions"], "value": "Show HN: Promptproof \u2013 <em>GitHub Action</em> to test LLM prompts, catch bad JSON schemas"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/geminimir/promptproof-action"}}, "_tags": ["story", "author_geminimir", "story_45105844", "show_hn"], "author": "geminimir", "created_at": "2025-09-02T17:03:25Z", "created_at_i": 1756832605, "num_comments": 0, "objectID": "45105844", "points": 2, "story_id": 45105844, "story_text": "We kept breaking production with small prompt edits \u2014 suddenly outputs weren\u2019t valid JSON, fields disappeared, or formats changed silently.\nSo we built Promptproof, a GitHub Action that runs in CI and blocks PRs when prompts produce invalid outputs.<p>Features:<p>- Validates JSON output<p>- Enforces required keys &amp; schemas<p>- Runs fast in CI (no external infra)<p>- Works with OpenAI, Anthropic, and local models<p>- Adds PR comments so reviewers see failures immediately<p>We\u2019d love feedback: which rules or integrations would make this most useful for you?<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;geminimir&#x2F;promptproof-action\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;geminimir&#x2F;promptproof-action</a>", "title": "Show HN: Promptproof \u2013 GitHub Action to test LLM prompts, catch bad JSON schemas", "updated_at": "2025-09-03T06:49:14Z", "url": "https://github.com/geminimir/promptproof-action"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Parbhat-Kapila"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "I'm an Engineer with a track record of turning ideas into <em>production</em>-ready tools. Over the past year, I've architected and launched multiple developer tools from concept to <em>production</em>, focusing on solving real pain points in the development workflow.\nRecent projects/products I've built:<p>- Vector Mail: An enterprise-grade, privacy-first email system engineered for speed, scalability, and security.<p>- Repo Docs: A developer-first platform that transforms codebases into living, auto-updating documentation.<p>- Visura: A high-performance data visualization suite delivering instant, actionable insights.<p>Tech stack:<p>- Frontend: React, Next.js, TypeScript, Tailwind CSS<p>- Backend: Node.js, Python, PostgreSQL, MongoDB, Redis<p>- DevOps: Docker, AWS/Vercel, <em>GitHub Actions</em>, CI/CD<p>- Currently exploring: PyTorch, LangChain, and building RAG applications<p>What I bring:<p>- Self-directed learner who can own features end-to-end<p>- Experience across the entire product lifecycle from ideation to deployment<p>- Strong focus on developer experience and clean, maintainable code<p>- Remote-first mindset with excellent async communication skills<p>Seeking a remote full-stack role where I can contribute to products that developers love. Excited about startups where I can wear multiple hats and grow with the team.<p>Email: parbhatkapila4@gmail.com<p>Portfolio: www.parbhat.dev"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN-(Remote Full-Stack Roles)Engineer Who Ships \u2013 From Idea \u2192 <em>Production</em>"}}, "_tags": ["story", "author_Parbhat-Kapila", "story_45381883", "ask_hn"], "author": "Parbhat-Kapila", "children": [45392796], "created_at": "2025-09-26T02:19:15Z", "created_at_i": 1758853155, "num_comments": 1, "objectID": "45381883", "points": 1, "story_id": 45381883, "story_text": "I&#x27;m an Engineer with a track record of turning ideas into production-ready tools. Over the past year, I&#x27;ve architected and launched multiple developer tools from concept to production, focusing on solving real pain points in the development workflow.\nRecent projects&#x2F;products I&#x27;ve built:<p>- Vector Mail: An enterprise-grade, privacy-first email system engineered for speed, scalability, and security.<p>- Repo Docs: A developer-first platform that transforms codebases into living, auto-updating documentation.<p>- Visura: A high-performance data visualization suite delivering instant, actionable insights.<p>Tech stack:<p>- Frontend: React, Next.js, TypeScript, Tailwind CSS<p>- Backend: Node.js, Python, PostgreSQL, MongoDB, Redis<p>- DevOps: Docker, AWS&#x2F;Vercel, GitHub Actions, CI&#x2F;CD<p>- Currently exploring: PyTorch, LangChain, and building RAG applications<p>What I bring:<p>- Self-directed learner who can own features end-to-end<p>- Experience across the entire product lifecycle from ideation to deployment<p>- Strong focus on developer experience and clean, maintainable code<p>- Remote-first mindset with excellent async communication skills<p>Seeking a remote full-stack role where I can contribute to products that developers love. Excited about startups where I can wear multiple hats and grow with the team.<p>Email: parbhatkapila4@gmail.com<p>Portfolio: www.parbhat.dev", "title": "Ask HN-(Remote Full-Stack Roles)Engineer Who Ships \u2013 From Idea \u2192 Production", "updated_at": "2025-09-27T02:17:55Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bakkerinho"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["github", "actions", "production"], "value": "I built a <em>GitHub Action</em> that solves a common pain point in CI/CD pipelines with Vercel deployments. Instead of relying on GitHub's deployment_status events, it actively polls Vercel's API to ensure deployments are truly ready before running tests or other checks.<p>Key technical features:\n- Direct integration with Vercel's API for real-time deployment status\n- Configurable polling with timeout and interval settings\n- Support for team projects and branch aliases\n- Works with both preview and <em>production</em> deployments\n- No special webhook configuration needed<p>The main advantage is reliability: Many CI pipelines fail intermittently because they start running tests before Vercel deployments are actually ready. Traditional solutions rely on GitHub's deployment_status events, which can be unreliable or require complex setup.<p>This action solves that by:\n1. Retrieving the latest deployment for your project/branch\n2. Extracting the preview/<em>production</em> URL\n3. Actively polling until the deployment is genuinely ready<p>Example use cases:\n- Running E2E tests against preview deployments\n- Multi-app testing workflows\n- Post-<em>production</em> deployment validation\n- Accessibility testing on live environments<p>I'd love feedback from the HN community, especially from those dealing with similar CI/CD challenges."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["github", "actions"], "value": "Show HN: I Built a <em>GitHub Action</em> to Wait for Vercel Deployments Before CI"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/marketplace/actions/vercel-preview-url-with-status-polling"}}, "_tags": ["story", "author_bakkerinho", "story_44728362", "show_hn"], "author": "bakkerinho", "children": [44730270], "created_at": "2025-07-29T21:17:58Z", "created_at_i": 1753823878, "num_comments": 1, "objectID": "44728362", "points": 1, "story_id": 44728362, "story_text": "I built a GitHub Action that solves a common pain point in CI&#x2F;CD pipelines with Vercel deployments. Instead of relying on GitHub&#x27;s deployment_status events, it actively polls Vercel&#x27;s API to ensure deployments are truly ready before running tests or other checks.<p>Key technical features:\n- Direct integration with Vercel&#x27;s API for real-time deployment status\n- Configurable polling with timeout and interval settings\n- Support for team projects and branch aliases\n- Works with both preview and production deployments\n- No special webhook configuration needed<p>The main advantage is reliability: Many CI pipelines fail intermittently because they start running tests before Vercel deployments are actually ready. Traditional solutions rely on GitHub&#x27;s deployment_status events, which can be unreliable or require complex setup.<p>This action solves that by:\n1. Retrieving the latest deployment for your project&#x2F;branch\n2. Extracting the preview&#x2F;production URL\n3. Actively polling until the deployment is genuinely ready<p>Example use cases:\n- Running E2E tests against preview deployments\n- Multi-app testing workflows\n- Post-production deployment validation\n- Accessibility testing on live environments<p>I&#x27;d love feedback from the HN community, especially from those dealing with similar CI&#x2F;CD challenges.", "title": "Show HN: I Built a GitHub Action to Wait for Vercel Deployments Before CI", "updated_at": "2025-07-30T01:53:53Z", "url": "https://github.com/marketplace/actions/vercel-preview-url-with-status-polling"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mlgraham"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Hey HN,<p>I built CodeRocket Deploy because I was tired of the CI/CD setup ritual every developer knows: search for a workflow template, copy YAML, tweak it, watch it fail, debug, repeat.<p>What it does:\n- Install as a GitHub App\n- Analyzes your repo (package.json, requirements.txt, config files, etc.)\n- Detects language, framework, and deploy target automatically\n- Uses Claude to generate a workflow optimized for your specific stack\n- Creates a PR so you can review before merging<p>Technical details:\n- Deep static analysis using GitHub's Contents API\n- 20+ framework detectors (Next.js, Django, FastAPI, Rails, Spring Boot, etc.)\n- Few-shot prompting with <em>production</em>-ready workflow examples\n- Fallback templates for edge cases\n- YAML validation with security scanning (no hardcoded secrets)<p>Stack: Django + DRF backend, React + TypeScript frontend, PostgreSQL, Redis for Celery tasks.<p>Free tier: 100 generations/month, 3 repos. No credit card.<p>GitHub Marketplace: <a href=\"https://github.com/marketplace/coderocket-deploy\" rel=\"nofollow\">https://github.com/marketplace/coderocket-deploy</a><p>Would love feedback, especially:\n- What frameworks are we missing?\n- What would make this useful for your workflow?\n- Any concerns about AI-generated CI/CD configs?<p>Happy to answer questions about the technical implementation."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["github", "actions"], "value": "Show HN: CodeRocket Deploy \u2013 AI generates <em>GitHub Actions</em> workflows in 60 seconds"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://deploy.coderocket.com"}}, "_tags": ["story", "author_mlgraham", "story_47129214", "show_hn"], "author": "mlgraham", "created_at": "2026-02-23T21:35:08Z", "created_at_i": 1771882508, "num_comments": 0, "objectID": "47129214", "points": 1, "story_id": 47129214, "story_text": "Hey HN,<p>I built CodeRocket Deploy because I was tired of the CI&#x2F;CD setup ritual every developer knows: search for a workflow template, copy YAML, tweak it, watch it fail, debug, repeat.<p>What it does:\n- Install as a GitHub App\n- Analyzes your repo (package.json, requirements.txt, config files, etc.)\n- Detects language, framework, and deploy target automatically\n- Uses Claude to generate a workflow optimized for your specific stack\n- Creates a PR so you can review before merging<p>Technical details:\n- Deep static analysis using GitHub&#x27;s Contents API\n- 20+ framework detectors (Next.js, Django, FastAPI, Rails, Spring Boot, etc.)\n- Few-shot prompting with production-ready workflow examples\n- Fallback templates for edge cases\n- YAML validation with security scanning (no hardcoded secrets)<p>Stack: Django + DRF backend, React + TypeScript frontend, PostgreSQL, Redis for Celery tasks.<p>Free tier: 100 generations&#x2F;month, 3 repos. No credit card.<p>GitHub Marketplace: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;marketplace&#x2F;coderocket-deploy\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;marketplace&#x2F;coderocket-deploy</a><p>Would love feedback, especially:\n- What frameworks are we missing?\n- What would make this useful for your workflow?\n- Any concerns about AI-generated CI&#x2F;CD configs?<p>Happy to answer questions about the technical implementation.", "title": "Show HN: CodeRocket Deploy \u2013 AI generates GitHub Actions workflows in 60 seconds", "updated_at": "2026-02-23T21:36:41Z", "url": "https://deploy.coderocket.com"}], "hitsPerPage": 15, "nbHits": 64, "nbPages": 5, "page": 0, "params": "query=github-actions+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 19, "processingTimingsMS": {"_request": {"roundTrip": 17}, "afterFetch": {"format": {"highlighting": 2, "total": 2}}, "fetch": {"query": 11, "scanning": 6, "total": 18}, "total": 19}, "query": "github-actions production", "serverTimeMS": 22}}