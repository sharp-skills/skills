{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ricardodevelop"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "Hello everyone,<p>I\u2019ve created this starter project for creating <em>production</em> ready web apps in Vite and React that I hope some might find useful.<p>This template came about as a necessity to provide some standardization across new projects at work. A few of the initial goals when creating this project were to:<p>- Reduce setup time<p>- Standardize codebase with ESLint and Prettier<p>- Improve commit messages with tools like husky, commitizen and commitlint<p>- Improve codebase maintainability and scalability by providing a reasonable folder structure<p>- Simplify React Component development through use of tools like Storybook<p>- Improving codebase stability with unit and E2E tests via Vitest + React Testing Library and Playwright respectively<p>- Ease the deployment process by providing a simple starter Dockerfile<p>In addition to all the aforementioned goals, I also wanted to use modern tools such as React Query + Zustand for state management, React Hook Form + <em>Zod</em> for creating and validating forms, Tailwind CSS for building out UI\u2019s, etc.<p>I tried to cover everything I, and others, might need but recognize that everyones requirements are different. Luckily, this isn\u2019t a framework so removing unneeded packages or adding new ones is as simple it would normally be. The project itself doesn\u2019t come with a demo as its purpose is to simply provide a foundation for any new projects you might have in mind.<p>Feedback is always welcome and I appreciate anyone willing to checkout this project.<p>Thank you and have a great day."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Vite React Boilerplate \u2013 A <em>Production</em> Ready, Scalable Starter Template"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/RicardoValdovinos/vite-react-boilerplate"}}, "_tags": ["story", "author_ricardodevelop", "story_36926095", "show_hn"], "author": "ricardodevelop", "children": [36927690, 36928003], "created_at": "2023-07-29T23:49:15Z", "created_at_i": 1690674555, "num_comments": 5, "objectID": "36926095", "points": 18, "story_id": 36926095, "story_text": "Hello everyone,<p>I\u2019ve created this starter project for creating production ready web apps in Vite and React that I hope some might find useful.<p>This template came about as a necessity to provide some standardization across new projects at work. A few of the initial goals when creating this project were to:<p>- Reduce setup time<p>- Standardize codebase with ESLint and Prettier<p>- Improve commit messages with tools like husky, commitizen and commitlint<p>- Improve codebase maintainability and scalability by providing a reasonable folder structure<p>- Simplify React Component development through use of tools like Storybook<p>- Improving codebase stability with unit and E2E tests via Vitest + React Testing Library and Playwright respectively<p>- Ease the deployment process by providing a simple starter Dockerfile<p>In addition to all the aforementioned goals, I also wanted to use modern tools such as React Query + Zustand for state management, React Hook Form + Zod for creating and validating forms, Tailwind CSS for building out UI\u2019s, etc.<p>I tried to cover everything I, and others, might need but recognize that everyones requirements are different. Luckily, this isn\u2019t a framework so removing unneeded packages or adding new ones is as simple it would normally be. The project itself doesn\u2019t come with a demo as its purpose is to simply provide a foundation for any new projects you might have in mind.<p>Feedback is always welcome and I appreciate anyone willing to checkout this project.<p>Thank you and have a great day.", "title": "Show HN: Vite React Boilerplate \u2013 A Production Ready, Scalable Starter Template", "updated_at": "2024-09-20T14:45:12Z", "url": "https://github.com/RicardoValdovinos/vite-react-boilerplate"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "daynablackwell"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "Hi HN,<p>I kept shipping features where the frontend accepted data the backend rejected. Validation rules lived in two places (Go struct tags and TypeScript <em>Zod</em> schemas), and they'd drift.<p>I built goldenthread to solve this. It's a build-time compiler that generates TypeScript <em>Zod</em> schemas from Go struct tags. Write validation once, use it\neverywhere.<p>Example:<p><pre><code>    // Go backend\n    type User struct {\n        Username string `json:&quot;username&quot; gt:&quot;required,len:3..20&quot;`\n        Email    string `json:&quot;email&quot; gt:&quot;email&quot;`\n        Age      int    `json:&quot;age&quot; gt:&quot;min:13,max:130&quot;`\n    }\n</code></pre>\nRun `goldenthread generate ./models` and you get:<p><pre><code>    // TypeScript (auto-generated)\n    export const UserSchema = z.object({\n      username: z.string().min(3).max(20),\n      email: z.string().email(),\n      age: z.number().int().min(13).max(130)\n    })\n\n    export type User = z.infer&lt;typeof UserSchema&gt;\n</code></pre>\nThe compiler uses go/packages and go/types for accurate type resolution. It handles nested objects, enums, maps, embedded structs, and 37+ validation rules.<p>The best feature: drift detection. Run `goldenthread check` in CI - it computes SHA-256 hashes of your schemas and fails the build if generated TypeScript doesn't match Go source. No more &quot;frontend works locally but breaks in <em>production</em> because someone changed the backend struct.&quot;<p>Before releasing v0.1.0, I ran continuous fuzzing (10 targets, hourly in GitHub Actions). It found two bugs my test suite missed:<p><pre><code>  1. UTF-8 corruption: Japanese field names with empty JSON tags triggered byte-slicing bugs in camelCase conversion. Took 444,553 executions to discover.\n\n  2. Regex escaping: Patterns with newline characters produced broken JavaScript output. Found in 180 executions.\n</code></pre>\nBoth required specific intersections of conditions that manual testing wouldn't cover. I wrote up the full fuzzing setup here: <a href=\"https://blackwell-systems.github.io/blog/posts/continuous-fuzzing-go/\" rel=\"nofollow\">https://blackwell-systems.github.io/blog/posts/continuous-fu...</a><p>The tool is <em>production</em>-ready:<p>Zero runtime dependencies (generated code only imports <em>Zod</em>)\nGenerates readable TypeScript (looks hand-written)\nComplete Go type support (primitives, arrays, maps, nested objects)\nWorks with any Go project structure\nMIT OR Apache 2.0 dual licensed<p>I built this because I was tired of frontend/backend validation bugs in my day job (hospitality platform with booking/payment APIs). We use it internally now.<p>Real-world example in the repo: 9-schema e-commerce system with Customer records (E.164 phone validation), Product SKUs (regex patterns), and Order workflows (nested objects + array constraints).<p>GitHub: <a href=\"https://github.com/blackwell-systems/goldenthread\" rel=\"nofollow\">https://github.com/blackwell-systems/goldenthread</a>\nDocs: <a href=\"https://github.com/blackwell-systems/goldenthread/blob/main/README.md\" rel=\"nofollow\">https://github.com/blackwell-systems/goldenthread/blob/main/...</a>\nTag syntax: <a href=\"https://github.com/blackwell-systems/goldenthread/blob/main/docs/TAG_SPEC.md\" rel=\"nofollow\">https://github.com/blackwell-systems/goldenthread/blob/main/...</a><p>Would love feedback<p>Thanks for looking!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["zod"], "value": "Show HN: Goldenthread \u2013 Compile Go to TypeScript <em>Zod</em> for type-safe validation"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/blackwell-systems/goldenthread"}}, "_tags": ["story", "author_daynablackwell", "story_46781766", "show_hn"], "author": "daynablackwell", "created_at": "2026-01-27T16:03:11Z", "created_at_i": 1769529791, "num_comments": 0, "objectID": "46781766", "points": 1, "story_id": 46781766, "story_text": "Hi HN,<p>I kept shipping features where the frontend accepted data the backend rejected. Validation rules lived in two places (Go struct tags and TypeScript Zod schemas), and they&#x27;d drift.<p>I built goldenthread to solve this. It&#x27;s a build-time compiler that generates TypeScript Zod schemas from Go struct tags. Write validation once, use it\neverywhere.<p>Example:<p><pre><code>    &#x2F;&#x2F; Go backend\n    type User struct {\n        Username string `json:&quot;username&quot; gt:&quot;required,len:3..20&quot;`\n        Email    string `json:&quot;email&quot; gt:&quot;email&quot;`\n        Age      int    `json:&quot;age&quot; gt:&quot;min:13,max:130&quot;`\n    }\n</code></pre>\nRun `goldenthread generate .&#x2F;models` and you get:<p><pre><code>    &#x2F;&#x2F; TypeScript (auto-generated)\n    export const UserSchema = z.object({\n      username: z.string().min(3).max(20),\n      email: z.string().email(),\n      age: z.number().int().min(13).max(130)\n    })\n\n    export type User = z.infer&lt;typeof UserSchema&gt;\n</code></pre>\nThe compiler uses go&#x2F;packages and go&#x2F;types for accurate type resolution. It handles nested objects, enums, maps, embedded structs, and 37+ validation rules.<p>The best feature: drift detection. Run `goldenthread check` in CI - it computes SHA-256 hashes of your schemas and fails the build if generated TypeScript doesn&#x27;t match Go source. No more &quot;frontend works locally but breaks in production because someone changed the backend struct.&quot;<p>Before releasing v0.1.0, I ran continuous fuzzing (10 targets, hourly in GitHub Actions). It found two bugs my test suite missed:<p><pre><code>  1. UTF-8 corruption: Japanese field names with empty JSON tags triggered byte-slicing bugs in camelCase conversion. Took 444,553 executions to discover.\n\n  2. Regex escaping: Patterns with newline characters produced broken JavaScript output. Found in 180 executions.\n</code></pre>\nBoth required specific intersections of conditions that manual testing wouldn&#x27;t cover. I wrote up the full fuzzing setup here: <a href=\"https:&#x2F;&#x2F;blackwell-systems.github.io&#x2F;blog&#x2F;posts&#x2F;continuous-fuzzing-go&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;blackwell-systems.github.io&#x2F;blog&#x2F;posts&#x2F;continuous-fu...</a><p>The tool is production-ready:<p>Zero runtime dependencies (generated code only imports Zod)\nGenerates readable TypeScript (looks hand-written)\nComplete Go type support (primitives, arrays, maps, nested objects)\nWorks with any Go project structure\nMIT OR Apache 2.0 dual licensed<p>I built this because I was tired of frontend&#x2F;backend validation bugs in my day job (hospitality platform with booking&#x2F;payment APIs). We use it internally now.<p>Real-world example in the repo: 9-schema e-commerce system with Customer records (E.164 phone validation), Product SKUs (regex patterns), and Order workflows (nested objects + array constraints).<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;blackwell-systems&#x2F;goldenthread\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;blackwell-systems&#x2F;goldenthread</a>\nDocs: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;blackwell-systems&#x2F;goldenthread&#x2F;blob&#x2F;main&#x2F;README.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;blackwell-systems&#x2F;goldenthread&#x2F;blob&#x2F;main&#x2F;...</a>\nTag syntax: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;blackwell-systems&#x2F;goldenthread&#x2F;blob&#x2F;main&#x2F;docs&#x2F;TAG_SPEC.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;blackwell-systems&#x2F;goldenthread&#x2F;blob&#x2F;main&#x2F;...</a><p>Would love feedback<p>Thanks for looking!", "title": "Show HN: Goldenthread \u2013 Compile Go to TypeScript Zod for type-safe validation", "updated_at": "2026-01-27T16:19:13Z", "url": "https://github.com/blackwell-systems/goldenthread"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "anerli"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "Hey HN, Anders and Tom here. We had a post about our AI test automation framework 2 months ago that got a decent amount of traction (<a href=\"https://news.ycombinator.com/item?id=43796003\">https://news.ycombinator.com/item?id=43796003</a>).<p>We got some great feedback from the community, with the most positive response being about our vision-first approach used in our browser agent. However, many wanted to use the underlying agent outside the testing domain. So today, we're releasing our fully featured AI browser automation framework.<p>You can use it to automate tasks on the web, integrate between apps without APIs, extract data, test your web apps, or as a building block for your own browser agents.<p>Traditionally, browser automation could only be done via the DOM, even though that\u2019s not how humans use browsers. Most browser agents are still stuck in this paradigm. With a vision-first approach, we avoid relying on flaky DOM navigation and perform better on complex interactions found in a broad variety of sites, for example:<p>- Drag and drop interactions<p>- Data visualizations, charts, and tables<p>- Legacy apps with nested iframes<p>- Canvas and webGL-heavy sites (like design tools or photo editing)<p>- Remote desktops streamed into the browser<p>To interact accurately with the browser, we use visually grounded models to execute precise actions based on pixel coordinates. The model used by Magnitude must be smart enough to plan out actions but also able to execute them. Not many models are both smart *and* visually grounded. We highly recommend Claude Sonnet 4 for the best performance, but if you prefer open source, we also support Qwen-2.5-VL 72B.<p>Most browser agents never make it to <em>production</em>. This is because of (1) the flaky DOM navigation mentioned above, but (2) the lack of control most browser agents offer. The dominant paradigm is you give the agent a high-level task + tools and hope for the best. This quickly falls apart for <em>production</em> automations that need to be reliable and specific. With Magnitude, you have fine-grained control over the agent with our `act()` and `extract()` syntax, and can mix it with your own code as needed. You also have full control of the prompts at both the action and agent level.<p>```ts<p>// Magnitude can handle high-level tasks<p>await agent.act('Create an issue', {<p><pre><code>  // Optionally pass data that the agent will use where appropriate\n\n  data: {\n\n    title: 'Use Magnitude',\n\n    description: 'Run &quot;npx create-magnitude-app&quot; and follow the instructions',\n\n  },\n</code></pre>\n});<p>// It can also handle low-level actions<p>await agent.act('Drag &quot;Use Magnitude&quot; to the top of the in progress column');<p>// Intelligently extract data based on the DOM content matching a provided <em>zod</em> schema<p>const tasks = await agent.extract(<p><pre><code>    'List in progress issues',\n\n    z.array(z.object({\n\n        title: z.string(),\n\n        description: z.string(),\n\n        // Agent can extract existing data or new insights\n\n        difficulty: z.number().describe('Rate the difficulty between 1-5')\n\n    })),\n</code></pre>\n);<p>```<p>We have a setup script that makes it trivial to get started with an example, just run &quot;npx create-magnitude-app&quot;. We\u2019d love to hear what you think!<p>Repo: <a href=\"https://github.com/magnitudedev/magnitude\">https://github.com/magnitudedev/magnitude</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Magnitude \u2013 Open-source AI browser automation framework"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/magnitudedev/magnitude"}}, "_tags": ["story", "author_anerli", "story_44390005", "show_hn"], "author": "anerli", "children": [44390344, 44390403, 44390548, 44391985, 44392057, 44392725, 44393178, 44393183, 44394532, 44394718, 44395459], "created_at": "2025-06-26T18:30:56Z", "created_at_i": 1750962656, "num_comments": 40, "objectID": "44390005", "points": 145, "story_id": 44390005, "story_text": "Hey HN, Anders and Tom here. We had a post about our AI test automation framework 2 months ago that got a decent amount of traction (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43796003\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43796003</a>).<p>We got some great feedback from the community, with the most positive response being about our vision-first approach used in our browser agent. However, many wanted to use the underlying agent outside the testing domain. So today, we&#x27;re releasing our fully featured AI browser automation framework.<p>You can use it to automate tasks on the web, integrate between apps without APIs, extract data, test your web apps, or as a building block for your own browser agents.<p>Traditionally, browser automation could only be done via the DOM, even though that\u2019s not how humans use browsers. Most browser agents are still stuck in this paradigm. With a vision-first approach, we avoid relying on flaky DOM navigation and perform better on complex interactions found in a broad variety of sites, for example:<p>- Drag and drop interactions<p>- Data visualizations, charts, and tables<p>- Legacy apps with nested iframes<p>- Canvas and webGL-heavy sites (like design tools or photo editing)<p>- Remote desktops streamed into the browser<p>To interact accurately with the browser, we use visually grounded models to execute precise actions based on pixel coordinates. The model used by Magnitude must be smart enough to plan out actions but also able to execute them. Not many models are both smart *and* visually grounded. We highly recommend Claude Sonnet 4 for the best performance, but if you prefer open source, we also support Qwen-2.5-VL 72B.<p>Most browser agents never make it to production. This is because of (1) the flaky DOM navigation mentioned above, but (2) the lack of control most browser agents offer. The dominant paradigm is you give the agent a high-level task + tools and hope for the best. This quickly falls apart for production automations that need to be reliable and specific. With Magnitude, you have fine-grained control over the agent with our `act()` and `extract()` syntax, and can mix it with your own code as needed. You also have full control of the prompts at both the action and agent level.<p>```ts<p>&#x2F;&#x2F; Magnitude can handle high-level tasks<p>await agent.act(&#x27;Create an issue&#x27;, {<p><pre><code>  &#x2F;&#x2F; Optionally pass data that the agent will use where appropriate\n\n  data: {\n\n    title: &#x27;Use Magnitude&#x27;,\n\n    description: &#x27;Run &quot;npx create-magnitude-app&quot; and follow the instructions&#x27;,\n\n  },\n</code></pre>\n});<p>&#x2F;&#x2F; It can also handle low-level actions<p>await agent.act(&#x27;Drag &quot;Use Magnitude&quot; to the top of the in progress column&#x27;);<p>&#x2F;&#x2F; Intelligently extract data based on the DOM content matching a provided zod schema<p>const tasks = await agent.extract(<p><pre><code>    &#x27;List in progress issues&#x27;,\n\n    z.array(z.object({\n\n        title: z.string(),\n\n        description: z.string(),\n\n        &#x2F;&#x2F; Agent can extract existing data or new insights\n\n        difficulty: z.number().describe(&#x27;Rate the difficulty between 1-5&#x27;)\n\n    })),\n</code></pre>\n);<p>```<p>We have a setup script that makes it trivial to get started with an example, just run &quot;npx create-magnitude-app&quot;. We\u2019d love to hear what you think!<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;magnitudedev&#x2F;magnitude\">https:&#x2F;&#x2F;github.com&#x2F;magnitudedev&#x2F;magnitude</a>", "title": "Show HN: Magnitude \u2013 Open-source AI browser automation framework", "updated_at": "2025-12-03T12:31:00Z", "url": "https://github.com/magnitudedev/magnitude"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "itssimon"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "G\u2019day Hacker News, I\u2019m Simon Gurcke, the sole founder of Apitally (<a href=\"https://apitally.io\" rel=\"nofollow\">https://apitally.io</a>).<p>I\u2019m building a simple API monitoring and analytics tool for Python / Node.js apps. It helps users understand API usage and performance, spot issues early and troubleshoot effectively when something goes wrong.<p>Features include:<p>- <i>Dashboards:</i> Provide insights into API traffic, errors, performance and consumers.<p>- <i>Request logging:</i> Opt-in and highly configurable in terms of what data is logged. Users can drill down from aggregated metrics to individual requests (proven to be super helpful when troubleshooting issues).<p>- <i>Custom alerts:</i> Based on 14 different API metrics with notifications delivered via email, Slack or Microsoft Teams.<p>- <i>Validation error tracking:</i> Captures metrics about which fields failed validation and why. Works for web frameworks with built-in validation (e.g. FastAPI with pydantic), or that integrate with popular third-party validation libraries (e.g. <em>Zod</em> for Hono).<p>- <i>Server error tracking:</i> Captures exception details and stack traces for 500 error responses. An integration with the Sentry SDK also captures event IDs, allowing users to click through to the relevant Sentry issue for more context.<p>I first started developing Apitally to scratch my own itch. While working at a health tech company where I was responsible for API-based software products, I became frustrated with the monitoring tools we had in place - Datadog and the ELK stack. They were too complex for my API-centric use cases, and often a pain to use.<p>As a result, I focused on making Apitally as simple as possible. This involved not just refining the UX of the dashboard, but also optimizing the developer experience with the open-source SDKs:<p>- <a href=\"https://github.com/apitally/apitally-py\">https://github.com/apitally/apitally-py</a> - Python SDK (supports FastAPI, Flask, Django, Litestar, Starlette)<p>- <a href=\"https://github.com/apitally/apitally-js\">https://github.com/apitally/apitally-js</a> - Node.js SDK (supports Express, NestJS, Fastify, Koa, Hono)<p>My other focus was on data privacy, as that is a strict requirement in the healthcare industry. By default, Apitally doesn\u2019t capture any sensitive data - metrics are aggregated on the client side (similar to Prometheus) and sent in the background in regular intervals.<p>The hardest part has been implementing integrations for various web frameworks and supporting a wide range of versions. I learned a lot about the inner workings of web frameworks in the process. Good test coverage and an extensive test matrix were really important to not break people\u2019s <em>production</em> APIs with buggy middleware.<p>Apitally\u2019s backend is built in Python and runs on a small Kubernetes cluster on DigitalOcean. It uses PostgreSQL and ClickHouse to store data and NATS JetStream as a message queue. I chose NATS for being lightweight and its exactly-once processing capabilities. I\u2019m also impressed by ClickHouse\u2019s performance given the low hardware specs of my server (4 vCPUs, 8 GB RAM).<p>Apitally is free to use for small hobby projects (with limitations), and I offer two paid tiers for $39 and $119 (USD) per month. The dashboard has a demo mode, allowing people to explore the product without having to set up their own app first.<p>Thank you for reading about my bootstrapped indie product. Please let me know your thoughts and questions in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Apitally \u2013 A simple, privacy-focused API monitoring and analytics tool"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://apitally.io"}}, "_tags": ["story", "author_itssimon", "story_42915435", "show_hn"], "author": "itssimon", "children": [42915477, 42915683, 42929740, 42949349, 42949483, 42959486, 42960715], "created_at": "2025-02-03T06:07:00Z", "created_at_i": 1738562820, "num_comments": 21, "objectID": "42915435", "points": 46, "story_id": 42915435, "story_text": "G\u2019day Hacker News, I\u2019m Simon Gurcke, the sole founder of Apitally (<a href=\"https:&#x2F;&#x2F;apitally.io\" rel=\"nofollow\">https:&#x2F;&#x2F;apitally.io</a>).<p>I\u2019m building a simple API monitoring and analytics tool for Python &#x2F; Node.js apps. It helps users understand API usage and performance, spot issues early and troubleshoot effectively when something goes wrong.<p>Features include:<p>- <i>Dashboards:</i> Provide insights into API traffic, errors, performance and consumers.<p>- <i>Request logging:</i> Opt-in and highly configurable in terms of what data is logged. Users can drill down from aggregated metrics to individual requests (proven to be super helpful when troubleshooting issues).<p>- <i>Custom alerts:</i> Based on 14 different API metrics with notifications delivered via email, Slack or Microsoft Teams.<p>- <i>Validation error tracking:</i> Captures metrics about which fields failed validation and why. Works for web frameworks with built-in validation (e.g. FastAPI with pydantic), or that integrate with popular third-party validation libraries (e.g. Zod for Hono).<p>- <i>Server error tracking:</i> Captures exception details and stack traces for 500 error responses. An integration with the Sentry SDK also captures event IDs, allowing users to click through to the relevant Sentry issue for more context.<p>I first started developing Apitally to scratch my own itch. While working at a health tech company where I was responsible for API-based software products, I became frustrated with the monitoring tools we had in place - Datadog and the ELK stack. They were too complex for my API-centric use cases, and often a pain to use.<p>As a result, I focused on making Apitally as simple as possible. This involved not just refining the UX of the dashboard, but also optimizing the developer experience with the open-source SDKs:<p>- <a href=\"https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-py\">https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-py</a> - Python SDK (supports FastAPI, Flask, Django, Litestar, Starlette)<p>- <a href=\"https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-js\">https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-js</a> - Node.js SDK (supports Express, NestJS, Fastify, Koa, Hono)<p>My other focus was on data privacy, as that is a strict requirement in the healthcare industry. By default, Apitally doesn\u2019t capture any sensitive data - metrics are aggregated on the client side (similar to Prometheus) and sent in the background in regular intervals.<p>The hardest part has been implementing integrations for various web frameworks and supporting a wide range of versions. I learned a lot about the inner workings of web frameworks in the process. Good test coverage and an extensive test matrix were really important to not break people\u2019s production APIs with buggy middleware.<p>Apitally\u2019s backend is built in Python and runs on a small Kubernetes cluster on DigitalOcean. It uses PostgreSQL and ClickHouse to store data and NATS JetStream as a message queue. I chose NATS for being lightweight and its exactly-once processing capabilities. I\u2019m also impressed by ClickHouse\u2019s performance given the low hardware specs of my server (4 vCPUs, 8 GB RAM).<p>Apitally is free to use for small hobby projects (with limitations), and I offer two paid tiers for $39 and $119 (USD) per month. The dashboard has a demo mode, allowing people to explore the product without having to set up their own app first.<p>Thank you for reading about my bootstrapped indie product. Please let me know your thoughts and questions in the comments.", "title": "Show HN: Apitally \u2013 A simple, privacy-focused API monitoring and analytics tool", "updated_at": "2025-11-19T05:50:23Z", "url": "https://apitally.io"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "creativedg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "I started this boilerplate in July 2020 and I\u2019ve been maintaining it for 5 years. It began on Next.js 9 and kept upgrading to Next.js 15+ (App Router), while upgrading the stack over time (Tailwind 1 \u2192 4, ESLint 8, swapping Cypress \u2192 Playwright, etc.). The goal is simple: I kept rebuilding the same setup, so I packaged it and kept it updated.<p>What you get (preconfigured, keep only what you need):<p>- Next.js 15 (App Router) + TypeScript + Tailwind 4<p>- Auth with Clerk (magic links, MFA, social, passkeys)<p>- I18n via next-intl<p>- DB with Drizzle ORM (PGlite locally)<p>- Forms with React Hook Form + <em>Zod</em> validation<p>- Testing: Vitest (unit), Playwright (integration/E2E)<p>- CI with GitHub Actions; Storybook for UI work<p>- SEO (Open Graph, JSON-LD, sitemap, robots)<p>- Observability: Sentry, logging with LogTape, log management &amp; uptime/monitoring<p>- Security: Arcjet (bot detection, rate limiting, shield rules)<p>- DX details: ESLint/Prettier, Lefthook + lint-staged, Commitlint, absolute imports, bundle analyzer<p>- AI code review<p>It\u2019s free and open source (MIT). Today the project sits around 11.8k GitHub stars and 2.2k forks. I\u2019m still actively maintaining it and adding features.<p>Repo: <a href=\"https://github.com/ixartz/Next-js-Boilerplate\" rel=\"nofollow\">https://github.com/ixartz/Next-js-Boilerplate</a><p>Why I built it<p>Spinning up auth, a DB, i18n, tests, and lint/format/CI for each new app was repetitive. This gives me (and hopefully you) a <em>production</em>-ready base in minutes, with opinionated defaults you can start.<p>I\u2019m open to suggestions and feedback, what would you like to see next? I\u2019ll hang around in the comments to answer questions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Open-source Next.js 15 boilerplate \u2013 auth, DB, intl, tests, monitoring"}}, "_tags": ["story", "author_creativedg", "story_45052744", "show_hn"], "author": "creativedg", "created_at": "2025-08-28T14:38:31Z", "created_at_i": 1756391911, "num_comments": 0, "objectID": "45052744", "points": 4, "story_id": 45052744, "story_text": "I started this boilerplate in July 2020 and I\u2019ve been maintaining it for 5 years. It began on Next.js 9 and kept upgrading to Next.js 15+ (App Router), while upgrading the stack over time (Tailwind 1 \u2192 4, ESLint 8, swapping Cypress \u2192 Playwright, etc.). The goal is simple: I kept rebuilding the same setup, so I packaged it and kept it updated.<p>What you get (preconfigured, keep only what you need):<p>- Next.js 15 (App Router) + TypeScript + Tailwind 4<p>- Auth with Clerk (magic links, MFA, social, passkeys)<p>- I18n via next-intl<p>- DB with Drizzle ORM (PGlite locally)<p>- Forms with React Hook Form + Zod validation<p>- Testing: Vitest (unit), Playwright (integration&#x2F;E2E)<p>- CI with GitHub Actions; Storybook for UI work<p>- SEO (Open Graph, JSON-LD, sitemap, robots)<p>- Observability: Sentry, logging with LogTape, log management &amp; uptime&#x2F;monitoring<p>- Security: Arcjet (bot detection, rate limiting, shield rules)<p>- DX details: ESLint&#x2F;Prettier, Lefthook + lint-staged, Commitlint, absolute imports, bundle analyzer<p>- AI code review<p>It\u2019s free and open source (MIT). Today the project sits around 11.8k GitHub stars and 2.2k forks. I\u2019m still actively maintaining it and adding features.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ixartz&#x2F;Next-js-Boilerplate\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ixartz&#x2F;Next-js-Boilerplate</a><p>Why I built it<p>Spinning up auth, a DB, i18n, tests, and lint&#x2F;format&#x2F;CI for each new app was repetitive. This gives me (and hopefully you) a production-ready base in minutes, with opinionated defaults you can start.<p>I\u2019m open to suggestions and feedback, what would you like to see next? I\u2019ll hang around in the comments to answer questions.", "title": "Show HN: Open-source Next.js 15 boilerplate \u2013 auth, DB, intl, tests, monitoring", "updated_at": "2025-08-29T18:59:31Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "saadahmad"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "Hi folks, I am very new to the hacker news community and would like to share something I have been working on.<p>I've spent the past 3 years building Arvo, a TypeScript toolkit for applications where AI agents, humans, workflows, and business logic need to work together.<p>My core thesis is to treat the main units of work as event handlers. Agents, workflows, services, and humans all modelled as event handlers communicating across an event fabric. No architectural privileges. An agent coordinating with a workflow uses identical patterns to a workflow coordinating with a human.<p>Key characteristics:\n- Contract-first design with runtime validation (<em>Zod</em>-based)\n- Event driven durable execution with suspend/resume for orchestrations\n    - State machines (XState) for structured workflows\n    - Imperative orchestrations (Temporal style) for when state-machines becomes limiting\n- Infrastructure-agnostic (same code runs locally or on cloud)\n- Native OpenTelemetry integration<p>Built on CloudEvents, implements virtual orchestration through physical choreography. Handlers are stateless, workflows persist as JSON, everything composes naturally.<p>Open source, <em>production</em>-ready core, growing standard library (@arvo-tools).<p>Happy to discuss the architecture or answer questions about the design decisions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Arvo \u2013 TypeScript toolkit for event-driven agentic systems and mesh"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.arvo.land/"}}, "_tags": ["story", "author_saadahmad", "story_46451417", "show_hn"], "author": "saadahmad", "created_at": "2026-01-01T04:52:31Z", "created_at_i": 1767243151, "num_comments": 0, "objectID": "46451417", "points": 3, "story_id": 46451417, "story_text": "Hi folks, I am very new to the hacker news community and would like to share something I have been working on.<p>I&#x27;ve spent the past 3 years building Arvo, a TypeScript toolkit for applications where AI agents, humans, workflows, and business logic need to work together.<p>My core thesis is to treat the main units of work as event handlers. Agents, workflows, services, and humans all modelled as event handlers communicating across an event fabric. No architectural privileges. An agent coordinating with a workflow uses identical patterns to a workflow coordinating with a human.<p>Key characteristics:\n- Contract-first design with runtime validation (Zod-based)\n- Event driven durable execution with suspend&#x2F;resume for orchestrations\n    - State machines (XState) for structured workflows\n    - Imperative orchestrations (Temporal style) for when state-machines becomes limiting\n- Infrastructure-agnostic (same code runs locally or on cloud)\n- Native OpenTelemetry integration<p>Built on CloudEvents, implements virtual orchestration through physical choreography. Handlers are stateless, workflows persist as JSON, everything composes naturally.<p>Open source, production-ready core, growing standard library (@arvo-tools).<p>Happy to discuss the architecture or answer questions about the design decisions.", "title": "Show HN: Arvo \u2013 TypeScript toolkit for event-driven agentic systems and mesh", "updated_at": "2026-01-05T19:03:39Z", "url": "https://www.arvo.land/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justvugg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "I built PolyMCP to make it trivial to expose existing functions as MCP tools, without rewriting logic or adding much glue code.<p>The goal: take \u201cnormal\u201d Python or TypeScript functions and instantly make them usable by MCP clients (Claude Desktop, agents, Ollama, etc.).<p>Python example<p>from polymcp.polymcp_toolkit import expose_tools<p>def greet(name: str) -&gt; str:\n    &quot;&quot;&quot;Say hello.&quot;&quot;&quot;\n    return f&quot;Hello, {name}!&quot;<p>def add(a: int, b: int) -&gt; int:\n    &quot;&quot;&quot;Add two numbers.&quot;&quot;&quot;\n    return a + b<p>app = expose_tools([greet, add], title=&quot;My MCP Tools&quot;)<p>Run with:<p>uvicorn server:app --reload<p>MCP endpoints appear at:\n \u2022 /mcp/list_tools\n \u2022 /mcp/invoke<p>TypeScript example<p>import { z } from &quot;<em>zod</em>&quot;;\nimport { tool, exposeTools } from &quot;polymcp&quot;;<p>const uppercaseTool = tool({\n  name: &quot;uppercase&quot;,\n  description: &quot;Convert text to uppercase&quot;,\n  inputSchema: z.object({ text: z.string() }),\n  function: async ({ text }) =&gt; text.toUpperCase(),\n});<p>const app = exposeTools([uppercaseTool], { title: &quot;Text Tools&quot; });\napp.listen(3000);<p>More \u201creal\u201d example (Python)<p>import pandas as pd\nfrom polymcp.polymcp_toolkit import expose_tools<p>def calculate_commissions(sales_data: list[dict]):\n    df = pd.DataFrame(sales_data)\n    df[&quot;commission&quot;] = df[&quot;sales_amount&quot;] * 0.05\n    return df.to_dict(orient=&quot;records&quot;)<p>app = expose_tools([calculate_commissions], title=&quot;Business Tools&quot;)<p>What you get\n \u2022 Reuse existing code with minimal changes\n \u2022 MCP-compatible (Claude Desktop, agents, Ollama, etc.)\n \u2022 HTTP, stdio, and WASM support\n \u2022 Automatic input validation\n \u2022 Basic <em>production</em> features (budgets, retries, redaction, logs)\n \u2022 Built-in inspector for testing and monitoring<p>Install\n \u2022 Python: pip install polymcp\n \u2022 TypeScript: clone repo \u2192 cd polymcp-ts \u2192 npm install \u2192 npm run build<p>Repo:\n<a href=\"https://github.com/poly-mcp/Polymcp\" rel=\"nofollow\">https://github.com/poly-mcp/Polymcp</a><p>Curious what kind of function people would expose first if it was this easy.\nFeedback very welcome."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: PolyMCP \u2013 Expose Python/TS functions as MCP tools easily"}}, "_tags": ["story", "author_justvugg", "story_46842573", "show_hn"], "author": "justvugg", "created_at": "2026-02-01T00:58:27Z", "created_at_i": 1769907507, "num_comments": 0, "objectID": "46842573", "points": 1, "story_id": 46842573, "story_text": "I built PolyMCP to make it trivial to expose existing functions as MCP tools, without rewriting logic or adding much glue code.<p>The goal: take \u201cnormal\u201d Python or TypeScript functions and instantly make them usable by MCP clients (Claude Desktop, agents, Ollama, etc.).<p>Python example<p>from polymcp.polymcp_toolkit import expose_tools<p>def greet(name: str) -&gt; str:\n    &quot;&quot;&quot;Say hello.&quot;&quot;&quot;\n    return f&quot;Hello, {name}!&quot;<p>def add(a: int, b: int) -&gt; int:\n    &quot;&quot;&quot;Add two numbers.&quot;&quot;&quot;\n    return a + b<p>app = expose_tools([greet, add], title=&quot;My MCP Tools&quot;)<p>Run with:<p>uvicorn server:app --reload<p>MCP endpoints appear at:\n \u2022 &#x2F;mcp&#x2F;list_tools\n \u2022 &#x2F;mcp&#x2F;invoke<p>TypeScript example<p>import { z } from &quot;zod&quot;;\nimport { tool, exposeTools } from &quot;polymcp&quot;;<p>const uppercaseTool = tool({\n  name: &quot;uppercase&quot;,\n  description: &quot;Convert text to uppercase&quot;,\n  inputSchema: z.object({ text: z.string() }),\n  function: async ({ text }) =&gt; text.toUpperCase(),\n});<p>const app = exposeTools([uppercaseTool], { title: &quot;Text Tools&quot; });\napp.listen(3000);<p>More \u201creal\u201d example (Python)<p>import pandas as pd\nfrom polymcp.polymcp_toolkit import expose_tools<p>def calculate_commissions(sales_data: list[dict]):\n    df = pd.DataFrame(sales_data)\n    df[&quot;commission&quot;] = df[&quot;sales_amount&quot;] * 0.05\n    return df.to_dict(orient=&quot;records&quot;)<p>app = expose_tools([calculate_commissions], title=&quot;Business Tools&quot;)<p>What you get\n \u2022 Reuse existing code with minimal changes\n \u2022 MCP-compatible (Claude Desktop, agents, Ollama, etc.)\n \u2022 HTTP, stdio, and WASM support\n \u2022 Automatic input validation\n \u2022 Basic production features (budgets, retries, redaction, logs)\n \u2022 Built-in inspector for testing and monitoring<p>Install\n \u2022 Python: pip install polymcp\n \u2022 TypeScript: clone repo \u2192 cd polymcp-ts \u2192 npm install \u2192 npm run build<p>Repo:\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;Polymcp\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;Polymcp</a><p>Curious what kind of function people would expose first if it was this easy.\nFeedback very welcome.", "title": "Show HN: PolyMCP \u2013 Expose Python/TS functions as MCP tools easily", "updated_at": "2026-02-01T01:00:13Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justvugg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "I made PolyMCP so I could quickly turn normal functions (Python or TypeScript) into MCP tools without much extra work.\nPython example:\nfrom polymcp.polymcp_toolkit import expose_tools<p>def greet(name: str) -&gt; str:\n    &quot;&quot;&quot;Say hello.&quot;&quot;&quot;\n    return f&quot;Hello, {name}!&quot;<p>def add(a: int, b: int) -&gt; int:\n    &quot;&quot;&quot;Add two numbers.&quot;&quot;&quot;\n    return a + b<p>app = expose_tools([greet, add], title=&quot;My MCP Tools&quot;)\nRun with:\nuvicorn server:app --reload\nMCP endpoints appear at /mcp/list_tools and /mcp/invoke/.\nTypeScript example:\nimport { z } from '<em>zod</em>';\nimport { tool, exposeTools } from 'polymcp';<p>const uppercaseTool = tool({\n  name: 'uppercase',\n  description: 'Convert text to uppercase',\n  inputSchema: z.object({ text: z.string() }),\n  function: async ({ text }) =&gt; text.toUpperCase(),\n});<p>const app = exposeTools([uppercaseTool], { title: &quot;Text Tools&quot; });\napp.listen(3000);\nBusiness example (Python):\nimport pandas as pd\nfrom polymcp.polymcp_toolkit import expose_tools<p>def calculate_commissions(sales_data: list[dict]):\n    df = pd.DataFrame(sales_data)\n    df[&quot;commission&quot;] = df[&quot;sales_amount&quot;] * 0.05\n    return df.to_dict(orient=&quot;records&quot;)<p>app = expose_tools([calculate_commissions], title=&quot;Business Tools&quot;)\nWhat you get:\n \u2022 Reuse existing code with minimal changes\n \u2022 Compatible with MCP clients (Claude Desktop, agents, Ollama, etc.)\n \u2022 HTTP and stdio and Wasm support\n \u2022 Automatic validation\n \u2022 Basic <em>production</em> features (budgets, retries, redaction, logs)\n \u2022 Inspector: polymcp inspector for testing/monitoring\nInstall:\n \u2022 Python: pip install polymcp\n \u2022 TypeScript: clone repo \u2192 cd polymcp-ts \u2192 npm install \u2192 npm run build\nRepo: https://github.com/poly-mcp/Polymcp<p>Curious what kind of function people would expose first if it was this simple. Feedback welcome."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: PolyMCP \u2013 Expose Python/TS functions as MCP tools easily"}}, "_tags": ["story", "author_justvugg", "story_46794329", "show_hn"], "author": "justvugg", "created_at": "2026-01-28T12:13:22Z", "created_at_i": 1769602402, "num_comments": 0, "objectID": "46794329", "points": 1, "story_id": 46794329, "story_text": "I made PolyMCP so I could quickly turn normal functions (Python or TypeScript) into MCP tools without much extra work.\nPython example:\nfrom polymcp.polymcp_toolkit import expose_tools<p>def greet(name: str) -&gt; str:\n    &quot;&quot;&quot;Say hello.&quot;&quot;&quot;\n    return f&quot;Hello, {name}!&quot;<p>def add(a: int, b: int) -&gt; int:\n    &quot;&quot;&quot;Add two numbers.&quot;&quot;&quot;\n    return a + b<p>app = expose_tools([greet, add], title=&quot;My MCP Tools&quot;)\nRun with:\nuvicorn server:app --reload\nMCP endpoints appear at &#x2F;mcp&#x2F;list_tools and &#x2F;mcp&#x2F;invoke&#x2F;.\nTypeScript example:\nimport { z } from &#x27;zod&#x27;;\nimport { tool, exposeTools } from &#x27;polymcp&#x27;;<p>const uppercaseTool = tool({\n  name: &#x27;uppercase&#x27;,\n  description: &#x27;Convert text to uppercase&#x27;,\n  inputSchema: z.object({ text: z.string() }),\n  function: async ({ text }) =&gt; text.toUpperCase(),\n});<p>const app = exposeTools([uppercaseTool], { title: &quot;Text Tools&quot; });\napp.listen(3000);\nBusiness example (Python):\nimport pandas as pd\nfrom polymcp.polymcp_toolkit import expose_tools<p>def calculate_commissions(sales_data: list[dict]):\n    df = pd.DataFrame(sales_data)\n    df[&quot;commission&quot;] = df[&quot;sales_amount&quot;] * 0.05\n    return df.to_dict(orient=&quot;records&quot;)<p>app = expose_tools([calculate_commissions], title=&quot;Business Tools&quot;)\nWhat you get:\n \u2022 Reuse existing code with minimal changes\n \u2022 Compatible with MCP clients (Claude Desktop, agents, Ollama, etc.)\n \u2022 HTTP and stdio and Wasm support\n \u2022 Automatic validation\n \u2022 Basic production features (budgets, retries, redaction, logs)\n \u2022 Inspector: polymcp inspector for testing&#x2F;monitoring\nInstall:\n \u2022 Python: pip install polymcp\n \u2022 TypeScript: clone repo \u2192 cd polymcp-ts \u2192 npm install \u2192 npm run build\nRepo: https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;Polymcp<p>Curious what kind of function people would expose first if it was this simple. Feedback welcome.", "title": "Show HN: PolyMCP \u2013 Expose Python/TS functions as MCP tools easily", "updated_at": "2026-01-28T12:14:45Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "hongyeon"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "Hi HN,<p>I'm releasing LLM Newsletter Kit, a <em>production</em>-tested TypeScript toolkit designed to build AI-driven newsletter pipelines. It handles the full lifecycle: crawling \u2192 analysis \u2192 generation \u2192 delivery.<p>The Context: I\u2019m an archaeologist-turned-engineer. I built this engine for &quot;Research Radar&quot; (a cultural heritage newsletter) to automate my own manual research aggregation. It currently maintains a 15% CTR with near-zero maintenance, costing ~$0.20-1 per issue.<p>Core Features:<p>&quot;Bring Your Own Scraper&quot;: Async injection lets you use Cheerio, Puppeteer, or LLM-based parsers without framework lock-in.<p>Provider-based DI: Swap out crawling, analysis, or storage components via clean interfaces.<p><em>Production</em>-First: 100% test coverage, built-in retries, cost controls, and observability baked in.<p>Tech Stack: TypeScript ESM, LangChain runnables, Vercel AI SDK (structured outputs), and <em>Zod</em>.<p>Why Code instead of No-Code? To optimize costs and quality, you need granular control over context windows, token limits, and retry strategies. This toolkit allows for advanced workflows (e.g., self-reflection loops) that are often prohibitively expensive or impossible in drag-and-drop tools.<p>Links:<p>Code (GitHub): <a href=\"https://github.com/kimhongyeon/heripo-research-radar\" rel=\"nofollow\">https://github.com/kimhongyeon/heripo-research-radar</a><p>Live Example: <a href=\"https://heripo.com/research-radar-newsletter-example.html\" rel=\"nofollow\">https://heripo.com/research-radar-newsletter-example.html</a><p>npm: @llm-newsletter-kit/core<p>I'd love your feedback on the architecture and DX!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LLM Newsletter Kit \u2013 A TypeScript Framework for AI Newsletters"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/kimhongyeon/llm-newsletter-kit-core"}}, "_tags": ["story", "author_hongyeon", "story_46094018", "show_hn"], "author": "hongyeon", "created_at": "2025-11-30T05:16:16Z", "created_at_i": 1764479776, "num_comments": 0, "objectID": "46094018", "points": 1, "story_id": 46094018, "story_text": "Hi HN,<p>I&#x27;m releasing LLM Newsletter Kit, a production-tested TypeScript toolkit designed to build AI-driven newsletter pipelines. It handles the full lifecycle: crawling \u2192 analysis \u2192 generation \u2192 delivery.<p>The Context: I\u2019m an archaeologist-turned-engineer. I built this engine for &quot;Research Radar&quot; (a cultural heritage newsletter) to automate my own manual research aggregation. It currently maintains a 15% CTR with near-zero maintenance, costing ~$0.20-1 per issue.<p>Core Features:<p>&quot;Bring Your Own Scraper&quot;: Async injection lets you use Cheerio, Puppeteer, or LLM-based parsers without framework lock-in.<p>Provider-based DI: Swap out crawling, analysis, or storage components via clean interfaces.<p>Production-First: 100% test coverage, built-in retries, cost controls, and observability baked in.<p>Tech Stack: TypeScript ESM, LangChain runnables, Vercel AI SDK (structured outputs), and Zod.<p>Why Code instead of No-Code? To optimize costs and quality, you need granular control over context windows, token limits, and retry strategies. This toolkit allows for advanced workflows (e.g., self-reflection loops) that are often prohibitively expensive or impossible in drag-and-drop tools.<p>Links:<p>Code (GitHub): <a href=\"https:&#x2F;&#x2F;github.com&#x2F;kimhongyeon&#x2F;heripo-research-radar\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;kimhongyeon&#x2F;heripo-research-radar</a><p>Live Example: <a href=\"https:&#x2F;&#x2F;heripo.com&#x2F;research-radar-newsletter-example.html\" rel=\"nofollow\">https:&#x2F;&#x2F;heripo.com&#x2F;research-radar-newsletter-example.html</a><p>npm: @llm-newsletter-kit&#x2F;core<p>I&#x27;d love your feedback on the architecture and DX!", "title": "Show HN: LLM Newsletter Kit \u2013 A TypeScript Framework for AI Newsletters", "updated_at": "2025-11-30T05:20:48Z", "url": "https://github.com/kimhongyeon/llm-newsletter-kit-core"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "eldiabolo"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "Hey Hackernews! We're excited to share the latest version of LaunchIt, the first fully GDPR-compliant SaaS boilerplate that lets founders build and ship in hours, not months.<p>What is LaunchIt?\nLaunchIt is a complete SaaS foundation that gives you everything you need to launch your next big idea without reinventing the wheel. Our users have saved $2.5k+ in development costs while reducing time-to-market from months to days.<p>What's new in this release?\n Full NextJS 15 support with Server Components\n Enhanced GDPR compliance with built-in cookie consent\n Enterprise-grade security features\n Complete i18n support for global scaling\n Improved Lighthouse scores &amp; SEO optimization\n Type-safe development with <em>Zod</em>\n Microservices-ready architecture<p>Why our users love it (4.8/5 rating)\n&quot;LaunchIt saved us 2 months of development time and solved our compliance headaches before they started.&quot;\nLaunchIt was built after we spent thousands of hours developing SaaS products and repeatedly solving the same problems. We packaged all those lessons into a <em>production</em>-ready foundation.<p>Launch special\nTo celebrate our NextJS 15 update, we're offering 50%. No monthly fees, just a one-time purchase.<p>Try the demo or get started today! We'd love your feedback."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LaunchIt \u2013 The first GDPR-compliant NextJS Kit"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.getlaunch.it/en/"}}, "_tags": ["story", "author_eldiabolo", "story_43458424", "show_hn"], "author": "eldiabolo", "created_at": "2025-03-24T07:35:23Z", "created_at_i": 1742801723, "num_comments": 0, "objectID": "43458424", "points": 1, "story_id": 43458424, "story_text": "Hey Hackernews! We&#x27;re excited to share the latest version of LaunchIt, the first fully GDPR-compliant SaaS boilerplate that lets founders build and ship in hours, not months.<p>What is LaunchIt?\nLaunchIt is a complete SaaS foundation that gives you everything you need to launch your next big idea without reinventing the wheel. Our users have saved $2.5k+ in development costs while reducing time-to-market from months to days.<p>What&#x27;s new in this release?\n Full NextJS 15 support with Server Components\n Enhanced GDPR compliance with built-in cookie consent\n Enterprise-grade security features\n Complete i18n support for global scaling\n Improved Lighthouse scores &amp; SEO optimization\n Type-safe development with Zod\n Microservices-ready architecture<p>Why our users love it (4.8&#x2F;5 rating)\n&quot;LaunchIt saved us 2 months of development time and solved our compliance headaches before they started.&quot;\nLaunchIt was built after we spent thousands of hours developing SaaS products and repeatedly solving the same problems. We packaged all those lessons into a production-ready foundation.<p>Launch special\nTo celebrate our NextJS 15 update, we&#x27;re offering 50%. No monthly fees, just a one-time purchase.<p>Try the demo or get started today! We&#x27;d love your feedback.", "title": "Show HN: LaunchIt \u2013 The first GDPR-compliant NextJS Kit", "updated_at": "2025-03-24T07:37:24Z", "url": "https://www.getlaunch.it/en/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "DAlperin"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "GPUI 2 is now in <em>production</em> \u2013 <em>Zed</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["zod"], "value": "https://<em>zed</em>.dev/blog/gpui-2-on-preview"}}, "_tags": ["story", "author_DAlperin", "story_38871732"], "author": "DAlperin", "children": [38872711, 38872918, 38872982, 38873104, 38873108, 38873251, 38873395, 38874187, 38874847, 38907971], "created_at": "2024-01-04T20:19:00Z", "created_at_i": 1704399540, "num_comments": 18, "objectID": "38871732", "points": 50, "story_id": 38871732, "title": "GPUI 2 is now in production \u2013 Zed", "updated_at": "2025-10-25T06:20:51Z", "url": "https://zed.dev/blog/gpui-2-on-preview"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "colmtuite"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "After two years of development (and endless bikeshedding), we\u2019ve finally pushed Base UI v1 out the door.<p>Base UI is an unstyled UI library for building accessible component libraries and user interfaces with React. 35 accessible, configurable, composable, customizable components with a focus on a11y.<p>Base UI is already in <em>production</em> at startups like Paper, <em>Zed</em>, and Unsplash, and being trialled by larger companies like GitHub and Vercel. There is a growing community of open-source libs built on top of it, such as Coss UI, Basecn, 9UI, and Pure UI.<p>Since Base UI doesn\u2019t bundle any styles, it plays nice with Tailwind, CSS Modules, CSS-in-JS, plain CSS, or whatever styling solution you prefer. It also works with JavaScript animation libraries like Motion, or just plain CSS transitions.<p>The project is backed by MUI (who are profitable and UI-focused), and maintained by a full-time team of 7 engineers, designers, and managers.<p><a href=\"https://base-ui.com\" rel=\"nofollow\">https://base-ui.com</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Base UI v1.0 Unstyled UI Components from the Creators of Radix and MUI"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/mui/base-ui"}}, "_tags": ["story", "author_colmtuite", "story_46245401", "show_hn"], "author": "colmtuite", "children": [46245435, 46245454, 46245472, 46245480, 46245638, 46254072], "created_at": "2025-12-12T16:04:22Z", "created_at_i": 1765555462, "num_comments": 14, "objectID": "46245401", "points": 16, "story_id": 46245401, "story_text": "After two years of development (and endless bikeshedding), we\u2019ve finally pushed Base UI v1 out the door.<p>Base UI is an unstyled UI library for building accessible component libraries and user interfaces with React. 35 accessible, configurable, composable, customizable components with a focus on a11y.<p>Base UI is already in production at startups like Paper, Zed, and Unsplash, and being trialled by larger companies like GitHub and Vercel. There is a growing community of open-source libs built on top of it, such as Coss UI, Basecn, 9UI, and Pure UI.<p>Since Base UI doesn\u2019t bundle any styles, it plays nice with Tailwind, CSS Modules, CSS-in-JS, plain CSS, or whatever styling solution you prefer. It also works with JavaScript animation libraries like Motion, or just plain CSS transitions.<p>The project is backed by MUI (who are profitable and UI-focused), and maintained by a full-time team of 7 engineers, designers, and managers.<p><a href=\"https:&#x2F;&#x2F;base-ui.com\" rel=\"nofollow\">https:&#x2F;&#x2F;base-ui.com</a>", "title": "Show HN: Base UI v1.0 Unstyled UI Components from the Creators of Radix and MUI", "updated_at": "2026-02-19T13:25:25Z", "url": "https://github.com/mui/base-ui"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "simanyay"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "These days I often see extraordinary claims from reputable industry and other insiders that AI writes most of the code and human programmers are simply supervising it.<p>For example, this morning I was reading [1] and it had the following claim: \u201c If you really want to grasp how much better A.I. has gotten recently, talk to a programmer. A year or two ago, A.I. coding tools existed, but were aimed more at speeding up human coders than at replacing them. Today, software engineers tell me that A.I. does most of the actual coding for them, and that they increasingly feel that their job is to supervise the A.I. systems.<p>Jared Friedman, a partner at Y Combinator, a start-up accelerator, recently said a quarter of the accelerator\u2019s current batch of start-ups were using A.I. to write nearly all their code.\u201d<p>As a programmer myself, whenever I read this I feel like there\u2019s a giant group chat going on and I\u2019m excluded. I use AI in my work, mostly through <em>Zed</em> assistant, but with the latest available models the output and reasoning is nowhere in quality where I\u2019d let it generate majority of the code and ship it to <em>production</em>.<p>Am I missing something? Do people really generate majority of the code that then successfully operates in <em>production</em> at any significant business scale?<p>[1] - https://archive.ph/XykVf"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: Do you use AI to generate majority of your <em>production</em> code?"}}, "_tags": ["story", "author_simanyay", "story_43462171", "ask_hn"], "author": "simanyay", "children": [43462180, 43462298, 43462340, 43462506, 43463773, 43466286], "created_at": "2025-03-24T15:28:25Z", "created_at_i": 1742830105, "num_comments": 8, "objectID": "43462171", "points": 4, "story_id": 43462171, "story_text": "These days I often see extraordinary claims from reputable industry and other insiders that AI writes most of the code and human programmers are simply supervising it.<p>For example, this morning I was reading [1] and it had the following claim: \u201c If you really want to grasp how much better A.I. has gotten recently, talk to a programmer. A year or two ago, A.I. coding tools existed, but were aimed more at speeding up human coders than at replacing them. Today, software engineers tell me that A.I. does most of the actual coding for them, and that they increasingly feel that their job is to supervise the A.I. systems.<p>Jared Friedman, a partner at Y Combinator, a start-up accelerator, recently said a quarter of the accelerator\u2019s current batch of start-ups were using A.I. to write nearly all their code.\u201d<p>As a programmer myself, whenever I read this I feel like there\u2019s a giant group chat going on and I\u2019m excluded. I use AI in my work, mostly through Zed assistant, but with the latest available models the output and reasoning is nowhere in quality where I\u2019d let it generate majority of the code and ship it to production.<p>Am I missing something? Do people really generate majority of the code that then successfully operates in production at any significant business scale?<p>[1] - https:&#x2F;&#x2F;archive.ph&#x2F;XykVf", "title": "Ask HN: Do you use AI to generate majority of your production code?", "updated_at": "2026-02-14T15:33:23Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "araghuvanshi"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "Hello HN! We\u2019re Emily and Aman, the cofounders of Pyq (<a href=\"https://www.pyqai.com\">https://www.pyqai.com</a>). We make it easy for developers to build features powered by AI. We do this by identifying specific tasks that AI can solve well and providing simple APIs that any developer can start using straight from our website.<p>We built Pyq because it took too long to build features that were powered by AI at our previous jobs. A lot of people want to get started using AI, but struggle because of the difficulties involved in managing infrastructure, finding the right model and learning how to call it. There are many interesting and useful models in places like Github or Hugging Face, as well as specific applications of popular models like OpenAI\u2019s GPT, but they require a decent amount of work/knowledge to get working in your app.<p>The first issue is determining if and how your problem can be solved with AI. This generally involves experimenting with different models and (more recently) prompts, followed by potentially fine-tuning your model, at which point you\u2019ll have to repeat this process with datasets. Then you move onto the set of challenges posed by getting that model deployed in <em>production</em>, including messing around with Docker, cloud infrastructure etc. This process can take weeks or even months. We aim to make it easy to match a problem to an AI solution and get it working in your application quickly.<p>Aman was leading a product team at a startup and was told that an already-built AI model would take an additional 3 weeks to bring to <em>production</em>. The only solution was to hire an engineer to do this and potentially pay for an enterprise MLOps platform on top of that. Simultaneously, Emily at Microsoft found herself asking the Azure team directly for help to hook up a model into the HoloLens application she was working on. The ensuing frustration resulted in our first principle: bringing an AI model to <em>production</em> should take minutes, not weeks!<p>Infrastructure is only one part of the problem. With all of the new possibilities afforded by modern AI models, it can be difficult to understand what business applications they can be used for. We decided to apply our knowledge of building AI-powered products to finding practical use cases that are easy for any developer to understand, even if they don\u2019t have any AI knowledge.<p>We identify use cases of various AI models and provide straightforward APIs tailored to those use cases. We use both open-source models and popular providers such as OpenAI. This allows for easy and fast integration into apps. Rather than starting with the model, experimenting to see if it can actually do what you want it to, learning about deployment and serving, developers can just make a POST call to start using AI.<p>We serve our models with FastAPI, containerize them, and then deploy them to our GKE clusters. Depending on the model, we choose different machines - some require GPUs, most are decent on CPU. We take models up or down based on usage, so we have cold starts unless otherwise specified by customers. We expose access to the model via a POST call through our cloud app. We track inputs and outputs, as we expect that people will become interested in fine tuning models based on their past usage.<p>Pyq is not meant for AI experts or specialists, but for people who are building features which are powered by AI. We have a curated list of models that are good at specific tasks and are inexpensive to use. Some have been used thousands of times already!<p>Deploying your own model with us is also a very straightforward process and can usually be done within an hour. For those requiring low latency and high volume, we also offer a high performance API at additional cost.<p>Shortly after the launch of Chat GPT, we created a GPT Detector (<a href=\"https://www.gpt-detector.com\" rel=\"nofollow\">https://www.gpt-detector.com</a>, also available via API through our website) in collaboration with another YC company. This got a surprising amount of traction due to the virality of ChatGPT itself. Building the entire website took less than a day - we fine-tuned an existing text classification model, deployed it on Pyq and our partner integrated it with their front-end. It has been used 10,000+ times since then, and has been quite performant and inexpensive.<p>We have seen several other applications created in a similar way using Pyq. These include document OCR apps, chatbots, stock image generators and more.<p>We have a prepaid, usage-based pricing model. Every model has a \u201cspot price\u201d - the cost of 1 second of compute. This is available on each model\u2019s page in our \u2018<em>Zoo</em>.\u2019 If you deploy your own model, we will give you your initial price manually and adjust it up or down over time depending on your needs.<p>We also provide $10 of free computing credit upon signup. This is enough to experiment with all of our models and, for some of them, enough to run a few hundred or even a thousand inferences. We add more credits on an ad-hoc basis, so feel free to email us at team[at]pyqai.com describing what you\u2019re working on and we\u2019ll do our best to accommodate you!<p>We are so excited to show this product. Our hope is that it helps you bring a project to life, finish that feature you\u2019ve been working on, or just gives you ideas for what to build next. Please weigh in and tell us what you think!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Pyq (YC W23) \u2013 Simple APIs to Popular AI Models"}}, "_tags": ["story", "author_araghuvanshi", "story_34971883", "launch_hn"], "author": "araghuvanshi", "children": [34972015, 34972483, 34972620, 34972667, 34972706, 34973062, 34975217, 34975556], "created_at": "2023-02-28T17:14:59Z", "created_at_i": 1677604499, "num_comments": 24, "objectID": "34971883", "points": 123, "story_id": 34971883, "story_text": "Hello HN! We\u2019re Emily and Aman, the cofounders of Pyq (<a href=\"https:&#x2F;&#x2F;www.pyqai.com\">https:&#x2F;&#x2F;www.pyqai.com</a>). We make it easy for developers to build features powered by AI. We do this by identifying specific tasks that AI can solve well and providing simple APIs that any developer can start using straight from our website.<p>We built Pyq because it took too long to build features that were powered by AI at our previous jobs. A lot of people want to get started using AI, but struggle because of the difficulties involved in managing infrastructure, finding the right model and learning how to call it. There are many interesting and useful models in places like Github or Hugging Face, as well as specific applications of popular models like OpenAI\u2019s GPT, but they require a decent amount of work&#x2F;knowledge to get working in your app.<p>The first issue is determining if and how your problem can be solved with AI. This generally involves experimenting with different models and (more recently) prompts, followed by potentially fine-tuning your model, at which point you\u2019ll have to repeat this process with datasets. Then you move onto the set of challenges posed by getting that model deployed in production, including messing around with Docker, cloud infrastructure etc. This process can take weeks or even months. We aim to make it easy to match a problem to an AI solution and get it working in your application quickly.<p>Aman was leading a product team at a startup and was told that an already-built AI model would take an additional 3 weeks to bring to production. The only solution was to hire an engineer to do this and potentially pay for an enterprise MLOps platform on top of that. Simultaneously, Emily at Microsoft found herself asking the Azure team directly for help to hook up a model into the HoloLens application she was working on. The ensuing frustration resulted in our first principle: bringing an AI model to production should take minutes, not weeks!<p>Infrastructure is only one part of the problem. With all of the new possibilities afforded by modern AI models, it can be difficult to understand what business applications they can be used for. We decided to apply our knowledge of building AI-powered products to finding practical use cases that are easy for any developer to understand, even if they don\u2019t have any AI knowledge.<p>We identify use cases of various AI models and provide straightforward APIs tailored to those use cases. We use both open-source models and popular providers such as OpenAI. This allows for easy and fast integration into apps. Rather than starting with the model, experimenting to see if it can actually do what you want it to, learning about deployment and serving, developers can just make a POST call to start using AI.<p>We serve our models with FastAPI, containerize them, and then deploy them to our GKE clusters. Depending on the model, we choose different machines - some require GPUs, most are decent on CPU. We take models up or down based on usage, so we have cold starts unless otherwise specified by customers. We expose access to the model via a POST call through our cloud app. We track inputs and outputs, as we expect that people will become interested in fine tuning models based on their past usage.<p>Pyq is not meant for AI experts or specialists, but for people who are building features which are powered by AI. We have a curated list of models that are good at specific tasks and are inexpensive to use. Some have been used thousands of times already!<p>Deploying your own model with us is also a very straightforward process and can usually be done within an hour. For those requiring low latency and high volume, we also offer a high performance API at additional cost.<p>Shortly after the launch of Chat GPT, we created a GPT Detector (<a href=\"https:&#x2F;&#x2F;www.gpt-detector.com\" rel=\"nofollow\">https:&#x2F;&#x2F;www.gpt-detector.com</a>, also available via API through our website) in collaboration with another YC company. This got a surprising amount of traction due to the virality of ChatGPT itself. Building the entire website took less than a day - we fine-tuned an existing text classification model, deployed it on Pyq and our partner integrated it with their front-end. It has been used 10,000+ times since then, and has been quite performant and inexpensive.<p>We have seen several other applications created in a similar way using Pyq. These include document OCR apps, chatbots, stock image generators and more.<p>We have a prepaid, usage-based pricing model. Every model has a \u201cspot price\u201d - the cost of 1 second of compute. This is available on each model\u2019s page in our \u2018Zoo.\u2019 If you deploy your own model, we will give you your initial price manually and adjust it up or down over time depending on your needs.<p>We also provide $10 of free computing credit upon signup. This is enough to experiment with all of our models and, for some of them, enough to run a few hundred or even a thousand inferences. We add more credits on an ad-hoc basis, so feel free to email us at team[at]pyqai.com describing what you\u2019re working on and we\u2019ll do our best to accommodate you!<p>We are so excited to show this product. Our hope is that it helps you bring a project to life, finish that feature you\u2019ve been working on, or just gives you ideas for what to build next. Please weigh in and tell us what you think!", "title": "Launch HN: Pyq (YC W23) \u2013 Simple APIs to Popular AI Models", "updated_at": "2025-10-06T20:43:46Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bsg75"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["zod", "production"], "value": "Looking to implement a platform with a native ZFS port to run PostgreSQL DBs on. These databases support analytic workloads (OLAP), and initial testing with ZFS On Linux (<em>ZoL</em>) is promising[1], but I have encountered some issues that are driving me to investigate a &quot;built-in&quot; implementation.<p>Coming from Linux (CentOS), I find FreeBSD to be very friendly. From installation to adding apps via ports or pkg, FreeBSD was a snap to get started with.<p>Some in the PostgreSQL community favor Illumos derivatives, OmniOS for example. Initial explorations show it is a greater deviation from the familiar, and will require more time investment before we are proficient enough for <em>production</em>.<p>This said, are there any significant advantages that an Illumos platform has over FreeBSD, when looking at a host that will principally run PostgreSQL databases? Performance? Stability?<p>The only other workloads will be Python based ETL processes - I have found Pandas and similar run fine on BSD, not yet sure about OmniOS.<p>[1] I have great hopes for the OpenZFS project, and eventually ZFS On Linux. Other data platforms I use such as Vertica are not supported on BSD, and <em>ZoL</em> brings ZFS to a lot of new platform possibilities.<p>Edit: It turns out I asked this two years ago (https://news.ycombinator.com/item?id=5665800), but perhaps new perspectives will be added."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: FreeBSD or Illumos for Postgres DWH?"}, "url": {"matchLevel": "none", "matchedWords": [], "value": ""}}, "_tags": ["story", "author_bsg75", "story_8884125", "ask_hn"], "author": "bsg75", "children": [8888168, 8888459], "created_at": "2015-01-14T03:36:52Z", "created_at_i": 1421206612, "num_comments": 13, "objectID": "8884125", "points": 5, "story_id": 8884125, "story_text": "Looking to implement a platform with a native ZFS port to run PostgreSQL DBs on. These databases support analytic workloads (OLAP), and initial testing with ZFS On Linux (ZoL) is promising[1], but I have encountered some issues that are driving me to investigate a &quot;built-in&quot; implementation.<p>Coming from Linux (CentOS), I find FreeBSD to be very friendly. From installation to adding apps via ports or pkg, FreeBSD was a snap to get started with.<p>Some in the PostgreSQL community favor Illumos derivatives, OmniOS for example. Initial explorations show it is a greater deviation from the familiar, and will require more time investment before we are proficient enough for production.<p>This said, are there any significant advantages that an Illumos platform has over FreeBSD, when looking at a host that will principally run PostgreSQL databases? Performance? Stability?<p>The only other workloads will be Python based ETL processes - I have found Pandas and similar run fine on BSD, not yet sure about OmniOS.<p>[1] I have great hopes for the OpenZFS project, and eventually ZFS On Linux. Other data platforms I use such as Vertica are not supported on BSD, and ZoL brings ZFS to a lot of new platform possibilities.<p>Edit: It turns out I asked this two years ago (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=5665800), but perhaps new perspectives will be added.", "title": "Ask HN: FreeBSD or Illumos for Postgres DWH?", "updated_at": "2023-09-06T23:16:11Z", "url": ""}], "hitsPerPage": 15, "nbHits": 16, "nbPages": 2, "page": 0, "params": "query=zod+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 10, "processingTimingsMS": {"_request": {"roundTrip": 24}, "afterFetch": {"format": {"highlighting": 2, "total": 2}}, "fetch": {"query": 5, "scanning": 3, "total": 9}, "total": 10}, "query": "zod production", "serverTimeMS": 14}}