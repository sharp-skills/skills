{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "trutic"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Episodic buffer to bring LLM apps to prod"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "https://topoteretes.notion.site/Going-beyond-Langchain-<em>Weaviate</em>-and-towards-a-<em>production</em>-ready-modern-data-platform-7351d77a1eba40aab4394c24bef3a278?pvs=4"}}, "_tags": ["story", "author_trutic", "story_37166019"], "author": "trutic", "children": [37166020], "created_at": "2023-08-17T18:31:18Z", "created_at_i": 1692297078, "num_comments": 1, "objectID": "37166019", "points": 1, "story_id": 37166019, "title": "Episodic buffer to bring LLM apps to prod", "updated_at": "2024-09-20T14:52:13Z", "url": "https://topoteretes.notion.site/Going-beyond-Langchain-Weaviate-and-towards-a-production-ready-modern-data-platform-7351d77a1eba40aab4394c24bef3a278?pvs=4"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bobvanluijt"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["weaviate"], "value": "<em>Weaviate</em> releases hot, warm, and cold storage tiers"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "https://<em>weaviate</em>.io/blog/launching-into-<em>production</em>"}}, "_tags": ["story", "author_bobvanluijt", "story_41108818"], "author": "bobvanluijt", "children": [41108819], "created_at": "2024-07-30T13:13:02Z", "created_at_i": 1722345182, "num_comments": 1, "objectID": "41108818", "points": 3, "story_id": 41108818, "title": "Weaviate releases hot, warm, and cold storage tiers", "updated_at": "2024-09-20T17:30:10Z", "url": "https://weaviate.io/blog/launching-into-production"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kmassimilian"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["weaviate"], "value": "Been working on a robust Q&amp;A app for enterprise. I used llamaindex (+langchain) as a pipeline. Started using Chroma for my vector db, which worked pretty well, but I realized that my app runs faster when I store the indices in an S3 bucket rather than use Chroma to store my embeddings and generate the index from these embeddings at query time. Are there tradeoffs I'm making in using a pre-built index in S3 rather than a vector db to stash embeddings? Has anyone come across this kind of consideration? I've looked at <em>Weaviate</em> (offers hybrid search) but haven't decided to retool code based around it. Basically, I'm just looking for whichever implementation will result in the fastest response times (knowledge base size is 'large' ~40GB).<p>RE <em>Weaviate</em>, this looks interesting: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/<em>weaviate</em>/hybrid-search-with-<em>weaviate</em>-and-openai.ipynb<p>Further and related, has anyone tried to embed a larger amount of data before? I estimated total time using CPU ~29 hours. With GPU I've seen demos reducing this to minutes. https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray<p>TY"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Design considerations for RAG application in <em>production</em> mode"}}, "_tags": ["story", "author_kmassimilian", "story_37123940", "ask_hn"], "author": "kmassimilian", "created_at": "2023-08-14T17:34:27Z", "created_at_i": 1692034467, "num_comments": 0, "objectID": "37123940", "points": 3, "story_id": 37123940, "story_text": "Been working on a robust Q&amp;A app for enterprise. I used llamaindex (+langchain) as a pipeline. Started using Chroma for my vector db, which worked pretty well, but I realized that my app runs faster when I store the indices in an S3 bucket rather than use Chroma to store my embeddings and generate the index from these embeddings at query time. Are there tradeoffs I&#x27;m making in using a pre-built index in S3 rather than a vector db to stash embeddings? Has anyone come across this kind of consideration? I&#x27;ve looked at Weaviate (offers hybrid search) but haven&#x27;t decided to retool code based around it. Basically, I&#x27;m just looking for whichever implementation will result in the fastest response times (knowledge base size is &#x27;large&#x27; ~40GB).<p>RE Weaviate, this looks interesting: https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-cookbook&#x2F;blob&#x2F;main&#x2F;examples&#x2F;vector_databases&#x2F;weaviate&#x2F;hybrid-search-with-weaviate-and-openai.ipynb<p>Further and related, has anyone tried to embed a larger amount of data before? I estimated total time using CPU ~29 hours. With GPU I&#x27;ve seen demos reducing this to minutes. https:&#x2F;&#x2F;www.anyscale.com&#x2F;blog&#x2F;build-and-scale-a-powerful-query-engine-with-llamaindex-ray<p>TY", "title": "Design considerations for RAG application in production mode", "updated_at": "2024-09-20T14:47:35Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "lokahdev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "I built [VectorLiteDB (<a href=\"https://github.com/vectorlitedb/vectorlitedb\" rel=\"nofollow\">https://github.com/vectorlitedb/vectorlitedb</a>)<p>\u2014 a simple, embedded vector database that stores everything in a single file, just like SQLite.<p>The problem:<p>If you\u2019re a developer building AI apps, you usually have two choices for vector search<p>- Set up a server (e.g. Chroma, <em>Weaviate</em>)  \n- Use a cloud service (e.g. Pinecone)<p>That works for <em>production</em>, but it\u2019s overkill when you just want to:<p>- Quickly prototype with embeddings  \n- Run offline without cloud dependencies  \n- Keep your data portable in a single file<p>The inspiration was *SQLite* during development \u2014 simple, local, and reliable.<p>The solution:<p>So I built VectorLiteDB<p>- Single-file, embedded, no server  \n- Stores vectors + metadata, persists to disk  \n- Supports cosine / L2 / dot similarity  \n- Works offline, ~100ms for 10K vectors  \n- Perfect for local RAG, prototyping or personal AI memory<p>Feedback on both the tool and the approach would be really helpful.<p>- Is this something that would be useful\n- Use cases you\u2019d try this for"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: VectorLiteDB \u2013 a vector DB for local dev, like SQLite but for vectors"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/vectorlitedb/vectorlitedb"}}, "_tags": ["story", "author_lokahdev", "story_45319922", "show_hn"], "author": "lokahdev", "children": [45322272, 45336263], "created_at": "2025-09-21T03:54:25Z", "created_at_i": 1758426865, "num_comments": 4, "objectID": "45319922", "points": 13, "story_id": 45319922, "story_text": "I built [VectorLiteDB (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;vectorlitedb&#x2F;vectorlitedb\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vectorlitedb&#x2F;vectorlitedb</a>)<p>\u2014 a simple, embedded vector database that stores everything in a single file, just like SQLite.<p>The problem:<p>If you\u2019re a developer building AI apps, you usually have two choices for vector search<p>- Set up a server (e.g. Chroma, Weaviate)  \n- Use a cloud service (e.g. Pinecone)<p>That works for production, but it\u2019s overkill when you just want to:<p>- Quickly prototype with embeddings  \n- Run offline without cloud dependencies  \n- Keep your data portable in a single file<p>The inspiration was *SQLite* during development \u2014 simple, local, and reliable.<p>The solution:<p>So I built VectorLiteDB<p>- Single-file, embedded, no server  \n- Stores vectors + metadata, persists to disk  \n- Supports cosine &#x2F; L2 &#x2F; dot similarity  \n- Works offline, ~100ms for 10K vectors  \n- Perfect for local RAG, prototyping or personal AI memory<p>Feedback on both the tool and the approach would be really helpful.<p>- Is this something that would be useful\n- Use cases you\u2019d try this for", "title": "Show HN: VectorLiteDB \u2013 a vector DB for local dev, like SQLite but for vectors", "updated_at": "2025-09-26T11:39:07Z", "url": "https://github.com/vectorlitedb/vectorlitedb"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tikkun"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "I have a list of ai tools that I've used or want to use in my notes and I'm cleaning it up so I can share it.<p>Here's what I have so far of tools for building ai products:<p>You'll probably start with an LLM API - OpenAI\n - Cohere and others aren't as good\n - Anthropic's isn't available<p>If you're using embeddings\n - If you're working with a lot of items, you'll want a vector database, like Pinecone, or <em>Weaviate</em>, or pgvector<p>If you're building Q&amp;A over a document\n - I'd suggest using GPT Index<p>If you need to be able to interact with external data sources, do google searches, database lookups, python REPL\n - I'd suggest using langchain<p>If you're doing chained prompts\n - Check out dust tt and langchain<p>If you want to deploy a little app quickly\n - Check out Streamlit<p>If you need to use something like stable diffusion or whisper in your product\n - banana dev, modal, replicate, tiyaro ai, beam cloud, inferrd, or pipeline ai<p>If you need something to optimize your prompts\n - Check out Humanloop and Everyprompt<p>If you're building models and need an ml framework\n - PyTorch, Keras, TensorFlow<p>If you're deploying models to <em>production</em>\n -  Check out MLOps tools like MLflow, Kubeflow, Metaflow, Airflow, Seldon Core, TFServing<p>If you need to check out example projects for inspiration\n - Check out the pinecone op stack, the langchain gallery, the gpt index showcase, and the openai cookbook<p>If you want to browse the latest research, check out arXiv, of course"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: What tools for building AI products do you wish you'd discovered sooner?"}}, "_tags": ["story", "author_tikkun", "story_34764554", "ask_hn"], "author": "tikkun", "children": [34764572, 34764832, 34764922, 34765018], "created_at": "2023-02-12T17:11:48Z", "created_at_i": 1676221908, "num_comments": 7, "objectID": "34764554", "points": 6, "story_id": 34764554, "story_text": "I have a list of ai tools that I&#x27;ve used or want to use in my notes and I&#x27;m cleaning it up so I can share it.<p>Here&#x27;s what I have so far of tools for building ai products:<p>You&#x27;ll probably start with an LLM API - OpenAI\n - Cohere and others aren&#x27;t as good\n - Anthropic&#x27;s isn&#x27;t available<p>If you&#x27;re using embeddings\n - If you&#x27;re working with a lot of items, you&#x27;ll want a vector database, like Pinecone, or Weaviate, or pgvector<p>If you&#x27;re building Q&amp;A over a document\n - I&#x27;d suggest using GPT Index<p>If you need to be able to interact with external data sources, do google searches, database lookups, python REPL\n - I&#x27;d suggest using langchain<p>If you&#x27;re doing chained prompts\n - Check out dust tt and langchain<p>If you want to deploy a little app quickly\n - Check out Streamlit<p>If you need to use something like stable diffusion or whisper in your product\n - banana dev, modal, replicate, tiyaro ai, beam cloud, inferrd, or pipeline ai<p>If you need something to optimize your prompts\n - Check out Humanloop and Everyprompt<p>If you&#x27;re building models and need an ml framework\n - PyTorch, Keras, TensorFlow<p>If you&#x27;re deploying models to production\n -  Check out MLOps tools like MLflow, Kubeflow, Metaflow, Airflow, Seldon Core, TFServing<p>If you need to check out example projects for inspiration\n - Check out the pinecone op stack, the langchain gallery, the gpt index showcase, and the openai cookbook<p>If you want to browse the latest research, check out arXiv, of course", "title": "Ask HN: What tools for building AI products do you wish you'd discovered sooner?", "updated_at": "2025-05-26T04:16:27Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tcarambat1010"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "Hey HN,<p>At Mintplex Labs are building developer tools for AI applications. One area we encountered frustration was the use of Vector Databases like Pinecone, Chroma, QDrant, or <em>Weaviate</em> to &quot;unlock&quot; long-term memory and contextual answers. It is nearly impossible to manage this data when in use for <em>production</em>.<p>The craziest thing was how you cannot atomically CRUD any vectors in most of these vector databases. Let alone easily copy, clone, or migrate data or entire indexes without paying for re-embedding - among other things.<p>With VectorAdmin you get a database level UI with the ability to easily search for embeddings and atomically manage them on top of being a general tool suite for those using vector databases with LLMs.<p>Some things we have unlocked with Vector Admin:\n- Upload data directly into the vector db via a text doc or PDF\n- One click sync of entire existing vector databases\n- Migrating entire db's to another provider to escape vendor lock in\n- Ability to duplicate collections/namespaces to create dev-environments off <em>production</em> data at no cost\n- Be able to finally reset a vector database (provider agnostic)<p>and soon, be able to detect &quot;drift&quot; in semantic search results and catch it early before your <em>production</em> system starts providing wild context snippets.<p>VectorAdmin is open-source or hosted and has a 3-day trial. We really want HN's feedback on the issues or problem you are having wrangling the actual data in a vector database while building any LLM application."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: VectorAdmin \u2013 An open-source vector database management system"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://vectoradmin.com/"}}, "_tags": ["story", "author_tcarambat1010", "story_37846029", "show_hn"], "author": "tcarambat1010", "children": [37855104], "created_at": "2023-10-11T15:48:42Z", "created_at_i": 1697039322, "num_comments": 1, "objectID": "37846029", "points": 6, "story_id": 37846029, "story_text": "Hey HN,<p>At Mintplex Labs are building developer tools for AI applications. One area we encountered frustration was the use of Vector Databases like Pinecone, Chroma, QDrant, or Weaviate to &quot;unlock&quot; long-term memory and contextual answers. It is nearly impossible to manage this data when in use for production.<p>The craziest thing was how you cannot atomically CRUD any vectors in most of these vector databases. Let alone easily copy, clone, or migrate data or entire indexes without paying for re-embedding - among other things.<p>With VectorAdmin you get a database level UI with the ability to easily search for embeddings and atomically manage them on top of being a general tool suite for those using vector databases with LLMs.<p>Some things we have unlocked with Vector Admin:\n- Upload data directly into the vector db via a text doc or PDF\n- One click sync of entire existing vector databases\n- Migrating entire db&#x27;s to another provider to escape vendor lock in\n- Ability to duplicate collections&#x2F;namespaces to create dev-environments off production data at no cost\n- Be able to finally reset a vector database (provider agnostic)<p>and soon, be able to detect &quot;drift&quot; in semantic search results and catch it early before your production system starts providing wild context snippets.<p>VectorAdmin is open-source or hosted and has a 3-day trial. We really want HN&#x27;s feedback on the issues or problem you are having wrangling the actual data in a vector database while building any LLM application.", "title": "Show HN: VectorAdmin \u2013 An open-source vector database management system", "updated_at": "2024-09-20T15:24:21Z", "url": "https://vectoradmin.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "behnamoh"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "There are many options out there:<p>- FAISS<p>- Pinecone<p>- Milvus<p>- Qdrant<p>- <em>Weaviate</em><p>- Elasticsearch<p>- Vespa<p>- pgvector<p>- ScaNN<p>- Vald<p>I wonder which one is <em>production</em>-ready and has better features. I'm trying to stay away from Langchain, so if there are no integrations with Langchain that's fine."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Which Vector Database do you recommend for LLM applications?"}}, "_tags": ["story", "author_behnamoh", "story_36413626", "ask_hn"], "author": "behnamoh", "children": [36414342, 36414352, 36414672, 36415115, 36417118, 36420088, 36520456], "created_at": "2023-06-21T02:25:45Z", "created_at_i": 1687314345, "num_comments": 9, "objectID": "36413626", "points": 5, "story_id": 36413626, "story_text": "There are many options out there:<p>- FAISS<p>- Pinecone<p>- Milvus<p>- Qdrant<p>- Weaviate<p>- Elasticsearch<p>- Vespa<p>- pgvector<p>- ScaNN<p>- Vald<p>I wonder which one is production-ready and has better features. I&#x27;m trying to stay away from Langchain, so if there are no integrations with Langchain that&#x27;s fine.", "title": "Ask HN: Which Vector Database do you recommend for LLM applications?", "updated_at": "2024-09-20T14:22:41Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "manadu"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "Hi HN,\nNow that Elasticsearch supports ANN search with pre-filtering, version 8.2 onward, and there are many open source solutions such as Vespa, <em>Weaviate</em>, etc. that could also be easily hosted, what is a good solution to choose for a <em>production</em> system? Is there any benchmarking/comparisons that could be referred to in order to make the choice?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "What is the best ANN search solution with metadata pre-filtering?"}}, "_tags": ["story", "author_manadu", "story_32345501", "ask_hn"], "author": "manadu", "children": [32348472], "created_at": "2022-08-04T17:05:32Z", "created_at_i": 1659632732, "num_comments": 1, "objectID": "32345501", "points": 5, "story_id": 32345501, "story_text": "Hi HN,\nNow that Elasticsearch supports ANN search with pre-filtering, version 8.2 onward, and there are many open source solutions such as Vespa, Weaviate, etc. that could also be easily hosted, what is a good solution to choose for a production system? Is there any benchmarking&#x2F;comparisons that could be referred to in order to make the choice?", "title": "What is the best ANN search solution with metadata pre-filtering?", "updated_at": "2024-09-20T11:42:48Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pvpv"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "Hey HN, we are releasing IRPAPERS to answer a highly pragmatic question: when building a RAG pipeline over PDFs, should you OCR the text or just embed the raw page images?<p>Processing PDFs in <em>production</em> usually involves stringing together brittle OCR heuristics. While recent multimodal embeddings (like ColModernVBERT or ColPali) allow you to skip OCR entirely and retrieve directly from visual layouts, we wanted to measure if the computational overhead is actually worth the utility.<p>The short answer: Transformer-based image pipelines won't be perfect for every use-case, but they fix exactly what OCR breaks.<p>Here is what we found benchmarking 3,230 pages of dense scientific literature:<p>Complementary Bottlenecks: Text representations (BM25 + dense vectors) are highly efficient for exact lexical constraints (e.g., finding a specific acronym like &quot;HyDE&quot;). Conversely, image embeddings shine on spatial architecture diagrams and t-SNE plots where OCR serialization just turns into structural garbage.<p>Multimodal Hybrid Search: Because these failure modes are almost perfectly orthogonal, fusing the two signals gives you the best performance out of the box. By combining them, we pushed top-1 recall to 49% (beating text alone at 46%).<p>The Memory Constraint: Late-interaction image embeddings produce thousands of vectors per page, creating a massive storage bottleneck. To address this need, we evaluate MUVERA encoding. Under the hood, this compresses multi-vector representations into a single fixed-dimensional encoding via SimHash, allowing you to use standard HNSW indexing without the paralyzing memory overhead.<p>In practice, if you are building a RAG workflow today, text-based context still provides higher downstream utility for the actual generation step (0.82 vs 0.71 alignment). Instead of picking one modality and dealing with its blind spots, start with hybrid text search as a sensible default, and inject multi-vector image embeddings to catch the visual edge-cases.<p>We\u2019ve open-sourced the benchmark and the evaluation recipes:<p>Paper <a href=\"https://arxiv.org/abs/2602.17687\" rel=\"nofollow\">https://arxiv.org/abs/2602.17687</a>\nIRPAPERS dataset on HuggingFace at huggingface.co/<em>weaviate</em>/IRPAPERS and GitHub\nat github.com/<em>weaviate</em>/IRPAPERS<p>Our experimental code is also available on GitHub at\ngithub.com/<em>weaviate</em>/query-agent-benchmarking<p>Happy to answer any questions about the evaluation pipeline, the cold start problem of visual benchmarks, or the specific retrieval trade-offs we saw."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Irpapers \u2013 Visual embeddings vs. OCR trade-offs in scientific PDFs"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["weaviate"], "value": "https://github.com/<em>weaviate</em>/query-agent-benchmarking"}}, "_tags": ["story", "author_pvpv", "story_47125210", "show_hn"], "author": "pvpv", "created_at": "2026-02-23T17:10:50Z", "created_at_i": 1771866650, "num_comments": 0, "objectID": "47125210", "points": 5, "story_id": 47125210, "story_text": "Hey HN, we are releasing IRPAPERS to answer a highly pragmatic question: when building a RAG pipeline over PDFs, should you OCR the text or just embed the raw page images?<p>Processing PDFs in production usually involves stringing together brittle OCR heuristics. While recent multimodal embeddings (like ColModernVBERT or ColPali) allow you to skip OCR entirely and retrieve directly from visual layouts, we wanted to measure if the computational overhead is actually worth the utility.<p>The short answer: Transformer-based image pipelines won&#x27;t be perfect for every use-case, but they fix exactly what OCR breaks.<p>Here is what we found benchmarking 3,230 pages of dense scientific literature:<p>Complementary Bottlenecks: Text representations (BM25 + dense vectors) are highly efficient for exact lexical constraints (e.g., finding a specific acronym like &quot;HyDE&quot;). Conversely, image embeddings shine on spatial architecture diagrams and t-SNE plots where OCR serialization just turns into structural garbage.<p>Multimodal Hybrid Search: Because these failure modes are almost perfectly orthogonal, fusing the two signals gives you the best performance out of the box. By combining them, we pushed top-1 recall to 49% (beating text alone at 46%).<p>The Memory Constraint: Late-interaction image embeddings produce thousands of vectors per page, creating a massive storage bottleneck. To address this need, we evaluate MUVERA encoding. Under the hood, this compresses multi-vector representations into a single fixed-dimensional encoding via SimHash, allowing you to use standard HNSW indexing without the paralyzing memory overhead.<p>In practice, if you are building a RAG workflow today, text-based context still provides higher downstream utility for the actual generation step (0.82 vs 0.71 alignment). Instead of picking one modality and dealing with its blind spots, start with hybrid text search as a sensible default, and inject multi-vector image embeddings to catch the visual edge-cases.<p>We\u2019ve open-sourced the benchmark and the evaluation recipes:<p>Paper <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2602.17687\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2602.17687</a>\nIRPAPERS dataset on HuggingFace at huggingface.co&#x2F;weaviate&#x2F;IRPAPERS and GitHub\nat github.com&#x2F;weaviate&#x2F;IRPAPERS<p>Our experimental code is also available on GitHub at\ngithub.com&#x2F;weaviate&#x2F;query-agent-benchmarking<p>Happy to answer any questions about the evaluation pipeline, the cold start problem of visual benchmarks, or the specific retrieval trade-offs we saw.", "title": "Show HN: Irpapers \u2013 Visual embeddings vs. OCR trade-offs in scientific PDFs", "updated_at": "2026-02-24T06:49:56Z", "url": "https://github.com/weaviate/query-agent-benchmarking"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "hjm1980"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "I\u2019m the creator of Spring AI Playground, a self-hosted web UI built on Spring AI\u2014designed to make testing and experimenting with AI tools more straightforward for Java developers.<p>What it is\nRuns locally, lets you experiment with:\n \u2022 LLM providers (Ollama is the default, but you can switch to OpenAI, Anthropic, Microsoft, Google, and more.).\n \u2022 RAG workflows: upload documents, chunk, embed, search with scoring, and filter metadata.\n \u2022 A visual MCP (Model Context Protocol, a spec for connecting models to external tools) Playground to set up tool integrations (HTTP, STDIO, SSE), inspect tool metadata, and call them from a chat interface.<p>I wanted a place to play with RAG workflows and external tool calls without wiring up a full app or handling API boilerplate.<p>GitHub: <a href=\"https://github.com/JM-Lab/spring-ai-playground\" rel=\"nofollow\">https://github.com/JM-Lab/spring-ai-playground</a><p>Why it\u2019s different\n \u2022 Built with Spring AI\u2014no new language or framework if you\u2019re already in the Spring ecosystem.\n \u2022 Zero API key setup (thanks to Ollama support), though you can switch to OpenAI easily.\n \u2022 Vector DB agnostic\u2014plug in providers like Pinecone, Milvus, PGVector, <em>Weaviate</em>, Elasticsearch, Redis, and many more.\n \u2022 Live MCP debugging\u2014inspect tools, tweak arguments, see execution history in one place.\n \u2022 Everything runs locally\u2014your data stays on your machine (Docker or native).\n \u2022 Built with Spring Boot DevTools\u2014fast application restarts when you modify configurations or experiment with different settings.<p>Background\nI built this after repeating the same setup tasks for Spring AI. I wanted a simple sandbox to prototype ideas faster. Currently lacks authentication, user management, and <em>production</em>-ready features, but it already cuts my prototyping time in half.<p>Feedback welcome\nIf you try it, I\u2019d love to know:\n \u2022 Was anything confusing or hard to use?\n \u2022 Any rough edges or gotchas you\u2019d like fixed?<p>Jemin Huh"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Spring AI Playground \u2013 Self-Hosted Web UI for MCP, RAG and LLM"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/JM-Lab/spring-ai-playground"}}, "_tags": ["story", "author_hjm1980", "story_45036481", "show_hn"], "author": "hjm1980", "created_at": "2025-08-27T07:24:32Z", "created_at_i": 1756279472, "num_comments": 0, "objectID": "45036481", "points": 3, "story_id": 45036481, "story_text": "I\u2019m the creator of Spring AI Playground, a self-hosted web UI built on Spring AI\u2014designed to make testing and experimenting with AI tools more straightforward for Java developers.<p>What it is\nRuns locally, lets you experiment with:\n \u2022 LLM providers (Ollama is the default, but you can switch to OpenAI, Anthropic, Microsoft, Google, and more.).\n \u2022 RAG workflows: upload documents, chunk, embed, search with scoring, and filter metadata.\n \u2022 A visual MCP (Model Context Protocol, a spec for connecting models to external tools) Playground to set up tool integrations (HTTP, STDIO, SSE), inspect tool metadata, and call them from a chat interface.<p>I wanted a place to play with RAG workflows and external tool calls without wiring up a full app or handling API boilerplate.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;JM-Lab&#x2F;spring-ai-playground\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;JM-Lab&#x2F;spring-ai-playground</a><p>Why it\u2019s different\n \u2022 Built with Spring AI\u2014no new language or framework if you\u2019re already in the Spring ecosystem.\n \u2022 Zero API key setup (thanks to Ollama support), though you can switch to OpenAI easily.\n \u2022 Vector DB agnostic\u2014plug in providers like Pinecone, Milvus, PGVector, Weaviate, Elasticsearch, Redis, and many more.\n \u2022 Live MCP debugging\u2014inspect tools, tweak arguments, see execution history in one place.\n \u2022 Everything runs locally\u2014your data stays on your machine (Docker or native).\n \u2022 Built with Spring Boot DevTools\u2014fast application restarts when you modify configurations or experiment with different settings.<p>Background\nI built this after repeating the same setup tasks for Spring AI. I wanted a simple sandbox to prototype ideas faster. Currently lacks authentication, user management, and production-ready features, but it already cuts my prototyping time in half.<p>Feedback welcome\nIf you try it, I\u2019d love to know:\n \u2022 Was anything confusing or hard to use?\n \u2022 Any rough edges or gotchas you\u2019d like fixed?<p>Jemin Huh", "title": "Show HN: Spring AI Playground \u2013 Self-Hosted Web UI for MCP, RAG and LLM", "updated_at": "2025-09-03T19:55:47Z", "url": "https://github.com/JM-Lab/spring-ai-playground"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mimchak"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "I launched Embex two weeks ago and hit 9,000 downloads (7K PyPI, 2K npm).<p>Embex is a universal ORM for vector databases. One API that works across LanceDB, Qdrant, Pinecone, Chroma, PgVector, Milvus, and <em>Weaviate</em>.<p>The problem: Every vector database has a different API. Switching from Pinecone to Qdrant means rewriting your data layer.<p>Example:\n```python\n# Works with ANY provider\nclient = await EmbexClient.new_async(provider=&quot;lancedb&quot;, url=&quot;./data&quot;)\nawait client.insert(&quot;products&quot;, vectors)\nresults = await client.search(&quot;products&quot;, vector=query, top_k=5)<p># Switch to Qdrant? Change one line:\nclient = await EmbexClient.new_async(provider=&quot;qdrant&quot;, url=&quot;http://localhost:6333&quot;)\n```<p>Built with Rust core + SIMD acceleration (4x faster than pure Python/JS). Available for Python and Node.js.<p>What happened after launch:\n- Published to PyPI and npm\n- Downloads started coming in organically\n- 9K downloads in 2 weeks<p>I'm honestly not sure where most of the traffic came from. PyPI/npm search probably, but I haven't dug into the analytics deeply. I just made one LinkedIn post which didn't get any likes/comments.<p>Start local with LanceDB (embedded, zero Docker), then switch to <em>production</em> databases (Qdrant, Pinecone, Milvus) without changing code.<p>GitHub: <a href=\"https://github.com/bridgerust/bridgerust\" rel=\"nofollow\">https://github.com/bridgerust/bridgerust</a>\nDocs: <a href=\"https://bridgerust.dev/embex/introduction\" rel=\"nofollow\">https://bridgerust.dev/embex/introduction</a><p>Would love feedback on:\n- API design decisions\n- Which databases to support next\n- Performance optimizations<p>Built using BridgeRust, a framework for creating cross-language Rust libraries (also open source in this repo)."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Embex \u2013 9K organic downloads in 2 weeks with zero marketing"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.bridgerust.dev/embex/introduction/"}}, "_tags": ["story", "author_mimchak", "story_46571103", "show_hn"], "author": "mimchak", "created_at": "2026-01-10T23:37:26Z", "created_at_i": 1768088246, "num_comments": 0, "objectID": "46571103", "points": 2, "story_id": 46571103, "story_text": "I launched Embex two weeks ago and hit 9,000 downloads (7K PyPI, 2K npm).<p>Embex is a universal ORM for vector databases. One API that works across LanceDB, Qdrant, Pinecone, Chroma, PgVector, Milvus, and Weaviate.<p>The problem: Every vector database has a different API. Switching from Pinecone to Qdrant means rewriting your data layer.<p>Example:\n```python\n# Works with ANY provider\nclient = await EmbexClient.new_async(provider=&quot;lancedb&quot;, url=&quot;.&#x2F;data&quot;)\nawait client.insert(&quot;products&quot;, vectors)\nresults = await client.search(&quot;products&quot;, vector=query, top_k=5)<p># Switch to Qdrant? Change one line:\nclient = await EmbexClient.new_async(provider=&quot;qdrant&quot;, url=&quot;http:&#x2F;&#x2F;localhost:6333&quot;)\n```<p>Built with Rust core + SIMD acceleration (4x faster than pure Python&#x2F;JS). Available for Python and Node.js.<p>What happened after launch:\n- Published to PyPI and npm\n- Downloads started coming in organically\n- 9K downloads in 2 weeks<p>I&#x27;m honestly not sure where most of the traffic came from. PyPI&#x2F;npm search probably, but I haven&#x27;t dug into the analytics deeply. I just made one LinkedIn post which didn&#x27;t get any likes&#x2F;comments.<p>Start local with LanceDB (embedded, zero Docker), then switch to production databases (Qdrant, Pinecone, Milvus) without changing code.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;bridgerust&#x2F;bridgerust\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;bridgerust&#x2F;bridgerust</a>\nDocs: <a href=\"https:&#x2F;&#x2F;bridgerust.dev&#x2F;embex&#x2F;introduction\" rel=\"nofollow\">https:&#x2F;&#x2F;bridgerust.dev&#x2F;embex&#x2F;introduction</a><p>Would love feedback on:\n- API design decisions\n- Which databases to support next\n- Performance optimizations<p>Built using BridgeRust, a framework for creating cross-language Rust libraries (also open source in this repo).", "title": "Show HN: Embex \u2013 9K organic downloads in 2 weeks with zero marketing", "updated_at": "2026-01-11T08:55:11Z", "url": "https://www.bridgerust.dev/embex/introduction/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "CULPRITCHAOS"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "Hi HN \u2014 I built this and I\u2019m explicitly asking skeptics to tear it apart.\u201d\nInterlock is a safety and certification layer for AI infrastructure, not an optimizer or a vector database.<p>The problem I am solving for is that AI systems (vector search, RAG pipelines, agent frameworks) don\u2019t usually fail cleanly \u2014 they degrade silently, oscillate under load, or keep returning corrupted results until something crashes. Monitoring tells you after the fact; circuit breakers tend to be static and blind to context.<p>Interlock tries to address that by:<p>forecasting time-to-failure under stress<p>intervening before hard limits are reached<p>refusing to serve results when confidence collapses<p>producing cryptographically signed evidence of what happened (control vs protected runs)<p>It includes:<p>integrations with FAISS, Pinecone, <em>Weaviate</em>, Milvus, LangChain, LlamaIndex (Elasticsearch experimental)<p>TypeScript + Python support<p>automated stress tests (control vs protected)<p>long-run stability tests<p>certification classes (I\u2013V) derived from actual configuration + behavior, not labels<p>Importantly: Interlock does not guarantee correctness or uptime. It certifies that a given configuration survived a defined stress test without crashing, oscillating, or serving degraded results \u2014 similar to a structural load rating rather than a promise.<p>The repo is fully open source, and all claims link to test artifacts and CI runs. I\u2019m especially interested in feedback on:<p>failure modes this wouldn\u2019t catch<p>where the certification model is too strict or too weak<p>whether this is actually useful in real <em>production</em> AI systems<p>Repo: <a href=\"https://github.com/CULPRITCHAOS/Interlock\" rel=\"nofollow\">https://github.com/CULPRITCHAOS/Interlock</a><p>Happy to answer questions or be told why this is a bad idea lol"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: I built a circuit breaker that predicts AI failures"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/CULPRITCHAOS/Interlock"}}, "_tags": ["story", "author_CULPRITCHAOS", "story_46264972", "show_hn"], "author": "CULPRITCHAOS", "created_at": "2025-12-14T17:40:05Z", "created_at_i": 1765734005, "num_comments": 0, "objectID": "46264972", "points": 1, "story_id": 46264972, "story_text": "Hi HN \u2014 I built this and I\u2019m explicitly asking skeptics to tear it apart.\u201d\nInterlock is a safety and certification layer for AI infrastructure, not an optimizer or a vector database.<p>The problem I am solving for is that AI systems (vector search, RAG pipelines, agent frameworks) don\u2019t usually fail cleanly \u2014 they degrade silently, oscillate under load, or keep returning corrupted results until something crashes. Monitoring tells you after the fact; circuit breakers tend to be static and blind to context.<p>Interlock tries to address that by:<p>forecasting time-to-failure under stress<p>intervening before hard limits are reached<p>refusing to serve results when confidence collapses<p>producing cryptographically signed evidence of what happened (control vs protected runs)<p>It includes:<p>integrations with FAISS, Pinecone, Weaviate, Milvus, LangChain, LlamaIndex (Elasticsearch experimental)<p>TypeScript + Python support<p>automated stress tests (control vs protected)<p>long-run stability tests<p>certification classes (I\u2013V) derived from actual configuration + behavior, not labels<p>Importantly: Interlock does not guarantee correctness or uptime. It certifies that a given configuration survived a defined stress test without crashing, oscillating, or serving degraded results \u2014 similar to a structural load rating rather than a promise.<p>The repo is fully open source, and all claims link to test artifacts and CI runs. I\u2019m especially interested in feedback on:<p>failure modes this wouldn\u2019t catch<p>where the certification model is too strict or too weak<p>whether this is actually useful in real production AI systems<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;CULPRITCHAOS&#x2F;Interlock\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;CULPRITCHAOS&#x2F;Interlock</a><p>Happy to answer questions or be told why this is a bad idea lol", "title": "Show HN: I built a circuit breaker that predicts AI failures", "updated_at": "2025-12-14T17:43:00Z", "url": "https://github.com/CULPRITCHAOS/Interlock"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Applied-AI-Dev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "I\u2019ve turned my open-source Intelligence Hub into a hosted service so teams can ship reliable, model-agnostic LLM features without wrestling infra.<p>The service includes a robust free tier, and until October 1st, all free user's will automatically have access to the &quot;developer&quot; paid tier features.<p>What you get:<p>One API for Azure OpenAI, OpenAI, Anthropic, allowing you to swap models on the fly<p>AI Agent &quot;Profiles&quot; for consistent and secure prompting and configuration<p>RAG pipeline setup for <em>Weaviate</em> (Azure AI Search is also supported for enterprises) + tool calls running in parallel<p>Tool Call Execution, allowing you to send tool call arguments directly to 3rd party APIs<p>Built-in conversation history retrieved via a conversation id<p>Baked-in security and resiliency; retries, backoff, fallbacks ensuring your apps are secure and never fail<p>Don't worry, the open source version isn\u2019t going anywhere\u2014DIY folks can still self-host. But if you want boringly reliable, <em>production</em>-ready LLM plumbing, the managed service is for you.<p>Live service:  <a href=\"https://theintelligencehub.azurewebsites.net/\" rel=\"nofollow\">https://theintelligencehub.azurewebsites.net/</a>\n Open-source repo:  <a href=\"https://github.com/AppliedAI-Org/IntelligenceHub\" rel=\"nofollow\">https://github.com/AppliedAI-Org/IntelligenceHub</a><p>If you\u2019re building with AI, I\u2019d love to hear your use cases, feedback, or any features you'd like added."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: AI Interoperability to the Max \u2013 The Intelligence Hub"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://theintelligencehub.azurewebsites.net/"}}, "_tags": ["story", "author_Applied-AI-Dev", "story_44892476", "show_hn"], "author": "Applied-AI-Dev", "created_at": "2025-08-13T19:05:56Z", "created_at_i": 1755111956, "num_comments": 0, "objectID": "44892476", "points": 1, "story_id": 44892476, "story_text": "I\u2019ve turned my open-source Intelligence Hub into a hosted service so teams can ship reliable, model-agnostic LLM features without wrestling infra.<p>The service includes a robust free tier, and until October 1st, all free user&#x27;s will automatically have access to the &quot;developer&quot; paid tier features.<p>What you get:<p>One API for Azure OpenAI, OpenAI, Anthropic, allowing you to swap models on the fly<p>AI Agent &quot;Profiles&quot; for consistent and secure prompting and configuration<p>RAG pipeline setup for Weaviate (Azure AI Search is also supported for enterprises) + tool calls running in parallel<p>Tool Call Execution, allowing you to send tool call arguments directly to 3rd party APIs<p>Built-in conversation history retrieved via a conversation id<p>Baked-in security and resiliency; retries, backoff, fallbacks ensuring your apps are secure and never fail<p>Don&#x27;t worry, the open source version isn\u2019t going anywhere\u2014DIY folks can still self-host. But if you want boringly reliable, production-ready LLM plumbing, the managed service is for you.<p>Live service:  <a href=\"https:&#x2F;&#x2F;theintelligencehub.azurewebsites.net&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;theintelligencehub.azurewebsites.net&#x2F;</a>\n Open-source repo:  <a href=\"https:&#x2F;&#x2F;github.com&#x2F;AppliedAI-Org&#x2F;IntelligenceHub\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;AppliedAI-Org&#x2F;IntelligenceHub</a><p>If you\u2019re building with AI, I\u2019d love to hear your use cases, feedback, or any features you&#x27;d like added.", "title": "Show HN: AI Interoperability to the Max \u2013 The Intelligence Hub", "updated_at": "2025-08-19T00:11:21Z", "url": "https://theintelligencehub.azurewebsites.net/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "CShorten"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "Hey everyone! I am SUPER EXCITED to share our new interview with Jason Liu! Jason is one of the world's rising stars in AI, and I am so proud to publish this podcast. It was jointly one of the most fun and technical podcasts we've done!\nJason has a very interesting background and positioning in the space of AI. I would say perhaps firstly, he is the creator of Instructor, one of the fastest growing LLM frameworks. Instructor leverages Pydantic type checking to achieve structured output parsing. This jointly cleans up code bases leveraging LLMs and prompting, as well as achieves tons of new functionality, especially applicable to RAG-based systems.\nThis is where Jason's background picks up in novelty, thanks to his experience at StitchFix, Jason has a ton of expertise in Recommendation systems and he describes in tremendous detail throughout the podcast how these lessons translate to RAG. Jason is currently working as an independent consultant at the forefront of bringing this technology to <em>production</em> software. Jason has also written many amazing blog posts on RAG, one of my favorite ones published on January 23rd, 2024 is &quot;Embedding English Wikipedia in under 15 minutes&quot; on modal.com, as well as his writings on https://jxnl.github.io/blog/.\nThank you so much Jason, one of my favorite <em>Weaviate</em> Podcast of all time!\nYouTube: https://www.youtube.com/watch?v=higlHgYDc5E\nSpotify: https://spotifyanchor-web.app.link/e/NQPIpdHzbHb"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Instructor with Jason Liu"}}, "_tags": ["story", "author_CShorten", "story_39370812", "ask_hn"], "author": "CShorten", "created_at": "2024-02-14T15:22:05Z", "created_at_i": 1707924125, "num_comments": 0, "objectID": "39370812", "points": 1, "story_id": 39370812, "story_text": "Hey everyone! I am SUPER EXCITED to share our new interview with Jason Liu! Jason is one of the world&#x27;s rising stars in AI, and I am so proud to publish this podcast. It was jointly one of the most fun and technical podcasts we&#x27;ve done!\nJason has a very interesting background and positioning in the space of AI. I would say perhaps firstly, he is the creator of Instructor, one of the fastest growing LLM frameworks. Instructor leverages Pydantic type checking to achieve structured output parsing. This jointly cleans up code bases leveraging LLMs and prompting, as well as achieves tons of new functionality, especially applicable to RAG-based systems.\nThis is where Jason&#x27;s background picks up in novelty, thanks to his experience at StitchFix, Jason has a ton of expertise in Recommendation systems and he describes in tremendous detail throughout the podcast how these lessons translate to RAG. Jason is currently working as an independent consultant at the forefront of bringing this technology to production software. Jason has also written many amazing blog posts on RAG, one of my favorite ones published on January 23rd, 2024 is &quot;Embedding English Wikipedia in under 15 minutes&quot; on modal.com, as well as his writings on https:&#x2F;&#x2F;jxnl.github.io&#x2F;blog&#x2F;.\nThank you so much Jason, one of my favorite Weaviate Podcast of all time!\nYouTube: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=higlHgYDc5E\nSpotify: https:&#x2F;&#x2F;spotifyanchor-web.app.link&#x2F;e&#x2F;NQPIpdHzbHb", "title": "Instructor with Jason Liu", "updated_at": "2024-09-20T16:28:17Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yubainu"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["weaviate", "production"], "value": "I built SIB-ENGINE, a real-time hallucination detection system\nthat monitors LLM internal structure rather than output content.<p>KEY RESULTS (Gemma-2B, N=1000):<p>\u2022 54% hallucination detection with 7% false positive rate<p>\u2022 &lt;1% computational overhead (runs on RTX 3050 with 4GB VRAM)<p>\u2022 ROC-AUC: 0.8995<p>WHY IT'S DIFFERENT:<p>Traditional methods analyze the output text semantically.<p>SIB-ENGINE monitors &quot;geometric drift&quot; in hidden states during generation - identifying the structural collapse of the latent space before the first incorrect token is sampled.<p>This approach offers unique advantages:<p>\u2022 Real-time intervention: Stop generation mid-stream<p>\u2022 Language-agnostic: No semantic analysis needed<p>\u2022 Privacy-preserving: Never reads the actual content<p>\u2022 Extremely lightweight: Works on consumer hardware<p>HOW IT WORKS:\nSIB-ENGINE monitors the internal stability of the model's computation. While the system utilizes multiple structural signals to detect instability, two primary indicators include:<p>Representation Stability: Tracking how the initial intent is preserved or distorted as it moves through the model's transformation space.<p>Cross-Layer Alignment: Monitoring the consensus of information processing across different neural depths to identify early-stage divergence.<p>When these (and other proprietary structural signals) <em>deviate</em> from the expected stable manifold, the system flags a potential hallucination before it manifests in the output.<p>DEMO &amp; CODE:<p>\u2022 Demo video: <a href=\"https://www.youtube.com/watch?v=H1_zDC0SXQ8\" rel=\"nofollow\">https://www.youtube.com/watch?v=H1_zDC0SXQ8</a><p>\u2022 GitHub: <a href=\"https://github.com/yubainu/sibainu-engine\" rel=\"nofollow\">https://github.com/yubainu/sibainu-engine</a><p>\u2022 Raw data: raw_logs.csv (full transparency)<p>LIMITATIONS:<p>\u2022 Tested on Gemma-2B only (2.5B parameters)<p>\u2022 Designed to scale, but needs validation on larger models<p>\u2022 Catches &quot;structurally unstable&quot; hallucinations (about half)<p>\u2022 Best used as first-line defense in ensemble systems<p>TECHNICAL NOTES:<p>\u2022 No external models needed (unlike self-consistency methods)<p>\u2022 No knowledge bases required (unlike RAG approaches)<p>\u2022 Adds ~1% inference time vs. 300-500% for semantic methods<p>\u2022 Works by monitoring the process not the product<p>I'd love feedback on:<p>\u2022 Validation on larger models (Seeking strategic partnerships and compute resources for large-scale validation.)<p>\u2022 Integration patterns for <em>production</em> systems<p>\u2022 Comparison with other structural approaches<p>\u2022 Edge cases where geometric signals fail<p>This represents a fundamentally different paradigm: instead of\nasking &quot;is this text correct?&quot;, we ask &quot;was the generation process\nunstable?&quot; The answer is surprisingly informative.<p>Happy to discuss technical details in the comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Running hallucination detection on a $200 GPU (RTX 3050, 4GB)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/yubainu/sibainu-engine"}}, "_tags": ["story", "author_yubainu", "story_47159047", "show_hn"], "author": "yubainu", "children": [47159061, 47159697], "created_at": "2026-02-25T22:36:49Z", "created_at_i": 1772059009, "num_comments": 3, "objectID": "47159047", "points": 2, "story_id": 47159047, "story_text": "I built SIB-ENGINE, a real-time hallucination detection system\nthat monitors LLM internal structure rather than output content.<p>KEY RESULTS (Gemma-2B, N=1000):<p>\u2022 54% hallucination detection with 7% false positive rate<p>\u2022 &lt;1% computational overhead (runs on RTX 3050 with 4GB VRAM)<p>\u2022 ROC-AUC: 0.8995<p>WHY IT&#x27;S DIFFERENT:<p>Traditional methods analyze the output text semantically.<p>SIB-ENGINE monitors &quot;geometric drift&quot; in hidden states during generation - identifying the structural collapse of the latent space before the first incorrect token is sampled.<p>This approach offers unique advantages:<p>\u2022 Real-time intervention: Stop generation mid-stream<p>\u2022 Language-agnostic: No semantic analysis needed<p>\u2022 Privacy-preserving: Never reads the actual content<p>\u2022 Extremely lightweight: Works on consumer hardware<p>HOW IT WORKS:\nSIB-ENGINE monitors the internal stability of the model&#x27;s computation. While the system utilizes multiple structural signals to detect instability, two primary indicators include:<p>Representation Stability: Tracking how the initial intent is preserved or distorted as it moves through the model&#x27;s transformation space.<p>Cross-Layer Alignment: Monitoring the consensus of information processing across different neural depths to identify early-stage divergence.<p>When these (and other proprietary structural signals) deviate from the expected stable manifold, the system flags a potential hallucination before it manifests in the output.<p>DEMO &amp; CODE:<p>\u2022 Demo video: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=H1_zDC0SXQ8\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=H1_zDC0SXQ8</a><p>\u2022 GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;yubainu&#x2F;sibainu-engine\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;yubainu&#x2F;sibainu-engine</a><p>\u2022 Raw data: raw_logs.csv (full transparency)<p>LIMITATIONS:<p>\u2022 Tested on Gemma-2B only (2.5B parameters)<p>\u2022 Designed to scale, but needs validation on larger models<p>\u2022 Catches &quot;structurally unstable&quot; hallucinations (about half)<p>\u2022 Best used as first-line defense in ensemble systems<p>TECHNICAL NOTES:<p>\u2022 No external models needed (unlike self-consistency methods)<p>\u2022 No knowledge bases required (unlike RAG approaches)<p>\u2022 Adds ~1% inference time vs. 300-500% for semantic methods<p>\u2022 Works by monitoring the process not the product<p>I&#x27;d love feedback on:<p>\u2022 Validation on larger models (Seeking strategic partnerships and compute resources for large-scale validation.)<p>\u2022 Integration patterns for production systems<p>\u2022 Comparison with other structural approaches<p>\u2022 Edge cases where geometric signals fail<p>This represents a fundamentally different paradigm: instead of\nasking &quot;is this text correct?&quot;, we ask &quot;was the generation process\nunstable?&quot; The answer is surprisingly informative.<p>Happy to discuss technical details in the comments!", "title": "Show HN: Running hallucination detection on a $200 GPU (RTX 3050, 4GB)", "updated_at": "2026-02-26T22:56:22Z", "url": "https://github.com/yubainu/sibainu-engine"}], "hitsPerPage": 15, "nbHits": 15, "nbPages": 1, "page": 0, "params": "query=weaviate+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 11, "processingTimingsMS": {"_request": {"roundTrip": 18}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 8, "scanning": 1, "total": 10}, "total": 11}, "query": "weaviate production", "serverTimeMS": 13}}