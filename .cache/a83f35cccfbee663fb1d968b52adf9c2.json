{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "defilan"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "Hi HN! I built LLMKube, a Kubernetes operator for deploying GPU-accelerated LLMs in <em>production</em>. One command gets you from zero to inference with full observability.<p>Why this exists: Regulated industries (healthcare, defense, finance) need air-gapped LLM\ndeployments, but existing tools are either single-node only (Ollama) or lack GPU optimization and\nSLO enforcement. LLMKube bridges the gap.<p>What's working:<p>- 17x speedup with NVIDIA GPUs (64 tok/s on Llama 3.2 3B vs 4.6 tok/s CPU)<p>- One command: llmkube deploy llama-3b --gpu (auto CUDA setup, scheduling, layer offloading)<p>- <em>Production</em> observability: <em>Prometheus</em> + Grafana + DCGM GPU metrics out of the box<p>- OpenAI-compatible API endpoints<p>- Terraform configs for GKE GPU clusters with auto-scale to zero<p>Tech: Kubernetes CRDs, llama.cpp with CUDA, NVIDIA GPU Operator, cost-optimized spot instances\n(~$50-150/mo dev workloads).<p>Status: v0.2.0 <em>production</em>-ready for single-GPU deployments on standard K8s clusters. Multi-GPU and\nmulti-node model sharding on the roadmap.<p>Apache 2.0 licensed. Would love feedback from anyone running LLMs in <em>production</em>!<p>Website: <a href=\"https://llmkube.com\" rel=\"nofollow\">https://llmkube.com</a><p>GitHub: <a href=\"https://github.com/Defilan/LLMKube\" rel=\"nofollow\">https://github.com/Defilan/LLMKube</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LLMKube \u2013 Kubernetes for Local LLMs with GPU Acceleration"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/defilantech/LLMKube"}}, "_tags": ["story", "author_defilan", "story_45968719", "show_hn"], "author": "defilan", "created_at": "2025-11-18T16:47:49Z", "created_at_i": 1763484469, "num_comments": 0, "objectID": "45968719", "points": 5, "story_id": 45968719, "story_text": "Hi HN! I built LLMKube, a Kubernetes operator for deploying GPU-accelerated LLMs in production. One command gets you from zero to inference with full observability.<p>Why this exists: Regulated industries (healthcare, defense, finance) need air-gapped LLM\ndeployments, but existing tools are either single-node only (Ollama) or lack GPU optimization and\nSLO enforcement. LLMKube bridges the gap.<p>What&#x27;s working:<p>- 17x speedup with NVIDIA GPUs (64 tok&#x2F;s on Llama 3.2 3B vs 4.6 tok&#x2F;s CPU)<p>- One command: llmkube deploy llama-3b --gpu (auto CUDA setup, scheduling, layer offloading)<p>- Production observability: Prometheus + Grafana + DCGM GPU metrics out of the box<p>- OpenAI-compatible API endpoints<p>- Terraform configs for GKE GPU clusters with auto-scale to zero<p>Tech: Kubernetes CRDs, llama.cpp with CUDA, NVIDIA GPU Operator, cost-optimized spot instances\n(~$50-150&#x2F;mo dev workloads).<p>Status: v0.2.0 production-ready for single-GPU deployments on standard K8s clusters. Multi-GPU and\nmulti-node model sharding on the roadmap.<p>Apache 2.0 licensed. Would love feedback from anyone running LLMs in production!<p>Website: <a href=\"https:&#x2F;&#x2F;llmkube.com\" rel=\"nofollow\">https:&#x2F;&#x2F;llmkube.com</a><p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Defilan&#x2F;LLMKube\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Defilan&#x2F;LLMKube</a>", "title": "Show HN: LLMKube \u2013 Kubernetes for Local LLMs with GPU Acceleration", "updated_at": "2025-11-18T17:12:51Z", "url": "https://github.com/defilantech/LLMKube"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "nbroyal"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["prometheus"], "value": "M3 v1.0 released: <em>Prometheus</em> metrics store scalable to 100s of nodes"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "https://medium.com/chronosphere/m3-v1-0-released-a-<em>production</em>-ready-<em>prometheus</em>-metrics-store-scalable-to-hundreds-of-nodes-7f673fac4334"}}, "_tags": ["story", "author_nbroyal", "story_25290193"], "author": "nbroyal", "children": [25290210, 25290913], "created_at": "2020-12-03T16:03:02Z", "created_at_i": 1607011382, "num_comments": 2, "objectID": "25290193", "points": 34, "story_id": 25290193, "title": "M3 v1.0 released: Prometheus metrics store scalable to 100s of nodes", "updated_at": "2024-09-20T07:31:11Z", "url": "https://medium.com/chronosphere/m3-v1-0-released-a-production-ready-prometheus-metrics-store-scalable-to-hundreds-of-nodes-7f673fac4334"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justvugg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "Hi HN,<p>I built *LLM-Use*, an open-source intelligent router that helps reduce LLM API costs by automatically selecting the most appropriate model for each prompt.<p>I created it after realizing I was using GPT-4 for everything \u2014 including simple prompts like \u201ctranslate hello to Spanish\u201d \u2014 which cost $0.03 per call. Models like Mixtral can do the same for $0.0003.<p>### How it works:\n- Uses NLP (spaCy + transformers) to analyze prompt complexity\n- Routes to the optimal model (GPT-4, Claude, LLaMA, Mixtral, etc.)\n- Uses semantic similarity scoring to preserve output quality\n- Falls back gracefully if a model fails or gives poor results<p>### Key features:\n- Real-time streaming support for all providers\n- A/B testing with statistical significance\n- Response caching (LRU + TTL)\n- Circuit breakers for <em>production</em> stability\n- FastAPI backend with <em>Prometheus</em> metrics<p>### Early results:\n- Personal tests show up to 80% cost reduction\n- Output quality preserved (verified via internal A/B testing)<p>### Technical notes:\n- 2000+ lines of Python\n- Supports OpenAI, Anthropic, Google, Groq, Ollama\n- Complexity scoring: lexical diversity, prompt length, semantic analysis\n- Quality checks: relevance, coherence, grammar<p>Repo: [<a href=\"https://github.com/JustVugg/llm-use\" rel=\"nofollow\">https://github.com/JustVugg/llm-use</a>](<a href=\"https://github.com/JustVugg/llm-use\" rel=\"nofollow\">https://github.com/JustVugg/llm-use</a>)<p>Thanks! Happy to answer questions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LLM-Use \u2013 An LLM router that chooses the right model for each prompt"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/JustVugg/llm-use"}}, "_tags": ["story", "author_justvugg", "story_45504149", "show_hn"], "author": "justvugg", "children": [45505532], "created_at": "2025-10-07T15:13:39Z", "created_at_i": 1759850019, "num_comments": 2, "objectID": "45504149", "points": 3, "story_id": 45504149, "story_text": "Hi HN,<p>I built *LLM-Use*, an open-source intelligent router that helps reduce LLM API costs by automatically selecting the most appropriate model for each prompt.<p>I created it after realizing I was using GPT-4 for everything \u2014 including simple prompts like \u201ctranslate hello to Spanish\u201d \u2014 which cost $0.03 per call. Models like Mixtral can do the same for $0.0003.<p>### How it works:\n- Uses NLP (spaCy + transformers) to analyze prompt complexity\n- Routes to the optimal model (GPT-4, Claude, LLaMA, Mixtral, etc.)\n- Uses semantic similarity scoring to preserve output quality\n- Falls back gracefully if a model fails or gives poor results<p>### Key features:\n- Real-time streaming support for all providers\n- A&#x2F;B testing with statistical significance\n- Response caching (LRU + TTL)\n- Circuit breakers for production stability\n- FastAPI backend with Prometheus metrics<p>### Early results:\n- Personal tests show up to 80% cost reduction\n- Output quality preserved (verified via internal A&#x2F;B testing)<p>### Technical notes:\n- 2000+ lines of Python\n- Supports OpenAI, Anthropic, Google, Groq, Ollama\n- Complexity scoring: lexical diversity, prompt length, semantic analysis\n- Quality checks: relevance, coherence, grammar<p>Repo: [<a href=\"https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;llm-use\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;llm-use</a>](<a href=\"https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;llm-use\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;llm-use</a>)<p>Thanks! Happy to answer questions.", "title": "Show HN: LLM-Use \u2013 An LLM router that chooses the right model for each prompt", "updated_at": "2025-10-07T21:59:51Z", "url": "https://github.com/JustVugg/llm-use"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "netingle"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["prometheus"], "value": "CNCF's Cortex v1.0: scalable, fast <em>Prometheus</em> implementation"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "https://grafana.com/blog/2020/04/02/cortex-v1.0-released-the-highly-scalable-fast-<em>prometheus</em>-implementation-is-generally-available-for-<em>production</em>-use/"}}, "_tags": ["story", "author_netingle", "story_22758402"], "author": "netingle", "children": [22758664, 22758706, 22758722, 22758902, 22759297, 22759504, 22760907, 22761547, 22763325, 22764002, 22765089, 22766955], "created_at": "2020-04-02T12:59:12Z", "created_at_i": 1585832352, "num_comments": 46, "objectID": "22758402", "points": 181, "story_id": 22758402, "title": "CNCF's Cortex v1.0: scalable, fast Prometheus implementation", "updated_at": "2024-09-20T05:54:36Z", "url": "https://grafana.com/blog/2020/04/02/cortex-v1.0-released-the-highly-scalable-fast-prometheus-implementation-is-generally-available-for-production-use/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "madhusudancs"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "Hello. I am Madhu, a Software Engineer at Resolve AI. We launched our product today and we are thrilled to share it with you all and get feedback: <a href=\"https://resolve.ai/\" rel=\"nofollow\">https://resolve.ai/</a><p>Our team at Resolve AI comes with a wealth of experience in this space. I was an early contributor to Kubernetes at Google where I worked on Kubernetes and associated technologies for ~6 years. More recently, I was the tech lead for the Kubernetes-based compute platform at Robinhood where my teams were in a number of SEVs per year, not necessarily caused by the platform itself but still supported (pretty much the story of life for Infrastructure Engineers everywhere). Our co-founders, Spiros Xanthos and Mayank Agarwal co-created OpenTelemetry at their previous startup Omnition (acquired by Splunk). More recently, Spiros was the GM and Senior Vice President of Splunk Observability and Mayank was the lead architect for all of Splunk's observability product lines. We have all lived the problems we are trying to solve.<p>Resolve is AI for <em>production</em> engineers. <em>Production</em> systems are dynamic and complex. Addressing common <em>production</em> engineering concerns like incident troubleshooting, cloud operations, security, compliance and cost involves painfully piecing together information from many teams (service on-call rotations, Platform, SRE, etc) and multiple (routinely 10+) different tools (observability, CI/CD, infrastructure, paging, chat, etc). These tools were not designed to work together, pushing the complexity on humans.<p>Resolve AI is tackling this challenge by building an AI <em>Production</em> Engineer with the goal of automating the majority of tasks across incident management, cloud operations, security engineering, compliance, and cost management. As the first step in our ambitious journey, we are automating incident troubleshooting as it is the most direct way to prevent outages and improve reliability while relieving engineers from the most stressful part of their job. Our goal is to automate the resolution of 80%+ of alerts and incidents without human involvement.<p>Resolve AI automatically maps and keeps up-to-date a complete knowledge graph of any <em>production</em> environment, without needing any upfront training or user input. It builds knowledge of which tools and signals are relevant for any situation. It comes pre-built with models for various tool categories such as metrics, logs, traces, alerts, seamlessly connecting with category- and vendor-specific products like <em>Prometheus</em>, Splunk, GCP, AWS, Azure and others. These models automatically and continuously adapt to each customer's environment.<p>With the state-of-the-art reasoning engine that\u2019s composed of multiple agents, Resolve AI is able to investigate novel incidents, accurately determine causality, learn and adapt as it encounters new situations and perform various complex actions.<p>Generative AI is inherently probabilistic and not always 100% accurate. Without full context, AI models may hallucinate, potentially misleading users. For an AI that takes actions, building user trust is paramount; it must present clear evidence for any decision or action. We address these challenges by building an interface that supports claims with evidence, present findings with context and allow humans to collaborate with the system so that they can guide the system when needed.<p>Our video demo is on the website. Please take a look. We really appreciate your feedback. We are also happy to hop on a call to show a demo live if you are interested (madhu@resolve.ai)."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Resolve AI \u2013 Your AI <em>Production</em> Engineer"}}, "_tags": ["story", "author_madhusudancs", "story_41712089", "show_hn"], "author": "madhusudancs", "children": [41717514, 41731108], "created_at": "2024-10-01T18:17:23Z", "created_at_i": 1727806643, "num_comments": 2, "objectID": "41712089", "points": 10, "story_id": 41712089, "story_text": "Hello. I am Madhu, a Software Engineer at Resolve AI. We launched our product today and we are thrilled to share it with you all and get feedback: <a href=\"https:&#x2F;&#x2F;resolve.ai&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;resolve.ai&#x2F;</a><p>Our team at Resolve AI comes with a wealth of experience in this space. I was an early contributor to Kubernetes at Google where I worked on Kubernetes and associated technologies for ~6 years. More recently, I was the tech lead for the Kubernetes-based compute platform at Robinhood where my teams were in a number of SEVs per year, not necessarily caused by the platform itself but still supported (pretty much the story of life for Infrastructure Engineers everywhere). Our co-founders, Spiros Xanthos and Mayank Agarwal co-created OpenTelemetry at their previous startup Omnition (acquired by Splunk). More recently, Spiros was the GM and Senior Vice President of Splunk Observability and Mayank was the lead architect for all of Splunk&#x27;s observability product lines. We have all lived the problems we are trying to solve.<p>Resolve is AI for production engineers. Production systems are dynamic and complex. Addressing common production engineering concerns like incident troubleshooting, cloud operations, security, compliance and cost involves painfully piecing together information from many teams (service on-call rotations, Platform, SRE, etc) and multiple (routinely 10+) different tools (observability, CI&#x2F;CD, infrastructure, paging, chat, etc). These tools were not designed to work together, pushing the complexity on humans.<p>Resolve AI is tackling this challenge by building an AI Production Engineer with the goal of automating the majority of tasks across incident management, cloud operations, security engineering, compliance, and cost management. As the first step in our ambitious journey, we are automating incident troubleshooting as it is the most direct way to prevent outages and improve reliability while relieving engineers from the most stressful part of their job. Our goal is to automate the resolution of 80%+ of alerts and incidents without human involvement.<p>Resolve AI automatically maps and keeps up-to-date a complete knowledge graph of any production environment, without needing any upfront training or user input. It builds knowledge of which tools and signals are relevant for any situation. It comes pre-built with models for various tool categories such as metrics, logs, traces, alerts, seamlessly connecting with category- and vendor-specific products like Prometheus, Splunk, GCP, AWS, Azure and others. These models automatically and continuously adapt to each customer&#x27;s environment.<p>With the state-of-the-art reasoning engine that\u2019s composed of multiple agents, Resolve AI is able to investigate novel incidents, accurately determine causality, learn and adapt as it encounters new situations and perform various complex actions.<p>Generative AI is inherently probabilistic and not always 100% accurate. Without full context, AI models may hallucinate, potentially misleading users. For an AI that takes actions, building user trust is paramount; it must present clear evidence for any decision or action. We address these challenges by building an interface that supports claims with evidence, present findings with context and allow humans to collaborate with the system so that they can guide the system when needed.<p>Our video demo is on the website. Please take a look. We really appreciate your feedback. We are also happy to hop on a call to show a demo live if you are interested (madhu@resolve.ai).", "title": "Show HN: Resolve AI \u2013 Your AI Production Engineer", "updated_at": "2025-03-11T21:29:55Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "veinar_gh"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "We\u2019re a team of two building Valiant after too many incidents where \u201csomething changed\u201d but nobody could tell what actually caused <em>production</em> issues.<p>Valiant correlates intent (Git commits, CI/CD signals) with actual execution (Kubernetes rollouts) and links them to <em>Prometheus</em> metrics, so you can see the real impact of each change - not just what was deployed.<p>It\u2019s open-source, still under active development, and very much a work in progress. Feedback, ideas, or contributors are welcome.<p>No website yet \u2014 just the GitHub repo ;)"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Why it's hard to know which deployment caused a <em>production</em> incident"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/BytePeaks/valiant"}}, "_tags": ["story", "author_veinar_gh", "story_46902702", "show_hn"], "author": "veinar_gh", "children": [46937341, 46937353], "created_at": "2026-02-05T18:12:05Z", "created_at_i": 1770315125, "num_comments": 4, "objectID": "46902702", "points": 4, "story_id": 46902702, "story_text": "We\u2019re a team of two building Valiant after too many incidents where \u201csomething changed\u201d but nobody could tell what actually caused production issues.<p>Valiant correlates intent (Git commits, CI&#x2F;CD signals) with actual execution (Kubernetes rollouts) and links them to Prometheus metrics, so you can see the real impact of each change - not just what was deployed.<p>It\u2019s open-source, still under active development, and very much a work in progress. Feedback, ideas, or contributors are welcome.<p>No website yet \u2014 just the GitHub repo ;)", "title": "Show HN: Why it's hard to know which deployment caused a production incident", "updated_at": "2026-02-09T00:16:00Z", "url": "https://github.com/BytePeaks/valiant"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tschuehly"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "I am currently developing a project that I want to start a business with.<p>I've read a lot about Observability, Logging, Testing etc.  But what techniques and tools are important when moving to a <em>production</em> setting?<p>Is it necessary to setup a Grafana + <em>Prometheus</em> Stack to observe my application?<p>Is it necessary to create full end to end test on a staging environment?<p>Is it necessary to move from my current docker-compose setup to kubernetes and implement automatic health checks and so on?<p>Is it necessary to figure out how to load test my application with multiple servers to circumvent my slow internet connection?<p>If you have resources or tipps to share, it would help me very much."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: What is important when moving a project to <em>production</em> as a solo dev?"}}, "_tags": ["story", "author_tschuehly", "story_32859079", "ask_hn"], "author": "tschuehly", "children": [32866327], "created_at": "2022-09-15T22:08:30Z", "created_at_i": 1663279710, "num_comments": 1, "objectID": "32859079", "points": 1, "story_id": 32859079, "story_text": "I am currently developing a project that I want to start a business with.<p>I&#x27;ve read a lot about Observability, Logging, Testing etc.  But what techniques and tools are important when moving to a production setting?<p>Is it necessary to setup a Grafana + Prometheus Stack to observe my application?<p>Is it necessary to create full end to end test on a staging environment?<p>Is it necessary to move from my current docker-compose setup to kubernetes and implement automatic health checks and so on?<p>Is it necessary to figure out how to load test my application with multiple servers to circumvent my slow internet connection?<p>If you have resources or tipps to share, it would help me very much.", "title": "Ask HN: What is important when moving a project to production as a solo dev?", "updated_at": "2024-09-20T12:03:53Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yayajacky"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Old Node Packages in <em>production</em>? You can start monitoring them in 15 minutes"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["prometheus"], "value": "https://medium.com/teamzerolabs/staying-on-track-of-aging-node-packages-with-monitoring-in-<em>prometheus</em>-f915540e72df"}}, "_tags": ["story", "author_yayajacky", "story_22259384"], "author": "yayajacky", "created_at": "2020-02-06T19:27:31Z", "created_at_i": 1581017251, "num_comments": 0, "objectID": "22259384", "points": 1, "story_id": 22259384, "title": "Old Node Packages in production? You can start monitoring them in 15 minutes", "updated_at": "2024-09-20T05:41:34Z", "url": "https://medium.com/teamzerolabs/staying-on-track-of-aging-node-packages-with-monitoring-in-prometheus-f915540e72df"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "martons"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Using spot instances in <em>production</em> grade Kubernetes clusters"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["prometheus"], "value": "https://banzaicloud.com/blog/<em>prometheus</em>-spot-exporter/"}}, "_tags": ["story", "author_martons", "story_16343535"], "author": "martons", "created_at": "2018-02-09T20:23:43Z", "created_at_i": 1518207823, "num_comments": 0, "objectID": "16343535", "points": 1, "story_id": 16343535, "title": "Using spot instances in production grade Kubernetes clusters", "updated_at": "2024-09-20T02:03:13Z", "url": "https://banzaicloud.com/blog/prometheus-spot-exporter/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "9dev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "Our Hosting provider, Hetzner, has recently started charging for public IPv4 addresses - as they should! Those numbers started getting expensive. This prompted me to try and set up a new server cluster using IPv6 exclusively, and see how far I could get before having to give in and purchase an additional v4 address.<p>The experiment ended much sooner than I had anticipated. Some of the road blocks I hit along the way:<p><pre><code>  - The GitHub API and its code load endpoints are not reachable via IPv6, making it impossible to download release artefacts from many projects, lots of which distribute their software via GitHub exclusively (<em>Prometheus</em> for instance).\n  - The default Ubuntu key servers aren't reachable via IPv6, making it difficult to install packages from third-party registries, such as Docker or Grafana. While debugging, I noticed huge swaths of the GPG infrastructure are defunct: There aren't many key servers left at all, and the only one I found actually working via IPv6 was pgpkeys.eu.\n  - BitBucket cannot deploy to IPv6 hosts, as pipelines don't support IPv6 at all. You can self-host a pipeline runner and connect to it via v6, BUT it needs to have a dual stack - otherwise the runner won't start.\n  - Hetzner itself doesn't even provide their own API via IPv6 (which we talk to for in-cluster service discovery. Oh, the irony.\n</code></pre>\nIt seems IPv6 is still not viable, more than a decade after launch. Do you use it in <em>production</em>? If so, how? What issues did you hit?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Tell HN: IPv6-only still pretty much unusable"}}, "_tags": ["story", "author_9dev", "story_33894933", "ask_hn"], "author": "9dev", "children": [33895147, 33895173, 33895592, 33895714, 33895772, 33895791, 33895825, 33895853, 33895858, 33895922, 33895983, 33896027, 33896038, 33896092, 33896218, 33896219, 33896222, 33896235, 33896242, 33896283, 33896538, 33896554, 33896642, 33896700, 33896925, 33896928, 33897053, 33897071, 33897091, 33897093, 33897130, 33897251, 33897398, 33897488, 33897667, 33897674, 33897688, 33897818, 33898030, 33898111, 33898161, 33898323, 33898416, 33898466, 33899101, 33899102, 33899360, 33899400, 33899516, 33899797, 33899814, 33899831, 33899846, 33899851, 33900251, 33900330, 33900978, 33900998, 33900999, 33901060, 33901116, 33901137, 33901271, 33901341, 33901388, 33902441, 33902779, 33902916, 33903019, 33903037, 33903210, 33903706, 33903946, 33904433, 33906472, 33906804, 33907732, 33908705, 33916008, 33930651], "created_at": "2022-12-07T14:51:09Z", "created_at_i": 1670424669, "num_comments": 631, "objectID": "33894933", "points": 686, "story_id": 33894933, "story_text": "Our Hosting provider, Hetzner, has recently started charging for public IPv4 addresses - as they should! Those numbers started getting expensive. This prompted me to try and set up a new server cluster using IPv6 exclusively, and see how far I could get before having to give in and purchase an additional v4 address.<p>The experiment ended much sooner than I had anticipated. Some of the road blocks I hit along the way:<p><pre><code>  - The GitHub API and its code load endpoints are not reachable via IPv6, making it impossible to download release artefacts from many projects, lots of which distribute their software via GitHub exclusively (Prometheus for instance).\n  - The default Ubuntu key servers aren&#x27;t reachable via IPv6, making it difficult to install packages from third-party registries, such as Docker or Grafana. While debugging, I noticed huge swaths of the GPG infrastructure are defunct: There aren&#x27;t many key servers left at all, and the only one I found actually working via IPv6 was pgpkeys.eu.\n  - BitBucket cannot deploy to IPv6 hosts, as pipelines don&#x27;t support IPv6 at all. You can self-host a pipeline runner and connect to it via v6, BUT it needs to have a dual stack - otherwise the runner won&#x27;t start.\n  - Hetzner itself doesn&#x27;t even provide their own API via IPv6 (which we talk to for in-cluster service discovery. Oh, the irony.\n</code></pre>\nIt seems IPv6 is still not viable, more than a decade after launch. Do you use it in production? If so, how? What issues did you hit?", "title": "Tell HN: IPv6-only still pretty much unusable", "updated_at": "2026-02-14T18:56:38Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fabienpenso"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "Hey HN. I'm Fabien, principal engineer, 25 years shipping <em>production</em> systems (Ruby, Swift, now Rust). I built Moltis because I wanted an AI assistant I could run myself, trust end to end, and make extensible in the Rust way using traits and the type system. It shares some ideas with OpenClaw (same memory approach, Pi-inspired self-extension) but is Rust-native from the ground up. The agent can create its own skills at runtime.<p>Moltis is one Rust binary, 150k lines, ~60MB, web UI included. No Node, no Python, no runtime deps. Multi-provider LLM routing (OpenAI, local GGUF/MLX, Hugging Face), sandboxed execution (Docker/Podman/Apple Containers), hybrid vector + full-text memory, MCP tool servers with auto-restart, and multi-channel (web, Telegram, API) with shared context. MIT licensed. No telemetry phoning home, but full observability built in (OpenTelemetry, <em>Prometheus</em>).<p>I've included 1-click deploys on DigitalOcean and Fly.io, but since a Docker image is provided you can easily run it on your own servers as well. I've written before about owning your content (<a href=\"https://pen.so/2020/11/07/own-your-content/\" rel=\"nofollow\">https://pen.so/2020/11/07/own-your-content/</a>) and owning your email (<a href=\"https://pen.so/2020/12/10/own-your-email/\" rel=\"nofollow\">https://pen.so/2020/12/10/own-your-email/</a>). Same logic here: if something touches your files, credentials, and daily workflow, you should be able to inspect it, audit it, and fork it if the project changes direction.<p>It's alpha. I use it daily and I'm shipping because it's useful, not because it's done.<p>Longer architecture deep-dive: <a href=\"https://pen.so/2026/02/12/moltis-a-personal-ai-assistant-built-in-rust/\" rel=\"nofollow\">https://pen.so/2026/02/12/moltis-a-personal-ai-assistant-bui...</a><p>Happy to discuss the Rust architecture, security model, or local LLM setup. Would love feedback."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Moltis \u2013 AI assistant with memory, tools, and self-extending skills"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.moltis.org"}}, "_tags": ["story", "author_fabienpenso", "story_46993587", "show_hn"], "author": "fabienpenso", "children": [46994819, 46996806, 47007030, 47007174, 47007263, 47007514, 47007525, 47007667, 47007824, 47008032, 47008298, 47008585, 47008614, 47009148, 47009604, 47010364, 47010835, 47011060, 47011375, 47012050, 47013592, 47014345, 47017051, 47018401, 47024452, 47054036, 47165498], "created_at": "2026-02-12T19:15:21Z", "created_at_i": 1770923721, "num_comments": 51, "objectID": "46993587", "points": 131, "story_id": 46993587, "story_text": "Hey HN. I&#x27;m Fabien, principal engineer, 25 years shipping production systems (Ruby, Swift, now Rust). I built Moltis because I wanted an AI assistant I could run myself, trust end to end, and make extensible in the Rust way using traits and the type system. It shares some ideas with OpenClaw (same memory approach, Pi-inspired self-extension) but is Rust-native from the ground up. The agent can create its own skills at runtime.<p>Moltis is one Rust binary, 150k lines, ~60MB, web UI included. No Node, no Python, no runtime deps. Multi-provider LLM routing (OpenAI, local GGUF&#x2F;MLX, Hugging Face), sandboxed execution (Docker&#x2F;Podman&#x2F;Apple Containers), hybrid vector + full-text memory, MCP tool servers with auto-restart, and multi-channel (web, Telegram, API) with shared context. MIT licensed. No telemetry phoning home, but full observability built in (OpenTelemetry, Prometheus).<p>I&#x27;ve included 1-click deploys on DigitalOcean and Fly.io, but since a Docker image is provided you can easily run it on your own servers as well. I&#x27;ve written before about owning your content (<a href=\"https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;11&#x2F;07&#x2F;own-your-content&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;11&#x2F;07&#x2F;own-your-content&#x2F;</a>) and owning your email (<a href=\"https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;12&#x2F;10&#x2F;own-your-email&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;12&#x2F;10&#x2F;own-your-email&#x2F;</a>). Same logic here: if something touches your files, credentials, and daily workflow, you should be able to inspect it, audit it, and fork it if the project changes direction.<p>It&#x27;s alpha. I use it daily and I&#x27;m shipping because it&#x27;s useful, not because it&#x27;s done.<p>Longer architecture deep-dive: <a href=\"https:&#x2F;&#x2F;pen.so&#x2F;2026&#x2F;02&#x2F;12&#x2F;moltis-a-personal-ai-assistant-built-in-rust&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pen.so&#x2F;2026&#x2F;02&#x2F;12&#x2F;moltis-a-personal-ai-assistant-bui...</a><p>Happy to discuss the Rust architecture, security model, or local LLM setup. Would love feedback.", "title": "Show HN: Moltis \u2013 AI assistant with memory, tools, and self-extending skills", "updated_at": "2026-02-27T04:18:37Z", "url": "https://www.moltis.org"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "empath-nirvana"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "I built this kubernetes operator as a proof of concept this weekend..  It only has a single required item in the spec, a freeform description field.  The operator will use chatgpt to generate a spec, then immediately apply it to the cluster.  It makes some attempt to correct errors if there's a problem with the syntax.  It will leave additional comments, questions or instructions in the status field of the object.  I built this in a weekend and it's still quite unrefined.  It's in no way <em>production</em> ready, please don't use it for anything real, but it works better than you would think, considering how simple it is.  If you're going to use it, run it on a local cluster like 'kind'.<p>Some descriptions to try:<p>* install a redis namespace with a redis cluster and a service in it\n* create an argocd application in the argocd namespace to install velero.\n* write a python script that lists all ec2 instances in us-east-1, and run it as a k8s job with the aws credentials already saved in the default namespace..<p>a somewhat longer description that also worked:\ngiven the following spec:\n    ---\n    kind: MagicHappens\n    apiVersion: gptmagic.io/v1\n    metadata:\n      name: foo\n    spec:\n      description: this is a freeform description field that will be sent to chatgpt to generate kubernetes resources\n      dryRun: false\n    ---\n    Can you create more magic happens resources, each of which describes an argocd application that needs to be created to install a helm chart for one of the standard cluster addons that need to be installed on a cluster for it to be <em>production</em> ready.  The description should be be freeform text like the following: &quot;Create an argocd application in the argocd namespace to install istio from the helm chart with all the defaults&quot; or &quot;Create an argocd application in the argocd namespace to install <em>prometheus</em> and grafana, with an ingress enabled for grafana&quot;. Be very thorough and included as many apps that might be needed for a prod ready cluster using industry standard CNCF projects if possible.<p>(this produces a list of additional resources for the operator, which the operator then goes on to create argocd applications for -- it also left comments with instructions on one of the resources for how configure it to work with your cloud provider<p>something to note is that since you can run arbitrary containers with arbitrary commands, and chatgpt can write arbitrary code, you don't have to limit yourself to k8s stuff.. if you've got saas credentials on the cluster, you can just tell it to run a python script as a job to do whatever you want.<p>Since most people are cowards, there's a dryRun field that defaults to true, so it only attaches the spec to the object.<p>It is <i>scary</i> how well this works."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Magic Happens \u2013 let ChatGPT manage your Kubernetes cluster"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/empath-nirvana/magic-happens"}}, "_tags": ["story", "author_empath-nirvana", "story_35604715", "show_hn"], "author": "empath-nirvana", "children": [35605883, 35605948, 35606084, 35606114, 35606151, 35606176, 35606190, 35606233, 35606750], "created_at": "2023-04-17T18:39:22Z", "created_at_i": 1681756762, "num_comments": 32, "objectID": "35604715", "points": 48, "story_id": 35604715, "story_text": "I built this kubernetes operator as a proof of concept this weekend..  It only has a single required item in the spec, a freeform description field.  The operator will use chatgpt to generate a spec, then immediately apply it to the cluster.  It makes some attempt to correct errors if there&#x27;s a problem with the syntax.  It will leave additional comments, questions or instructions in the status field of the object.  I built this in a weekend and it&#x27;s still quite unrefined.  It&#x27;s in no way production ready, please don&#x27;t use it for anything real, but it works better than you would think, considering how simple it is.  If you&#x27;re going to use it, run it on a local cluster like &#x27;kind&#x27;.<p>Some descriptions to try:<p>* install a redis namespace with a redis cluster and a service in it\n* create an argocd application in the argocd namespace to install velero.\n* write a python script that lists all ec2 instances in us-east-1, and run it as a k8s job with the aws credentials already saved in the default namespace..<p>a somewhat longer description that also worked:\ngiven the following spec:\n    ---\n    kind: MagicHappens\n    apiVersion: gptmagic.io&#x2F;v1\n    metadata:\n      name: foo\n    spec:\n      description: this is a freeform description field that will be sent to chatgpt to generate kubernetes resources\n      dryRun: false\n    ---\n    Can you create more magic happens resources, each of which describes an argocd application that needs to be created to install a helm chart for one of the standard cluster addons that need to be installed on a cluster for it to be production ready.  The description should be be freeform text like the following: &quot;Create an argocd application in the argocd namespace to install istio from the helm chart with all the defaults&quot; or &quot;Create an argocd application in the argocd namespace to install prometheus and grafana, with an ingress enabled for grafana&quot;. Be very thorough and included as many apps that might be needed for a prod ready cluster using industry standard CNCF projects if possible.<p>(this produces a list of additional resources for the operator, which the operator then goes on to create argocd applications for -- it also left comments with instructions on one of the resources for how configure it to work with your cloud provider<p>something to note is that since you can run arbitrary containers with arbitrary commands, and chatgpt can write arbitrary code, you don&#x27;t have to limit yourself to k8s stuff.. if you&#x27;ve got saas credentials on the cluster, you can just tell it to run a python script as a job to do whatever you want.<p>Since most people are cowards, there&#x27;s a dryRun field that defaults to true, so it only attaches the spec to the object.<p>It is <i>scary</i> how well this works.", "title": "Show HN: Magic Happens \u2013 let ChatGPT manage your Kubernetes cluster", "updated_at": "2024-09-20T13:45:32Z", "url": "https://github.com/empath-nirvana/magic-happens"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "itssimon"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "G\u2019day Hacker News, I\u2019m Simon Gurcke, the sole founder of Apitally (<a href=\"https://apitally.io\" rel=\"nofollow\">https://apitally.io</a>).<p>I\u2019m building a simple API monitoring and analytics tool for Python / Node.js apps. It helps users understand API usage and performance, spot issues early and troubleshoot effectively when something goes wrong.<p>Features include:<p>- <i>Dashboards:</i> Provide insights into API traffic, errors, performance and consumers.<p>- <i>Request logging:</i> Opt-in and highly configurable in terms of what data is logged. Users can drill down from aggregated metrics to individual requests (proven to be super helpful when troubleshooting issues).<p>- <i>Custom alerts:</i> Based on 14 different API metrics with notifications delivered via email, Slack or Microsoft Teams.<p>- <i>Validation error tracking:</i> Captures metrics about which fields failed validation and why. Works for web frameworks with built-in validation (e.g. FastAPI with pydantic), or that integrate with popular third-party validation libraries (e.g. Zod for Hono).<p>- <i>Server error tracking:</i> Captures exception details and stack traces for 500 error responses. An integration with the Sentry SDK also captures event IDs, allowing users to click through to the relevant Sentry issue for more context.<p>I first started developing Apitally to scratch my own itch. While working at a health tech company where I was responsible for API-based software products, I became frustrated with the monitoring tools we had in place - Datadog and the ELK stack. They were too complex for my API-centric use cases, and often a pain to use.<p>As a result, I focused on making Apitally as simple as possible. This involved not just refining the UX of the dashboard, but also optimizing the developer experience with the open-source SDKs:<p>- <a href=\"https://github.com/apitally/apitally-py\">https://github.com/apitally/apitally-py</a> - Python SDK (supports FastAPI, Flask, Django, Litestar, Starlette)<p>- <a href=\"https://github.com/apitally/apitally-js\">https://github.com/apitally/apitally-js</a> - Node.js SDK (supports Express, NestJS, Fastify, Koa, Hono)<p>My other focus was on data privacy, as that is a strict requirement in the healthcare industry. By default, Apitally doesn\u2019t capture any sensitive data - metrics are aggregated on the client side (similar to <em>Prometheus</em>) and sent in the background in regular intervals.<p>The hardest part has been implementing integrations for various web frameworks and supporting a wide range of versions. I learned a lot about the inner workings of web frameworks in the process. Good test coverage and an extensive test matrix were really important to not break people\u2019s <em>production</em> APIs with buggy middleware.<p>Apitally\u2019s backend is built in Python and runs on a small Kubernetes cluster on DigitalOcean. It uses PostgreSQL and ClickHouse to store data and NATS JetStream as a message queue. I chose NATS for being lightweight and its exactly-once processing capabilities. I\u2019m also impressed by ClickHouse\u2019s performance given the low hardware specs of my server (4 vCPUs, 8 GB RAM).<p>Apitally is free to use for small hobby projects (with limitations), and I offer two paid tiers for $39 and $119 (USD) per month. The dashboard has a demo mode, allowing people to explore the product without having to set up their own app first.<p>Thank you for reading about my bootstrapped indie product. Please let me know your thoughts and questions in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Apitally \u2013 A simple, privacy-focused API monitoring and analytics tool"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://apitally.io"}}, "_tags": ["story", "author_itssimon", "story_42915435", "show_hn"], "author": "itssimon", "children": [42915477, 42915683, 42929740, 42949349, 42949483, 42959486, 42960715], "created_at": "2025-02-03T06:07:00Z", "created_at_i": 1738562820, "num_comments": 21, "objectID": "42915435", "points": 46, "story_id": 42915435, "story_text": "G\u2019day Hacker News, I\u2019m Simon Gurcke, the sole founder of Apitally (<a href=\"https:&#x2F;&#x2F;apitally.io\" rel=\"nofollow\">https:&#x2F;&#x2F;apitally.io</a>).<p>I\u2019m building a simple API monitoring and analytics tool for Python &#x2F; Node.js apps. It helps users understand API usage and performance, spot issues early and troubleshoot effectively when something goes wrong.<p>Features include:<p>- <i>Dashboards:</i> Provide insights into API traffic, errors, performance and consumers.<p>- <i>Request logging:</i> Opt-in and highly configurable in terms of what data is logged. Users can drill down from aggregated metrics to individual requests (proven to be super helpful when troubleshooting issues).<p>- <i>Custom alerts:</i> Based on 14 different API metrics with notifications delivered via email, Slack or Microsoft Teams.<p>- <i>Validation error tracking:</i> Captures metrics about which fields failed validation and why. Works for web frameworks with built-in validation (e.g. FastAPI with pydantic), or that integrate with popular third-party validation libraries (e.g. Zod for Hono).<p>- <i>Server error tracking:</i> Captures exception details and stack traces for 500 error responses. An integration with the Sentry SDK also captures event IDs, allowing users to click through to the relevant Sentry issue for more context.<p>I first started developing Apitally to scratch my own itch. While working at a health tech company where I was responsible for API-based software products, I became frustrated with the monitoring tools we had in place - Datadog and the ELK stack. They were too complex for my API-centric use cases, and often a pain to use.<p>As a result, I focused on making Apitally as simple as possible. This involved not just refining the UX of the dashboard, but also optimizing the developer experience with the open-source SDKs:<p>- <a href=\"https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-py\">https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-py</a> - Python SDK (supports FastAPI, Flask, Django, Litestar, Starlette)<p>- <a href=\"https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-js\">https:&#x2F;&#x2F;github.com&#x2F;apitally&#x2F;apitally-js</a> - Node.js SDK (supports Express, NestJS, Fastify, Koa, Hono)<p>My other focus was on data privacy, as that is a strict requirement in the healthcare industry. By default, Apitally doesn\u2019t capture any sensitive data - metrics are aggregated on the client side (similar to Prometheus) and sent in the background in regular intervals.<p>The hardest part has been implementing integrations for various web frameworks and supporting a wide range of versions. I learned a lot about the inner workings of web frameworks in the process. Good test coverage and an extensive test matrix were really important to not break people\u2019s production APIs with buggy middleware.<p>Apitally\u2019s backend is built in Python and runs on a small Kubernetes cluster on DigitalOcean. It uses PostgreSQL and ClickHouse to store data and NATS JetStream as a message queue. I chose NATS for being lightweight and its exactly-once processing capabilities. I\u2019m also impressed by ClickHouse\u2019s performance given the low hardware specs of my server (4 vCPUs, 8 GB RAM).<p>Apitally is free to use for small hobby projects (with limitations), and I offer two paid tiers for $39 and $119 (USD) per month. The dashboard has a demo mode, allowing people to explore the product without having to set up their own app first.<p>Thank you for reading about my bootstrapped indie product. Please let me know your thoughts and questions in the comments.", "title": "Show HN: Apitally \u2013 A simple, privacy-focused API monitoring and analytics tool", "updated_at": "2025-11-19T05:50:23Z", "url": "https://apitally.io"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "whispem"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "Hi HN,<p>I\u2019m releasing minikv, a distributed key-value and object store in Rust.<p>What is minikv?\nminikv is an open-source, distributed storage engine built for learning, experimentation, and self-hosted setups. \nIt combines a strongly-consistent key-value database (Raft), S3-compatible object storage, and basic multi-tenancy. \nI started minikv as a learning project about distributed systems, and it grew into something <em>production</em>-ready and fun to extend.<p>Features/highlights:<p>- Raft consensus with automatic failover and sharding\n- S3-compatible HTTP API (plus REST/gRPC APIs)\n- Pluggable storage backends: in-memory, RocksDB, Sled\n- Multi-tenant: per-tenant namespaces, role-based access, quotas, and audit\n- Metrics (<em>Prometheus</em>), TLS, JWT-based API keys\n- Easy to deploy (single binary, works with Docker/Kubernetes)<p>Quick demo (single node):<p>git clone <a href=\"https://github.com/whispem/minikv.git\" rel=\"nofollow\">https://github.com/whispem/minikv.git</a>\ncd minikv\ncargo run --release -- --config config.example.toml\ncurl localhost:8080/health/ready\n# S3 upload + read\ncurl -X PUT localhost:8080/s3/mybucket/hello -d &quot;hi HN&quot;\ncurl localhost:8080/s3/mybucket/hello<p>Docs, cluster setup, and architecture details are in the repo.\nI\u2019d love to hear feedback, questions, ideas, or your stories running distributed infra in Rust!<p>Repo: <a href=\"https://github.com/whispem/minikv\" rel=\"nofollow\">https://github.com/whispem/minikv</a>\nCrate: <a href=\"https://crates.io/crates/minikv\" rel=\"nofollow\">https://crates.io/crates/minikv</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Minikv \u2013 Distributed key-value and object store in Rust (Raft, S3 API)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/whispem/minikv"}}, "_tags": ["story", "author_whispem", "story_46661308", "show_hn"], "author": "whispem", "children": [46661419, 46661538, 46662030, 46662152, 46662175, 46664118, 46673079, 46675357], "created_at": "2026-01-17T19:39:36Z", "created_at_i": 1768678776, "num_comments": 39, "objectID": "46661308", "points": 35, "story_id": 46661308, "story_text": "Hi HN,<p>I\u2019m releasing minikv, a distributed key-value and object store in Rust.<p>What is minikv?\nminikv is an open-source, distributed storage engine built for learning, experimentation, and self-hosted setups. \nIt combines a strongly-consistent key-value database (Raft), S3-compatible object storage, and basic multi-tenancy. \nI started minikv as a learning project about distributed systems, and it grew into something production-ready and fun to extend.<p>Features&#x2F;highlights:<p>- Raft consensus with automatic failover and sharding\n- S3-compatible HTTP API (plus REST&#x2F;gRPC APIs)\n- Pluggable storage backends: in-memory, RocksDB, Sled\n- Multi-tenant: per-tenant namespaces, role-based access, quotas, and audit\n- Metrics (Prometheus), TLS, JWT-based API keys\n- Easy to deploy (single binary, works with Docker&#x2F;Kubernetes)<p>Quick demo (single node):<p>git clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;whispem&#x2F;minikv.git\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;whispem&#x2F;minikv.git</a>\ncd minikv\ncargo run --release -- --config config.example.toml\ncurl localhost:8080&#x2F;health&#x2F;ready\n# S3 upload + read\ncurl -X PUT localhost:8080&#x2F;s3&#x2F;mybucket&#x2F;hello -d &quot;hi HN&quot;\ncurl localhost:8080&#x2F;s3&#x2F;mybucket&#x2F;hello<p>Docs, cluster setup, and architecture details are in the repo.\nI\u2019d love to hear feedback, questions, ideas, or your stories running distributed infra in Rust!<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;whispem&#x2F;minikv\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;whispem&#x2F;minikv</a>\nCrate: <a href=\"https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;minikv\" rel=\"nofollow\">https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;minikv</a>", "title": "Show HN: Minikv \u2013 Distributed key-value and object store in Rust (Raft, S3 API)", "updated_at": "2026-02-03T14:01:18Z", "url": "https://github.com/whispem/minikv"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "sweaver"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["prometheus", "production"], "value": "Hi HN!<p>Sam and Michael here. We are building an open-source management platform that deploys and operates any open source infra software on kubernetes in a <em>production</em>-ready way: https://www.plural.sh<p>I\u2019ve been in open source for over a decade. I spent 4 years at Red Hat before spending the next 6 years building products at MongoDB. Most recently I was the Head of Product at Unqork where we saw how difficult it was to create a deployment for our app stack and its related services.<p>Michael has spent his career at Vine, Amazon, Frame.io and Facebook where he has built and scaled systems using open source solutions for millions of users.<p>We met each other in early 2021 and got excited to tackle this problem together: making multi-cloud open source infrastructure zero effort to install and manage.<p>We\u2019ve interviewed over 60 development and devops teams and learned that existing solutions always fell short. Either they were too complex to build or too costly to manage. In particular we heard many times that data infrastructure was a complete bear to stand up and integrate. We wanted to build a platform that would supercharge devops team\u2019s capabilities and we wanted to give users the ability to do this all in their own cloud: aws, azure, gcp or bare metal.<p>It was important for us to work with the open source vendors. By making it trivial for users to get going with open source software, we can bring great solutions directly from vendors to the platform, and most importantly, compensate the vendors for their work.<p>Here\u2019s a short demo video if you want to check out how it works when installing something operationally complex like Airflow:\nhttps://www.plural.sh/video-plural-product-demo<p>Plural can run on AWS, Azure, GCP and in alpha on Equinix (bare) metal, deploying applications in Kubernetes, with logging and monitoring (<em>prometheus</em>/grafana) enabled out of the box. We have built full dashboarding and run books for managing and scaling your cluster and a simple install interface for quickly getting going with apps like Airflow, Kubeflow, Airbyte etc.<p>Today our catalog has over 30 apps ready to solve the most interesting use cases from AI/ML &amp; data infrastructure to observability to caching. We hope our community will help us add more solutions to our catalog and we will focus on building our open source product. We will eventually create a paid edition (open core model) with enterprise-level features to address the needs of our most demanding users.<p>Try it out: https://github.com/pluralsh/plural &amp; app.plural.sh/ and let us know what you think!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Plural \u2013 Multi-Cloud OSS Application Deployments on Kubernetes"}}, "_tags": ["story", "author_sweaver", "story_30710481", "launch_hn"], "author": "sweaver", "children": [30710556, 30720058, 30720069], "created_at": "2022-03-17T12:39:50Z", "created_at_i": 1647520790, "num_comments": 7, "objectID": "30710481", "points": 23, "story_id": 30710481, "story_text": "Hi HN!<p>Sam and Michael here. We are building an open-source management platform that deploys and operates any open source infra software on kubernetes in a production-ready way: https:&#x2F;&#x2F;www.plural.sh<p>I\u2019ve been in open source for over a decade. I spent 4 years at Red Hat before spending the next 6 years building products at MongoDB. Most recently I was the Head of Product at Unqork where we saw how difficult it was to create a deployment for our app stack and its related services.<p>Michael has spent his career at Vine, Amazon, Frame.io and Facebook where he has built and scaled systems using open source solutions for millions of users.<p>We met each other in early 2021 and got excited to tackle this problem together: making multi-cloud open source infrastructure zero effort to install and manage.<p>We\u2019ve interviewed over 60 development and devops teams and learned that existing solutions always fell short. Either they were too complex to build or too costly to manage. In particular we heard many times that data infrastructure was a complete bear to stand up and integrate. We wanted to build a platform that would supercharge devops team\u2019s capabilities and we wanted to give users the ability to do this all in their own cloud: aws, azure, gcp or bare metal.<p>It was important for us to work with the open source vendors. By making it trivial for users to get going with open source software, we can bring great solutions directly from vendors to the platform, and most importantly, compensate the vendors for their work.<p>Here\u2019s a short demo video if you want to check out how it works when installing something operationally complex like Airflow:\nhttps:&#x2F;&#x2F;www.plural.sh&#x2F;video-plural-product-demo<p>Plural can run on AWS, Azure, GCP and in alpha on Equinix (bare) metal, deploying applications in Kubernetes, with logging and monitoring (prometheus&#x2F;grafana) enabled out of the box. We have built full dashboarding and run books for managing and scaling your cluster and a simple install interface for quickly getting going with apps like Airflow, Kubeflow, Airbyte etc.<p>Today our catalog has over 30 apps ready to solve the most interesting use cases from AI&#x2F;ML &amp; data infrastructure to observability to caching. We hope our community will help us add more solutions to our catalog and we will focus on building our open source product. We will eventually create a paid edition (open core model) with enterprise-level features to address the needs of our most demanding users.<p>Try it out: https:&#x2F;&#x2F;github.com&#x2F;pluralsh&#x2F;plural &amp; app.plural.sh&#x2F; and let us know what you think!", "title": "Launch HN: Plural \u2013 Multi-Cloud OSS Application Deployments on Kubernetes", "updated_at": "2024-09-20T10:42:42Z"}], "hitsPerPage": 15, "nbHits": 38, "nbPages": 3, "page": 0, "params": "query=prometheus+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 13, "processingTimingsMS": {"_request": {"queue": 1, "roundTrip": 15}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 8, "scanning": 2, "total": 12}, "total": 13}, "query": "prometheus production", "serverTimeMS": 16}}