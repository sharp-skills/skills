{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mikeshi42"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "Hi HN, Michael and Warren here - cofounders of DeploySentinel (<a href=\"https://deploysentinel.com\" rel=\"nofollow\">https://deploysentinel.com</a>). We make end-to-end testing easier and more reliable.<p>At my last job, it dawned upon me how many <em>production</em> incidents and unhappy customers could have been avoided with more test automation - \u201can ounce of prevention is worth a pound of cure\u201d. However, it wasn\u2019t clear that you can get prevention for just an ounce. Our teams ramped up on investment into testing, especially end-to-end tests (via spinning up a headless browser in CI and testing as an end user), but it quickly became clear that these were incredibly expensive to build and maintain, especially as test suite and application complexity grow. When we asked around other engineering teams from different companies, we consistently heard how time-intensive test maintenance was.<p>The worst part of end to end tests is when they fail occasionally in CI but never locally\u2014a heisenbug in your test code, or what\u2019s usually referred to as a flaky test. The conventional way to debug such an issue is to replay a video of your CI\u2019s test browser, stepping between video frames to try to parse what could be happening under the hood. Otherwise, your CI is just a complete black box.<p>Anyone that finds this story familiar can probably attest to days spent trying to debug an issue like this, possibly losing some hair in the process, and \u201cresolving\u201d it in the end by just deleting the test and regaining their sanity. Some teams even try to put front-end monitoring tools for <em>production</em> into their CI process, only to realize they aren\u2019t able to handle recording hundreds of test actions executed by a machine over just a few seconds.<p>After realizing how painful debugging these tests could be, we started putting together a debugger that can help developers pinpoint issues, more like how you debug issues locally. Teams have told us there\u2019s a night and day difference between trying to debug test failures with just video, and having a tool that can finally tell them what\u2019s happening in their CI browser, with the same information they\u2019re used to having in their browser\u2019s devtools.<p>We give you the ability to inspect DOM snapshots, network events, and console logs for any step taken in a <em>Cypress</em> test running in CI, to give more insight into why a particular test might be failing. It\u2019s like Fullstory/LogRocket, but for CI failures instead of <em>production</em> bugs. (We\u2019re starting with <em>Cypress</em> tests, with plans to extend further.)<p>Our tool integrates with <em>Cypress</em> via their plugin API, so we\u2019re able to plug in and record tests in CI with just an NPM install and 2 lines of code. From there we\u2019re able to hook into <em>Cypress</em>/Mocha events to capture everything happening within the test runner (ex. when a test is starting, when a command is fired, when an element is found, etc.) as well as open a debugger protocol port with the browser to listen for network and console events. While a test suite is running, the debugger is consistently collecting what\u2019s happening during a test run, and uploads the information (minus user-configured censored events) after every test completes.<p>While this may sound similar to shoving a LogRocket/FullStory into your test suite, there\u2019s actually quite a few differences. The most practical one is that those tools typically have a low rate limit that work well for human traffic interacting with web apps at human speeds, but break when dealing with parallelized test runner traffic interacting with web app at machine speeds. Other minor details revolve around us associating replays with test metadata as opposed to user metadata, having full access to all network requests/console messages emitted within a test at the browser level, and us indexing playback information based on test commands rather than timestamp (time is an unreliable concept in tests!).<p>Once a test fails, a Github PR comment is created and an engineer can immediately access our web app to start debugging their test failure. Alternatively, they can check our web dashboard as well. Instead of playing a video of the failure in slow motion to understand the issue, an engineer can step through the test command-by-command, inspect the DOM with their browser inspect element tool at any point, view what elements the test interacted with, if any console messages were emitted during the action, or take a look at every network request made along with HTTP error codes or browser network error messages.<p>Typically with this kind of information, engineers can quickly find out if they have a network-based race condition, a console warning emitted in their frontend, a server-side bug, or a test failure from an edge case triggered by randomly generated test data.<p>We dream of a world where applications have minimal bugs, happy customers, built with engineering teams that don\u2019t see testing as an expensive chore! Although the first pain we\u2019re addressing is tests that fail in CI, we\u2019re working on a bunch of things beyond that, including the second biggest issue in testing: test runtime length.<p>We have a free trial available for you to try out with your own tests, along with a few live demos of what our debugger looks like on an example test. You can get started here: <a href=\"https://deploysentinel.com/\" rel=\"nofollow\">https://deploysentinel.com/</a><p>We\u2019re looking forward to hearing everyone else\u2019s experiences with end to end tests, and what you think of what we\u2019re doing!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: DeploySentinel (YC S22) \u2013 End-to-end tests that don't flake"}}, "_tags": ["story", "author_mikeshi42", "story_32319404", "launch_hn"], "author": "mikeshi42", "children": [32319620, 32319850, 32320185, 32321302, 32323705, 32327204, 32329757, 32330012, 32330175, 32330327, 32373936], "created_at": "2022-08-02T15:01:29Z", "created_at_i": 1659452489, "num_comments": 36, "objectID": "32319404", "points": 56, "story_id": 32319404, "story_text": "Hi HN, Michael and Warren here - cofounders of DeploySentinel (<a href=\"https:&#x2F;&#x2F;deploysentinel.com\" rel=\"nofollow\">https:&#x2F;&#x2F;deploysentinel.com</a>). We make end-to-end testing easier and more reliable.<p>At my last job, it dawned upon me how many production incidents and unhappy customers could have been avoided with more test automation - \u201can ounce of prevention is worth a pound of cure\u201d. However, it wasn\u2019t clear that you can get prevention for just an ounce. Our teams ramped up on investment into testing, especially end-to-end tests (via spinning up a headless browser in CI and testing as an end user), but it quickly became clear that these were incredibly expensive to build and maintain, especially as test suite and application complexity grow. When we asked around other engineering teams from different companies, we consistently heard how time-intensive test maintenance was.<p>The worst part of end to end tests is when they fail occasionally in CI but never locally\u2014a heisenbug in your test code, or what\u2019s usually referred to as a flaky test. The conventional way to debug such an issue is to replay a video of your CI\u2019s test browser, stepping between video frames to try to parse what could be happening under the hood. Otherwise, your CI is just a complete black box.<p>Anyone that finds this story familiar can probably attest to days spent trying to debug an issue like this, possibly losing some hair in the process, and \u201cresolving\u201d it in the end by just deleting the test and regaining their sanity. Some teams even try to put front-end monitoring tools for production into their CI process, only to realize they aren\u2019t able to handle recording hundreds of test actions executed by a machine over just a few seconds.<p>After realizing how painful debugging these tests could be, we started putting together a debugger that can help developers pinpoint issues, more like how you debug issues locally. Teams have told us there\u2019s a night and day difference between trying to debug test failures with just video, and having a tool that can finally tell them what\u2019s happening in their CI browser, with the same information they\u2019re used to having in their browser\u2019s devtools.<p>We give you the ability to inspect DOM snapshots, network events, and console logs for any step taken in a Cypress test running in CI, to give more insight into why a particular test might be failing. It\u2019s like Fullstory&#x2F;LogRocket, but for CI failures instead of production bugs. (We\u2019re starting with Cypress tests, with plans to extend further.)<p>Our tool integrates with Cypress via their plugin API, so we\u2019re able to plug in and record tests in CI with just an NPM install and 2 lines of code. From there we\u2019re able to hook into Cypress&#x2F;Mocha events to capture everything happening within the test runner (ex. when a test is starting, when a command is fired, when an element is found, etc.) as well as open a debugger protocol port with the browser to listen for network and console events. While a test suite is running, the debugger is consistently collecting what\u2019s happening during a test run, and uploads the information (minus user-configured censored events) after every test completes.<p>While this may sound similar to shoving a LogRocket&#x2F;FullStory into your test suite, there\u2019s actually quite a few differences. The most practical one is that those tools typically have a low rate limit that work well for human traffic interacting with web apps at human speeds, but break when dealing with parallelized test runner traffic interacting with web app at machine speeds. Other minor details revolve around us associating replays with test metadata as opposed to user metadata, having full access to all network requests&#x2F;console messages emitted within a test at the browser level, and us indexing playback information based on test commands rather than timestamp (time is an unreliable concept in tests!).<p>Once a test fails, a Github PR comment is created and an engineer can immediately access our web app to start debugging their test failure. Alternatively, they can check our web dashboard as well. Instead of playing a video of the failure in slow motion to understand the issue, an engineer can step through the test command-by-command, inspect the DOM with their browser inspect element tool at any point, view what elements the test interacted with, if any console messages were emitted during the action, or take a look at every network request made along with HTTP error codes or browser network error messages.<p>Typically with this kind of information, engineers can quickly find out if they have a network-based race condition, a console warning emitted in their frontend, a server-side bug, or a test failure from an edge case triggered by randomly generated test data.<p>We dream of a world where applications have minimal bugs, happy customers, built with engineering teams that don\u2019t see testing as an expensive chore! Although the first pain we\u2019re addressing is tests that fail in CI, we\u2019re working on a bunch of things beyond that, including the second biggest issue in testing: test runtime length.<p>We have a free trial available for you to try out with your own tests, along with a few live demos of what our debugger looks like on an example test. You can get started here: <a href=\"https:&#x2F;&#x2F;deploysentinel.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;deploysentinel.com&#x2F;</a><p>We\u2019re looking forward to hearing everyone else\u2019s experiences with end to end tests, and what you think of what we\u2019re doing!", "title": "Launch HN: DeploySentinel (YC S22) \u2013 End-to-end tests that don't flake", "updated_at": "2024-09-20T11:39:58Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jaraganittah"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "I'm Nagaraj, the founder of Tesmon. Having been both a developer and an engineering leader, from big tech companies to startups, I've personally encountered the challenges of integration testing. It's often so cumbersome that we resort to manual testing.<p>The world of testing is fragmented across multiple frameworks and platforms like Postman, RestAssured, pytest, Selenium, <em>Cypress</em>, Testim, and various custom in-house code using open source frameworks. Building and maintaining this testing infrastructure is not only time-consuming but can be a real pain.<p>Our mission is to revolutionize testing into an assertion-free, near zero effort process by leveraging AI/ML.<p>Current challenges?<p>- Environment Differences: Tests often fail across different environments\u2014local, staging, <em>production</em>\u2014due to setup discrepancies and versioning issues between environments.<p>- Complex Negative Testing: Difficult to manage sequences like receiving an HTTP 200 followed by a 407 error for the same resource.<p>- Frequent Updates: Tests require extensive updates with each application change, including writing assertions for all critical fields in large response bodies.<p>- Unreliable Self-Healing: Self-healing tests frequently fail to correct issues automatically, necessitating manual intervention.<p>- Scalability Issues: No-code/low-code UI tests struggle to scale, becoming a significant time drain as the test suite grows.<p>- Trade-offs Between Automation and Deadlines: Testing automation is often deprioritized due to urgent deadlines, leading to increased manual testing and the risk of regressions.<p>- Resource Limitations: Restricted access to essential databases and services due to compliance requirements, including the need to set up VPN access and manage Jenkins hosted in public clouds because of private subnet restrictions.<p>- Isolation Challenges: Testing everything from only the frontend or backend is often impractical, highlighting the need for integrated testing solutions.<p>- Effort in Framework Construction: Building a testing framework requires significant effort, especially when dealing with modularization and managing different testing environments.<p>How does testing code differ from product code?<p>Testing usually involves sending requests and checking responses, a process that doesn't need to run continuously like microservices or batch jobs. This makes it perfect for AI/ML automation, as the predictable nature of testing tasks allows for efficient learning and optimization without the complexity of continuous monitoring. Traditional programming adds unnecessary complexity to testing, which Tesmon simplifies.<p>Unit tests vs integration tests?<p>We cannot ship code without thorough integration testing, whether automated or manual. This step is critical for identifying significant issues that might not be caught by unit testing alone. Unit tests are primarily used for testing common code and libraries, ensuring that each individual component functions correctly in isolation.<p>Tesmon's Approach: We've completely rethought testing with the Tesmon Platform:<p>- Interactive Local Testing: Directly interact with your APIs, databases, caches, Kafka, and more through Tesmon Desktop, which autonomously creates tests.<p>- Single-Click Test Updates: Tesmon adapts to changes in your system and can integrate updates with just a click.<p>- Extended Lifecycle with Tesmon Cloud: From local development through staging to <em>production</em>, Tesmon covers it all.<p>- Unified Frontend and Backend Testing: Execute comprehensive testing across both frontend and backend within a single test.<p>- Zero Assertions: Utilize AI/ML models to ensure testing requires no manual assertions.<p>Get Started Now. Download Tesmon Desktop today on Mac/Windows and transform your testing.<p><a href=\"https://tesmon.io/desktop\" rel=\"nofollow\">https://tesmon.io/desktop</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["cypress"], "value": "Show HN: Tesmon Platform == Postman + <em>Cypress</em> + RestAssured + Database + More"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://tesmon.io/"}}, "_tags": ["story", "author_jaraganittah", "story_40497644", "show_hn"], "author": "jaraganittah", "children": [40497735, 40507803], "created_at": "2024-05-28T05:36:14Z", "created_at_i": 1716874574, "num_comments": 2, "objectID": "40497644", "points": 14, "story_id": 40497644, "story_text": "I&#x27;m Nagaraj, the founder of Tesmon. Having been both a developer and an engineering leader, from big tech companies to startups, I&#x27;ve personally encountered the challenges of integration testing. It&#x27;s often so cumbersome that we resort to manual testing.<p>The world of testing is fragmented across multiple frameworks and platforms like Postman, RestAssured, pytest, Selenium, Cypress, Testim, and various custom in-house code using open source frameworks. Building and maintaining this testing infrastructure is not only time-consuming but can be a real pain.<p>Our mission is to revolutionize testing into an assertion-free, near zero effort process by leveraging AI&#x2F;ML.<p>Current challenges?<p>- Environment Differences: Tests often fail across different environments\u2014local, staging, production\u2014due to setup discrepancies and versioning issues between environments.<p>- Complex Negative Testing: Difficult to manage sequences like receiving an HTTP 200 followed by a 407 error for the same resource.<p>- Frequent Updates: Tests require extensive updates with each application change, including writing assertions for all critical fields in large response bodies.<p>- Unreliable Self-Healing: Self-healing tests frequently fail to correct issues automatically, necessitating manual intervention.<p>- Scalability Issues: No-code&#x2F;low-code UI tests struggle to scale, becoming a significant time drain as the test suite grows.<p>- Trade-offs Between Automation and Deadlines: Testing automation is often deprioritized due to urgent deadlines, leading to increased manual testing and the risk of regressions.<p>- Resource Limitations: Restricted access to essential databases and services due to compliance requirements, including the need to set up VPN access and manage Jenkins hosted in public clouds because of private subnet restrictions.<p>- Isolation Challenges: Testing everything from only the frontend or backend is often impractical, highlighting the need for integrated testing solutions.<p>- Effort in Framework Construction: Building a testing framework requires significant effort, especially when dealing with modularization and managing different testing environments.<p>How does testing code differ from product code?<p>Testing usually involves sending requests and checking responses, a process that doesn&#x27;t need to run continuously like microservices or batch jobs. This makes it perfect for AI&#x2F;ML automation, as the predictable nature of testing tasks allows for efficient learning and optimization without the complexity of continuous monitoring. Traditional programming adds unnecessary complexity to testing, which Tesmon simplifies.<p>Unit tests vs integration tests?<p>We cannot ship code without thorough integration testing, whether automated or manual. This step is critical for identifying significant issues that might not be caught by unit testing alone. Unit tests are primarily used for testing common code and libraries, ensuring that each individual component functions correctly in isolation.<p>Tesmon&#x27;s Approach: We&#x27;ve completely rethought testing with the Tesmon Platform:<p>- Interactive Local Testing: Directly interact with your APIs, databases, caches, Kafka, and more through Tesmon Desktop, which autonomously creates tests.<p>- Single-Click Test Updates: Tesmon adapts to changes in your system and can integrate updates with just a click.<p>- Extended Lifecycle with Tesmon Cloud: From local development through staging to production, Tesmon covers it all.<p>- Unified Frontend and Backend Testing: Execute comprehensive testing across both frontend and backend within a single test.<p>- Zero Assertions: Utilize AI&#x2F;ML models to ensure testing requires no manual assertions.<p>Get Started Now. Download Tesmon Desktop today on Mac&#x2F;Windows and transform your testing.<p><a href=\"https:&#x2F;&#x2F;tesmon.io&#x2F;desktop\" rel=\"nofollow\">https:&#x2F;&#x2F;tesmon.io&#x2F;desktop</a>", "title": "Show HN: Tesmon Platform == Postman + Cypress + RestAssured + Database + More", "updated_at": "2025-03-30T20:17:12Z", "url": "https://tesmon.io/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mikenikles"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "Hi,<p>I spent the last three months writing a 200+ page book, with 19 corresponding pull requests.<p>https://www.mikenikles.com/cloud-native-web-development<p>It's a hands-on guidebook on how to develop a cloud-native web application based on technologies I am familiar with. Svelte, Tailwind CSS, <em>Cypress</em>.io, Gitpod.io, Firebase &amp; Google Cloud at a high-level.<p>The goal was to walk through the entire end-to-end process, from zero to <em>production</em>! It walks through building a foundation needed to develop on top of. CI / CD, testing, feature toggles, <em>production</em> monitoring, etc.<p>To write it, I used Google Docs and had two git repositories: one to experiment and one that contains the final source code - where the git commit history looks like I never struggled :-).<p>AMA on self-publishing or the book's topic itself!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Cloud Native Web Development \u2013 I self-published my first book"}}, "_tags": ["story", "author_mikenikles", "story_23670911", "show_hn"], "author": "mikenikles", "children": [23676356], "created_at": "2020-06-28T18:26:38Z", "created_at_i": 1593368798, "num_comments": 2, "objectID": "23670911", "points": 9, "story_id": 23670911, "story_text": "Hi,<p>I spent the last three months writing a 200+ page book, with 19 corresponding pull requests.<p>https:&#x2F;&#x2F;www.mikenikles.com&#x2F;cloud-native-web-development<p>It&#x27;s a hands-on guidebook on how to develop a cloud-native web application based on technologies I am familiar with. Svelte, Tailwind CSS, Cypress.io, Gitpod.io, Firebase &amp; Google Cloud at a high-level.<p>The goal was to walk through the entire end-to-end process, from zero to production! It walks through building a foundation needed to develop on top of. CI &#x2F; CD, testing, feature toggles, production monitoring, etc.<p>To write it, I used Google Docs and had two git repositories: one to experiment and one that contains the final source code - where the git commit history looks like I never struggled :-).<p>AMA on self-publishing or the book&#x27;s topic itself!", "title": "Show HN: Cloud Native Web Development \u2013 I self-published my first book", "updated_at": "2024-09-20T06:28:36Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mouzogu"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "I took this from a job ad for front end developer in the UK (you can probably google it).<p>&gt; Proven experience of infrastructure planning and working with development teams to enable high availability of services in-line with SLOs and error budgets<p>&gt; Evangelist for infrastructure as code and working with immutable infrastructure and configuration tools such as Terraform to achieve end-to-end automation.<p>&gt; Experience working with CI/CD solutions and gitops to enable automation of infrastructure at scale (pipeline design, testing and best practices using Jenkins/Concourse/GoCD/Gitlab)<p>&gt; Programming and scripting to a <em>production</em> level including testing (Go/Python etc)\nCloud Platforms (designing infrastructure deployment patterns within GCP and using managed services, such as big data tooling effectively)<p>&gt; Logging systems (deployment of and scaling of logging solutions, both cloud native and self-hosted e.g. Stackdriver, Elastic)<p>I don't think any of this is &quot;front end&quot; to me.<p>Here is another one:<p>&gt; JavaScript, Typescript, React + Hooks, Redux, Node.js, HTML5 and SASS/CSS<p>&gt;  Webpack, Babel, Jest, <em>Cypress</em> and React Testing Library<p>&gt; UX/UI design<p>&gt; Docker and Kubernetes<p>&gt; CI/CD and modern DevOps tools<p>From this list, the only ones that are &quot;front end&quot; to me are JavaScript (for DOM only, no server-side), HTML, CSS/SASS. UX/UI design is not front end development to me, it is a separate field, albeit with some overlap like accessibility.<p>I really believe the traditional role of pure front end no longer exist.<p>When JavaScript became the <i>everything</i> language (thanks nodejs), the role of front end became more integrated with back-end due to the influx of back-end devs writing JS for server side. Now we see that most &quot;front end&quot; devs don't even write plain JavaScript, first it was TypeScript - a back-end language imo, and now we are seeing WASM as perhaps another nail in the coffin.<p>I remember reading an article on HN, proposing a &quot;front of the front end&quot; developer [0], which I think is pretty much symptomatic of the convoluted situation.<p>Before when i used to apply for jobs, i would get some kind of html/css test. Now when i apply i get a hacker-rank test to deal with some algorithms, and i have to choose from js,ts,python,c++ etc. I think it says it all.<p>[0] https://bradfrost.com/blog/post/front-of-the-front-end-and-back-of-the-front-end-web-development/"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Does pure Front End still exist?"}}, "_tags": ["story", "author_mouzogu", "story_32922214", "ask_hn"], "author": "mouzogu", "children": [32922537, 32924242, 32924357, 32925705, 32930888], "created_at": "2022-09-21T08:31:48Z", "created_at_i": 1663749108, "num_comments": 5, "objectID": "32922214", "points": 6, "story_id": 32922214, "story_text": "I took this from a job ad for front end developer in the UK (you can probably google it).<p>&gt; Proven experience of infrastructure planning and working with development teams to enable high availability of services in-line with SLOs and error budgets<p>&gt; Evangelist for infrastructure as code and working with immutable infrastructure and configuration tools such as Terraform to achieve end-to-end automation.<p>&gt; Experience working with CI&#x2F;CD solutions and gitops to enable automation of infrastructure at scale (pipeline design, testing and best practices using Jenkins&#x2F;Concourse&#x2F;GoCD&#x2F;Gitlab)<p>&gt; Programming and scripting to a production level including testing (Go&#x2F;Python etc)\nCloud Platforms (designing infrastructure deployment patterns within GCP and using managed services, such as big data tooling effectively)<p>&gt; Logging systems (deployment of and scaling of logging solutions, both cloud native and self-hosted e.g. Stackdriver, Elastic)<p>I don&#x27;t think any of this is &quot;front end&quot; to me.<p>Here is another one:<p>&gt; JavaScript, Typescript, React + Hooks, Redux, Node.js, HTML5 and SASS&#x2F;CSS<p>&gt;  Webpack, Babel, Jest, Cypress and React Testing Library<p>&gt; UX&#x2F;UI design<p>&gt; Docker and Kubernetes<p>&gt; CI&#x2F;CD and modern DevOps tools<p>From this list, the only ones that are &quot;front end&quot; to me are JavaScript (for DOM only, no server-side), HTML, CSS&#x2F;SASS. UX&#x2F;UI design is not front end development to me, it is a separate field, albeit with some overlap like accessibility.<p>I really believe the traditional role of pure front end no longer exist.<p>When JavaScript became the <i>everything</i> language (thanks nodejs), the role of front end became more integrated with back-end due to the influx of back-end devs writing JS for server side. Now we see that most &quot;front end&quot; devs don&#x27;t even write plain JavaScript, first it was TypeScript - a back-end language imo, and now we are seeing WASM as perhaps another nail in the coffin.<p>I remember reading an article on HN, proposing a &quot;front of the front end&quot; developer [0], which I think is pretty much symptomatic of the convoluted situation.<p>Before when i used to apply for jobs, i would get some kind of html&#x2F;css test. Now when i apply i get a hacker-rank test to deal with some algorithms, and i have to choose from js,ts,python,c++ etc. I think it says it all.<p>[0] https:&#x2F;&#x2F;bradfrost.com&#x2F;blog&#x2F;post&#x2F;front-of-the-front-end-and-back-of-the-front-end-web-development&#x2F;", "title": "Ask HN: Does pure Front End still exist?", "updated_at": "2024-09-20T12:03:00Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "creativedg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "I started this boilerplate in July 2020 and I\u2019ve been maintaining it for 5 years. It began on Next.js 9 and kept upgrading to Next.js 15+ (App Router), while upgrading the stack over time (Tailwind 1 \u2192 4, ESLint 8, swapping <em>Cypress</em> \u2192 Playwright, etc.). The goal is simple: I kept rebuilding the same setup, so I packaged it and kept it updated.<p>What you get (preconfigured, keep only what you need):<p>- Next.js 15 (App Router) + TypeScript + Tailwind 4<p>- Auth with Clerk (magic links, MFA, social, passkeys)<p>- I18n via next-intl<p>- DB with Drizzle ORM (PGlite locally)<p>- Forms with React Hook Form + Zod validation<p>- Testing: Vitest (unit), Playwright (integration/E2E)<p>- CI with GitHub Actions; Storybook for UI work<p>- SEO (Open Graph, JSON-LD, sitemap, robots)<p>- Observability: Sentry, logging with LogTape, log management &amp; uptime/monitoring<p>- Security: Arcjet (bot detection, rate limiting, shield rules)<p>- DX details: ESLint/Prettier, Lefthook + lint-staged, Commitlint, absolute imports, bundle analyzer<p>- AI code review<p>It\u2019s free and open source (MIT). Today the project sits around 11.8k GitHub stars and 2.2k forks. I\u2019m still actively maintaining it and adding features.<p>Repo: <a href=\"https://github.com/ixartz/Next-js-Boilerplate\" rel=\"nofollow\">https://github.com/ixartz/Next-js-Boilerplate</a><p>Why I built it<p>Spinning up auth, a DB, i18n, tests, and lint/format/CI for each new app was repetitive. This gives me (and hopefully you) a <em>production</em>-ready base in minutes, with opinionated defaults you can start.<p>I\u2019m open to suggestions and feedback, what would you like to see next? I\u2019ll hang around in the comments to answer questions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Open-source Next.js 15 boilerplate \u2013 auth, DB, intl, tests, monitoring"}}, "_tags": ["story", "author_creativedg", "story_45052744", "show_hn"], "author": "creativedg", "created_at": "2025-08-28T14:38:31Z", "created_at_i": 1756391911, "num_comments": 0, "objectID": "45052744", "points": 4, "story_id": 45052744, "story_text": "I started this boilerplate in July 2020 and I\u2019ve been maintaining it for 5 years. It began on Next.js 9 and kept upgrading to Next.js 15+ (App Router), while upgrading the stack over time (Tailwind 1 \u2192 4, ESLint 8, swapping Cypress \u2192 Playwright, etc.). The goal is simple: I kept rebuilding the same setup, so I packaged it and kept it updated.<p>What you get (preconfigured, keep only what you need):<p>- Next.js 15 (App Router) + TypeScript + Tailwind 4<p>- Auth with Clerk (magic links, MFA, social, passkeys)<p>- I18n via next-intl<p>- DB with Drizzle ORM (PGlite locally)<p>- Forms with React Hook Form + Zod validation<p>- Testing: Vitest (unit), Playwright (integration&#x2F;E2E)<p>- CI with GitHub Actions; Storybook for UI work<p>- SEO (Open Graph, JSON-LD, sitemap, robots)<p>- Observability: Sentry, logging with LogTape, log management &amp; uptime&#x2F;monitoring<p>- Security: Arcjet (bot detection, rate limiting, shield rules)<p>- DX details: ESLint&#x2F;Prettier, Lefthook + lint-staged, Commitlint, absolute imports, bundle analyzer<p>- AI code review<p>It\u2019s free and open source (MIT). Today the project sits around 11.8k GitHub stars and 2.2k forks. I\u2019m still actively maintaining it and adding features.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ixartz&#x2F;Next-js-Boilerplate\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ixartz&#x2F;Next-js-Boilerplate</a><p>Why I built it<p>Spinning up auth, a DB, i18n, tests, and lint&#x2F;format&#x2F;CI for each new app was repetitive. This gives me (and hopefully you) a production-ready base in minutes, with opinionated defaults you can start.<p>I\u2019m open to suggestions and feedback, what would you like to see next? I\u2019ll hang around in the comments to answer questions.", "title": "Show HN: Open-source Next.js 15 boilerplate \u2013 auth, DB, intl, tests, monitoring", "updated_at": "2025-08-29T18:59:31Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ajith-joseph"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "Hello HN! We\u2019re excited to share PerfAgents, a synthetic monitoring tool built for startups and enterprises seeking robust and proactive monitoring across global regions.<p>TL;DR:<p>PerfAgents is a synthetic monitoring platform that uses your existing E2E automation scripts (Playwright, Puppeteer, <em>Cypress</em>, and Selenium) for web app monitoring, reducing setup time by 80%. No complex integrations, no vendor lock-in, and proactive alerts that let you catch bugs before users do.<p>Problem We\u2019re Solving:<p>Ensuring application reliability across all regions is tough, especially when existing monitoring tools either require extensive DevOps/QA setup or lock users into proprietary workflows. This results in inefficient workflows, delayed issue detection, and, often, user-facing bugs in <em>production</em> environments.<p>How We Solved It:<p>PerfAgents offers a multi-framework approach that lets you monitor app functionality without needing new integrations or script re-recording. By reusing existing end-to-end scripts, PerfAgents makes setup fast, keeps monitoring flexible, and allows teams to detect and resolve issues faster.<p>Features include:<p>-&gt; Multi-framework support for Playwright, Puppeteer, <em>Cypress</em>, and Selenium<p>-&gt; AI-driven monitoring script generation to automate monitoring setups with no code<p>-&gt; Global test execution for instant insights across regions<p>-&gt; Real-time alerts integrated with popular tools (Slack, PagerDuty, Jira)<p>-&gt; Flexible pricing based on execution frequency, not script complexity<p>How It Works:<p>-&gt; Setup: Connect your GitHub repository to import existing scripts, or use our built-in AI tools for zero-code setups.<p>-&gt; Monitor &amp; Alert: Configure regional monitoring and alert channels, with real-time notifications when an issue occurs.<p>-&gt; Optimize &amp; Scale: Review logs, performance reports, and leverage our multi-framework support to refine application flow monitoring.<p>Key Benefits:<p>-&gt; Faster issue resolution: Early detection and instant alerts prevent downtime and improve stability.<p>-&gt; Cross-team collaboration: Centralized data helps DevOps, QA, and product teams collaborate more effectively.<p>-&gt; Flexible framework support: Avoid vendor lock-in with multi-framework compatibility.<p>-&gt; High scalability: Configurable monitoring counts ensure you\u2019re only paying for what you use.<p>PerfAgents is already in use by Fortune 500 companies and has helped SaaS and e-commerce teams reduce downtime by 40% and cut support tickets by nearly 57%.<p>Who It\u2019s For:<p>-&gt; DevOps, QA, and Engineering leaders looking to optimize monitoring setup and execution<p>-&gt; Teams already using frameworks like Playwright, Puppeteer, <em>Cypress</em>, or Selenium<p>-&gt; SaaS and e-commerce platforms where reliable, global user flows are critical<p>PerfAgents is available for a free trial now. We\u2019d love to hear your thoughts, and if you have any questions, feel free to ask!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: PerfAgents \u2013 Find Issues Before Your Users Do with Synthetic Monitoring"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.perfagents.com/"}}, "_tags": ["story", "author_ajith-joseph", "story_42149010", "show_hn"], "author": "ajith-joseph", "created_at": "2024-11-15T17:36:39Z", "created_at_i": 1731692199, "num_comments": 0, "objectID": "42149010", "points": 2, "story_id": 42149010, "story_text": "Hello HN! We\u2019re excited to share PerfAgents, a synthetic monitoring tool built for startups and enterprises seeking robust and proactive monitoring across global regions.<p>TL;DR:<p>PerfAgents is a synthetic monitoring platform that uses your existing E2E automation scripts (Playwright, Puppeteer, Cypress, and Selenium) for web app monitoring, reducing setup time by 80%. No complex integrations, no vendor lock-in, and proactive alerts that let you catch bugs before users do.<p>Problem We\u2019re Solving:<p>Ensuring application reliability across all regions is tough, especially when existing monitoring tools either require extensive DevOps&#x2F;QA setup or lock users into proprietary workflows. This results in inefficient workflows, delayed issue detection, and, often, user-facing bugs in production environments.<p>How We Solved It:<p>PerfAgents offers a multi-framework approach that lets you monitor app functionality without needing new integrations or script re-recording. By reusing existing end-to-end scripts, PerfAgents makes setup fast, keeps monitoring flexible, and allows teams to detect and resolve issues faster.<p>Features include:<p>-&gt; Multi-framework support for Playwright, Puppeteer, Cypress, and Selenium<p>-&gt; AI-driven monitoring script generation to automate monitoring setups with no code<p>-&gt; Global test execution for instant insights across regions<p>-&gt; Real-time alerts integrated with popular tools (Slack, PagerDuty, Jira)<p>-&gt; Flexible pricing based on execution frequency, not script complexity<p>How It Works:<p>-&gt; Setup: Connect your GitHub repository to import existing scripts, or use our built-in AI tools for zero-code setups.<p>-&gt; Monitor &amp; Alert: Configure regional monitoring and alert channels, with real-time notifications when an issue occurs.<p>-&gt; Optimize &amp; Scale: Review logs, performance reports, and leverage our multi-framework support to refine application flow monitoring.<p>Key Benefits:<p>-&gt; Faster issue resolution: Early detection and instant alerts prevent downtime and improve stability.<p>-&gt; Cross-team collaboration: Centralized data helps DevOps, QA, and product teams collaborate more effectively.<p>-&gt; Flexible framework support: Avoid vendor lock-in with multi-framework compatibility.<p>-&gt; High scalability: Configurable monitoring counts ensure you\u2019re only paying for what you use.<p>PerfAgents is already in use by Fortune 500 companies and has helped SaaS and e-commerce teams reduce downtime by 40% and cut support tickets by nearly 57%.<p>Who It\u2019s For:<p>-&gt; DevOps, QA, and Engineering leaders looking to optimize monitoring setup and execution<p>-&gt; Teams already using frameworks like Playwright, Puppeteer, Cypress, or Selenium<p>-&gt; SaaS and e-commerce platforms where reliable, global user flows are critical<p>PerfAgents is available for a free trial now. We\u2019d love to hear your thoughts, and if you have any questions, feel free to ask!", "title": "Show HN: PerfAgents \u2013 Find Issues Before Your Users Do with Synthetic Monitoring", "updated_at": "2024-11-15T18:08:38Z", "url": "https://www.perfagents.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ashish_sharda"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "I built this tool to generate comprehensive test cases from user stories using GPT-4, then auto-generate automation code in multiple frameworks (Selenium, Playwright, <em>Cypress</em>).<p>What takes a QA engineer 3-4 hours now takes 30 seconds.<p>Features:\n- Multi-industry support (Insurance, Fintech, E-commerce, Healthcare)\n- Generates positive, negative, edge cases, and security tests\n- Auto-generates <em>production</em>-ready automation code\n- Free tier: 5 test generations per day<p>Built and deployed in one day using Python, Streamlit, and GPT-4 to validate the concept.<p>Live demo: <a href=\"https://genai-test-gen.streamlit.app\" rel=\"nofollow\">https://genai-test-gen.streamlit.app</a><p>Happy to answer questions about the architecture, prompt engineering techniques, or how this fits into modern QA workflows."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: GenAI Test Case Generator \u2013 Reduces QA time by 80% using GPT-4"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://genai-test-gen.streamlit.app/"}}, "_tags": ["story", "author_ashish_sharda", "story_45592016", "show_hn"], "author": "ashish_sharda", "created_at": "2025-10-15T13:18:13Z", "created_at_i": 1760534293, "num_comments": 0, "objectID": "45592016", "points": 1, "story_id": 45592016, "story_text": "I built this tool to generate comprehensive test cases from user stories using GPT-4, then auto-generate automation code in multiple frameworks (Selenium, Playwright, Cypress).<p>What takes a QA engineer 3-4 hours now takes 30 seconds.<p>Features:\n- Multi-industry support (Insurance, Fintech, E-commerce, Healthcare)\n- Generates positive, negative, edge cases, and security tests\n- Auto-generates production-ready automation code\n- Free tier: 5 test generations per day<p>Built and deployed in one day using Python, Streamlit, and GPT-4 to validate the concept.<p>Live demo: <a href=\"https:&#x2F;&#x2F;genai-test-gen.streamlit.app\" rel=\"nofollow\">https:&#x2F;&#x2F;genai-test-gen.streamlit.app</a><p>Happy to answer questions about the architecture, prompt engineering techniques, or how this fits into modern QA workflows.", "title": "Show HN: GenAI Test Case Generator \u2013 Reduces QA time by 80% using GPT-4", "updated_at": "2025-10-15T13:21:51Z", "url": "https://genai-test-gen.streamlit.app/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bredren"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "I have a legacy ecommerce codebase that is primarily wordpress / php that has many plugins that want to be updated regularly.<p>While I'm triaging bugs and working toward a sane deployment I need to be able to more safely test that plugin updates are not breaking the site.<p>Ideally, I'd like to avoid anchoring the company to a paid service, and instead invest in building infrastructure if feasible.<p>I have the flexibility of running any testing tools I want, (I can set up a VPS and run anything against staging and <em>production</em>.)<p>I prefer / am best at Python and prefer containerization where possible.<p>I'm very experienced with github actions and ultimately want visual regression to be a part of CI/CD.<p>There hasn't been a post about this in some time, the active FOSS options I'm seeing include:<p>- BackstopJS<p>- <em>Cypress</em><p>Active commercial solutions include:<p>- Applitude<p>- Diffy<p>- Screenster<p>Any feedback on picking a headless browser or really the entire technology stack for visual testing would be appreciated."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: What do you recommend for Visual Regression Testing in 2021?"}}, "_tags": ["story", "author_bredren", "story_26780256", "ask_hn"], "author": "bredren", "created_at": "2021-04-12T15:11:49Z", "created_at_i": 1618240309, "num_comments": 0, "objectID": "26780256", "points": 1, "story_id": 26780256, "story_text": "I have a legacy ecommerce codebase that is primarily wordpress &#x2F; php that has many plugins that want to be updated regularly.<p>While I&#x27;m triaging bugs and working toward a sane deployment I need to be able to more safely test that plugin updates are not breaking the site.<p>Ideally, I&#x27;d like to avoid anchoring the company to a paid service, and instead invest in building infrastructure if feasible.<p>I have the flexibility of running any testing tools I want, (I can set up a VPS and run anything against staging and production.)<p>I prefer &#x2F; am best at Python and prefer containerization where possible.<p>I&#x27;m very experienced with github actions and ultimately want visual regression to be a part of CI&#x2F;CD.<p>There hasn&#x27;t been a post about this in some time, the active FOSS options I&#x27;m seeing include:<p>- BackstopJS<p>- Cypress<p>Active commercial solutions include:<p>- Applitude<p>- Diffy<p>- Screenster<p>Any feedback on picking a headless browser or really the entire technology stack for visual testing would be appreciated.", "title": "Ask HN: What do you recommend for Visual Regression Testing in 2021?", "updated_at": "2024-09-20T08:23:49Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "bertrand_charp"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "Hello Hacker News!<p>I am Bertrand from Pruna AI. With my associates, John, Rayan, and Stephan, we are fellow researchers in AI efficiency and reliability coming from TUM.<p>We are building an optimization engine that combines compression methods (e.g. quantization, pruning, compilation, batching\u2026) in the aim of saving compute power when running AI models. This optimization engine take one base model as input and returns a compressed model as output. It aims to help for two things:<p>- Make various AI models faster and/or smaller for various hardware (because they can require significant compute power to run).<p>- Easily apply one, but also, multiple compression methods on AI models (because it can take a lot of development time to <em>compress</em> models for <em>production</em>).<p>Currently, Pruna is designed only for inference optimization, not yet training. It focuses on Pytorch models and runs on Linux. It can be deployed in Docker, and is available either self-hosted via CLI (<a href=\"https://docs.pruna.ai/en/latest/setup/pip.html\" rel=\"nofollow\">https://docs.pruna.ai/en/latest/setup/pip.html</a>) or via the AWS Marketplace (<a href=\"https://aws.amazon.com/marketplace/pp/prodview-nqi4r52e2qnry\" rel=\"nofollow\">https://aws.amazon.com/marketplace/pp/prodview-nqi4r52e2qnry</a>).<p>For the last year, to ensure that our product was good enough, we did multiple things:<p>- We built a workflow to automatically scrape various Hugging Face models, run them through our tool, and push back the compressed version to Hugging Face (see 7,500 models available on Hugging Face (<a href=\"https://huggingface.co/PrunaAI\" rel=\"nofollow\">https://huggingface.co/PrunaAI</a>).<p>- We also created a benchmark page (special for Flux, soon for Llama) to showcase the results of all our internal testing: Flux Pruna Benchmark(<a href=\"https://flux-pruna-benchmark.vercel.app/\" rel=\"nofollow\">https://flux-pruna-benchmark.vercel.app/</a>). Every company we meet asks, \u201cDo you have numbers?\u201d\u2014and this isn\u2019t just a feature, it\u2019s our way of being transparent about what we bring to the table.<p>- We\u2019ve prepared examples loaded in Google Colabs and documentation to explain what the compression methods do (<a href=\"https://docs.pruna.ai/en/latest/index.html\" rel=\"nofollow\">https://docs.pruna.ai/en/latest/index.html</a>). In terms of compression methods, we aimed to integrate both existing and new compression methods that lead to efficiency gains. We are naturally interested if you have suggestions for other ones.<p>On the backend, we\u2019ve implemented a token system (<a href=\"https://docs.pruna.ai/en/latest/setup/token.html\" rel=\"nofollow\">https://docs.pruna.ai/en/latest/setup/token.html</a>). The token serves as a unique identifier when using the package. Upon your first call to the smash function <a href=\"https://docs.pruna.ai/en/latest/user_manual/smash.html\" rel=\"nofollow\">https://docs.pruna.ai/en/latest/user_manual/smash.html</a>), your token is automatically generated and printed in the console.<p>FYI, for now, we\u2019ve adopted a freemium model (up to 100 hours of runtime per month) with a soft limit (you can exceed it, theoretically, to avoid downtime \u2013 we\u2019ll see how it goes if there\u2019s abuse) as we\u2019re still evaluating the best monetization strategy. Our end goal is to combine open-source with feature-gating for enterprises, but we\u2019re not quite there yet. Think of this as an intermediate step.<p>I\u2019m really happy we get to share this with you all. Thanks for reading! Please let us know your thoughts and questions in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Pruna AI \u2013 Inference Optimization Engine"}}, "_tags": ["story", "author_bertrand_charp", "story_42369029", "show_hn"], "author": "bertrand_charp", "created_at": "2024-12-09T18:38:04Z", "created_at_i": 1733769484, "num_comments": 0, "objectID": "42369029", "points": 6, "story_id": 42369029, "story_text": "Hello Hacker News!<p>I am Bertrand from Pruna AI. With my associates, John, Rayan, and Stephan, we are fellow researchers in AI efficiency and reliability coming from TUM.<p>We are building an optimization engine that combines compression methods (e.g. quantization, pruning, compilation, batching\u2026) in the aim of saving compute power when running AI models. This optimization engine take one base model as input and returns a compressed model as output. It aims to help for two things:<p>- Make various AI models faster and&#x2F;or smaller for various hardware (because they can require significant compute power to run).<p>- Easily apply one, but also, multiple compression methods on AI models (because it can take a lot of development time to compress models for production).<p>Currently, Pruna is designed only for inference optimization, not yet training. It focuses on Pytorch models and runs on Linux. It can be deployed in Docker, and is available either self-hosted via CLI (<a href=\"https:&#x2F;&#x2F;docs.pruna.ai&#x2F;en&#x2F;latest&#x2F;setup&#x2F;pip.html\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.pruna.ai&#x2F;en&#x2F;latest&#x2F;setup&#x2F;pip.html</a>) or via the AWS Marketplace (<a href=\"https:&#x2F;&#x2F;aws.amazon.com&#x2F;marketplace&#x2F;pp&#x2F;prodview-nqi4r52e2qnry\" rel=\"nofollow\">https:&#x2F;&#x2F;aws.amazon.com&#x2F;marketplace&#x2F;pp&#x2F;prodview-nqi4r52e2qnry</a>).<p>For the last year, to ensure that our product was good enough, we did multiple things:<p>- We built a workflow to automatically scrape various Hugging Face models, run them through our tool, and push back the compressed version to Hugging Face (see 7,500 models available on Hugging Face (<a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;PrunaAI\" rel=\"nofollow\">https:&#x2F;&#x2F;huggingface.co&#x2F;PrunaAI</a>).<p>- We also created a benchmark page (special for Flux, soon for Llama) to showcase the results of all our internal testing: Flux Pruna Benchmark(<a href=\"https:&#x2F;&#x2F;flux-pruna-benchmark.vercel.app&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;flux-pruna-benchmark.vercel.app&#x2F;</a>). Every company we meet asks, \u201cDo you have numbers?\u201d\u2014and this isn\u2019t just a feature, it\u2019s our way of being transparent about what we bring to the table.<p>- We\u2019ve prepared examples loaded in Google Colabs and documentation to explain what the compression methods do (<a href=\"https:&#x2F;&#x2F;docs.pruna.ai&#x2F;en&#x2F;latest&#x2F;index.html\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.pruna.ai&#x2F;en&#x2F;latest&#x2F;index.html</a>). In terms of compression methods, we aimed to integrate both existing and new compression methods that lead to efficiency gains. We are naturally interested if you have suggestions for other ones.<p>On the backend, we\u2019ve implemented a token system (<a href=\"https:&#x2F;&#x2F;docs.pruna.ai&#x2F;en&#x2F;latest&#x2F;setup&#x2F;token.html\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.pruna.ai&#x2F;en&#x2F;latest&#x2F;setup&#x2F;token.html</a>). The token serves as a unique identifier when using the package. Upon your first call to the smash function <a href=\"https:&#x2F;&#x2F;docs.pruna.ai&#x2F;en&#x2F;latest&#x2F;user_manual&#x2F;smash.html\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.pruna.ai&#x2F;en&#x2F;latest&#x2F;user_manual&#x2F;smash.html</a>), your token is automatically generated and printed in the console.<p>FYI, for now, we\u2019ve adopted a freemium model (up to 100 hours of runtime per month) with a soft limit (you can exceed it, theoretically, to avoid downtime \u2013 we\u2019ll see how it goes if there\u2019s abuse) as we\u2019re still evaluating the best monetization strategy. Our end goal is to combine open-source with feature-gating for enterprises, but we\u2019re not quite there yet. Think of this as an intermediate step.<p>I\u2019m really happy we get to share this with you all. Thanks for reading! Please let us know your thoughts and questions in the comments.", "title": "Show HN: Pruna AI \u2013 Inference Optimization Engine", "updated_at": "2024-12-10T10:15:06Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "antonap"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "Hi HN, we are the founders of Relari, the company behind continuous-eval (<a href=\"https://github.com/relari-ai/continuous-eval\">https://github.com/relari-ai/continuous-eval</a>), an evaluation framework that lets you test your GenAI systems at the component level, pinpointing issues where they originate.<p>We experienced the need for this when we were building a copilot for bankers. Our RAG pipeline blew up in complexity as we added components: a query classifier (to triage user intent), multiple retrievers (to grab information from different sources), filtering LLM (to rerank / <em>compress</em> context), a calculator agent (to call financial functions) and finally the synthesizer LLM that gives the answer. Ensuring reliability became more difficult with each of these we added.<p>When a bad response was detected by our answer evaluator, we had to backtrack multiple steps to understand which component(s) made a mistake. But this quickly became unscalable beyond a few samples.<p>I did my Ph.D. in fault detection for autonomous vehicles, and I see a strong parallel between the complexity of autonomous driving software and today's LLM pipelines. In self-driving systems, sensors, perception, prediction, planning, and control modules are all chained together. To ensure system-level safety, we use granular metrics to measure the performance of each module individually. When the vehicle makes an unexpected decision, we use these metrics to pinpoint the problem to a specific component. Only then we can make targeted improvements, systematically.<p>Based on this thinking, we developed the first version of continuous-eval for ourselves. Since then we\u2019ve made it more flexible to fit various types of GenAI pipelines. Continuous-eval allows you to describe (programmatically) your pipeline and modules, and select metrics for each module. We developed 30+ metrics to cover retrieval, text generation, code generation, classification, agent tool use, etc. We now have a number of companies using us to test complex pipelines like finance copilots, enterprise search, coding agents, etc.<p>As an example, one customer was trying to understand why their RAG system did poorly on trend analysis queries. Through continuous-eval, they realized that the \u201cretriever\u201d component was retrieving 80%+ of all relevant chunks, but the \u201creranker\u201d component, that filters out \u201cirrelevant\u201d context, was dropping that to below 50%. This enabled them to fix the problem, in their case by skipping the reranker for certain queries.<p>We\u2019ve also built ensemble metrics that do a surprisingly good job of predicting user feedback. Users often rate LLM-generated answers by giving a thumbs up/down about how good the answer was. We train our custom metrics on this user data, and then use those metrics to generate thumbs up/down ratings on future LLM answers. The results turn out to be 90% aligned with what the users say. This gives developers a feedback loop from <em>production</em> data to offline testing and development. Some customers have found this to be our most unique advantage.<p>Lastly, to make the most out of evaluation, you should use a diverse dataset\u2014ideally with ground truth labels for comprehensive and consistent assessment. Because ground truth labels are costly and time-consuming to curate manually, we also have a synthetic data generation pipeline that allows you to get started quickly. Try it here (<a href=\"https://www.relari.ai/#synthetic_data_demo\" rel=\"nofollow\">https://www.relari.ai/#synthetic_data_demo</a>)<p>What\u2019s been your experience testing and iterating LLM apps? Please let us know your thoughts and feedback on our approaches (modular framework, leveraging user feedback, testing with synthetic data)."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Relari (YC W24) \u2013 Identify the root cause of problems in LLM apps"}}, "_tags": ["story", "author_antonap", "story_39641105", "launch_hn"], "author": "antonap", "children": [39641803, 39642193, 39642521, 39643395, 39644477, 39653898], "created_at": "2024-03-08T14:00:09Z", "created_at_i": 1709906409, "num_comments": 15, "objectID": "39641105", "points": 106, "story_id": 39641105, "story_text": "Hi HN, we are the founders of Relari, the company behind continuous-eval (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;relari-ai&#x2F;continuous-eval\">https:&#x2F;&#x2F;github.com&#x2F;relari-ai&#x2F;continuous-eval</a>), an evaluation framework that lets you test your GenAI systems at the component level, pinpointing issues where they originate.<p>We experienced the need for this when we were building a copilot for bankers. Our RAG pipeline blew up in complexity as we added components: a query classifier (to triage user intent), multiple retrievers (to grab information from different sources), filtering LLM (to rerank &#x2F; compress context), a calculator agent (to call financial functions) and finally the synthesizer LLM that gives the answer. Ensuring reliability became more difficult with each of these we added.<p>When a bad response was detected by our answer evaluator, we had to backtrack multiple steps to understand which component(s) made a mistake. But this quickly became unscalable beyond a few samples.<p>I did my Ph.D. in fault detection for autonomous vehicles, and I see a strong parallel between the complexity of autonomous driving software and today&#x27;s LLM pipelines. In self-driving systems, sensors, perception, prediction, planning, and control modules are all chained together. To ensure system-level safety, we use granular metrics to measure the performance of each module individually. When the vehicle makes an unexpected decision, we use these metrics to pinpoint the problem to a specific component. Only then we can make targeted improvements, systematically.<p>Based on this thinking, we developed the first version of continuous-eval for ourselves. Since then we\u2019ve made it more flexible to fit various types of GenAI pipelines. Continuous-eval allows you to describe (programmatically) your pipeline and modules, and select metrics for each module. We developed 30+ metrics to cover retrieval, text generation, code generation, classification, agent tool use, etc. We now have a number of companies using us to test complex pipelines like finance copilots, enterprise search, coding agents, etc.<p>As an example, one customer was trying to understand why their RAG system did poorly on trend analysis queries. Through continuous-eval, they realized that the \u201cretriever\u201d component was retrieving 80%+ of all relevant chunks, but the \u201creranker\u201d component, that filters out \u201cirrelevant\u201d context, was dropping that to below 50%. This enabled them to fix the problem, in their case by skipping the reranker for certain queries.<p>We\u2019ve also built ensemble metrics that do a surprisingly good job of predicting user feedback. Users often rate LLM-generated answers by giving a thumbs up&#x2F;down about how good the answer was. We train our custom metrics on this user data, and then use those metrics to generate thumbs up&#x2F;down ratings on future LLM answers. The results turn out to be 90% aligned with what the users say. This gives developers a feedback loop from production data to offline testing and development. Some customers have found this to be our most unique advantage.<p>Lastly, to make the most out of evaluation, you should use a diverse dataset\u2014ideally with ground truth labels for comprehensive and consistent assessment. Because ground truth labels are costly and time-consuming to curate manually, we also have a synthetic data generation pipeline that allows you to get started quickly. Try it here (<a href=\"https:&#x2F;&#x2F;www.relari.ai&#x2F;#synthetic_data_demo\" rel=\"nofollow\">https:&#x2F;&#x2F;www.relari.ai&#x2F;#synthetic_data_demo</a>)<p>What\u2019s been your experience testing and iterating LLM apps? Please let us know your thoughts and feedback on our approaches (modular framework, leveraging user feedback, testing with synthetic data).", "title": "Launch HN: Relari (YC W24) \u2013 Identify the root cause of problems in LLM apps", "updated_at": "2024-09-20T16:36:50Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mogoman"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "I'm trying to think up ideas of what I could do with an intermittent &quot;excess&quot; of electricity - only at certain hours of the day up to a certain (fixed) amount. Let's say around 10kW hours a day between 12pm and 3pm (I've described my case below, but other people would have different scenarios).<p>Ideally the extra energy would be used to help reduce environmental impact rather than making money, so crypto mining (for example) is not an option.<p>Here are some of my thoughts:<p>- heat water for home usage (heat pump); this solution is already deployed, but there is only so much hot water one needs\n- charge an EV; I don't own one presently, but it an option for the future\n- fight the current 70% rule in Germany (this is the cause of my excess, see below)\n- generate H2 via electrolysis; not sure what to do with H2 then. What would be interesting would be to be able to &quot;charge&quot; fuel cells and store them for use in winter, or <em>compress</em> H2 into cylinders? Not sure what can fit into my garage / would be affordable.\n- pumped water storage; I suspect the energies involved at home are insignificant vs industrial hydro solutions\n- convert to microwave and beam into space\n- run an industrial press to <em>compress</em> my lawn cuttings / garden waste into pellets and dump them into a deep lake\n- a magical machine that extracts Co2 from the air and produces a fine black powder which I could <em>compress</em> and dump into a deep lake\n- ...<p>Some background:<p>In Germany, excess domestic solar panel <em>production</em> can be pushed into the grid for a small (cash) refund, but there is a strange rule : only 70% of the maximum panel output is allowed to be pushed. So if the panels could generate 10kW at midday, without anything turned on at home, a maximum of 7kW can be pushed. This effect is only for a few &quot;peak sun&quot; hours (say 12pm to 3pm), but means that roughly 10kW hours of electricity is &quot;lost&quot; (the system is throttled back via the charge controller, so the energy isn't lost or wasted, it just isn't produced)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Ideas for useful uses of excess electricity?"}}, "_tags": ["story", "author_mogoman", "story_31593183", "ask_hn"], "author": "mogoman", "children": [31594066, 31594379, 31594827, 31594844, 31594847, 31595027, 31595182, 31595287, 31595320, 31595619, 31596757], "created_at": "2022-06-02T10:08:09Z", "created_at_i": 1654164489, "num_comments": 19, "objectID": "31593183", "points": 5, "story_id": 31593183, "story_text": "I&#x27;m trying to think up ideas of what I could do with an intermittent &quot;excess&quot; of electricity - only at certain hours of the day up to a certain (fixed) amount. Let&#x27;s say around 10kW hours a day between 12pm and 3pm (I&#x27;ve described my case below, but other people would have different scenarios).<p>Ideally the extra energy would be used to help reduce environmental impact rather than making money, so crypto mining (for example) is not an option.<p>Here are some of my thoughts:<p>- heat water for home usage (heat pump); this solution is already deployed, but there is only so much hot water one needs\n- charge an EV; I don&#x27;t own one presently, but it an option for the future\n- fight the current 70% rule in Germany (this is the cause of my excess, see below)\n- generate H2 via electrolysis; not sure what to do with H2 then. What would be interesting would be to be able to &quot;charge&quot; fuel cells and store them for use in winter, or compress H2 into cylinders? Not sure what can fit into my garage &#x2F; would be affordable.\n- pumped water storage; I suspect the energies involved at home are insignificant vs industrial hydro solutions\n- convert to microwave and beam into space\n- run an industrial press to compress my lawn cuttings &#x2F; garden waste into pellets and dump them into a deep lake\n- a magical machine that extracts Co2 from the air and produces a fine black powder which I could compress and dump into a deep lake\n- ...<p>Some background:<p>In Germany, excess domestic solar panel production can be pushed into the grid for a small (cash) refund, but there is a strange rule : only 70% of the maximum panel output is allowed to be pushed. So if the panels could generate 10kW at midday, without anything turned on at home, a maximum of 7kW can be pushed. This effect is only for a few &quot;peak sun&quot; hours (say 12pm to 3pm), but means that roughly 10kW hours of electricity is &quot;lost&quot; (the system is throttled back via the charge controller, so the energy isn&#x27;t lost or wasted, it just isn&#x27;t produced)", "title": "Ask HN: Ideas for useful uses of excess electricity?", "updated_at": "2025-02-09T02:04:53Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "VladCovaci"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "This is the development process we use to build MVPs and internal tools.<p>To move fast, we combine multiple tools, AI agents, and systems. This lets us <em>compress</em> the product development lifecycle down to 1\u20132 days.<p>Here\u2019s the high-level flow:\nIdea \u2192 Boilerplate \u2192 AI Planning Agents \u2192 Core Features (Claude / Codex / Gemini) \u2192 Deployment<p>Every tool includes repeatable features such as emails, payments, and marketing pages. To avoid rebuilding these each time, we created a modular internal boilerplate which can be seen here. This modular approach allows us to change the design very easily and focus only on the core features of the product. Once the boilerplate is set up, we are ready to go. The documentation can also be found here. The boilerplate features are outlined below:<p>Boilerplate features:\nMarketing pages: Home, About, Pricing, Blog, Contact, Services, Legal Pages\nAuthentification: NextAuth &amp; Google Auth\nPayment\nEmails\nNotifications\nDashboard Structure\nFeature Gating\nSEO &amp; GEO ready\nDatabase Setup<p>AI Planning Agents<p>AI Planning Agents act as our internal agile team.<p>When building with AI, strong planning is essential to ensure the development agent operates within clear guardrails. These agents live directly inside our codebase, making it easy to provide full context for the features we want to build.<p>A simple flow looks like this:<p>Analyst Agent \u2192 creates the Product Brief (http://brief.md) \u2192 PM Agent \u2192 creates the PRD (http://prd.md) \u2192 Architect Agent \u2192 creates the System Architecture (http://architecture.md)\n\u2192 PM Agent \u2192 creates the Epics &amp; Stories (http://epics.md, http://stories.md)<p>Why are these so important? This process gives both us and the development AI agent a clear execution plan with strong guardrails. As a result, the agent does not hallucinate and builds exactly what is required, in the way it is required.<p>Here is an example of one story:<p>## Story 2.9: Send Email Notifications to Submitters on Status Changes\nAs a *feedback submitter*,\nI want *to receive an email when my feedback status changes (e.g., Doing \u2192 Testing \u2192 Finished)*,\nso that *I know the team is working on my suggestion and can see progress*.\n### Acceptance Criteria\n1. When team member changes feedback item status (Story 2.5 drag-and-drop), trigger email notification\n2. Email sent only if submitter provided email address during submission (FR17)\n3. Email subject: &quot;[Project Name] Update: Your feedback is now [Status]&quot;\n4. Email body includes: original feedback title, new status, team comment (if any), link to view on public board\n5. Email sent asynchronously (doesn't block status update)\n6. If email sending fails, log error but allow status update to succeed (NFR12)\n7. No duplicate emails if status changes multiple times quickly (debounce or queue)\n8. Unsubscribe link included (placeholder for now)\n9. Test email delivery in development and <em>production</em><p>Now that we have everything in place the boilerplate with all repeatable product features (login, dashboard, payments, emails, etc.) and the planning stage completed with clear focus, guardrails, user stories, and architecture we have all the context needed to build with AI (Claude, Codex, or Gemini).<p>In this phase, development happens story by story. With the full planning context in place, the AI agent implements exactly what is required. Depending on the number of features, we can deploy and have a live product ready for real user validation in 1\u20132 days.<p>Here is an example of what we manage to achieve:<p>https://startupkit.today\nhttps://founderspace.work"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Playbook: How to vibe code a successful app"}}, "_tags": ["story", "author_VladCovaci", "story_47048167", "ask_hn"], "author": "VladCovaci", "children": [47048335], "created_at": "2026-02-17T14:57:29Z", "created_at_i": 1771340249, "num_comments": 1, "objectID": "47048167", "points": 2, "story_id": 47048167, "story_text": "This is the development process we use to build MVPs and internal tools.<p>To move fast, we combine multiple tools, AI agents, and systems. This lets us compress the product development lifecycle down to 1\u20132 days.<p>Here\u2019s the high-level flow:\nIdea \u2192 Boilerplate \u2192 AI Planning Agents \u2192 Core Features (Claude &#x2F; Codex &#x2F; Gemini) \u2192 Deployment<p>Every tool includes repeatable features such as emails, payments, and marketing pages. To avoid rebuilding these each time, we created a modular internal boilerplate which can be seen here. This modular approach allows us to change the design very easily and focus only on the core features of the product. Once the boilerplate is set up, we are ready to go. The documentation can also be found here. The boilerplate features are outlined below:<p>Boilerplate features:\nMarketing pages: Home, About, Pricing, Blog, Contact, Services, Legal Pages\nAuthentification: NextAuth &amp; Google Auth\nPayment\nEmails\nNotifications\nDashboard Structure\nFeature Gating\nSEO &amp; GEO ready\nDatabase Setup<p>AI Planning Agents<p>AI Planning Agents act as our internal agile team.<p>When building with AI, strong planning is essential to ensure the development agent operates within clear guardrails. These agents live directly inside our codebase, making it easy to provide full context for the features we want to build.<p>A simple flow looks like this:<p>Analyst Agent \u2192 creates the Product Brief (http:&#x2F;&#x2F;brief.md) \u2192 PM Agent \u2192 creates the PRD (http:&#x2F;&#x2F;prd.md) \u2192 Architect Agent \u2192 creates the System Architecture (http:&#x2F;&#x2F;architecture.md)\n\u2192 PM Agent \u2192 creates the Epics &amp; Stories (http:&#x2F;&#x2F;epics.md, http:&#x2F;&#x2F;stories.md)<p>Why are these so important? This process gives both us and the development AI agent a clear execution plan with strong guardrails. As a result, the agent does not hallucinate and builds exactly what is required, in the way it is required.<p>Here is an example of one story:<p>## Story 2.9: Send Email Notifications to Submitters on Status Changes\nAs a *feedback submitter*,\nI want *to receive an email when my feedback status changes (e.g., Doing \u2192 Testing \u2192 Finished)*,\nso that *I know the team is working on my suggestion and can see progress*.\n### Acceptance Criteria\n1. When team member changes feedback item status (Story 2.5 drag-and-drop), trigger email notification\n2. Email sent only if submitter provided email address during submission (FR17)\n3. Email subject: &quot;[Project Name] Update: Your feedback is now [Status]&quot;\n4. Email body includes: original feedback title, new status, team comment (if any), link to view on public board\n5. Email sent asynchronously (doesn&#x27;t block status update)\n6. If email sending fails, log error but allow status update to succeed (NFR12)\n7. No duplicate emails if status changes multiple times quickly (debounce or queue)\n8. Unsubscribe link included (placeholder for now)\n9. Test email delivery in development and production<p>Now that we have everything in place the boilerplate with all repeatable product features (login, dashboard, payments, emails, etc.) and the planning stage completed with clear focus, guardrails, user stories, and architecture we have all the context needed to build with AI (Claude, Codex, or Gemini).<p>In this phase, development happens story by story. With the full planning context in place, the AI agent implements exactly what is required. Depending on the number of features, we can deploy and have a live product ready for real user validation in 1\u20132 days.<p>Here is an example of what we manage to achieve:<p>https:&#x2F;&#x2F;startupkit.today\nhttps:&#x2F;&#x2F;founderspace.work", "title": "Playbook: How to vibe code a successful app", "updated_at": "2026-02-18T10:22:06Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "manthangupta109"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "I went through both the blog and the code for Observational Memory, and really interesting direction, and I appreciate the transparency in sharing implementation details. I do have a few thoughts on the SOTA memory claim and the broader framing.<p>From what I can see:<p>1. The implementation appears heavily tuned toward performing well on LongMemEval. That's a useful signal, but it doesn't necessarily translate to robust long-term memory behavior in <em>production</em> environments.<p>2. It feels closer to context compression/context management than a durable long-term agent memory system. This will perform really well for a single long-running task<p>3. Both the Observer and Reflector rewrite memory in compressed form. That's helpful for token control, but compression is inherently lossy and can drop smaller details that might become important later.<p>4. The Reflector seems to validate success primarily via token thresholds, rather than checking whether the rewritten memory remains semantically faithful to the original. Over time, this could allow memory drift.<p>5. The Observer prompt may introduce assumptions (e.g., inferring that a planned action happened if enough time has passed), which risks creating incorrect memories.<p>6. The design appears to emphasize recency when rewriting observations. While that keeps context fresh, it may bias the system toward recent information and gradually <em>compress</em> away older but still important details. Durable memory systems usually need mechanisms to preserve salient long-term facts, not just recent activity.<p>7. The full observations block is repeatedly injected into context. This may increase token cost and introduce irrelevant noise depending on the task.<p>8. There appears to be limited grounding back to raw message evidence at response time, which makes it harder to detect and correct incorrect compressed memories.<p>9. Finally, I think we should be cautious about claiming &quot;SOTA&quot; based on performance on a single benchmark. LongMemEval results may demonstrate strong performance on that setup, but <em>production</em> workloads are much messier. Robustness, drift, grounding, and cost behavior typically show up only under sustained real-world usage.<p>Overall, this looks like a strong benchmark-oriented context handling. I am just less convinced that it yet qualifies as a robust, general-purpose long-term memory system. Curious how the team is thinking about these trade-offs beyond benchmark performance."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Views on Mastra's SOTA Memory?"}}, "_tags": ["story", "author_manthangupta109", "story_46992444", "ask_hn"], "author": "manthangupta109", "children": [47078600], "created_at": "2026-02-12T17:59:17Z", "created_at_i": 1770919157, "num_comments": 1, "objectID": "46992444", "points": 2, "story_id": 46992444, "story_text": "I went through both the blog and the code for Observational Memory, and really interesting direction, and I appreciate the transparency in sharing implementation details. I do have a few thoughts on the SOTA memory claim and the broader framing.<p>From what I can see:<p>1. The implementation appears heavily tuned toward performing well on LongMemEval. That&#x27;s a useful signal, but it doesn&#x27;t necessarily translate to robust long-term memory behavior in production environments.<p>2. It feels closer to context compression&#x2F;context management than a durable long-term agent memory system. This will perform really well for a single long-running task<p>3. Both the Observer and Reflector rewrite memory in compressed form. That&#x27;s helpful for token control, but compression is inherently lossy and can drop smaller details that might become important later.<p>4. The Reflector seems to validate success primarily via token thresholds, rather than checking whether the rewritten memory remains semantically faithful to the original. Over time, this could allow memory drift.<p>5. The Observer prompt may introduce assumptions (e.g., inferring that a planned action happened if enough time has passed), which risks creating incorrect memories.<p>6. The design appears to emphasize recency when rewriting observations. While that keeps context fresh, it may bias the system toward recent information and gradually compress away older but still important details. Durable memory systems usually need mechanisms to preserve salient long-term facts, not just recent activity.<p>7. The full observations block is repeatedly injected into context. This may increase token cost and introduce irrelevant noise depending on the task.<p>8. There appears to be limited grounding back to raw message evidence at response time, which makes it harder to detect and correct incorrect compressed memories.<p>9. Finally, I think we should be cautious about claiming &quot;SOTA&quot; based on performance on a single benchmark. LongMemEval results may demonstrate strong performance on that setup, but production workloads are much messier. Robustness, drift, grounding, and cost behavior typically show up only under sustained real-world usage.<p>Overall, this looks like a strong benchmark-oriented context handling. I am just less convinced that it yet qualifies as a robust, general-purpose long-term memory system. Curious how the team is thinking about these trade-offs beyond benchmark performance.", "title": "Ask HN: Views on Mastra's SOTA Memory?", "updated_at": "2026-02-26T16:51:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "SougataAS"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "Hi HN,\nI'm a solo developer from India who spent 3 months building ArchtSoft - an AI platform that takes your business requirements and generates a complete software architecture with:<p>- 6 architecture pattern suggestions with comparative scoring\n- Industry-specific tech stack recommendations (FinTech, Healthcare, etc.)\n- Editable architecture diagrams\n- Security models, compliance docs, IaC code<p>*The problem I'm solving:*\nTeams spend 2-3 weeks debating architecture decisions. I wanted to <em>compress</em> this to 2-3 hours with AI-powered analysis.<p>*Example:*\nUpload a BRD for a payment system \u2192 Get microservices vs monolith comparison \u2192 Choose one \u2192 Get complete diagram with Stripe, PostgreSQL, Redis recommendations \u2192 Export IaC code<p>*Try it:* https://archtsoft.com<p>*Challenges I'm facing:*\n1. Hard to get feedback (Indian users are polite but silent)\n2. Uncertain if AI quality is good enough for <em>production</em>\n3. Too many features? Should I simplify?<p>Would love brutal feedback from this community. What would make you trust AI for architecture decisions?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "ArchtSoft \u2013 AI generates software architecture from requirements"}}, "_tags": ["story", "author_SougataAS", "story_45992862", "ask_hn"], "author": "SougataAS", "children": [46081432], "created_at": "2025-11-20T14:24:03Z", "created_at_i": 1763648643, "num_comments": 0, "objectID": "45992862", "points": 2, "story_id": 45992862, "story_text": "Hi HN,\nI&#x27;m a solo developer from India who spent 3 months building ArchtSoft - an AI platform that takes your business requirements and generates a complete software architecture with:<p>- 6 architecture pattern suggestions with comparative scoring\n- Industry-specific tech stack recommendations (FinTech, Healthcare, etc.)\n- Editable architecture diagrams\n- Security models, compliance docs, IaC code<p>*The problem I&#x27;m solving:*\nTeams spend 2-3 weeks debating architecture decisions. I wanted to compress this to 2-3 hours with AI-powered analysis.<p>*Example:*\nUpload a BRD for a payment system \u2192 Get microservices vs monolith comparison \u2192 Choose one \u2192 Get complete diagram with Stripe, PostgreSQL, Redis recommendations \u2192 Export IaC code<p>*Try it:* https:&#x2F;&#x2F;archtsoft.com<p>*Challenges I&#x27;m facing:*\n1. Hard to get feedback (Indian users are polite but silent)\n2. Uncertain if AI quality is good enough for production\n3. Too many features? Should I simplify?<p>Would love brutal feedback from this community. What would make you trust AI for architecture decisions?", "title": "ArchtSoft \u2013 AI generates software architecture from requirements", "updated_at": "2025-12-03T10:13:14Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "barfbagginus"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["cypress", "production"], "value": "I've got a 20 gig database of business pictures in JPEG format - they're closeup pictures of boat decks from 700 different boats. However many boats are similar, and many images share identical pixels from slightly different views. So I'm wondering if I can <em>compress</em> them with a differ compression engine that better exploits the similarity and overlap between images. I'm also wondering if I could fine tune a neural network image compressor.<p>What tools would you use in <em>production</em>? Or is this still experimental territory? And if so, which projects should I invest in?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "CSI: Compressing Similar Images"}}, "_tags": ["story", "author_barfbagginus", "story_40600992", "ask_hn"], "author": "barfbagginus", "children": [40602404], "created_at": "2024-06-06T18:49:42Z", "created_at_i": 1717699782, "num_comments": 2, "objectID": "40600992", "points": 1, "story_id": 40600992, "story_text": "I&#x27;ve got a 20 gig database of business pictures in JPEG format - they&#x27;re closeup pictures of boat decks from 700 different boats. However many boats are similar, and many images share identical pixels from slightly different views. So I&#x27;m wondering if I can compress them with a differ compression engine that better exploits the similarity and overlap between images. I&#x27;m also wondering if I could fine tune a neural network image compressor.<p>What tools would you use in production? Or is this still experimental territory? And if so, which projects should I invest in?", "title": "CSI: Compressing Similar Images", "updated_at": "2024-09-20T17:09:18Z"}], "hitsPerPage": 15, "nbHits": 19, "nbPages": 2, "page": 0, "params": "query=cypress+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 15, "processingTimingsMS": {"_request": {"roundTrip": 16}, "afterFetch": {"format": {"highlighting": 2, "total": 2}}, "fetch": {"query": 11, "scanning": 2, "total": 14}, "total": 15}, "query": "cypress production", "serverTimeMS": 18}}