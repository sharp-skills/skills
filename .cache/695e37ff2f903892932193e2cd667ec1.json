{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "levkk"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "Hey HN! Lev and Justin here, authors of PgDog (<a href=\"https://pgdog.dev/\">https://pgdog.dev/</a>), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that works without requiring application code changes or database migrations.<p>Our post from last year: <a href=\"https://news.ycombinator.com/item?id=44099187\">https://news.ycombinator.com/item?id=44099187</a><p>The most important update: we are in <em>production</em>. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway.<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE/DROP/ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important because most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, Sequelize, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth a look.<p>If you\u2019re like us and prefer integers to <em>UUIDs</em> for your primary keys, we built a cross-shard unique sequence, directly inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, we can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a promoted primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with INSERT/UPDATE statements, but if you still prefer to handle your read/write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python/Ruby/Go apps, this helps by reducing memory usage, I/O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by an application crash, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending the Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off/on, so should you choose to give it a try, you can do so at your own pace. Our docs (<a href=\"https://docs.pgdog.dev\">https://docs.pgdog.dev</a>) should help too.<p>Thanks for reading and happy hacking!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: PgDog \u2013 Scale Postgres without changing the app"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/pgdogdev/pgdog"}}, "_tags": ["story", "author_levkk", "story_47123631", "show_hn"], "author": "levkk", "children": [47125519, 47125908, 47126058, 47126391, 47126668, 47126736, 47126921, 47127155, 47127202, 47127712, 47128494, 47128649, 47130284, 47130616, 47131018, 47131119, 47131282, 47132290, 47132869, 47133196, 47133746, 47135830, 47149003, 47154215], "created_at": "2026-02-23T15:33:24Z", "created_at_i": 1771860804, "num_comments": 62, "objectID": "47123631", "points": 322, "story_id": 47123631, "story_text": "Hey HN! Lev and Justin here, authors of PgDog (<a href=\"https:&#x2F;&#x2F;pgdog.dev&#x2F;\">https:&#x2F;&#x2F;pgdog.dev&#x2F;</a>), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that works without requiring application code changes or database migrations.<p>Our post from last year: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44099187\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44099187</a><p>The most important update: we are in production. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway.<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE&#x2F;DROP&#x2F;ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important because most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, Sequelize, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth a look.<p>If you\u2019re like us and prefer integers to UUIDs for your primary keys, we built a cross-shard unique sequence, directly inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, we can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a promoted primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with INSERT&#x2F;UPDATE statements, but if you still prefer to handle your read&#x2F;write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python&#x2F;Ruby&#x2F;Go apps, this helps by reducing memory usage, I&#x2F;O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by an application crash, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending the Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off&#x2F;on, so should you choose to give it a try, you can do so at your own pace. Our docs (<a href=\"https:&#x2F;&#x2F;docs.pgdog.dev\">https:&#x2F;&#x2F;docs.pgdog.dev</a>) should help too.<p>Thanks for reading and happy hacking!", "title": "Show HN: PgDog \u2013 Scale Postgres without changing the app", "updated_at": "2026-02-27T15:48:13Z", "url": "https://github.com/pgdogdev/pgdog"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "glimow"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "Hey HN! We\u2019re Tristan and Antoine, co-founders of Escape (<a href=\"https://escape.tech\">https://escape.tech</a>). We use AI inspired by chess to help security engineers and developers discover and secure APIs created by their organizations.<p>Here is our demo: <a href=\"https://youtu.be/qcCaegVElTY\" rel=\"nofollow\">https://youtu.be/qcCaegVElTY</a><p>Typical modern large orgs have hundreds if not thousands of APIs, and many of those handle sensitive data or are critical to business operations. The development of those APIs is distributed across different development teams that don\u2019t always have knowledge of API security best practices. API source codes are updated frequently, making it easy for new, easily exploitable security vulnerabilities to be introduced in <em>production</em> environments (think the 2018 FB 50M accounts data leak).<p>APIs make up 80% of global web traffic, and this share is growing. The responsibility for securing APIs is usually given to the organization's security engineers, not the API developers, which makes sense because they\u2019re the ones who know how to secure things. But in practice, it is almost an impossible job because the security engineers have no way to track everything the developers are exposing online, and usually, there\u2019s nobody to tell them! And when they do find out, they lack the right tooling for achieving it. This is a huge risk for organizations, a pain and personal risk for security engineers, and a great technical challenge.<p>Working as a software engineer a few years back, I faced a data scare: a pharmaceutical client's data was compromised due to a NoSQL injection. In this case the damage ended up controlled, but it led to the nightmarish thought of waking up one day with all the data from the applications gone, simply because cybercriminals exploited a security issue. When looking for solutions that allowed developers to ensure what they released in <em>production</em> was secure, we couldn\u2019t find anything particularly good. Security scanning tools like OWASP ZAP had been designed for people with penetration testing backgrounds. Code scanning tools were only finding the low-hanging fruit at the cost of many false positives, and ended up resembling security-oriented linters, turning the entire IDE red for minimal value. It felt like none of the existing security tools were built with real engineers in mind.\nWhen I met Antoine, who had previously been a security engineer at NATO and Apple, we decided to tackle this issue together and create a modern security tool that would appeal to both developers and security people. It needed to be fast, easy to set up yet configurable, have outstanding support for securing APIs, and find what was relevant with a low false positive rate.<p>The first step was to show security engineers and developers what APIs they had to secure. We needed to find an easy way to discover any organization\u2019s exposed and internal APIs.<p>To discover all APIs, we crafted a system that extracts all the API routes the organization exposes by scanning its domains, frontend websites, and SPAs. It then enriches this data by connecting to code repositories, API gateways, and API development tools to create a full list of all the exposed endpoints and the sensitivity of the data they handle. Other testing tools do not provide an inventory of all the API routes exposed by an organization, but as we mentioned above, the biggest problem security engineers face is often just finding out what it is they need to test!<p>Then, we needed to provide security engineers and developers with a list of security issues in their APIs.<p>Since APIs act as a business model layer, most of the critical security issues lie in the business processes underlying APIs. In security, issues obtained from breaking business processes are called Broken Object Level Authorization (BOLA), Broken Function Level Authorization (BFLA), and Broken Object Property Level Authorization (BOPLA).<p>To find them, we knew we couldn\u2019t rely on traditional techniques like fuzzing. We needed to find a way to model the Business Process underlying the API and attempt to break it.<p>Doing research on this topic, we discovered that modeling API business processes in a similar way to board games, like Chess or Go, worked surprisingly well. The underlying reason is simple: a board game is a state machine on which you can execute actions that must respect rules to change the game\u2019s state. Think about moving the pieces in a chess game, each piece has its specific moves, and their position on the board represents the state.<p>APIs are similar: they have a database, which represents the internal state, and API routes, which represent the actions you can run on the state. Of course, most APIs are more complex than a chess game because they have much more routes than there are chess pieces. In mathematics, we would say that the action space is much larger.<p>But the models are similar enough for us to try applying alpha-beta, Monte-Carlo Search Three, and more advanced Machine Learning techniques that have proven to work well in the context of large action space games like Go.<p>Those were the foundational ideas behind our in-house algorithm, Feedback-Driven API Exploration (FDAE), which automatically identifies the underlying business processes and generates sequences of API requests especially aimed at breaking them, uncovering potential security flaws and data leaks.<p>FDAE starts by ingesting the list of routes and parameters in an API. It first identifies the routes leading to sensitive data, like PII or financial information, and the parameters that have the most chances of being vulnerable to various kinds of injections and attacks.<p>Often, those routes require parameters like <em>UUIDs</em> or domain-specific values. That\u2019s where traditional security scanners fall short: they often fuzz randomly the parameters hoping to find some low-hanging fruit injection, but end up blocked at the data validation layer.<p>FDAE is smarter. If it detects that the route /user/:<em>uuid</em> might be sensible, it will first look at all the other routes in the API and try to find one that returns a valid user <em>UUID</em>. Once it gets the valid user <em>UUID</em>, it will use it to trigger the /user/:<em>uuid</em> route and try to exploit it in many different ways.<p>If there are no existing users in the database, but there is a route to create one, Escape\u2019s FDAE will even be able to create a user, get its <em>UUID</em>, and then attempt exploiting the routes that require a user <em>UUID</em>.<p>This process, very similar to what human penetration testers and bug hunters do, allows Escape to do extensive and deep testing of any API and business processes. It\u2019s specifically good at finding many access control bugs like tenant isolation problems, complex multi-step injections, and request forgeries.<p>To give a specific example, imagine you\u2019re building an e-commerce application, Escape can detect cases where users can bypass payment steps or modify input parameters in the request to access other user\u2019s orders or private information.<p>You can find a more detailed explanation of how Feedback Driven API Exploration works with graphics here: <a href=\"https://escape.tech/blog/feedback-driven-api-exploration/\">https://escape.tech/blog/feedback-driven-api-exploration/</a><p>Escape\u2019s entire scanning process takes minutes. It was very important to us, as former developers, to seamlessly integrate API testing in CI/CD pipelines and quickly implement relevant fixes. To verify that it was scalable, we scanned all public APIs on the internet and produced research reports on their quality: the State of GraphQL Security (<a href=\"https://26857953.fs1.hubspotusercontent-eu1.net/hubfs/26857953/State-of-GraphQL-security.pdf\" rel=\"nofollow\">https://26857953.fs1.hubspotusercontent-eu1.net/hubfs/268579...</a>), and the State of Public APIs (<a href=\"https://apirank.dev/state-of-public-api-2023/\" rel=\"nofollow\">https://apirank.dev/state-of-public-api-2023/</a>).<p>Apart from discovering and testing APIs in minutes, we wanted to make Escape actionable. Pinpointing a problem is one thing, but then how to fix it? Most dynamic scanners give vague remediation instructions. Escape actually generates code snippets to help developers.<p>We offer a few monthly and yearly subscription plans based on the number of APIs and developers in the org, with a free 7 days trial. The pricing is accessible in the app during a trial period. Since our product is highly technical, we wanted to make sure that users can explore our features, evaluate what Escape does, and understand its value before making a decision. Users can see pricing details at a point in their trial journey where it makes the most sense, aligning with their understanding of the product. You can try us without a credit card at <a href=\"https://escape.tech\">https://escape.tech</a>.<p>Our main SaaS product is closed source, but we publish many open source packages for security and developers on <a href=\"https://github.com/Escape-Technologies/\">https://github.com/Escape-Technologies/</a> , some of them being widely used like GraphQL Armor (<a href=\"https://github.com/Escape-Technologies/graphql-armor/\">https://github.com/Escape-Technologies/graphql-armor/</a>)<p>The number and complexity of APIs are constantly growing, and we\u2019re continuing to learn every day, so we would greatly appreciate and are eager for your feedback (no matter how big or small)! Thanks!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Escape (YC W23) \u2013 Discover and secure all your APIs"}}, "_tags": ["story", "author_glimow", "story_39215779", "launch_hn"], "author": "glimow", "children": [39215872, 39215935, 39215964, 39216096, 39216300, 39216901, 39217031, 39218387, 39218582, 39219508, 39219982, 39220021, 39221026, 39221659, 39225089], "created_at": "2024-02-01T13:38:42Z", "created_at_i": 1706794722, "num_comments": 34, "objectID": "39215779", "points": 96, "story_id": 39215779, "story_text": "Hey HN! We\u2019re Tristan and Antoine, co-founders of Escape (<a href=\"https:&#x2F;&#x2F;escape.tech\">https:&#x2F;&#x2F;escape.tech</a>). We use AI inspired by chess to help security engineers and developers discover and secure APIs created by their organizations.<p>Here is our demo: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;qcCaegVElTY\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;qcCaegVElTY</a><p>Typical modern large orgs have hundreds if not thousands of APIs, and many of those handle sensitive data or are critical to business operations. The development of those APIs is distributed across different development teams that don\u2019t always have knowledge of API security best practices. API source codes are updated frequently, making it easy for new, easily exploitable security vulnerabilities to be introduced in production environments (think the 2018 FB 50M accounts data leak).<p>APIs make up 80% of global web traffic, and this share is growing. The responsibility for securing APIs is usually given to the organization&#x27;s security engineers, not the API developers, which makes sense because they\u2019re the ones who know how to secure things. But in practice, it is almost an impossible job because the security engineers have no way to track everything the developers are exposing online, and usually, there\u2019s nobody to tell them! And when they do find out, they lack the right tooling for achieving it. This is a huge risk for organizations, a pain and personal risk for security engineers, and a great technical challenge.<p>Working as a software engineer a few years back, I faced a data scare: a pharmaceutical client&#x27;s data was compromised due to a NoSQL injection. In this case the damage ended up controlled, but it led to the nightmarish thought of waking up one day with all the data from the applications gone, simply because cybercriminals exploited a security issue. When looking for solutions that allowed developers to ensure what they released in production was secure, we couldn\u2019t find anything particularly good. Security scanning tools like OWASP ZAP had been designed for people with penetration testing backgrounds. Code scanning tools were only finding the low-hanging fruit at the cost of many false positives, and ended up resembling security-oriented linters, turning the entire IDE red for minimal value. It felt like none of the existing security tools were built with real engineers in mind.\nWhen I met Antoine, who had previously been a security engineer at NATO and Apple, we decided to tackle this issue together and create a modern security tool that would appeal to both developers and security people. It needed to be fast, easy to set up yet configurable, have outstanding support for securing APIs, and find what was relevant with a low false positive rate.<p>The first step was to show security engineers and developers what APIs they had to secure. We needed to find an easy way to discover any organization\u2019s exposed and internal APIs.<p>To discover all APIs, we crafted a system that extracts all the API routes the organization exposes by scanning its domains, frontend websites, and SPAs. It then enriches this data by connecting to code repositories, API gateways, and API development tools to create a full list of all the exposed endpoints and the sensitivity of the data they handle. Other testing tools do not provide an inventory of all the API routes exposed by an organization, but as we mentioned above, the biggest problem security engineers face is often just finding out what it is they need to test!<p>Then, we needed to provide security engineers and developers with a list of security issues in their APIs.<p>Since APIs act as a business model layer, most of the critical security issues lie in the business processes underlying APIs. In security, issues obtained from breaking business processes are called Broken Object Level Authorization (BOLA), Broken Function Level Authorization (BFLA), and Broken Object Property Level Authorization (BOPLA).<p>To find them, we knew we couldn\u2019t rely on traditional techniques like fuzzing. We needed to find a way to model the Business Process underlying the API and attempt to break it.<p>Doing research on this topic, we discovered that modeling API business processes in a similar way to board games, like Chess or Go, worked surprisingly well. The underlying reason is simple: a board game is a state machine on which you can execute actions that must respect rules to change the game\u2019s state. Think about moving the pieces in a chess game, each piece has its specific moves, and their position on the board represents the state.<p>APIs are similar: they have a database, which represents the internal state, and API routes, which represent the actions you can run on the state. Of course, most APIs are more complex than a chess game because they have much more routes than there are chess pieces. In mathematics, we would say that the action space is much larger.<p>But the models are similar enough for us to try applying alpha-beta, Monte-Carlo Search Three, and more advanced Machine Learning techniques that have proven to work well in the context of large action space games like Go.<p>Those were the foundational ideas behind our in-house algorithm, Feedback-Driven API Exploration (FDAE), which automatically identifies the underlying business processes and generates sequences of API requests especially aimed at breaking them, uncovering potential security flaws and data leaks.<p>FDAE starts by ingesting the list of routes and parameters in an API. It first identifies the routes leading to sensitive data, like PII or financial information, and the parameters that have the most chances of being vulnerable to various kinds of injections and attacks.<p>Often, those routes require parameters like UUIDs or domain-specific values. That\u2019s where traditional security scanners fall short: they often fuzz randomly the parameters hoping to find some low-hanging fruit injection, but end up blocked at the data validation layer.<p>FDAE is smarter. If it detects that the route &#x2F;user&#x2F;:uuid might be sensible, it will first look at all the other routes in the API and try to find one that returns a valid user UUID. Once it gets the valid user UUID, it will use it to trigger the &#x2F;user&#x2F;:uuid route and try to exploit it in many different ways.<p>If there are no existing users in the database, but there is a route to create one, Escape\u2019s FDAE will even be able to create a user, get its UUID, and then attempt exploiting the routes that require a user UUID.<p>This process, very similar to what human penetration testers and bug hunters do, allows Escape to do extensive and deep testing of any API and business processes. It\u2019s specifically good at finding many access control bugs like tenant isolation problems, complex multi-step injections, and request forgeries.<p>To give a specific example, imagine you\u2019re building an e-commerce application, Escape can detect cases where users can bypass payment steps or modify input parameters in the request to access other user\u2019s orders or private information.<p>You can find a more detailed explanation of how Feedback Driven API Exploration works with graphics here: <a href=\"https:&#x2F;&#x2F;escape.tech&#x2F;blog&#x2F;feedback-driven-api-exploration&#x2F;\">https:&#x2F;&#x2F;escape.tech&#x2F;blog&#x2F;feedback-driven-api-exploration&#x2F;</a><p>Escape\u2019s entire scanning process takes minutes. It was very important to us, as former developers, to seamlessly integrate API testing in CI&#x2F;CD pipelines and quickly implement relevant fixes. To verify that it was scalable, we scanned all public APIs on the internet and produced research reports on their quality: the State of GraphQL Security (<a href=\"https:&#x2F;&#x2F;26857953.fs1.hubspotusercontent-eu1.net&#x2F;hubfs&#x2F;26857953&#x2F;State-of-GraphQL-security.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;26857953.fs1.hubspotusercontent-eu1.net&#x2F;hubfs&#x2F;268579...</a>), and the State of Public APIs (<a href=\"https:&#x2F;&#x2F;apirank.dev&#x2F;state-of-public-api-2023&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;apirank.dev&#x2F;state-of-public-api-2023&#x2F;</a>).<p>Apart from discovering and testing APIs in minutes, we wanted to make Escape actionable. Pinpointing a problem is one thing, but then how to fix it? Most dynamic scanners give vague remediation instructions. Escape actually generates code snippets to help developers.<p>We offer a few monthly and yearly subscription plans based on the number of APIs and developers in the org, with a free 7 days trial. The pricing is accessible in the app during a trial period. Since our product is highly technical, we wanted to make sure that users can explore our features, evaluate what Escape does, and understand its value before making a decision. Users can see pricing details at a point in their trial journey where it makes the most sense, aligning with their understanding of the product. You can try us without a credit card at <a href=\"https:&#x2F;&#x2F;escape.tech\">https:&#x2F;&#x2F;escape.tech</a>.<p>Our main SaaS product is closed source, but we publish many open source packages for security and developers on <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Escape-Technologies&#x2F;\">https:&#x2F;&#x2F;github.com&#x2F;Escape-Technologies&#x2F;</a> , some of them being widely used like GraphQL Armor (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;Escape-Technologies&#x2F;graphql-armor&#x2F;\">https:&#x2F;&#x2F;github.com&#x2F;Escape-Technologies&#x2F;graphql-armor&#x2F;</a>)<p>The number and complexity of APIs are constantly growing, and we\u2019re continuing to learn every day, so we would greatly appreciate and are eager for your feedback (no matter how big or small)! Thanks!", "title": "Launch HN: Escape (YC W23) \u2013 Discover and secure all your APIs", "updated_at": "2024-09-20T16:13:36Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "_zfsy"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "Trading cards are awesome, but paying $30 for some cardboard isn\u2019t. I\u2019ve upscaled 60,000 cards from the entire catalog of Yugioh, Magic, Pokemon, &amp; a newer game, <a href=\"https://elestrals.com\" rel=\"nofollow\">https://elestrals.com</a>. I've made it easy to build a decklist, download it, and then print at home. Modern inkjet printers got really good when nobody was looking. While it\u2019s clear they\u2019re not real cards, the upscaling makes them look great for casual play (these are not tournament legal). It\u2019s totally free, give it a try!<p>Supplies: [url-redacted]\nPrinter Settings: [url-redacted]\nInstructions: [url-redacted]<p>Overview: I built [name-redacted] because I had some scripts to do this lying around, and wanted to explore the new Rails 8 magic. Kamal 2 (kamal-deploy.org/) is a game changer, SQLite in <em>production</em> is fine, and the database backed solid family of gems work like a charm.<p>Compute: I am renting a box on <a href=\"https://hetzner.com\" rel=\"nofollow\">https://hetzner.com</a> located in VA for $15/mo. This box has 8 gigs of ram and 2 vCPU's. This is such a deal compared to compute prices on <a href=\"https://render.com\" rel=\"nofollow\">https://render.com</a>.<p>Kamal 2: This thing is amazing. Kamal gives me everything I could want (easy console access, easy shell access, a way to manage secrets, a way to see my logs, and letsencrypt support for DNS), all without a PaaS tax. The best part is the accessories feature: <a href=\"https://kamal-deploy.org/docs/commands/accessory/\" rel=\"nofollow\">https://kamal-deploy.org/docs/commands/accessory/</a>. I am running my main app with two accessories: Meilisearch(<a href=\"https://meilisearch.com\" rel=\"nofollow\">https://meilisearch.com</a>) and OpenObserve (<a href=\"https://openobserve.ai\" rel=\"nofollow\">https://openobserve.ai</a>). Instead of paying Algolia to host search infrastructure and sentry to host monitoring infrastructure, I\u2019m hosting my own OSS without any fanfare.<p>Upscaling: To upscale the trading cards (a mandatory part of this build, scans are never high enough DPI). I am using this (<a href=\"https://replicate.com/nightmareai/real-esrgan\" rel=\"nofollow\">https://replicate.com/nightmareai/real-esrgan</a>) model. For upscaling every card, I've used under a hundred bucks of compute. This model was picked on a whim, but worked well enough that I didn\u2019t compare other models.<p>SQLite: I used SQLite combined with Litestream (litestream.io) for my database. While I considered Postgres, I hesitated due to uncertainties around handling backups on self-hosted infrastructure. This was my first time using SQLite in <em>production</em>, and it was functional but with some minor annoyances. Here\u2019s what I encountered: 1. No Default <em>UUID</em> Primary Key Type I had to set primary keys as strings and assign IDs manually from the application record. It\u2019s an annoying workaround but manageable. 2. No Native Array Columns Because SQLite doesn\u2019t support array columns, I had to use its native JSON column type, which just felt icky. If I were working with something like embeddings, this would be especially annoying, because you couldn\u2019t enforce all the records to have the same number of dimensions. 3. Cryptic Errors At one point, a migration failed silently, leaving a cryptic error in schema.rb. The issue was resolved by rolling back the migration and redoing it, but it was once again, annoying. 4. Litestream Defaults Litestream deletes snapshots after 24 hours by default, which is far too short. When I tried to recover some data, I found it had already been deleted. Adjusting these defaults fixed the problem.<p>Solid Queue/Cache/Cable: The solid family of gems are all backed by the database and were a pleasure to work with. Goal was to prevent needing to reach for redis, so you have one less thing to worry about. You end up with a little more latency, which is a totally reasonable tradeoff.<p>Conclusions: We are moving into a post platform as a service world. Instead of buying a bespoke render.com or heroku, you just buy commodity compute and use Kamal to manage. It's like, pretty much all there, excited to see how this space matures."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Free TCG Proxy Manager for Magic, Yugioh, and Pokemon"}}, "_tags": ["story", "author__zfsy", "story_42635049", "show_hn"], "author": "_zfsy", "children": [42636982, 42636999, 42637222, 42637266, 42637387, 42637630, 42638399, 42638901, 42639495, 42639830, 42655624, 42660872], "created_at": "2025-01-08T15:11:41Z", "created_at_i": 1736349101, "num_comments": 49, "objectID": "42635049", "points": 69, "story_id": 42635049, "story_text": "Trading cards are awesome, but paying $30 for some cardboard isn\u2019t. I\u2019ve upscaled 60,000 cards from the entire catalog of Yugioh, Magic, Pokemon, &amp; a newer game, <a href=\"https:&#x2F;&#x2F;elestrals.com\" rel=\"nofollow\">https:&#x2F;&#x2F;elestrals.com</a>. I&#x27;ve made it easy to build a decklist, download it, and then print at home. Modern inkjet printers got really good when nobody was looking. While it\u2019s clear they\u2019re not real cards, the upscaling makes them look great for casual play (these are not tournament legal). It\u2019s totally free, give it a try!<p>Supplies: [url-redacted]\nPrinter Settings: [url-redacted]\nInstructions: [url-redacted]<p>Overview: I built [name-redacted] because I had some scripts to do this lying around, and wanted to explore the new Rails 8 magic. Kamal 2 (kamal-deploy.org&#x2F;) is a game changer, SQLite in production is fine, and the database backed solid family of gems work like a charm.<p>Compute: I am renting a box on <a href=\"https:&#x2F;&#x2F;hetzner.com\" rel=\"nofollow\">https:&#x2F;&#x2F;hetzner.com</a> located in VA for $15&#x2F;mo. This box has 8 gigs of ram and 2 vCPU&#x27;s. This is such a deal compared to compute prices on <a href=\"https:&#x2F;&#x2F;render.com\" rel=\"nofollow\">https:&#x2F;&#x2F;render.com</a>.<p>Kamal 2: This thing is amazing. Kamal gives me everything I could want (easy console access, easy shell access, a way to manage secrets, a way to see my logs, and letsencrypt support for DNS), all without a PaaS tax. The best part is the accessories feature: <a href=\"https:&#x2F;&#x2F;kamal-deploy.org&#x2F;docs&#x2F;commands&#x2F;accessory&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;kamal-deploy.org&#x2F;docs&#x2F;commands&#x2F;accessory&#x2F;</a>. I am running my main app with two accessories: Meilisearch(<a href=\"https:&#x2F;&#x2F;meilisearch.com\" rel=\"nofollow\">https:&#x2F;&#x2F;meilisearch.com</a>) and OpenObserve (<a href=\"https:&#x2F;&#x2F;openobserve.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;openobserve.ai</a>). Instead of paying Algolia to host search infrastructure and sentry to host monitoring infrastructure, I\u2019m hosting my own OSS without any fanfare.<p>Upscaling: To upscale the trading cards (a mandatory part of this build, scans are never high enough DPI). I am using this (<a href=\"https:&#x2F;&#x2F;replicate.com&#x2F;nightmareai&#x2F;real-esrgan\" rel=\"nofollow\">https:&#x2F;&#x2F;replicate.com&#x2F;nightmareai&#x2F;real-esrgan</a>) model. For upscaling every card, I&#x27;ve used under a hundred bucks of compute. This model was picked on a whim, but worked well enough that I didn\u2019t compare other models.<p>SQLite: I used SQLite combined with Litestream (litestream.io) for my database. While I considered Postgres, I hesitated due to uncertainties around handling backups on self-hosted infrastructure. This was my first time using SQLite in production, and it was functional but with some minor annoyances. Here\u2019s what I encountered: 1. No Default UUID Primary Key Type I had to set primary keys as strings and assign IDs manually from the application record. It\u2019s an annoying workaround but manageable. 2. No Native Array Columns Because SQLite doesn\u2019t support array columns, I had to use its native JSON column type, which just felt icky. If I were working with something like embeddings, this would be especially annoying, because you couldn\u2019t enforce all the records to have the same number of dimensions. 3. Cryptic Errors At one point, a migration failed silently, leaving a cryptic error in schema.rb. The issue was resolved by rolling back the migration and redoing it, but it was once again, annoying. 4. Litestream Defaults Litestream deletes snapshots after 24 hours by default, which is far too short. When I tried to recover some data, I found it had already been deleted. Adjusting these defaults fixed the problem.<p>Solid Queue&#x2F;Cache&#x2F;Cable: The solid family of gems are all backed by the database and were a pleasure to work with. Goal was to prevent needing to reach for redis, so you have one less thing to worry about. You end up with a little more latency, which is a totally reasonable tradeoff.<p>Conclusions: We are moving into a post platform as a service world. Instead of buying a bespoke render.com or heroku, you just buy commodity compute and use Kamal to manage. It&#x27;s like, pretty much all there, excited to see how this space matures.", "title": "Show HN: Free TCG Proxy Manager for Magic, Yugioh, and Pokemon", "updated_at": "2026-02-03T03:11:06Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "lexokoh"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "I\u2019ve been experimenting with AI agents lately, and one problem kept coming up: they either get a raw API key with full access or nothing at all. That\u2019s risky, especially if you\u2019re testing agents that can make arbitrary calls.<p>So I hacked together a tiny package called Kage Keys - <a href=\"https://github.com/kagehq/keys\" rel=\"nofollow\">https://github.com/kagehq/keys</a><p>It lets you wrap agent actions with scoped, short-lived tokens instead of handing over your real API keys.<p>Example:<p>```js\nimport { withAgentKey, getLogs } from &quot;@kagehq/keys&quot;;<p>async function main() {\n  await withAgentKey(&quot;github:repos.read&quot;, async () =&gt; {\n    console.log(&quot;Agent is calling GitHub API...&quot;);\n  });<p><pre><code>  console.log(await getLogs());</code></pre>\n}<p>main();<p>Right now it:<p>- Generates scoped, expiring tokens (default 10s)<p>- Logs every action to kage-keys.log<p>- Works as a drop-in wrapper for async functions<p>It\u2019s just an MVP (tokens are fake <em>UUIDs</em>), but I want to see if developers find this helpful before building the <em>production</em> version with real crypto + proxy enforcement.<p>Repo: <a href=\"https://github.com/kagehq/keys\" rel=\"nofollow\">https://github.com/kagehq/keys</a><p>npm: <a href=\"https://www.npmjs.com/package/@kagehq/keys\" rel=\"nofollow\">https://www.npmjs.com/package/@kagehq/keys</a><p>Would love feedback, especially from anyone running agents in <em>production</em> or dealing with API key sprawl."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Scoped, expiring API keys for AI agents"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/kagehq/keys"}}, "_tags": ["story", "author_lexokoh", "story_44923881", "show_hn"], "author": "lexokoh", "children": [44924322, 44926955], "created_at": "2025-08-16T14:42:38Z", "created_at_i": 1755355358, "num_comments": 4, "objectID": "44923881", "points": 4, "story_id": 44923881, "story_text": "I\u2019ve been experimenting with AI agents lately, and one problem kept coming up: they either get a raw API key with full access or nothing at all. That\u2019s risky, especially if you\u2019re testing agents that can make arbitrary calls.<p>So I hacked together a tiny package called Kage Keys - <a href=\"https:&#x2F;&#x2F;github.com&#x2F;kagehq&#x2F;keys\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;kagehq&#x2F;keys</a><p>It lets you wrap agent actions with scoped, short-lived tokens instead of handing over your real API keys.<p>Example:<p>```js\nimport { withAgentKey, getLogs } from &quot;@kagehq&#x2F;keys&quot;;<p>async function main() {\n  await withAgentKey(&quot;github:repos.read&quot;, async () =&gt; {\n    console.log(&quot;Agent is calling GitHub API...&quot;);\n  });<p><pre><code>  console.log(await getLogs());</code></pre>\n}<p>main();<p>Right now it:<p>- Generates scoped, expiring tokens (default 10s)<p>- Logs every action to kage-keys.log<p>- Works as a drop-in wrapper for async functions<p>It\u2019s just an MVP (tokens are fake UUIDs), but I want to see if developers find this helpful before building the production version with real crypto + proxy enforcement.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;kagehq&#x2F;keys\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;kagehq&#x2F;keys</a><p>npm: <a href=\"https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@kagehq&#x2F;keys\" rel=\"nofollow\">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@kagehq&#x2F;keys</a><p>Would love feedback, especially from anyone running agents in production or dealing with API key sprawl.", "title": "Show HN: Scoped, expiring API keys for AI agents", "updated_at": "2025-08-19T14:06:26Z", "url": "https://github.com/kagehq/keys"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rsunnythota"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "Hi HN! \nI built HashCodeTools, a collection of 17 free, fast, privacy-focused developer utilities that run entirely in the browser \u2014 no data ever leaves your device.<p>As a backend engineer, I constantly found myself Googling things like \u201cjson formatter\u201d, \u201cjwt decoder\u201d, \u201c<em>uuid</em> generator\u201d, \u201cbase64 encode\u201d, etc. I realized I was using 6\u20137 different sites, many of which were slow, ad-heavy, or sent data to servers. So I decided to build one clean, unified toolbox.<p>What it currently includes<p>JSON Formatter / Validator<p>JWT Decoder<p>Base64 Encoder / Decoder<p><em>UUID</em> Generator<p>URL Encoder / Decoder<p>Epoch \u2194 Date Converter<p>HTML / CSS / JS Formatters<p>YAML \u21c4 JSON Converter<p>CSV \u21c4 JSON Converter<p>QR Code Generator<p>Text diff viewer\n\u2026and more.<p>Privacy Features<p>100% client-side<p>No tracking<p>No logins<p>No ads<p>No backend \u2014 everything runs in your browser<p>This means you can safely paste <em>production</em> JSON, JWTs, or logs without worrying about them being uploaded anywhere.<p>Why I built this<p>I wanted:<p>One tab for all dev utilities<p>Zero network requests<p>Instant formatting/decoding<p>Tools that don't break with large payloads<p>A simple interface without popups or ads<p>Tech Stack<p>TypeScript<p>Next.js<p>Monaco Editor<p>TailwindCSS<p>Local-only processing (no API calls)<p>Roadmap<p>VSCode extension<p>Chrome extension<p>More text &amp; data format tools<p>Open-source versions of each utility<p>Dark mode improvements<p>Feedback welcome!<p>I'm especially looking for feedback on:<p>Performance on large JSON/JWT payloads<p>Missing tools developers want<p>UI/UX improvements<p>Any privacy/security concerns<p>Thanks for checking it out!\nLink: <a href=\"https://hashcodetools.com/\" rel=\"nofollow\">https://hashcodetools.com/</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: HashCodeTools \u2013 17 Free Privacy-Focused Dev Tools"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://hashcodetools.com/"}}, "_tags": ["story", "author_rsunnythota", "story_45943918", "show_hn"], "author": "rsunnythota", "children": [45944317, 45945506, 45945512], "created_at": "2025-11-16T09:57:30Z", "created_at_i": 1763287050, "num_comments": 3, "objectID": "45943918", "points": 2, "story_id": 45943918, "story_text": "Hi HN! \nI built HashCodeTools, a collection of 17 free, fast, privacy-focused developer utilities that run entirely in the browser \u2014 no data ever leaves your device.<p>As a backend engineer, I constantly found myself Googling things like \u201cjson formatter\u201d, \u201cjwt decoder\u201d, \u201cuuid generator\u201d, \u201cbase64 encode\u201d, etc. I realized I was using 6\u20137 different sites, many of which were slow, ad-heavy, or sent data to servers. So I decided to build one clean, unified toolbox.<p>What it currently includes<p>JSON Formatter &#x2F; Validator<p>JWT Decoder<p>Base64 Encoder &#x2F; Decoder<p>UUID Generator<p>URL Encoder &#x2F; Decoder<p>Epoch \u2194 Date Converter<p>HTML &#x2F; CSS &#x2F; JS Formatters<p>YAML \u21c4 JSON Converter<p>CSV \u21c4 JSON Converter<p>QR Code Generator<p>Text diff viewer\n\u2026and more.<p>Privacy Features<p>100% client-side<p>No tracking<p>No logins<p>No ads<p>No backend \u2014 everything runs in your browser<p>This means you can safely paste production JSON, JWTs, or logs without worrying about them being uploaded anywhere.<p>Why I built this<p>I wanted:<p>One tab for all dev utilities<p>Zero network requests<p>Instant formatting&#x2F;decoding<p>Tools that don&#x27;t break with large payloads<p>A simple interface without popups or ads<p>Tech Stack<p>TypeScript<p>Next.js<p>Monaco Editor<p>TailwindCSS<p>Local-only processing (no API calls)<p>Roadmap<p>VSCode extension<p>Chrome extension<p>More text &amp; data format tools<p>Open-source versions of each utility<p>Dark mode improvements<p>Feedback welcome!<p>I&#x27;m especially looking for feedback on:<p>Performance on large JSON&#x2F;JWT payloads<p>Missing tools developers want<p>UI&#x2F;UX improvements<p>Any privacy&#x2F;security concerns<p>Thanks for checking it out!\nLink: <a href=\"https:&#x2F;&#x2F;hashcodetools.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;hashcodetools.com&#x2F;</a>", "title": "Show HN: HashCodeTools \u2013 17 Free Privacy-Focused Dev Tools", "updated_at": "2025-11-16T15:21:42Z", "url": "https://hashcodetools.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "devtoolbox"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["uuid"], "value": "I built a free REST API for common dev tasks that usually require pulling in heavy dependencies or writing boilerplate.<p>Features include:\n- URL shortener with click analytics\n- QR code generation (PNG/SVG/data URL)\n- Fake/mock data generator (person, address, email, company, etc.)\n- JWT decoder, regex tester, password generator\n- JSON diff/format/minify, YAML\u2194JSON, Markdown\u2192HTML\n- Base64, SHA256/MD5/SHA512 hashing, <em>UUID</em> v4\n- Cron expression explainer, number base converter\n- IP info, HTML tag stripper, list processor<p>No auth required for basic usage (rate limited). Pay 1 USDC on Base L2 for an unlimited API key.<p>Also available as a zero-dependency npm package: npm install conway-toolbox"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Free developer utility API \u2013 QR, fake data, URL shortener, 40 tools"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "https://conway-toolbox-<em>production</em>.up.railway.app/ui"}}, "_tags": ["story", "author_devtoolbox", "story_47071588", "show_hn"], "author": "devtoolbox", "children": [47071646], "created_at": "2026-02-19T09:00:33Z", "created_at_i": 1771491633, "num_comments": 3, "objectID": "47071588", "points": 1, "story_id": 47071588, "story_text": "I built a free REST API for common dev tasks that usually require pulling in heavy dependencies or writing boilerplate.<p>Features include:\n- URL shortener with click analytics\n- QR code generation (PNG&#x2F;SVG&#x2F;data URL)\n- Fake&#x2F;mock data generator (person, address, email, company, etc.)\n- JWT decoder, regex tester, password generator\n- JSON diff&#x2F;format&#x2F;minify, YAML\u2194JSON, Markdown\u2192HTML\n- Base64, SHA256&#x2F;MD5&#x2F;SHA512 hashing, UUID v4\n- Cron expression explainer, number base converter\n- IP info, HTML tag stripper, list processor<p>No auth required for basic usage (rate limited). Pay 1 USDC on Base L2 for an unlimited API key.<p>Also available as a zero-dependency npm package: npm install conway-toolbox", "title": "Show HN: Free developer utility API \u2013 QR, fake data, URL shortener, 40 tools", "updated_at": "2026-02-19T09:47:09Z", "url": "https://conway-toolbox-production.up.railway.app/ui"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "levkk"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "Hey HN!<p>Lev and Justin here, authors of PgDog (https://github.com/pgdogdev/pgdog), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that doesn\u2019t require application code changes or database migrations to work.<p>Our post from last year: https://news.ycombinator.com/item?id=44099187<p>The most important update: we are in <em>production</em>. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway:<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE/DROP/ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important since most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, Sequelize, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth another look.<p>If you\u2019re like us and prefer integers to <em>UUIDs</em> for your primary keys, we built a cross-shard unique sequence inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, PgDog can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a new primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with insert/update statements, but if you still prefer to handle your read/write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python/Ruby/Go apps, this helps by reducing memory usage, I/O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by PgBouncer, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending a Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off/on, so should you choose to give it a try, you can do so at your own pace. Our docs (https://docs.pgdog.dev) should help too.<p>Thanks for reading and happy hacking!<p>Lev &amp; Justin"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "PgDog: Connection pooler, load balancer and sharder for PostgreSQL"}}, "_tags": ["story", "author_levkk", "story_47048265", "ask_hn"], "author": "levkk", "created_at": "2026-02-17T15:05:55Z", "created_at_i": 1771340755, "num_comments": 0, "objectID": "47048265", "points": 1, "story_id": 47048265, "story_text": "Hey HN!<p>Lev and Justin here, authors of PgDog (https:&#x2F;&#x2F;github.com&#x2F;pgdogdev&#x2F;pgdog), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that doesn\u2019t require application code changes or database migrations to work.<p>Our post from last year: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44099187<p>The most important update: we are in production. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway:<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE&#x2F;DROP&#x2F;ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important since most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, Sequelize, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth another look.<p>If you\u2019re like us and prefer integers to UUIDs for your primary keys, we built a cross-shard unique sequence inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, PgDog can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a new primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with insert&#x2F;update statements, but if you still prefer to handle your read&#x2F;write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python&#x2F;Ruby&#x2F;Go apps, this helps by reducing memory usage, I&#x2F;O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by PgBouncer, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending a Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off&#x2F;on, so should you choose to give it a try, you can do so at your own pace. Our docs (https:&#x2F;&#x2F;docs.pgdog.dev) should help too.<p>Thanks for reading and happy hacking!<p>Lev &amp; Justin", "title": "PgDog: Connection pooler, load balancer and sharder for PostgreSQL", "updated_at": "2026-02-17T15:11:18Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ctalledo"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "Hi HN, this is Cesar and Rodny, developers of an open-source container runtime called Sysbox, and co-founders of a startup called Nestybox (YC S20).<p>We launched on HN almost a year ago and got excellent feedback then (https://news.ycombinator.com/item?id=24084758).<p>Happy to say that over the past year, Sysbox has continued to gain traction, particularly for securing containers in <em>production</em>, CI/CD, and containerized dev environments.<p>We wanted to announce an important new feature: integration between Sysbox and Kubernetes.<p>As a quick refresher, Sysbox is a &quot;runc&quot; that enhances containers in two key ways:<p>1) Hardens container isolation (Linux user-namespace on all containers, partial procfs &amp; sysfs virtualization, initial mount locking, and more).<p>2) Enables containers to run not just microservices, but also system software such as systemd, Docker, K8s, K3s, and more. This enables containers to replace slower/less-efficient VMs in many scenarios.<p>Prior to Sysbox this required insecure privileged containers, custom images, and special host mounts, or specialized tools like LXD, KinD and Minikube. With Sysbox, the container runtime sets up the container such that it can run the software securely and seamlessly, increasing security and reducing complexity.<p>Up to recently Sysbox only worked under Docker, but the latest release (v0.4.0) now works under Kubernetes too.<p>This means you can use Kubernetes to orchestrate pods that are rootless (i.e., root in the container maps to an unprivileged user on the host) and can run not just microservices, but full &quot;VM-like&quot; environments.<p>For example, you can create a pod that acts as a well isolated dev environment and inside of it run systemd, your favorite editor, plus Docker. Or create several pods that together form another K8s cluster for testing. Or run the K8s.io KinD inside a pod to create an entire K8s cluster inside one pod. Many interesting and powerful combinations are possible.<p>Sysbox has taken 2-years of very hard work, as it pushes the limits of OS virtualization (<em>uid</em>-shifting, syscall trapping, procfs virtualization, etc.) It was forked from the OCI runc in 2019, so we stand on the shoulders of the developers of that excellent project.<p>Would love to hear your feedback, if you think this new feature is useful, and for which use-cases. Would also encourage you to try it, we think you'll find it useful.<p>Thanks!\n- Cesar &amp; Rodny<p>Sysbox: https://github.com/nestybox/sysbox\nNestybox: https://www.nestybox.com/"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Rootless Containers/Pods that run systemd, Docker, and even Kubernetes"}}, "_tags": ["story", "author_ctalledo", "story_27974521", "show_hn"], "author": "ctalledo", "created_at": "2021-07-27T16:43:04Z", "created_at_i": 1627404184, "num_comments": 0, "objectID": "27974521", "points": 10, "story_id": 27974521, "story_text": "Hi HN, this is Cesar and Rodny, developers of an open-source container runtime called Sysbox, and co-founders of a startup called Nestybox (YC S20).<p>We launched on HN almost a year ago and got excellent feedback then (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24084758).<p>Happy to say that over the past year, Sysbox has continued to gain traction, particularly for securing containers in production, CI&#x2F;CD, and containerized dev environments.<p>We wanted to announce an important new feature: integration between Sysbox and Kubernetes.<p>As a quick refresher, Sysbox is a &quot;runc&quot; that enhances containers in two key ways:<p>1) Hardens container isolation (Linux user-namespace on all containers, partial procfs &amp; sysfs virtualization, initial mount locking, and more).<p>2) Enables containers to run not just microservices, but also system software such as systemd, Docker, K8s, K3s, and more. This enables containers to replace slower&#x2F;less-efficient VMs in many scenarios.<p>Prior to Sysbox this required insecure privileged containers, custom images, and special host mounts, or specialized tools like LXD, KinD and Minikube. With Sysbox, the container runtime sets up the container such that it can run the software securely and seamlessly, increasing security and reducing complexity.<p>Up to recently Sysbox only worked under Docker, but the latest release (v0.4.0) now works under Kubernetes too.<p>This means you can use Kubernetes to orchestrate pods that are rootless (i.e., root in the container maps to an unprivileged user on the host) and can run not just microservices, but full &quot;VM-like&quot; environments.<p>For example, you can create a pod that acts as a well isolated dev environment and inside of it run systemd, your favorite editor, plus Docker. Or create several pods that together form another K8s cluster for testing. Or run the K8s.io KinD inside a pod to create an entire K8s cluster inside one pod. Many interesting and powerful combinations are possible.<p>Sysbox has taken 2-years of very hard work, as it pushes the limits of OS virtualization (uid-shifting, syscall trapping, procfs virtualization, etc.) It was forked from the OCI runc in 2019, so we stand on the shoulders of the developers of that excellent project.<p>Would love to hear your feedback, if you think this new feature is useful, and for which use-cases. Would also encourage you to try it, we think you&#x27;ll find it useful.<p>Thanks!\n- Cesar &amp; Rodny<p>Sysbox: https:&#x2F;&#x2F;github.com&#x2F;nestybox&#x2F;sysbox\nNestybox: https:&#x2F;&#x2F;www.nestybox.com&#x2F;", "title": "Show HN: Rootless Containers/Pods that run systemd, Docker, and even Kubernetes", "updated_at": "2024-09-20T09:07:03Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pclark"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "We're gearing up for the launch of our iPhone application early October, and we wanted to get a users testing it with our <em>production</em> setup.<p>If you're interested in beta testing it [we'll reward beta testers with a free app if you're US based] sign up at: http://bit.ly/testbroadersheet [prefix your name with HN for priority] or send me an email with the <em>UDID</em>: peter@broadersheet.com<p>Another part of our launch is our marketing site, so if you guys have any feedback on that site we'd really appreciate it.<p>Thanks HN!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Beta test iPhone Broadersheet + critique our marketing site"}, "url": {"matchLevel": "none", "matchedWords": [], "value": ""}}, "_tags": ["story", "author_pclark", "story_837361", "ask_hn"], "author": "pclark", "children": [837364], "created_at": "2009-09-22T16:43:54Z", "created_at_i": 1253637834, "num_comments": 1, "objectID": "837361", "points": 1, "story_id": 837361, "story_text": "We're gearing up for the launch of our iPhone application early October, and we wanted to get a users testing it with our production setup.<p>If you're interested in beta testing it [we'll reward beta testers with a free app if you're US based] sign up at: http://bit.ly/testbroadersheet [prefix your name with HN for priority] or send me an email with the UDID: peter@broadersheet.com<p>Another part of our launch is our marketing site, so if you guys have any feedback on that site we'd really appreciate it.<p>Thanks HN!", "title": "Ask HN: Beta test iPhone Broadersheet + critique our marketing site", "updated_at": "2024-09-19T16:41:39Z", "url": ""}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "trelliscoded"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "I'm turning a side project into a <em>production</em> application, and I was wondering how other people split up their apps into different internal services, and what those services are.  I'd also like to hear about any negative consequences anyone had from their decisions on where to draw the lines.<p>I also have some more specific questions:<p>1. How do the microservices map onto the unix security model?  Does each service get its own <em>UID</em>?  How do you keep the user databases in sync in <em>production</em>?<p>2. How do you handle debugging these systems?<p>3. How do databases integrate with your architecture?  Do you use the same communications bus as the rest of the system, or do the services communicate with the database using its native protocols?  Do you treat the database server as a microservice too?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: What granularity are your microservices?"}}, "_tags": ["story", "author_trelliscoded", "story_13862423", "ask_hn"], "author": "trelliscoded", "children": [13872860], "created_at": "2017-03-13T21:11:10Z", "created_at_i": 1489439470, "num_comments": 1, "objectID": "13862423", "points": 1, "story_id": 13862423, "story_text": "I&#x27;m turning a side project into a production application, and I was wondering how other people split up their apps into different internal services, and what those services are.  I&#x27;d also like to hear about any negative consequences anyone had from their decisions on where to draw the lines.<p>I also have some more specific questions:<p>1. How do the microservices map onto the unix security model?  Does each service get its own UID?  How do you keep the user databases in sync in production?<p>2. How do you handle debugging these systems?<p>3. How do databases integrate with your architecture?  Do you use the same communications bus as the rest of the system, or do the services communicate with the database using its native protocols?  Do you treat the database server as a microservice too?", "title": "Ask HN: What granularity are your microservices?", "updated_at": "2024-09-20T00:32:40Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "XAKEPEHOK"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "I\u2019ve been working on Linux for several years and now planning to switch to Mac. I\u2019m already familiar with ARM images and x86 emulation nuances, so I understand that everything should generally work. However, I\u2019m looking for insights into specific edge cases, especially when comparing to Windows, since Linux is straightforward in these areas.<p>Questions:<p>1. Performance on file systems: On Windows, with WSL2, there are significant performance issues at the boundary of the host filesystem and WSL.<p>If the project resides inside WSL\u2019s filesystem, applications run fast, but IDEs (JetBrains) lag due to file sync overhead.<p>Conversely, if the project is on the host machine and mounted in WSL, IDEs perform well, but the application runs painfully slow.\nAre there any similar performance issues on Mac? My question is not about specific IDEs or their configurations, nor about WSL, but rather about containerization and file system behavior on macOS.<p>2. Volumes and permissions: On Windows, all files and directories inside Docker volumes have chmod 777 within the container. While this works, it\u2019s far from ideal.<p>I prefer setting specific <em>uid</em>/gid and user configurations in my containers to align dev environments with <em>production</em>, ensuring no surprises like missing permissions or needing root access to back up volumes on the host machine.<p>I don\u2019t care about the exact chmod values on Mac as long as I can, for example, delete files from volumes without sudo and have the container enforce its own file permissions and ownership.\nDoes Mac allow for this level of control?<p>3. Performance with volumes: ChatGPT suggested that there might be significant performance drops when storing MongoDB or Elasticsearch data inside volumes on Mac.<p>I\u2019m not setting up <em>production</em> on a Mac, but I plan to use it for data processing tasks, where datasets in MongoDB or Elasticsearch could reach 10 GB.<p>How noticeable is the slowdown in practice? I\u2019m looking for subjective feedback rather than exact metrics.<p>This question is aimed at experienced users who understand the nuances of containerization. Please don\u2019t suggest going back to Windows/WSL2 or using VSCode, etc. My knowledge of WSL2 may be outdated, and Windows might already perform better, but my focus right now is on Mac."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "macOS and Docker volumes \u2013 chmod and performance?"}}, "_tags": ["story", "author_XAKEPEHOK", "story_42353833", "ask_hn"], "author": "XAKEPEHOK", "created_at": "2024-12-07T23:54:42Z", "created_at_i": 1733615682, "num_comments": 0, "objectID": "42353833", "points": 1, "story_id": 42353833, "story_text": "I\u2019ve been working on Linux for several years and now planning to switch to Mac. I\u2019m already familiar with ARM images and x86 emulation nuances, so I understand that everything should generally work. However, I\u2019m looking for insights into specific edge cases, especially when comparing to Windows, since Linux is straightforward in these areas.<p>Questions:<p>1. Performance on file systems: On Windows, with WSL2, there are significant performance issues at the boundary of the host filesystem and WSL.<p>If the project resides inside WSL\u2019s filesystem, applications run fast, but IDEs (JetBrains) lag due to file sync overhead.<p>Conversely, if the project is on the host machine and mounted in WSL, IDEs perform well, but the application runs painfully slow.\nAre there any similar performance issues on Mac? My question is not about specific IDEs or their configurations, nor about WSL, but rather about containerization and file system behavior on macOS.<p>2. Volumes and permissions: On Windows, all files and directories inside Docker volumes have chmod 777 within the container. While this works, it\u2019s far from ideal.<p>I prefer setting specific uid&#x2F;gid and user configurations in my containers to align dev environments with production, ensuring no surprises like missing permissions or needing root access to back up volumes on the host machine.<p>I don\u2019t care about the exact chmod values on Mac as long as I can, for example, delete files from volumes without sudo and have the container enforce its own file permissions and ownership.\nDoes Mac allow for this level of control?<p>3. Performance with volumes: ChatGPT suggested that there might be significant performance drops when storing MongoDB or Elasticsearch data inside volumes on Mac.<p>I\u2019m not setting up production on a Mac, but I plan to use it for data processing tasks, where datasets in MongoDB or Elasticsearch could reach 10 GB.<p>How noticeable is the slowdown in practice? I\u2019m looking for subjective feedback rather than exact metrics.<p>This question is aimed at experienced users who understand the nuances of containerization. Please don\u2019t suggest going back to Windows&#x2F;WSL2 or using VSCode, etc. My knowledge of WSL2 may be outdated, and Windows might already perform better, but my focus right now is on Mac.", "title": "macOS and Docker volumes \u2013 chmod and performance?", "updated_at": "2024-12-07T23:57:58Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tomneijman"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["uuid", "production"], "value": "I built an open format for self-contained knowledge cards.<p>Each card is a single directory:\n- standalone HTML (no CDN, no external fonts)\n- bundled CSS + assets\n- print-ready PDF\n- JSON-LD metadata<p>No account. No platform. No external dependencies.\nThe card itself is the product.<p>Goal: use LLMs to distill practical knowledge into small independent units \u2014 then make those units survive without the technology that created them.<p>&quot;The best use of AI is not automation, but preservation \u2014 helping humans rebuild when machines are gone.&quot;<p>---<p>Identifier model:<p>Each card receives a 128-bit <em>ULID</em>.\nThe bits are mapped to A/C/G/T, <em>producin</em>g a 64-character DNA-like sequence.<p>The first 9 characters map to three craft \u201ccodon\u201d words from a fixed vocabulary (e.g. klei\u00b7vuur\u00b7rots).<p>Properties:\n- Globally unique\n- Chronologically sortable\n- Bidirectionally decodable\n- No central registry<p>The DNA appears in the URL, footer, and print version.<p>---<p>Integrity:<p>Each build generates a SHA-256 manifest of all cards, signed with Ed25519.\nVerification happens client-side using the Web Crypto API (no server round-trip).<p>Verify here:\n<a href=\"https://stoutenburger.com/verify/\" rel=\"nofollow\">https://stoutenburger.com/verify/</a><p>---<p>Edition 1 is live (10 cards):\nWater purification, fire, clay, basic metallurgy.<p>Five card types:\nKnowledge (why), Instruction (how), Product (what), Maker (who), Network (connections).<p>Build pipeline:\nMarkdown + YAML \u2192 JSON Schema \u2192 Nunjucks \u2192 Puppeteer \u2192 signed manifest.<p>Cards are designed to roam: USB stick, any domain, or printed in a drawer.<p>The format is open.\n<a href=\"https://stoutenburger.com\" rel=\"nofollow\">https://stoutenburger.com</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["uuid"], "value": "Show HN: Self-contained offline knowledge cards with <em>ULID</em>-DNA and IDsEd25519"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://stoutenburger.com"}}, "_tags": ["story", "author_tomneijman", "story_47167429", "show_hn"], "author": "tomneijman", "created_at": "2026-02-26T15:31:30Z", "created_at_i": 1772119890, "num_comments": 0, "objectID": "47167429", "points": 1, "story_id": 47167429, "story_text": "I built an open format for self-contained knowledge cards.<p>Each card is a single directory:\n- standalone HTML (no CDN, no external fonts)\n- bundled CSS + assets\n- print-ready PDF\n- JSON-LD metadata<p>No account. No platform. No external dependencies.\nThe card itself is the product.<p>Goal: use LLMs to distill practical knowledge into small independent units \u2014 then make those units survive without the technology that created them.<p>&quot;The best use of AI is not automation, but preservation \u2014 helping humans rebuild when machines are gone.&quot;<p>---<p>Identifier model:<p>Each card receives a 128-bit ULID.\nThe bits are mapped to A&#x2F;C&#x2F;G&#x2F;T, producing a 64-character DNA-like sequence.<p>The first 9 characters map to three craft \u201ccodon\u201d words from a fixed vocabulary (e.g. klei\u00b7vuur\u00b7rots).<p>Properties:\n- Globally unique\n- Chronologically sortable\n- Bidirectionally decodable\n- No central registry<p>The DNA appears in the URL, footer, and print version.<p>---<p>Integrity:<p>Each build generates a SHA-256 manifest of all cards, signed with Ed25519.\nVerification happens client-side using the Web Crypto API (no server round-trip).<p>Verify here:\n<a href=\"https:&#x2F;&#x2F;stoutenburger.com&#x2F;verify&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;stoutenburger.com&#x2F;verify&#x2F;</a><p>---<p>Edition 1 is live (10 cards):\nWater purification, fire, clay, basic metallurgy.<p>Five card types:\nKnowledge (why), Instruction (how), Product (what), Maker (who), Network (connections).<p>Build pipeline:\nMarkdown + YAML \u2192 JSON Schema \u2192 Nunjucks \u2192 Puppeteer \u2192 signed manifest.<p>Cards are designed to roam: USB stick, any domain, or printed in a drawer.<p>The format is open.\n<a href=\"https:&#x2F;&#x2F;stoutenburger.com\" rel=\"nofollow\">https:&#x2F;&#x2F;stoutenburger.com</a>", "title": "Show HN: Self-contained offline knowledge cards with ULID-DNA and IDsEd25519", "updated_at": "2026-02-26T15:36:20Z", "url": "https://stoutenburger.com"}], "hitsPerPage": 15, "nbHits": 12, "nbPages": 1, "page": 0, "params": "query=uuid+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 11, "processingTimingsMS": {"_request": {"roundTrip": 15}, "afterFetch": {"format": {"highlighting": 3, "total": 3}, "merge": {"total": 1}, "total": 1}, "fetch": {"query": 6, "scanning": 2, "total": 9}, "total": 11}, "query": "uuid production", "serverTimeMS": 15}}