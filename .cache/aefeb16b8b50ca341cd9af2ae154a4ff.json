{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "elfakyn"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "Responsible disclosure: public read/write <em>AWS S3</em> buckets in <em>production</em> (Emtrain)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://sec.elfakyn.com/2018/12/responsible-disclosure-emtrain-open-aws.html"}}, "_tags": ["story", "author_elfakyn", "story_18732115"], "author": "elfakyn", "created_at": "2018-12-21T08:40:29Z", "created_at_i": 1545381629, "num_comments": 0, "objectID": "18732115", "points": 2, "story_id": 18732115, "title": "Responsible disclosure: public read/write AWS S3 buckets in production (Emtrain)", "updated_at": "2024-09-20T03:28:53Z", "url": "https://sec.elfakyn.com/2018/12/responsible-disclosure-emtrain-open-aws.html"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "do_considering"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "For those that are using Digital Ocean's managed database in <em>production</em>, what is your review of their services.<p>I previously tried Spaces and found a lot of things just didn't work properly compared to <em>AWS S3</em>.  So I am a little hesitant to use them for <em>production</em> for databases.  What's your review?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: Anyone using DO managed databases in <em>production</em>?"}}, "_tags": ["story", "author_do_considering", "story_20926056", "ask_hn"], "author": "do_considering", "created_at": "2019-09-10T08:09:01Z", "created_at_i": 1568102941, "num_comments": 0, "objectID": "20926056", "points": 1, "story_id": 20926056, "story_text": "For those that are using Digital Ocean&#x27;s managed database in production, what is your review of their services.<p>I previously tried Spaces and found a lot of things just didn&#x27;t work properly compared to AWS S3.  So I am a little hesitant to use them for production for databases.  What&#x27;s your review?", "title": "Ask HN: Anyone using DO managed databases in production?", "updated_at": "2024-09-20T04:46:37Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kiwicopple"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "This is a MVP for Serverless Postgres.<p>1/ It uses Fly.io[0], which can automatically pause your database after all connections are released (and start it again when new connections join).<p>2/ It uses Oriole[1], a Postgres extension with experimental support for S3 / Decoupled Storage[2].<p>3/ It uses Tigris[3], Globally Distributed S3-Compatible Object Storage. Oriole will automatically backup the data to Tigris using background workers.<p>I wouldn't recommend using this in <em>production</em>, but I think it's in a good spot to provoke some discussion and ideas. You can get it running on your own machine with the steps provided - connecting to a remote Tigris bucket (can also be an <em>AWS S3</em> bucket).<p>[0] <a href=\"https://fly.io\">https://fly.io</a><p>[1] <a href=\"https://www.orioledb.com/\" rel=\"nofollow\">https://www.orioledb.com/</a><p>[2] Oriole Experiemental s3: <a href=\"https://www.orioledb.com/docs/usage/decoupled-storage\" rel=\"nofollow\">https://www.orioledb.com/docs/usage/decoupled-storage</a><p>[3] Tigris: <a href=\"https://www.tigrisdata.com/\" rel=\"nofollow\">https://www.tigrisdata.com/</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Serverless Postgres"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/kiwicopple/serverless-postgres"}}, "_tags": ["story", "author_kiwicopple", "story_40514089", "show_hn"], "author": "kiwicopple", "children": [40514090, 40515764, 40516013, 40516090, 40516659, 40516704, 40516811, 40517418, 40517465, 40519153, 40519430, 40519453, 40520107, 40521683, 40524778], "created_at": "2024-05-29T16:54:19Z", "created_at_i": 1717001659, "num_comments": 61, "objectID": "40514089", "points": 159, "story_id": 40514089, "story_text": "This is a MVP for Serverless Postgres.<p>1&#x2F; It uses Fly.io[0], which can automatically pause your database after all connections are released (and start it again when new connections join).<p>2&#x2F; It uses Oriole[1], a Postgres extension with experimental support for S3 &#x2F; Decoupled Storage[2].<p>3&#x2F; It uses Tigris[3], Globally Distributed S3-Compatible Object Storage. Oriole will automatically backup the data to Tigris using background workers.<p>I wouldn&#x27;t recommend using this in production, but I think it&#x27;s in a good spot to provoke some discussion and ideas. You can get it running on your own machine with the steps provided - connecting to a remote Tigris bucket (can also be an AWS S3 bucket).<p>[0] <a href=\"https:&#x2F;&#x2F;fly.io\">https:&#x2F;&#x2F;fly.io</a><p>[1] <a href=\"https:&#x2F;&#x2F;www.orioledb.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.orioledb.com&#x2F;</a><p>[2] Oriole Experiemental s3: <a href=\"https:&#x2F;&#x2F;www.orioledb.com&#x2F;docs&#x2F;usage&#x2F;decoupled-storage\" rel=\"nofollow\">https:&#x2F;&#x2F;www.orioledb.com&#x2F;docs&#x2F;usage&#x2F;decoupled-storage</a><p>[3] Tigris: <a href=\"https:&#x2F;&#x2F;www.tigrisdata.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.tigrisdata.com&#x2F;</a>", "title": "Show HN: Serverless Postgres", "updated_at": "2025-12-11T13:28:31Z", "url": "https://github.com/kiwicopple/serverless-postgres"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "TommyDANGerous"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "TLDR: Product developers need tools designed for them to build ML models. We\u2019d love for you to try a demo of Mage without needing to sign up: https://www.mage.ai/onboarding<p>My name is Tommy DANGerous (or Tommy Dang) and I\u2019m the CEO and co-founder at Mage. I worked at Airbnb for over 5 years as a product developer building features for guests.<p>Mage is a web-based tool for building, training, and deploying ML models that make predictions based off your data.<p>Training and using ML models in <em>production</em> typically requires working knowledge of building data pipelines, algorithms, infrastructure for deployment and inference, and more. Because of this highly specialized skillset, mostly data scientists and ML engineers are the only ones able to build and use ML models. Existing ML tools cater to this audience.<p>Over my 5+ years at Airbnb, I helped build and launch the Airbnb Experiences product, created ML models before and after we had in-house tooling, and built a devtool platform called Omni. I worked with 100s of product developers across the company and saw that they knew how ML is being used and had ideas on how they would apply ML to their specific feature. However, they relied on data science resource to help them implement their ideas even though we had ML tools built in-house for data scientists.<p>Mage is a low-code tool that you can access via your web browser. You can build ML models through our user interface. How it works:<p>1. First, you add data by uploading a file or connecting to data source like Amplitude, AWS Redshift, S3, Snowflake, GCP BigQuery, etc. Once you add your data, we store it on <em>AWS S3</em> for fast retrieval and transformations. \n2. Next step is you are given suggestions on how to enhance your dataset. You can perform functions like filtering, aggregating, adding columns, etc. We provide a GUI for you to perform these transformations. Behind the scenes, we\u2019re translating your input into code using the Pandas API.\n3. Once you\u2019re done preparing and cleaning your data, we\u2019ll train your model by launching a few data pipelines in Airflow, use Spark to build your training data, and then run our proprietary ML pipeline to train your model.\n4. Finally, when you\u2019re ready to use the model, we\u2019ll deploy your model to an online API endpoint that is running on AWS ECS. You can get your model\u2019s predictions via a POST request.<p>Existing ML tools are designed and built for data scientists and ML engineers. Mage is designed and built for product developers by product developers. This means we designed our tool to be usable by someone with no ML experience and we provide guided suggestions throughout the process to help educate users and help them become an ML expert.<p>We\u2019d love for you to try a demo of Mage without needing to sign up: https://www.mage.ai/onboarding. If you love it and want to use the entire set of features, you can sign up and use it for free (the best developers tools are free!). Thank you so much, your support is ultra appreciated!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Build predictive models with no prior ML experience"}}, "_tags": ["story", "author_TommyDANGerous", "story_30706193", "show_hn"], "author": "TommyDANGerous", "children": [30706262, 30706421, 30792179, 30792300, 30792313, 30792381, 30859124], "created_at": "2022-03-17T00:54:23Z", "created_at_i": 1647478463, "num_comments": 11, "objectID": "30706193", "points": 70, "story_id": 30706193, "story_text": "TLDR: Product developers need tools designed for them to build ML models. We\u2019d love for you to try a demo of Mage without needing to sign up: https:&#x2F;&#x2F;www.mage.ai&#x2F;onboarding<p>My name is Tommy DANGerous (or Tommy Dang) and I\u2019m the CEO and co-founder at Mage. I worked at Airbnb for over 5 years as a product developer building features for guests.<p>Mage is a web-based tool for building, training, and deploying ML models that make predictions based off your data.<p>Training and using ML models in production typically requires working knowledge of building data pipelines, algorithms, infrastructure for deployment and inference, and more. Because of this highly specialized skillset, mostly data scientists and ML engineers are the only ones able to build and use ML models. Existing ML tools cater to this audience.<p>Over my 5+ years at Airbnb, I helped build and launch the Airbnb Experiences product, created ML models before and after we had in-house tooling, and built a devtool platform called Omni. I worked with 100s of product developers across the company and saw that they knew how ML is being used and had ideas on how they would apply ML to their specific feature. However, they relied on data science resource to help them implement their ideas even though we had ML tools built in-house for data scientists.<p>Mage is a low-code tool that you can access via your web browser. You can build ML models through our user interface. How it works:<p>1. First, you add data by uploading a file or connecting to data source like Amplitude, AWS Redshift, S3, Snowflake, GCP BigQuery, etc. Once you add your data, we store it on AWS S3 for fast retrieval and transformations. \n2. Next step is you are given suggestions on how to enhance your dataset. You can perform functions like filtering, aggregating, adding columns, etc. We provide a GUI for you to perform these transformations. Behind the scenes, we\u2019re translating your input into code using the Pandas API.\n3. Once you\u2019re done preparing and cleaning your data, we\u2019ll train your model by launching a few data pipelines in Airflow, use Spark to build your training data, and then run our proprietary ML pipeline to train your model.\n4. Finally, when you\u2019re ready to use the model, we\u2019ll deploy your model to an online API endpoint that is running on AWS ECS. You can get your model\u2019s predictions via a POST request.<p>Existing ML tools are designed and built for data scientists and ML engineers. Mage is designed and built for product developers by product developers. This means we designed our tool to be usable by someone with no ML experience and we provide guided suggestions throughout the process to help educate users and help them become an ML expert.<p>We\u2019d love for you to try a demo of Mage without needing to sign up: https:&#x2F;&#x2F;www.mage.ai&#x2F;onboarding. If you love it and want to use the entire set of features, you can sign up and use it for free (the best developers tools are free!). Thank you so much, your support is ultra appreciated!", "title": "Show HN: Build predictive models with no prior ML experience", "updated_at": "2024-09-20T10:42:17Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "kirankgollu"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "Hey HN -<p>Kiran and Vijay here.<p>We built Oodle after seeing teams forced to choose between low cost, great experience, zero ops, and no lock-in. Even with premium tools, debugging still meant switching between Grafana for metrics, OpenSearch for logs, and Jaeger or Tempo for traces - copying timestamps, losing context, and burning time during incidents.<p>So we decided to rethink observability from first principles - in both architecture and experience.<p>Architecturally, we borrowed ideas from Snowflake: separated storage and compute so each can scale independently. All telemetry - metrics, logs, and traces - is stored on S3 in a custom columnar format, while serverless compute scales on demand. The result is 3\u20135\u00d7 lower cost, massive scale, and zero operational overhead, with full compatibility for Grafana dashboards, PromQL, and OpenSearch queries.<p>On the experience side, Oodle unifies everything you already use. It works with your existing Grafana and OpenSearch setup, but when an alert fires, Oodle automatically correlates metrics, logs, and traces in one view - showing the latency spike, the related logs, and the exact service that caused it.<p>It\u2019s already in <em>production</em> across SaaS, fintech, and healthcare companies processing 10 TB+ logs/day and 50 M+ time-series/day.<p>We\u2019ve both spent years building large-scale data systems. Vijay worked on Rubrik\u2019s petabyte-scale file system on object storage, and I helped build <em>AWS S3</em> and DynamoDB before leading Rubrik\u2019s cloud platform. Oodle applies the same design principles to observability.<p>You can try a live OpenTelemetry demo in &lt; 5 minutes (no signup needed): \n<a href=\"https://play.oodle.ai/settings?isUnifiedExperienceTourModalOpen=true\" rel=\"nofollow\">https://play.oodle.ai/settings?isUnifiedExperienceTourModalO...</a><p>or watch a short product walkthrough here: \n<a href=\"https://www.youtube.com/watch?v=wdYWDG3dRkU\" rel=\"nofollow\">https://www.youtube.com/watch?v=wdYWDG3dRkU</a><p>Would love feedback - what\u2019s your biggest observability pain today: cost, debuggability, or lock-in?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Oodle \u2013 Unified Debugging with OpenSearch and Grafana"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://blog.oodle.ai/meet-oodle-unified-and-ai-native-observability/"}}, "_tags": ["story", "author_kirankgollu", "story_45812312", "show_hn"], "author": "kirankgollu", "children": [45816488], "created_at": "2025-11-04T15:49:08Z", "created_at_i": 1762271348, "num_comments": 3, "objectID": "45812312", "points": 11, "story_id": 45812312, "story_text": "Hey HN -<p>Kiran and Vijay here.<p>We built Oodle after seeing teams forced to choose between low cost, great experience, zero ops, and no lock-in. Even with premium tools, debugging still meant switching between Grafana for metrics, OpenSearch for logs, and Jaeger or Tempo for traces - copying timestamps, losing context, and burning time during incidents.<p>So we decided to rethink observability from first principles - in both architecture and experience.<p>Architecturally, we borrowed ideas from Snowflake: separated storage and compute so each can scale independently. All telemetry - metrics, logs, and traces - is stored on S3 in a custom columnar format, while serverless compute scales on demand. The result is 3\u20135\u00d7 lower cost, massive scale, and zero operational overhead, with full compatibility for Grafana dashboards, PromQL, and OpenSearch queries.<p>On the experience side, Oodle unifies everything you already use. It works with your existing Grafana and OpenSearch setup, but when an alert fires, Oodle automatically correlates metrics, logs, and traces in one view - showing the latency spike, the related logs, and the exact service that caused it.<p>It\u2019s already in production across SaaS, fintech, and healthcare companies processing 10 TB+ logs&#x2F;day and 50 M+ time-series&#x2F;day.<p>We\u2019ve both spent years building large-scale data systems. Vijay worked on Rubrik\u2019s petabyte-scale file system on object storage, and I helped build AWS S3 and DynamoDB before leading Rubrik\u2019s cloud platform. Oodle applies the same design principles to observability.<p>You can try a live OpenTelemetry demo in &lt; 5 minutes (no signup needed): \n<a href=\"https:&#x2F;&#x2F;play.oodle.ai&#x2F;settings?isUnifiedExperienceTourModalOpen=true\" rel=\"nofollow\">https:&#x2F;&#x2F;play.oodle.ai&#x2F;settings?isUnifiedExperienceTourModalO...</a><p>or watch a short product walkthrough here: \n<a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wdYWDG3dRkU\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wdYWDG3dRkU</a><p>Would love feedback - what\u2019s your biggest observability pain today: cost, debuggability, or lock-in?", "title": "Show HN: Oodle \u2013 Unified Debugging with OpenSearch and Grafana", "updated_at": "2025-11-05T02:42:45Z", "url": "https://blog.oodle.ai/meet-oodle-unified-and-ai-native-observability/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "gunapologist99"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "Seems like a few of the non-<em>AWS S3</em> object storage services (even big ones like R2) have had some <em>production</em> issues.<p>Anyone care to describe their experiences?<p>I'd like to store billions of smallish files (photos etc) in <em>production</em>. Price (incl bandwidth) is important, but uptime and latency (where S3 does suffer) are more important."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: S3/Cloudflare R2/GCS/Wasabi/B2/Bunny or Minio/Seaweed/Garage/Ceph?"}}, "_tags": ["story", "author_gunapologist99", "story_39895205", "ask_hn"], "author": "gunapologist99", "children": [39895286, 39895988, 39900965, 39906821], "created_at": "2024-04-01T15:31:01Z", "created_at_i": 1711985461, "num_comments": 7, "objectID": "39895205", "points": 4, "story_id": 39895205, "story_text": "Seems like a few of the non-AWS S3 object storage services (even big ones like R2) have had some production issues.<p>Anyone care to describe their experiences?<p>I&#x27;d like to store billions of smallish files (photos etc) in production. Price (incl bandwidth) is important, but uptime and latency (where S3 does suffer) are more important.", "title": "Ask HN: S3/Cloudflare R2/GCS/Wasabi/B2/Bunny or Minio/Seaweed/Garage/Ceph?", "updated_at": "2024-09-20T16:46:50Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rkwap"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "I needed to integrate BunnyCDN Storage with Rails Active Storage, but found no official adapter available.<p>So, I built one.<p>Bunny Storage (<a href=\"https://bunny.net/storage/\" rel=\"nofollow\">https://bunny.net/storage/</a>) is a cheap and solid alternative to <em>AWS S3</em>, and this has been working flawlessly for me in <em>production</em>.<p>Would love feedback, and happy if this helps anyone else trying to use BunnyCDN with Rails."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: I Built a Rails Active Storage Adapter for BunnyCDN Storage"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/rkwap/active_storage_bunny"}}, "_tags": ["story", "author_rkwap", "story_45147362", "show_hn"], "author": "rkwap", "created_at": "2025-09-06T07:36:51Z", "created_at_i": 1757144211, "num_comments": 0, "objectID": "45147362", "points": 4, "story_id": 45147362, "story_text": "I needed to integrate BunnyCDN Storage with Rails Active Storage, but found no official adapter available.<p>So, I built one.<p>Bunny Storage (<a href=\"https:&#x2F;&#x2F;bunny.net&#x2F;storage&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;bunny.net&#x2F;storage&#x2F;</a>) is a cheap and solid alternative to AWS S3, and this has been working flawlessly for me in production.<p>Would love feedback, and happy if this helps anyone else trying to use BunnyCDN with Rails.", "title": "Show HN: I Built a Rails Active Storage Adapter for BunnyCDN Storage", "updated_at": "2025-09-09T13:19:37Z", "url": "https://github.com/rkwap/active_storage_bunny"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "maxivak"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "As a team of DevOps engineers, we've helped people to design and implement reliable infrastructure.<p>A lot of our projects needed a solution for a secure storage and management of passwords, tokens and protecting of sensitive data.\nHashicorp Vault is a good choice for small and mid-size organizations.<p>We built a SaaS managed Hashicorp Vault solution and opened it for everyone. \nIt is suitable for small companies when solutions based on AWS KMS and Vault Enterprise are too expensive. \nWe setup a Hashicorp Vault cluster in the cloud which is fully-managed and supported by our team. It satisfies compliance needs and fulfill the <em>production</em> requirements by Hashicorp. Customer data is stored encrypted on <em>AWS S3</em>.<p>We're inviting early adopters to join our private beta. Get your Vault cluster ready in a few minutes.<p><a href=\"https://rockos.io/managed-vault\" rel=\"nofollow\">https://rockos.io/managed-vault</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Vault as a Service"}}, "_tags": ["story", "author_maxivak", "story_19372079", "show_hn"], "author": "maxivak", "children": [19372600], "created_at": "2019-03-12T19:36:55Z", "created_at_i": 1552419415, "num_comments": 1, "objectID": "19372079", "points": 2, "story_id": 19372079, "story_text": "As a team of DevOps engineers, we&#x27;ve helped people to design and implement reliable infrastructure.<p>A lot of our projects needed a solution for a secure storage and management of passwords, tokens and protecting of sensitive data.\nHashicorp Vault is a good choice for small and mid-size organizations.<p>We built a SaaS managed Hashicorp Vault solution and opened it for everyone. \nIt is suitable for small companies when solutions based on AWS KMS and Vault Enterprise are too expensive. \nWe setup a Hashicorp Vault cluster in the cloud which is fully-managed and supported by our team. It satisfies compliance needs and fulfill the production requirements by Hashicorp. Customer data is stored encrypted on AWS S3.<p>We&#x27;re inviting early adopters to join our private beta. Get your Vault cluster ready in a few minutes.<p><a href=\"https:&#x2F;&#x2F;rockos.io&#x2F;managed-vault\" rel=\"nofollow\">https:&#x2F;&#x2F;rockos.io&#x2F;managed-vault</a>", "title": "Show HN: Vault as a Service", "updated_at": "2024-09-20T03:54:44Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "badmonster"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "Hi HN,<p>I\u2019ve been working on CocoIndex, an open-source ultra-performant framework to transform data for AI. It is optimized for data freshness, with incremental processing out-of-box.<p>You can start a CocoIndex with `pip install cocoindex` and declare a data flow that can build data transformation like LEGO\n- vector embeddings\n- knowledge graphs, \n- or extract, transform data with LLMs<p>It is a data processing framework beyond SQL. When you run the data flow either with live mode or batch mode, it will process the data incrementally with minimal recomputation and make it super fast to update the target stores on source changes.<p>The core engine is written in Rust. I've been a big fan of Rust before I left my last job. It is my first choice on the open source project for the data framework because of 1) robustness  2) performance 3) ability to bind to different languages.<p>I\u2019ve made a few tutorials and new projects since last launch, with different use cases:\n- <a href=\"https://www.youtube.com/@cocoindex-io\" rel=\"nofollow\">https://www.youtube.com/@cocoindex-io</a>\n- <a href=\"https://cocoindex.io/blogs/tags/examples\" rel=\"nofollow\">https://cocoindex.io/blogs/tags/examples</a><p>Previously, I\u2019ve worked at Google on projects like search indexing and ETL infra for 8 years. After I left Google last year, I built various projects and went through pivoting hell.<p>In all the projects I\u2019ve built, data still sits in the center of the problem and I find myself focusing on building data infra other than the business logic I need for data transformation. The current prepackaged RAG-as-service doesn't serve my needs, because I need to choose a different strategy for the context, and I also need deduplication, clustering (items are related), and other custom features that are commonly needed. That\u2019s where CocoIndex starts.<p>A simple philosophy behind it - data transformation is similar to formulas in spreadsheets. The ground of truth is at the source data, and all the steps to transform, and final target store are derived data, and should be reactive based on the source change. If you use CocoIndex, you only need to worry about defining transformations like formulas.<p><i>Data flow paradigm</i> came in as an immediate choice. because there\u2019s no side effect, lineage and observability just come out of the box.<p><i>Incremental processing</i> - If you are a data expert, an analogy would be a materialized view beyond SQL. The framework tracks pipeline states in database (Postgres) and only re-processes necessary portions. When data has changed, the framework handles the change data capture comprehensively and combines the mechanism for push and pull. Then clear stale derived data/versions and re-index data based on tracking data/logic changes or data TTL settings. There\u2019s lots of edge cases to do it right, for example, when a row is referenced in other places, and the row changes. These should be handled at the level of the framework.<p><i>At the compute engine level</i> - the framework should consider the multiple processes and concurrent updates. It should consider how to resume existing states from terminated execution. In the end, we want to build a framework that is easy to build with exceptional velocity, but scalable and robust in <em>production</em>.<p><i>Standardized the interface throughout the data flow</i> - really easy to plugin custom logic like LEGO; with a variety of native built-in components. One example is that it takes a few lines to switch among Qdrant, Postgres, Neo4j.<p>CocoIndex is licensed under Apache 2.0 <a href=\"https://github.com/cocoindex-io/cocoindex\">https://github.com/cocoindex-io/cocoindex</a>\nGetting started: <a href=\"https://cocoindex.io/docs/getting_started/quickstart\" rel=\"nofollow\">https://cocoindex.io/docs/getting_started/quickstart</a><p>I have rolled out over 25 releases since last HN launch and it has significantly improved in all aspects, especially supporting property graphs (Neo4j, Kuzu), supporting queue based CDC (<em>AWS S3</em>, SQS) and lots of infra updates including CLI, resilience and error handlings.<p>Excited to learn your thoughts, and thank you so much!<p>Linghua"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: CocoIndex \u2013 Open-Source Real-time Data transformation framework"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/cocoindex-io/cocoindex"}}, "_tags": ["story", "author_badmonster", "story_44181821", "show_hn"], "author": "badmonster", "created_at": "2025-06-04T15:30:00Z", "created_at_i": 1749051000, "num_comments": 0, "objectID": "44181821", "points": 2, "story_id": 44181821, "story_text": "Hi HN,<p>I\u2019ve been working on CocoIndex, an open-source ultra-performant framework to transform data for AI. It is optimized for data freshness, with incremental processing out-of-box.<p>You can start a CocoIndex with `pip install cocoindex` and declare a data flow that can build data transformation like LEGO\n- vector embeddings\n- knowledge graphs, \n- or extract, transform data with LLMs<p>It is a data processing framework beyond SQL. When you run the data flow either with live mode or batch mode, it will process the data incrementally with minimal recomputation and make it super fast to update the target stores on source changes.<p>The core engine is written in Rust. I&#x27;ve been a big fan of Rust before I left my last job. It is my first choice on the open source project for the data framework because of 1) robustness  2) performance 3) ability to bind to different languages.<p>I\u2019ve made a few tutorials and new projects since last launch, with different use cases:\n- <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;@cocoindex-io\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;@cocoindex-io</a>\n- <a href=\"https:&#x2F;&#x2F;cocoindex.io&#x2F;blogs&#x2F;tags&#x2F;examples\" rel=\"nofollow\">https:&#x2F;&#x2F;cocoindex.io&#x2F;blogs&#x2F;tags&#x2F;examples</a><p>Previously, I\u2019ve worked at Google on projects like search indexing and ETL infra for 8 years. After I left Google last year, I built various projects and went through pivoting hell.<p>In all the projects I\u2019ve built, data still sits in the center of the problem and I find myself focusing on building data infra other than the business logic I need for data transformation. The current prepackaged RAG-as-service doesn&#x27;t serve my needs, because I need to choose a different strategy for the context, and I also need deduplication, clustering (items are related), and other custom features that are commonly needed. That\u2019s where CocoIndex starts.<p>A simple philosophy behind it - data transformation is similar to formulas in spreadsheets. The ground of truth is at the source data, and all the steps to transform, and final target store are derived data, and should be reactive based on the source change. If you use CocoIndex, you only need to worry about defining transformations like formulas.<p><i>Data flow paradigm</i> came in as an immediate choice. because there\u2019s no side effect, lineage and observability just come out of the box.<p><i>Incremental processing</i> - If you are a data expert, an analogy would be a materialized view beyond SQL. The framework tracks pipeline states in database (Postgres) and only re-processes necessary portions. When data has changed, the framework handles the change data capture comprehensively and combines the mechanism for push and pull. Then clear stale derived data&#x2F;versions and re-index data based on tracking data&#x2F;logic changes or data TTL settings. There\u2019s lots of edge cases to do it right, for example, when a row is referenced in other places, and the row changes. These should be handled at the level of the framework.<p><i>At the compute engine level</i> - the framework should consider the multiple processes and concurrent updates. It should consider how to resume existing states from terminated execution. In the end, we want to build a framework that is easy to build with exceptional velocity, but scalable and robust in production.<p><i>Standardized the interface throughout the data flow</i> - really easy to plugin custom logic like LEGO; with a variety of native built-in components. One example is that it takes a few lines to switch among Qdrant, Postgres, Neo4j.<p>CocoIndex is licensed under Apache 2.0 <a href=\"https:&#x2F;&#x2F;github.com&#x2F;cocoindex-io&#x2F;cocoindex\">https:&#x2F;&#x2F;github.com&#x2F;cocoindex-io&#x2F;cocoindex</a>\nGetting started: <a href=\"https:&#x2F;&#x2F;cocoindex.io&#x2F;docs&#x2F;getting_started&#x2F;quickstart\" rel=\"nofollow\">https:&#x2F;&#x2F;cocoindex.io&#x2F;docs&#x2F;getting_started&#x2F;quickstart</a><p>I have rolled out over 25 releases since last HN launch and it has significantly improved in all aspects, especially supporting property graphs (Neo4j, Kuzu), supporting queue based CDC (AWS S3, SQS) and lots of infra updates including CLI, resilience and error handlings.<p>Excited to learn your thoughts, and thank you so much!<p>Linghua", "title": "Show HN: CocoIndex \u2013 Open-Source Real-time Data transformation framework", "updated_at": "2025-06-04T15:46:49Z", "url": "https://github.com/cocoindex-io/cocoindex"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "antoniogo"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "I did a few tests with <em>AWS s3</em> competitors (wasabi, digitalocean spaces, exoscale,...). Usually the api requests limit is not mentioned in documentation. But after testing (with vegeta and artillery) I found out that there is a 200 requests/second limit (doesn't matter if it is a GET/download or POST/upload request). After reaching the limit you will get errors for all further requests.<p>It's the limit for DO account/bucket (not for ip)...\nThis means that your competitor can simply launch the 200 get requests per second process and all your files on DigitalOcean Spaces will become unavailable for downloading...<p>It's cheaper than <em>AWS s3</em> but be aware that it's not suitable for <em>production</em> use..."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "DigitalOcean Spaces (s3) requests limit vulnerability"}}, "_tags": ["story", "author_antoniogo", "story_20798402", "ask_hn"], "author": "antoniogo", "created_at": "2019-08-26T09:22:23Z", "created_at_i": 1566811343, "num_comments": 0, "objectID": "20798402", "points": 2, "story_id": 20798402, "story_text": "I did a few tests with AWS s3 competitors (wasabi, digitalocean spaces, exoscale,...). Usually the api requests limit is not mentioned in documentation. But after testing (with vegeta and artillery) I found out that there is a 200 requests&#x2F;second limit (doesn&#x27;t matter if it is a GET&#x2F;download or POST&#x2F;upload request). After reaching the limit you will get errors for all further requests.<p>It&#x27;s the limit for DO account&#x2F;bucket (not for ip)...\nThis means that your competitor can simply launch the 200 get requests per second process and all your files on DigitalOcean Spaces will become unavailable for downloading...<p>It&#x27;s cheaper than AWS s3 but be aware that it&#x27;s not suitable for production use...", "title": "DigitalOcean Spaces (s3) requests limit vulnerability", "updated_at": "2024-09-20T04:52:44Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "reinhardt"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["aws", "s3", "production"], "value": "At work we maintain passwords, keys and secrets for a few dozens internal and 3rd party services such <em>as S3</em>, Sendgrid, Xero and more. For most services we actually have at least two accounts, one being used exclusively on <em>production</em>. So far we have been been storing most credentials in plain text config files under version control but we are looking for something more secure in case the source code is compromised. Any suggestions?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: How do you manage server side credentials? "}, "url": {"matchLevel": "none", "matchedWords": [], "value": ""}}, "_tags": ["story", "author_reinhardt", "story_4829559", "ask_hn"], "author": "reinhardt", "children": [4829798, 4831291, 4834606], "created_at": "2012-11-25T22:50:58Z", "created_at_i": 1353883858, "num_comments": 4, "objectID": "4829559", "points": 11, "story_id": 4829559, "story_text": "At work we maintain passwords, keys and secrets for a few dozens internal and 3rd party services such as S3, Sendgrid, Xero and more. For most services we actually have at least two accounts, one being used exclusively on production. So far we have been been storing most credentials in plain text config files under version control but we are looking for something more secure in case the source code is compromised. Any suggestions?", "title": "Ask HN: How do you manage server side credentials? ", "updated_at": "2023-09-06T21:35:01Z", "url": ""}], "hitsPerPage": 15, "nbHits": 11, "nbPages": 1, "page": 0, "params": "query=aws-s3+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 10, "processingTimingsMS": {"_request": {"roundTrip": 21}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 4, "scanning": 4, "total": 9}, "total": 10}, "query": "aws-s3 production", "serverTimeMS": 11}}