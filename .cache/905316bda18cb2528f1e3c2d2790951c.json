{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "theogravity"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "As a longtime TypeScript/Node.js developer, I've often faced challenges with logging\u2014choosing, using, and maintaining the right logger for various projects. While most loggers offer the usual methods like &quot;info&quot;, &quot;warn&quot;, and &quot;error&quot;, they vary significantly in how they handle structured metadata or Error objects. This can lead to ad-hoc solutions, like serializing errors or writing custom pipelines, just to get logs formatted correctly.<p>I built LogLayer to address these pain points by introducing a fluid, expressive API. With methods like &quot;withMetadata&quot; and &quot;withError&quot;, LogLayer separates object injection from the log message itself, making your logging code both cleaner and more maintainable.<p>Logs are processed through a LogLayer Transport, which acts as an adapter for your preferred logging library. This design offers several key advantages:<p>- Multi-Transport Support: Send logs to multiple destinations (e.g., DataDog and New Relic) simultaneously. I've personally used this feature to ship logs directly to DataDog without relying on their APM package or sidecars.<p>- Easy Logger Swapping: If you\u2019ve ever used <em>Pino</em> with Next.js, you might have encountered issues where it doesn\u2019t work out of the box after a <em>production</em> build without webpack hacks. With LogLayer, you can swap in a better-suited library without touching your logging code.<p>I spent a good few months on and off and used my winter break to launch version 5 of LogLayer, and also created the documentation using Vitepress.<p>LogLayer has been battle-tested in <em>production</em> at Airtop (<a href=\"https://airtop.ai\" rel=\"nofollow\">https://airtop.ai</a>), where it\u2019s been an integral part of our systems for years (we were running as Switchboard for almost four years and pivoted late last year).<p>(Disclaimer: I work at Airtop, but LogLayer is not sponsored / affiliated with them.)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LogLayer \u2013 Unified logger that routes logs to various logging libraries"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://loglayer.dev/"}}, "_tags": ["story", "author_theogravity", "story_42606454", "show_hn"], "author": "theogravity", "children": [42606937, 42607220, 42607255, 42608049, 42610137], "created_at": "2025-01-06T00:52:24Z", "created_at_i": 1736124744, "num_comments": 26, "objectID": "42606454", "points": 56, "story_id": 42606454, "story_text": "As a longtime TypeScript&#x2F;Node.js developer, I&#x27;ve often faced challenges with logging\u2014choosing, using, and maintaining the right logger for various projects. While most loggers offer the usual methods like &quot;info&quot;, &quot;warn&quot;, and &quot;error&quot;, they vary significantly in how they handle structured metadata or Error objects. This can lead to ad-hoc solutions, like serializing errors or writing custom pipelines, just to get logs formatted correctly.<p>I built LogLayer to address these pain points by introducing a fluid, expressive API. With methods like &quot;withMetadata&quot; and &quot;withError&quot;, LogLayer separates object injection from the log message itself, making your logging code both cleaner and more maintainable.<p>Logs are processed through a LogLayer Transport, which acts as an adapter for your preferred logging library. This design offers several key advantages:<p>- Multi-Transport Support: Send logs to multiple destinations (e.g., DataDog and New Relic) simultaneously. I&#x27;ve personally used this feature to ship logs directly to DataDog without relying on their APM package or sidecars.<p>- Easy Logger Swapping: If you\u2019ve ever used Pino with Next.js, you might have encountered issues where it doesn\u2019t work out of the box after a production build without webpack hacks. With LogLayer, you can swap in a better-suited library without touching your logging code.<p>I spent a good few months on and off and used my winter break to launch version 5 of LogLayer, and also created the documentation using Vitepress.<p>LogLayer has been battle-tested in production at Airtop (<a href=\"https:&#x2F;&#x2F;airtop.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;airtop.ai</a>), where it\u2019s been an integral part of our systems for years (we were running as Switchboard for almost four years and pivoted late last year).<p>(Disclaimer: I work at Airtop, but LogLayer is not sponsored &#x2F; affiliated with them.)", "title": "Show HN: LogLayer \u2013 Unified logger that routes logs to various logging libraries", "updated_at": "2025-09-22T20:27:10Z", "url": "https://loglayer.dev/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "theogravity"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "This has been in use on our <em>production</em> systems for around two years now at Switchboard (<a href=\"https://switchboard.app\" rel=\"nofollow\">https://switchboard.app</a>).<p>The problem we had was that we were using two different logging libs for our frontend and backend at the time (specifically roarr, and bunyan), and the API for the logging libs were not compatible with each other.<p>Loglayer was designed to wrap around popular JS logging libraries where you can swap out any underlying library without re-writing your log entries to adapt to the new library.<p>One common example is using plain &quot;console&quot; as a starting integration, but later swapping to a logging library like Winston once you've nailed down which JS logging lib fits your use-case. You can also swap to another from there (which we have done from bunyan to <em>pino</em> on our backend) if you need to later on.<p>It also provides a fluid API that standardizes how one should feed error, set context, and metadata. This means regardless of the underlying logging library used, developers use the same API to write logs throughout.<p><pre><code>  log\n  .withMetadata({ some: 'data'})\n  .withError(new Error('test'))\n  .info('my message')\n</code></pre>\nWith this new 4.x release, I added a plugin system allowing you to manipulate data before it is shipped to the underlying JS logger)."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Loglayer: A fluid logging interface for JavaScript loggers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/theogravity/loglayer"}}, "_tags": ["story", "author_theogravity", "story_40311559", "show_hn"], "author": "theogravity", "created_at": "2024-05-09T18:45:28Z", "created_at_i": 1715280328, "num_comments": 0, "objectID": "40311559", "points": 8, "story_id": 40311559, "story_text": "This has been in use on our production systems for around two years now at Switchboard (<a href=\"https:&#x2F;&#x2F;switchboard.app\" rel=\"nofollow\">https:&#x2F;&#x2F;switchboard.app</a>).<p>The problem we had was that we were using two different logging libs for our frontend and backend at the time (specifically roarr, and bunyan), and the API for the logging libs were not compatible with each other.<p>Loglayer was designed to wrap around popular JS logging libraries where you can swap out any underlying library without re-writing your log entries to adapt to the new library.<p>One common example is using plain &quot;console&quot; as a starting integration, but later swapping to a logging library like Winston once you&#x27;ve nailed down which JS logging lib fits your use-case. You can also swap to another from there (which we have done from bunyan to pino on our backend) if you need to later on.<p>It also provides a fluid API that standardizes how one should feed error, set context, and metadata. This means regardless of the underlying logging library used, developers use the same API to write logs throughout.<p><pre><code>  log\n  .withMetadata({ some: &#x27;data&#x27;})\n  .withError(new Error(&#x27;test&#x27;))\n  .info(&#x27;my message&#x27;)\n</code></pre>\nWith this new 4.x release, I added a plugin system allowing you to manipulate data before it is shipped to the underlying JS logger).", "title": "Show HN: Loglayer: A fluid logging interface for JavaScript loggers", "updated_at": "2024-09-20T16:59:17Z", "url": "https://github.com/theogravity/loglayer"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rhinorackattack"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "Disclaimer - the goal isn't to build a fully fledged company from this. Rather build a project that developers/startups will find useful as well as generate some income for myself.<p>I\u2019m working on a project to streamline the setup of tech stacks for startups. There are countless free repos on GitHub designed to help startups hit the ground running, but they usually just give you the code, leaving a lot of configuration work to be done. My solution is different: I want to offer a unified platform that integrates with a range of 3rd-party vendors and automatically sets up their cloud environments using API keys or OAuth.<p>The Vision:\nIn my dashboard, users will provide the necessary API keys, and we'll take care of the rest, automatically configuring and deploying the entire tech stack. At the end of the process, they'll have a backend and frontend deployed on AWS, a working RDS, and everything fully terraformed. Think of it as a one-click solution to build and deploy a comprehensive, ready-to-go environment.<p>What\u2019s in the box:<p>The repo is comprised of a monorepo with a FE with a basic auth page for login/logout, backend, shared types, an admin FE dashboard and a whole bunch of necessary and modern technologies to get your *SaaS portal* up and running. (SEO is available but the expectation is that this product is behind a paywall/authwall.<p>The Code:<p>- Monorepo with FE, BE and shared types\n- React + Next.js (frontend)\n- pnpm\n- Nest.js (server)\n- Tailwind\n- Material UI\n- Apollo (GraphQL)\n- Jest (Testing)\n- Typescript + ESLint + Prettier + Husky\n- Turbo\n- TypeORM\n- Segment\n- Database migrations\n- Docker\n- Logging (<em>Pino</em>.js)<p>Configuration:<p>- Terraform\n- GitHub Actions (CI/CD)\n- Local DB env with docker compose<p>Deployment (3rd Parties):<p>- Sentry\n- Auth0\n- Analytics\n- Stripe\n- Managed staging/<em>production</em> on AWS\n- Storage with S3\n- Langchain (ChatGPT)\n- Feature Flags (LaunchDarkly)\n- Mail (mailgun) + Notifications (SNS)\n- Postgres (RDS)\n- Lambdas with SAM<p>Features:<p>- Shared types\n- Langchain\n- Admin dashboard\n- CLI\n- Docs<p>Extras:<p>- Storybook\n- SEO\n- i18n<p>The Value Proposition:<p>- Unified Setup: Quickly deploy a <em>production</em>-ready tech stack with minimal configuration.\n- Reduced Complexity: Even with bots and automations, setting up AWS and other tools can be complex. This tool simplifies the entire process.<p>Why Build This:<p>- Complexity Management: Managing roles and deployments across various cloud platforms can be a headache. This tool pulls together all the essential components and simplifies the deployment process.\n- Engineer-Level Setup: You typically need an engineer to set up AWS, but this solution makes it trivial for non-tech founders to deploy a professional-grade infrastructure across a wide range of necessary services to run your start up.<p>I'm looking for feedback and validation on this idea. What do you think?<p>Some Initial Questions:<p>- I\u2019m not presenting vendor alternatives for the MVP. Maybe later I can present more vendor options, but for now I\u2019m trying to take the vendor decision-making process away from the startups so they can just focus on building the products. Will this present some problems?\n- I\u2019ve chosen AWS because startups often have to spend time learning about AWS and having users, env, compute, DBs, storage etc auto set up will allow them to focus on building their product and not on complex infrastructure. Once they\u2019re in the ecosystem, they can leverage more of it. I know there are more modern and streamlined alternatives to some of these services e.g. Vercel, Cloudflare, Firebase, Railway, Digital Ocean app platform, Render, Aptible; or when it comes to databases; planetDB or SurrealDB etc.<p>Business Model:<p>- Offer the code for free, once off fee for auto deployment and set up"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Idea Validation"}}, "_tags": ["story", "author_rhinorackattack", "story_40314582", "ask_hn"], "author": "rhinorackattack", "children": [40314703, 40314823, 40380254], "created_at": "2024-05-10T01:08:44Z", "created_at_i": 1715303324, "num_comments": 3, "objectID": "40314582", "points": 1, "story_id": 40314582, "story_text": "Disclaimer - the goal isn&#x27;t to build a fully fledged company from this. Rather build a project that developers&#x2F;startups will find useful as well as generate some income for myself.<p>I\u2019m working on a project to streamline the setup of tech stacks for startups. There are countless free repos on GitHub designed to help startups hit the ground running, but they usually just give you the code, leaving a lot of configuration work to be done. My solution is different: I want to offer a unified platform that integrates with a range of 3rd-party vendors and automatically sets up their cloud environments using API keys or OAuth.<p>The Vision:\nIn my dashboard, users will provide the necessary API keys, and we&#x27;ll take care of the rest, automatically configuring and deploying the entire tech stack. At the end of the process, they&#x27;ll have a backend and frontend deployed on AWS, a working RDS, and everything fully terraformed. Think of it as a one-click solution to build and deploy a comprehensive, ready-to-go environment.<p>What\u2019s in the box:<p>The repo is comprised of a monorepo with a FE with a basic auth page for login&#x2F;logout, backend, shared types, an admin FE dashboard and a whole bunch of necessary and modern technologies to get your *SaaS portal* up and running. (SEO is available but the expectation is that this product is behind a paywall&#x2F;authwall.<p>The Code:<p>- Monorepo with FE, BE and shared types\n- React + Next.js (frontend)\n- pnpm\n- Nest.js (server)\n- Tailwind\n- Material UI\n- Apollo (GraphQL)\n- Jest (Testing)\n- Typescript + ESLint + Prettier + Husky\n- Turbo\n- TypeORM\n- Segment\n- Database migrations\n- Docker\n- Logging (Pino.js)<p>Configuration:<p>- Terraform\n- GitHub Actions (CI&#x2F;CD)\n- Local DB env with docker compose<p>Deployment (3rd Parties):<p>- Sentry\n- Auth0\n- Analytics\n- Stripe\n- Managed staging&#x2F;production on AWS\n- Storage with S3\n- Langchain (ChatGPT)\n- Feature Flags (LaunchDarkly)\n- Mail (mailgun) + Notifications (SNS)\n- Postgres (RDS)\n- Lambdas with SAM<p>Features:<p>- Shared types\n- Langchain\n- Admin dashboard\n- CLI\n- Docs<p>Extras:<p>- Storybook\n- SEO\n- i18n<p>The Value Proposition:<p>- Unified Setup: Quickly deploy a production-ready tech stack with minimal configuration.\n- Reduced Complexity: Even with bots and automations, setting up AWS and other tools can be complex. This tool simplifies the entire process.<p>Why Build This:<p>- Complexity Management: Managing roles and deployments across various cloud platforms can be a headache. This tool pulls together all the essential components and simplifies the deployment process.\n- Engineer-Level Setup: You typically need an engineer to set up AWS, but this solution makes it trivial for non-tech founders to deploy a professional-grade infrastructure across a wide range of necessary services to run your start up.<p>I&#x27;m looking for feedback and validation on this idea. What do you think?<p>Some Initial Questions:<p>- I\u2019m not presenting vendor alternatives for the MVP. Maybe later I can present more vendor options, but for now I\u2019m trying to take the vendor decision-making process away from the startups so they can just focus on building the products. Will this present some problems?\n- I\u2019ve chosen AWS because startups often have to spend time learning about AWS and having users, env, compute, DBs, storage etc auto set up will allow them to focus on building their product and not on complex infrastructure. Once they\u2019re in the ecosystem, they can leverage more of it. I know there are more modern and streamlined alternatives to some of these services e.g. Vercel, Cloudflare, Firebase, Railway, Digital Ocean app platform, Render, Aptible; or when it comes to databases; planetDB or SurrealDB etc.<p>Business Model:<p>- Offer the code for free, once off fee for auto deployment and set up", "title": "Ask HN: Idea Validation", "updated_at": "2025-02-26T22:31:53Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "EFFALO"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "<em>Piano</em> Roll <em>Production</em> at QRS Music"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.youtube.com/watch?v=i3FTaGwfXPM"}}, "_tags": ["story", "author_EFFALO", "story_20646391"], "author": "EFFALO", "created_at": "2019-08-08T17:14:59Z", "created_at_i": 1565284499, "num_comments": 0, "objectID": "20646391", "points": 1, "story_id": 20646391, "title": "Piano Roll Production at QRS Music", "updated_at": "2024-09-20T04:37:50Z", "url": "https://www.youtube.com/watch?v=i3FTaGwfXPM"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pseudolus"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Porn companies are embracing crowdfunding"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "https://www.theverge.com/2019/4/3/18283012/porn-pornography-companies-crowdfunding-indiegogo-patreon-vod-free-<em>pink</em>-white-<em>productions</em>"}}, "_tags": ["story", "author_pseudolus", "story_19564805"], "author": "pseudolus", "created_at": "2019-04-03T16:37:00Z", "created_at_i": 1554309420, "num_comments": 0, "objectID": "19564805", "points": 2, "story_id": 19564805, "title": "Porn companies are embracing crowdfunding", "updated_at": "2024-09-20T04:04:08Z", "url": "https://www.theverge.com/2019/4/3/18283012/porn-pornography-companies-crowdfunding-indiegogo-patreon-vod-free-pink-white-productions"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "goopthink"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "When Netflix first started out, it was an aggregator of movies and shows not unlike Spotify for music is today. Originally, the biggest challenge was the technology parts of the business. But today, most of Netflix\u2019s (and all of the competitors) challenges are not technology problems, but media creation problems - creating hit media. Meanwhile, all of the other <em>production</em> companies (HBO, Peacock, Disney) have caught up, while other technology entrants (Hulu, Apple TV) are likewise focusing on <em>production</em> of original content to stay competitive.<p>This is also starting to happen with Podcasts (Spotify as the leading example).<p>But why hasn\u2019t it happened for music, proper? I think Jay-Z\u2019s Tidal tried this but they weren\u2019t successful. But Sony Music Group, Universal Music Group, etc all have huge catalogs of music, and there is a trend of artists selling their catalogs to these companies. What\u2019s to stop them from pulling back their licensing and launching their own app competing with Spotify? And conversely, why hasn\u2019t Spotify moved into music <em>production</em> in the same way that it has moved into Podcasts? (Caveat being that they tried at some point with generic beats and <em>piano</em> and atmospheric music <em>production</em>, but those knockoffs fell flat and felt hidden)."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Why hasn\u2019t music been disrupted in the same ways movies have been?"}}, "_tags": ["story", "author_goopthink", "story_30048845", "ask_hn"], "author": "goopthink", "children": [30048916, 30049177, 30049986, 30050050, 30050410, 30050446, 30052084, 30052862, 30053299], "created_at": "2022-01-23T18:31:20Z", "created_at_i": 1642962680, "num_comments": 14, "objectID": "30048845", "points": 14, "story_id": 30048845, "story_text": "When Netflix first started out, it was an aggregator of movies and shows not unlike Spotify for music is today. Originally, the biggest challenge was the technology parts of the business. But today, most of Netflix\u2019s (and all of the competitors) challenges are not technology problems, but media creation problems - creating hit media. Meanwhile, all of the other production companies (HBO, Peacock, Disney) have caught up, while other technology entrants (Hulu, Apple TV) are likewise focusing on production of original content to stay competitive.<p>This is also starting to happen with Podcasts (Spotify as the leading example).<p>But why hasn\u2019t it happened for music, proper? I think Jay-Z\u2019s Tidal tried this but they weren\u2019t successful. But Sony Music Group, Universal Music Group, etc all have huge catalogs of music, and there is a trend of artists selling their catalogs to these companies. What\u2019s to stop them from pulling back their licensing and launching their own app competing with Spotify? And conversely, why hasn\u2019t Spotify moved into music production in the same way that it has moved into Podcasts? (Caveat being that they tried at some point with generic beats and piano and atmospheric music production, but those knockoffs fell flat and felt hidden).", "title": "Ask HN: Why hasn\u2019t music been disrupted in the same ways movies have been?", "updated_at": "2024-09-20T10:19:44Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "goopthink"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "When Netflix first started out, it was an aggregator of movies and shows not unlike Spotify today. Originally, the biggest challenge was the technology parts of the business. But today, most of Netflix\u2019s (and all of the competitors) challenges are not technology problems, but media creation problems - creating hit content to keep people engaged. Meanwhile, all of the other <em>production</em> companies (HBO, Peacock, Disney) have caught up technology-wise, while other technology entrants (Hulu, Apple TV) are likewise focusing on <em>production</em> of original content to stay competitive.<p>This is also starting to happen with Podcasts (Spotify as the leading example).<p>But why hasn\u2019t it happened for music, proper? I think Jay-Z\u2019s Tidal tried this but they weren\u2019t successful. But Sony Music Group, Universal Music Group, etc all have huge catalogs of music, and there is a trend of artists selling their catalogs to these companies. What\u2019s to stop them from pulling back their licensing and launching their own app competing with Spotify? And conversely, why hasn\u2019t Spotify moved into music <em>production</em> in the same way that it has moved into Podcasts? (Caveat being that they tried at some point with generic beats and <em>piano</em> and atmospheric music <em>production</em>, but those knockoffs felt flat and hidden - it wasn't promoted as Spotify original music in the same way Netflix or HBO content is).<p>Is it because licensing is much more distributed? (ownership primarily in artists' hands?)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Why hasn't music been disrupted yet in the same way movies have been?"}}, "_tags": ["story", "author_goopthink", "story_30048918", "ask_hn"], "author": "goopthink", "children": [30049904], "created_at": "2022-01-23T18:37:18Z", "created_at_i": 1642963038, "num_comments": 1, "objectID": "30048918", "points": 2, "story_id": 30048918, "story_text": "When Netflix first started out, it was an aggregator of movies and shows not unlike Spotify today. Originally, the biggest challenge was the technology parts of the business. But today, most of Netflix\u2019s (and all of the competitors) challenges are not technology problems, but media creation problems - creating hit content to keep people engaged. Meanwhile, all of the other production companies (HBO, Peacock, Disney) have caught up technology-wise, while other technology entrants (Hulu, Apple TV) are likewise focusing on production of original content to stay competitive.<p>This is also starting to happen with Podcasts (Spotify as the leading example).<p>But why hasn\u2019t it happened for music, proper? I think Jay-Z\u2019s Tidal tried this but they weren\u2019t successful. But Sony Music Group, Universal Music Group, etc all have huge catalogs of music, and there is a trend of artists selling their catalogs to these companies. What\u2019s to stop them from pulling back their licensing and launching their own app competing with Spotify? And conversely, why hasn\u2019t Spotify moved into music production in the same way that it has moved into Podcasts? (Caveat being that they tried at some point with generic beats and piano and atmospheric music production, but those knockoffs felt flat and hidden - it wasn&#x27;t promoted as Spotify original music in the same way Netflix or HBO content is).<p>Is it because licensing is much more distributed? (ownership primarily in artists&#x27; hands?)", "title": "Ask HN: Why hasn't music been disrupted yet in the same way movies have been?", "updated_at": "2024-09-20T10:19:44Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "noworld"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "Spruce <em>Pine</em> devastated by Helene \u2013 Semiconductor <em>production</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pino"], "value": "https://www.interest.co.nz/technology/129982/north-carolinas-spruce-<em>pine</em>-devastated-hurricane-helene-worlds-main-source-high"}}, "_tags": ["story", "author_noworld", "story_41696676"], "author": "noworld", "children": [41697514, 41697584], "created_at": "2024-09-30T13:05:59Z", "created_at_i": 1727701559, "num_comments": 1, "objectID": "41696676", "points": 4, "story_id": 41696676, "title": "Spruce Pine devastated by Helene \u2013 Semiconductor production", "updated_at": "2024-09-30T15:32:53Z", "url": "https://www.interest.co.nz/technology/129982/north-carolinas-spruce-pine-devastated-hurricane-helene-worlds-main-source-high"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "sergeyk"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "The Art of <em>Production</em>: Rrose (Techno, Player <em>Piano</em>)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://ra.co/features/3619"}}, "_tags": ["story", "author_sergeyk", "story_28010190"], "author": "sergeyk", "created_at": "2021-07-30T17:21:12Z", "created_at_i": 1627665672, "num_comments": 0, "objectID": "28010190", "points": 1, "story_id": 28010190, "title": "The Art of Production: Rrose (Techno, Player Piano)", "updated_at": "2024-09-20T09:01:55Z", "url": "https://ra.co/features/3619"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "janjongboom"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "Hi HN!<p>TL;DR: We\u2019ve launched StableBuild, a new tool to easily freeze and <em>pin</em> Docker images, operating system packages, Python packages, and arbitrary build dependencies; in 5 lines of code: <a href=\"https://stablebuild.com\" rel=\"nofollow\">https://stablebuild.com</a> .<p>As the CTO at an ML startup w/ 75 people (<a href=\"https://edgeimpulse.com/\" rel=\"nofollow\">https://edgeimpulse.com/</a>) I\u2019ve grown incredibly frustrated with non-deterministic builds. Last year basically every week one of our containers (we have 40+ unique ones in prod) would stop working properly because some dependency was updated or removed. This ranges from Nvidia deleting cuda base images from Docker Hub, to Chromium being removed from the Ubuntu package registry in favor of the snap version, to pandas 2 being published with breaking APIs - while everyone just depends on e.g. pandas&gt;=1.4.<p>This has been super disruptive because builds break for no apparent reason: someone pushes some unrelated code change, a container needs to be rebuilt, now it gets the latest dependencies =&gt; boom, either a compile error or an integration test fails. Many times this even blocks deployment. If the build system has decided that a container on master needs to be rebuilt, we can\u2019t deploy the complete system if a dependency has shifted. And, fixing this naturally falls on the most senior engineers.<p>Anyway, to fix this I\u2019ve funded (together w/ my Edge Impulse cofounder) StableBuild. It\u2019s a set of mirrors and registries that let you easily freeze and <em>pin</em> Docker images, apt/apk packages, Python packages, and arbitrary files and URLs from the internet. It currently consists of:<p>* A custom pull-through cache for Docker Hub, that makes any image pulled immutable. Protects against updated or removed images; and as a nice byproduct also bypasses pull-rate limits in Docker Hub.<p>* Full daily copies of the Ubuntu, Debian and Alpine package registries + the most popular PPAs; so you can <em>pin</em> to a specific date (give me the package registry as it was on 2023-12-15). Essentially what snapshot.debian.org does, but fast and highly available (and for more repos).<p>* Full daily copy of the PyPI registry, so you can also <em>pin</em> to a specific date. This has been super useful for resurrecting old Python code. Any Python example w/ dependencies is bitrotted the moment it gets published - StableBuild\u2019s historic registry helps tremendously (see <a href=\"https://docs.stablebuild.com/mirrors-and-caches/pypi-mirror#getting-older-python-examples-working\" rel=\"nofollow\">https://docs.stablebuild.com/mirrors-and-caches/pypi-mirror#...</a>)<p>* A generic file / URL cache for arbitrary things you need to pull from the internet during builds.<p>This has all been in <em>production</em> with SB\u2019s first customers and has basically eliminated random build failures due to changed dependencies for them. Naturally you still want to upgrade dependencies (security patches are nice!) - but you can do it at their own pace, rather than whenever a container rebuilds.<p>Long story short: we\u2019re releasing StableBuild to the public today. Pricing starts at $199 / month - which yes, is definitely more than free, but is cheaper than just running your own historic apt mirror on Amazon (which we used to do at Edge Impulse) - and there\u2019s a 7 day free trial (CC needed). Would love to hear people\u2019s thoughts :-)<p>Sign up: <a href=\"https://dashboard.stablebuild.com\" rel=\"nofollow\">https://dashboard.stablebuild.com</a><p>Docs: <a href=\"https://docs.stablebuild.com\" rel=\"nofollow\">https://docs.stablebuild.com</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pino"], "value": "Show HN: StableBuild \u2013 <em>pin</em> and freeze any build dependency"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pino"], "value": "https://www.stablebuild.com/blog/launching-stablebuild-freeze-and-<em>pin</em>-all-your-dependencies"}}, "_tags": ["story", "author_janjongboom", "story_39384828", "show_hn"], "author": "janjongboom", "created_at": "2024-02-15T16:37:21Z", "created_at_i": 1708015041, "num_comments": 0, "objectID": "39384828", "points": 4, "story_id": 39384828, "story_text": "Hi HN!<p>TL;DR: We\u2019ve launched StableBuild, a new tool to easily freeze and pin Docker images, operating system packages, Python packages, and arbitrary build dependencies; in 5 lines of code: <a href=\"https:&#x2F;&#x2F;stablebuild.com\" rel=\"nofollow\">https:&#x2F;&#x2F;stablebuild.com</a> .<p>As the CTO at an ML startup w&#x2F; 75 people (<a href=\"https:&#x2F;&#x2F;edgeimpulse.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;edgeimpulse.com&#x2F;</a>) I\u2019ve grown incredibly frustrated with non-deterministic builds. Last year basically every week one of our containers (we have 40+ unique ones in prod) would stop working properly because some dependency was updated or removed. This ranges from Nvidia deleting cuda base images from Docker Hub, to Chromium being removed from the Ubuntu package registry in favor of the snap version, to pandas 2 being published with breaking APIs - while everyone just depends on e.g. pandas&gt;=1.4.<p>This has been super disruptive because builds break for no apparent reason: someone pushes some unrelated code change, a container needs to be rebuilt, now it gets the latest dependencies =&gt; boom, either a compile error or an integration test fails. Many times this even blocks deployment. If the build system has decided that a container on master needs to be rebuilt, we can\u2019t deploy the complete system if a dependency has shifted. And, fixing this naturally falls on the most senior engineers.<p>Anyway, to fix this I\u2019ve funded (together w&#x2F; my Edge Impulse cofounder) StableBuild. It\u2019s a set of mirrors and registries that let you easily freeze and pin Docker images, apt&#x2F;apk packages, Python packages, and arbitrary files and URLs from the internet. It currently consists of:<p>* A custom pull-through cache for Docker Hub, that makes any image pulled immutable. Protects against updated or removed images; and as a nice byproduct also bypasses pull-rate limits in Docker Hub.<p>* Full daily copies of the Ubuntu, Debian and Alpine package registries + the most popular PPAs; so you can pin to a specific date (give me the package registry as it was on 2023-12-15). Essentially what snapshot.debian.org does, but fast and highly available (and for more repos).<p>* Full daily copy of the PyPI registry, so you can also pin to a specific date. This has been super useful for resurrecting old Python code. Any Python example w&#x2F; dependencies is bitrotted the moment it gets published - StableBuild\u2019s historic registry helps tremendously (see <a href=\"https:&#x2F;&#x2F;docs.stablebuild.com&#x2F;mirrors-and-caches&#x2F;pypi-mirror#getting-older-python-examples-working\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.stablebuild.com&#x2F;mirrors-and-caches&#x2F;pypi-mirror#...</a>)<p>* A generic file &#x2F; URL cache for arbitrary things you need to pull from the internet during builds.<p>This has all been in production with SB\u2019s first customers and has basically eliminated random build failures due to changed dependencies for them. Naturally you still want to upgrade dependencies (security patches are nice!) - but you can do it at their own pace, rather than whenever a container rebuilds.<p>Long story short: we\u2019re releasing StableBuild to the public today. Pricing starts at $199 &#x2F; month - which yes, is definitely more than free, but is cheaper than just running your own historic apt mirror on Amazon (which we used to do at Edge Impulse) - and there\u2019s a 7 day free trial (CC needed). Would love to hear people\u2019s thoughts :-)<p>Sign up: <a href=\"https:&#x2F;&#x2F;dashboard.stablebuild.com\" rel=\"nofollow\">https:&#x2F;&#x2F;dashboard.stablebuild.com</a><p>Docs: <a href=\"https:&#x2F;&#x2F;docs.stablebuild.com\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.stablebuild.com</a>", "title": "Show HN: StableBuild \u2013 pin and freeze any build dependency", "updated_at": "2024-09-20T16:29:49Z", "url": "https://www.stablebuild.com/blog/launching-stablebuild-freeze-and-pin-all-your-dependencies"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jimhi"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pino"], "value": "My original ideas is to just <em>ping</em> an API every second or so. But if I have 10,000 of these out there - will I just end up DDOSing myself?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Ask HN: How does an IoT device in <em>production</em> get instructions from a server?"}}, "_tags": ["story", "author_jimhi", "story_34985402", "ask_hn"], "author": "jimhi", "children": [34985539, 34985705], "created_at": "2023-03-01T17:37:45Z", "created_at_i": 1677692265, "num_comments": 4, "objectID": "34985402", "points": 3, "story_id": 34985402, "story_text": "My original ideas is to just ping an API every second or so. But if I have 10,000 of these out there - will I just end up DDOSing myself?", "title": "Ask HN: How does an IoT device in production get instructions from a server?", "updated_at": "2024-09-20T13:31:02Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "konstantina_ps"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "Today we launched the Jira for 3D <em>production</em> and called it Flow 3D.<p>We built Flow because 3D content teams were stuck managing <em>production</em> across 5\u20136 different tools\u2014Slack, spreadsheets, ShotGrid, Jira, email threads\u2026 you name it. That constant jumping between platforms slowed teams down and made creative work feel chaotic.<p>Over the past 5 years, there is one thing I\u2019ve been hearing over and over as I chat with founders, producers, art directors and artists in Gaming:\n&quot;our workflows are a nightmare \u2014 we hate ShotGrid, we need to update statuses in Jira, speadsheets, ShotGrid, Perforce as well as <em>ping</em> our team members on Slack to get things reviewed and moved along the pipeline&quot;<p>I always thought that was crazy. And what\u2019s even crazier? NONE of that software has the functionality for VIEWING a 3D model  wth<p>With Flow, we\u2019re bringing the entire 3D <em>production</em> pipeline into one place. Unlike Jira or ShotGrid, which were never designed for artists, Flow is built specifically for 3D teams. You can orchestrate your pipeline, track every asset\u2019s stage, and keep your team fully in sync\u2014without losing the creative flow.<p>Here\u2019s what\u2019s launching today:\n\u2022 Assigning tasks to team members\n\u2022 Sharing progress for a 3D asset\n\u2022 Real-time reviews \u2014 with revisions, approvals, and progress tracking all in one view.\n\u2022 Create saved views and filters to track every asset and keep projects moving with clarity.<p>What's coming next:\n\u2022 Build milestones, set deadlines, and design custom workflows tailored to your productions.<p>Flow is all about giving 3D artists and studios a modern, purpose-built alternative for managing 3D <em>production</em> workflows, offering clarity, speed, and fewer tools to juggle.<p>What do you think of this launch? We\u2019re always open to feedback\u2014drop a comment and we\u2019ll work to build the features you\u2019d love to see."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Jira for 3D <em>Production</em> Management"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.kaedim3d.com/flow"}}, "_tags": ["story", "author_konstantina_ps", "story_45375890", "show_hn"], "author": "konstantina_ps", "created_at": "2025-09-25T17:22:54Z", "created_at_i": 1758820974, "num_comments": 0, "objectID": "45375890", "points": 1, "story_id": 45375890, "story_text": "Today we launched the Jira for 3D production and called it Flow 3D.<p>We built Flow because 3D content teams were stuck managing production across 5\u20136 different tools\u2014Slack, spreadsheets, ShotGrid, Jira, email threads\u2026 you name it. That constant jumping between platforms slowed teams down and made creative work feel chaotic.<p>Over the past 5 years, there is one thing I\u2019ve been hearing over and over as I chat with founders, producers, art directors and artists in Gaming:\n&quot;our workflows are a nightmare \u2014 we hate ShotGrid, we need to update statuses in Jira, speadsheets, ShotGrid, Perforce as well as ping our team members on Slack to get things reviewed and moved along the pipeline&quot;<p>I always thought that was crazy. And what\u2019s even crazier? NONE of that software has the functionality for VIEWING a 3D model  wth<p>With Flow, we\u2019re bringing the entire 3D production pipeline into one place. Unlike Jira or ShotGrid, which were never designed for artists, Flow is built specifically for 3D teams. You can orchestrate your pipeline, track every asset\u2019s stage, and keep your team fully in sync\u2014without losing the creative flow.<p>Here\u2019s what\u2019s launching today:\n\u2022 Assigning tasks to team members\n\u2022 Sharing progress for a 3D asset\n\u2022 Real-time reviews \u2014 with revisions, approvals, and progress tracking all in one view.\n\u2022 Create saved views and filters to track every asset and keep projects moving with clarity.<p>What&#x27;s coming next:\n\u2022 Build milestones, set deadlines, and design custom workflows tailored to your productions.<p>Flow is all about giving 3D artists and studios a modern, purpose-built alternative for managing 3D production workflows, offering clarity, speed, and fewer tools to juggle.<p>What do you think of this launch? We\u2019re always open to feedback\u2014drop a comment and we\u2019ll work to build the features you\u2019d love to see.", "title": "Show HN: Jira for 3D Production Management", "updated_at": "2025-09-25T17:28:22Z", "url": "https://www.kaedim3d.com/flow"}, {"_highlightResult": {"author": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["pino"], "value": "<em>Pine</em>_Mushroom"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "The Death of Music <em>Production</em> Plugins [video]"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.youtube.com/watch?v=0tWHFRbooCY"}}, "_tags": ["story", "author_Pine_Mushroom", "story_41533546"], "author": "Pine_Mushroom", "created_at": "2024-09-13T18:02:43Z", "created_at_i": 1726250563, "num_comments": 0, "objectID": "41533546", "points": 1, "story_id": 41533546, "title": "The Death of Music Production Plugins [video]", "updated_at": "2024-09-20T17:48:09Z", "url": "https://www.youtube.com/watch?v=0tWHFRbooCY"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tommoor"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "A javascript library that allows easy manipulation of the favicon to include nice looking alert bubbles. This means that users can <em>pin</em> your site and still see when their attention is needed!<p>It's been done before, but in my opinion not in a way aesthetically pleasing enough for <em>production</em> use. This library also falls back to the standard number in page title approach for browsers that do no support canvas / dynamic favicons.<p>Thoughts welcome."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Favicon alert bubbles"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "http://tommoor.github.com/tinycon/"}}, "_tags": ["story", "author_tommoor", "story_3560321", "show_hn"], "author": "tommoor", "children": [3560333, 3560365, 3560460, 3560466, 3560497, 3560526, 3560647, 3560652, 3560660, 3560698, 3561166, 3561362, 3561446, 3561698, 3561902, 3562060, 3562396], "created_at": "2012-02-07T03:34:10Z", "created_at_i": 1328585650, "num_comments": 63, "objectID": "3560321", "points": 271, "story_id": 3560321, "story_text": "A javascript library that allows easy manipulation of the favicon to include nice looking alert bubbles. This means that users can pin your site and still see when their attention is needed!<p>It's been done before, but in my opinion not in a way aesthetically pleasing enough for production use. This library also falls back to the standard number in page title approach for browsers that do no support canvas / dynamic favicons.<p>Thoughts welcome.", "title": "Show HN: Favicon alert bubbles", "updated_at": "2024-09-19T18:18:16Z", "url": "http://tommoor.github.com/tinycon/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "tang8330"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["pino", "production"], "value": "Hey, I\u2019m Robin and I\u2019m the founder/CTO at Artie (<a href=\"https://www.artie.so/\">https://www.artie.so/</a>). We solve the problem of stale <em>production</em> data in the data warehouse. We\u2019re open source, and you can try it for free here: <a href=\"https://github.com/artie-labs/transfer\">https://github.com/artie-labs/transfer</a>.<p>Specifically, we do real time data replication from databases to data warehouses. We leverage change data capture (CDC) and stream processing to perform data transfers efficiently, enabling sub-minute latency and significant cost savings. Here\u2019s a quick demo (jumping straight to the important part): <a href=\"https://www.youtube.com/watch?v=uAi1tm4gd9U#t=81s\">https://www.youtube.com/watch?v=uAi1tm4gd9U#t=81s</a>.<p>I encountered this problem when I was a heavy data warehouse user at prior jobs. The data in our data warehouse was super lagged and analytics were always stale. Imagine a fintech company performing anti-fraud/transaction monitoring with batched ETLs and finding out that fraudulent transactions occurred 24 hours ago. This was very frustrating to me! Since my background was in distributed systems and database architecture, I knew that there was a better way to perform data transfers.<p>The more teams I spoke with, the more I realized this was a real pain point. People wanted real time data for analytics, fraud alerting, transaction monitoring, and training AI/ML models; but there wasn\u2019t an easy out-of-the-box solution. Companies were either constrained on latency or schema integrity/data usability or data accuracy. Companies started telling me that if I built a tool that is robust, near real time, but also maintained schema integrity and data accuracy, they would very happily pay for it.<p>So I built Artie, a real time, open-source data streaming solution to transfer data from databases to data warehouses and handle schemas automatically in-flight (DMLs and DDLs).<p>Typical ETL solutions leverage batched processes that run on a schedule (DAGs, Airflow), which cannot achieve real time data syncs. This means that when companies aggregate <em>production</em> data into their data warehouse, the underlying data is always stale.<p>Streaming change data capture (CDC) logs is a more efficient way to transfer data, and helps lower networking/data ingestion costs as well. However, building data pipelines with CDC streaming is complicated. I wanted Artie to be the tool that abstracts away that complexity so that any company can benefit from having real time data.<p>A big challenge is implementing CDC streaming for stateful data (i.e. row updates/deletes) and schema changes (DDLs), which most streaming solutions just don\u2019t address, meaning that complexity is passed down to the customer. A lot of in-house streaming solutions leverage some combination of Debezium + Kafka/Kinesis + Apache Flink and are able to achieve near real time syncs, but they only handle append-only events (inserts) and don't handle schema changes/schema evolution like DMLs and DDLs. Not handling thse means the data at the destination doesn't look exactly like the <em>production</em> database, which obscures the source of truth. You end up having to do additional work to make the data warehouse tables match the source DB.<p>So how do we offer a robust CDC streaming solution?  We grab CDC logs using Debezium and/or our custom connector (which solves for certain edge cases that Debezium doesn\u2019t handle) and push them into Kafka (or Google Pub/Sub). Kafka helps ensure ordering and ease of recovery upon an outage - we use one table per topic, and the partition key is the primary key(s) to ensure no out of order writes. Artie then consumes these events from Kafka and we have an in-memory DB with our typing library that can infer schemas (DML and DDL), performs optimizations like deduplications, and then flushes data to the data warehouse. When the data warehouse confirms a successful merge, we then commit the offset within Kafka. This all happens with sub-minute data latency even at high volumes (several TBs or billions of rows).<p>However the data looks in your database, it should look exactly the same in your data warehouse. We also strive to handle all data types with no exceptions - i.e. supporting TOAST columns, composite keys as primary keys, arrays, large rows (&gt;1MBs), etc. (I say \u201cstrive\u201d because I\u2019m sure we haven\u2019t seen all possible data types yet!).<p>We\u2019ve been live for a while now. Several companies use us to update all their analytic dashboards in real time (with dashboards built on top of their data warehouse). Fintech platforms use us to perform financial transaction monitoring. A utilities software platform uses us to grab video/photo data to perform risk/hazard assessment against ML models.<p>Artie is live and supports PostgreSQL, MySQL, MongoDB sources and Snowflake, BigQuery, Redshift destinations. We make money from our hosted service. We charge based on usage (# of rows transferred per month, not including initial snapshots).<p>We\u2019d love for you to try it out! You can get started with the open source version here: <a href=\"https://github.com/artie-labs/transfer\">https://github.com/artie-labs/transfer</a>. We have a small OSS Slack community (www.artie.so/slack) \u2013 feel free to <em>ping</em> us for help or any other requests.<p>For our hosted version, we need to ensure that we have enough storage and compute capacity provisioned, so we\u2019re asking cloud users to hop on a quick call with us before we activate your account. Eventually we\u2019ll have easy self-serve functionality but that\u2019s not 100% built yet, so for now we set up a Slack channel to ensure smooth deployments. If you\u2019re willing to work with us on that, we\u2019ll be super excited to show you what we\u2019ve got. Just email us at founders@artie.so or request access from <a href=\"https://www.artie.so\">https://www.artie.so</a>.<p>We\u2019d love for you to try the OSS or hosted solution and give us feedback! We\u2019re eager to improve the product and test it against various workloads and data types :)"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Artie (YC S23) \u2013 Real time data replication to data warehouses"}}, "_tags": ["story", "author_tang8330", "story_36847569", "launch_hn"], "author": "tang8330", "children": [36848273, 36848350, 36848483, 36849087, 36849163, 36849490, 36850805, 36850821, 36851097, 36852294, 36852454, 36855159, 36855219, 36859362, 36859492, 36860189, 36860202, 36861658, 36861686, 36866149], "created_at": "2023-07-24T13:21:19Z", "created_at_i": 1690204879, "num_comments": 56, "objectID": "36847569", "points": 123, "story_id": 36847569, "story_text": "Hey, I\u2019m Robin and I\u2019m the founder&#x2F;CTO at Artie (<a href=\"https:&#x2F;&#x2F;www.artie.so&#x2F;\">https:&#x2F;&#x2F;www.artie.so&#x2F;</a>). We solve the problem of stale production data in the data warehouse. We\u2019re open source, and you can try it for free here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;artie-labs&#x2F;transfer\">https:&#x2F;&#x2F;github.com&#x2F;artie-labs&#x2F;transfer</a>.<p>Specifically, we do real time data replication from databases to data warehouses. We leverage change data capture (CDC) and stream processing to perform data transfers efficiently, enabling sub-minute latency and significant cost savings. Here\u2019s a quick demo (jumping straight to the important part): <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=uAi1tm4gd9U#t=81s\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=uAi1tm4gd9U#t=81s</a>.<p>I encountered this problem when I was a heavy data warehouse user at prior jobs. The data in our data warehouse was super lagged and analytics were always stale. Imagine a fintech company performing anti-fraud&#x2F;transaction monitoring with batched ETLs and finding out that fraudulent transactions occurred 24 hours ago. This was very frustrating to me! Since my background was in distributed systems and database architecture, I knew that there was a better way to perform data transfers.<p>The more teams I spoke with, the more I realized this was a real pain point. People wanted real time data for analytics, fraud alerting, transaction monitoring, and training AI&#x2F;ML models; but there wasn\u2019t an easy out-of-the-box solution. Companies were either constrained on latency or schema integrity&#x2F;data usability or data accuracy. Companies started telling me that if I built a tool that is robust, near real time, but also maintained schema integrity and data accuracy, they would very happily pay for it.<p>So I built Artie, a real time, open-source data streaming solution to transfer data from databases to data warehouses and handle schemas automatically in-flight (DMLs and DDLs).<p>Typical ETL solutions leverage batched processes that run on a schedule (DAGs, Airflow), which cannot achieve real time data syncs. This means that when companies aggregate production data into their data warehouse, the underlying data is always stale.<p>Streaming change data capture (CDC) logs is a more efficient way to transfer data, and helps lower networking&#x2F;data ingestion costs as well. However, building data pipelines with CDC streaming is complicated. I wanted Artie to be the tool that abstracts away that complexity so that any company can benefit from having real time data.<p>A big challenge is implementing CDC streaming for stateful data (i.e. row updates&#x2F;deletes) and schema changes (DDLs), which most streaming solutions just don\u2019t address, meaning that complexity is passed down to the customer. A lot of in-house streaming solutions leverage some combination of Debezium + Kafka&#x2F;Kinesis + Apache Flink and are able to achieve near real time syncs, but they only handle append-only events (inserts) and don&#x27;t handle schema changes&#x2F;schema evolution like DMLs and DDLs. Not handling thse means the data at the destination doesn&#x27;t look exactly like the production database, which obscures the source of truth. You end up having to do additional work to make the data warehouse tables match the source DB.<p>So how do we offer a robust CDC streaming solution?  We grab CDC logs using Debezium and&#x2F;or our custom connector (which solves for certain edge cases that Debezium doesn\u2019t handle) and push them into Kafka (or Google Pub&#x2F;Sub). Kafka helps ensure ordering and ease of recovery upon an outage - we use one table per topic, and the partition key is the primary key(s) to ensure no out of order writes. Artie then consumes these events from Kafka and we have an in-memory DB with our typing library that can infer schemas (DML and DDL), performs optimizations like deduplications, and then flushes data to the data warehouse. When the data warehouse confirms a successful merge, we then commit the offset within Kafka. This all happens with sub-minute data latency even at high volumes (several TBs or billions of rows).<p>However the data looks in your database, it should look exactly the same in your data warehouse. We also strive to handle all data types with no exceptions - i.e. supporting TOAST columns, composite keys as primary keys, arrays, large rows (&gt;1MBs), etc. (I say \u201cstrive\u201d because I\u2019m sure we haven\u2019t seen all possible data types yet!).<p>We\u2019ve been live for a while now. Several companies use us to update all their analytic dashboards in real time (with dashboards built on top of their data warehouse). Fintech platforms use us to perform financial transaction monitoring. A utilities software platform uses us to grab video&#x2F;photo data to perform risk&#x2F;hazard assessment against ML models.<p>Artie is live and supports PostgreSQL, MySQL, MongoDB sources and Snowflake, BigQuery, Redshift destinations. We make money from our hosted service. We charge based on usage (# of rows transferred per month, not including initial snapshots).<p>We\u2019d love for you to try it out! You can get started with the open source version here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;artie-labs&#x2F;transfer\">https:&#x2F;&#x2F;github.com&#x2F;artie-labs&#x2F;transfer</a>. We have a small OSS Slack community (www.artie.so&#x2F;slack) \u2013 feel free to ping us for help or any other requests.<p>For our hosted version, we need to ensure that we have enough storage and compute capacity provisioned, so we\u2019re asking cloud users to hop on a quick call with us before we activate your account. Eventually we\u2019ll have easy self-serve functionality but that\u2019s not 100% built yet, so for now we set up a Slack channel to ensure smooth deployments. If you\u2019re willing to work with us on that, we\u2019ll be super excited to show you what we\u2019ve got. Just email us at founders@artie.so or request access from <a href=\"https:&#x2F;&#x2F;www.artie.so\">https:&#x2F;&#x2F;www.artie.so</a>.<p>We\u2019d love for you to try the OSS or hosted solution and give us feedback! We\u2019re eager to improve the product and test it against various workloads and data types :)", "title": "Launch HN: Artie (YC S23) \u2013 Real time data replication to data warehouses", "updated_at": "2024-09-20T14:38:10Z"}], "hitsPerPage": 15, "nbHits": 40, "nbPages": 3, "page": 0, "params": "query=pino+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 11, "processingTimingsMS": {"_request": {"queue": 1, "roundTrip": 20}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 5, "scanning": 3, "total": 9}, "total": 11}, "query": "pino production", "serverTimeMS": 14}}