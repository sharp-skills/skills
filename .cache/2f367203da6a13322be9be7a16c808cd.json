{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "loppers92"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "<em>gRPC</em> in <em>Production</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "https://about.sourcegraph.com/go/<em>grpc</em>-in-<em>production</em>-alan-shreve"}}, "_tags": ["story", "author_loppers92", "story_14822294"], "author": "loppers92", "children": [14823709, 14823731, 14823967, 14824037, 14824365, 14824566, 14824567, 14825084], "created_at": "2017-07-21T17:29:53Z", "created_at_i": 1500658193, "num_comments": 56, "objectID": "14822294", "points": 186, "story_id": 14822294, "title": "gRPC in Production", "updated_at": "2024-09-20T01:05:45Z", "url": "https://about.sourcegraph.com/go/grpc-in-production-alan-shreve"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Apssouza"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "<em>gRPC</em> for <em>Production</em>(Golang)"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "https://github.com/apssouza22/<em>grpc</em>-<em>production</em>-go"}}, "_tags": ["story", "author_Apssouza", "story_24031338"], "author": "Apssouza", "children": [24031339, 24031343], "created_at": "2020-08-02T20:15:21Z", "created_at_i": 1596399321, "num_comments": 2, "objectID": "24031338", "points": 2, "story_id": 24031338, "title": "gRPC for Production(Golang)", "updated_at": "2024-09-20T06:42:58Z", "url": "https://github.com/apssouza22/grpc-production-go"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "somesaba"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "What are people's experiences with <em>gRPC</em> in <em>production</em> in 2018?"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grpc"], "value": "Ask HN: <em>gRPC</em> in Prod 2018"}}, "_tags": ["story", "author_somesaba", "story_17525914", "ask_hn"], "author": "somesaba", "created_at": "2018-07-13T19:25:21Z", "created_at_i": 1531509921, "num_comments": 0, "objectID": "17525914", "points": 3, "story_id": 17525914, "story_text": "What are people&#x27;s experiences with gRPC in production in 2018?", "title": "Ask HN: gRPC in Prod 2018", "updated_at": "2024-09-20T02:44:30Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fanf2"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grpc"], "value": "Experience designing and building <em>gRPC</em> services"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "https://blog.bugsnag.com/using-<em>grpc</em>-in-<em>production</em>/"}}, "_tags": ["story", "author_fanf2", "story_16509427"], "author": "fanf2", "created_at": "2018-03-03T14:43:04Z", "created_at_i": 1520088184, "num_comments": 0, "objectID": "16509427", "points": 2, "story_id": 16509427, "title": "Experience designing and building gRPC services", "updated_at": "2024-09-20T02:05:57Z", "url": "https://blog.bugsnag.com/using-grpc-in-production/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ofrzeta"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grpc"], "value": "Our experience designing and building <em>gRPC</em> services"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "https://blog.bugsnag.com/using-<em>grpc</em>-in-<em>production</em>/"}}, "_tags": ["story", "author_ofrzeta", "story_17947644"], "author": "ofrzeta", "created_at": "2018-09-09T20:36:06Z", "created_at_i": 1536525366, "num_comments": 0, "objectID": "17947644", "points": 1, "story_id": 17947644, "title": "Our experience designing and building gRPC services", "updated_at": "2024-09-20T02:59:46Z", "url": "https://blog.bugsnag.com/using-grpc-in-production/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "developer_denn"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "Show HN: Crudler \u2013 <em>Production</em>-ready REST, <em>gRPC</em> APIs from your database schema"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://crudler.com"}}, "_tags": ["story", "author_developer_denn", "story_46321950", "show_hn"], "author": "developer_denn", "created_at": "2025-12-19T03:20:24Z", "created_at_i": 1766114424, "num_comments": 0, "objectID": "46321950", "points": 2, "story_id": 46321950, "title": "Show HN: Crudler \u2013 Production-ready REST, gRPC APIs from your database schema", "updated_at": "2025-12-19T20:32:06Z", "url": "https://crudler.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "SteveMorin"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "Elixir and <em>gRPC</em> strong road to <em>production</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "https://code.tubitv.com/elixir-<em>grpc</em>-the-road-to-<em>production</em>-5d7daad4945b"}}, "_tags": ["story", "author_SteveMorin", "story_20642322"], "author": "SteveMorin", "created_at": "2019-08-08T06:28:36Z", "created_at_i": 1565245716, "num_comments": 0, "objectID": "20642322", "points": 2, "story_id": 20642322, "title": "Elixir and gRPC strong road to production", "updated_at": "2024-09-20T04:37:29Z", "url": "https://code.tubitv.com/elixir-grpc-the-road-to-production-5d7daad4945b"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "funbitty"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "Hi HN, I built Funxy because I needed a scripting layer for Go services \u2014 \nsomething between hardcoding business logic in Go and shipping a full dynamic \nlanguage with no safety guarantees.<p>What it does:\n- Embed in Go with 3 lines: New(), Bind(), Call(), Eval()\n- Bind Go structs and functions directly \u2014 scripts see fields and call methods\n- Type inference catches errors before runtime, but you rarely write annotations\n- Option/Result instead of nil \u2014 no surprise panics in <em>production</em>\n- Stdlib covers HTTP, WebSocket, <em>gRPC</em>, SQLite, JSON, CSV, async tasks out of the box<p>Use cases I built it for:\n- Business rules that change without redeploying the Go host\n- Data pipelines and ETL scripts\n- Configuration that's more than YAML but less than a full app<p>Example embedding:<p>Use case: discount rules your ops team can change without redeploy<p>Go side:<p><pre><code>    type Discounts struct {\n        VIPPercent     float64\n        HolidayPercent float64\n        MinOrder       float64\n    }\n    func (d *Discounts) SetVIP(pct float64)     { d.VIPPercent = pct }\n    func (d *Discounts) SetHoliday(pct float64)  { d.HolidayPercent = pct }\n    func (d *Discounts) SetMinOrder(min float64)  { d.MinOrder = min }\n\n    discounts := &amp;Discounts{VIPPercent: 0.05, HolidayPercent: 0.0, MinOrder: 100}\n    vm := funxy.New()\n    vm.Bind(&quot;discounts&quot;, discounts)\n    vm.LoadFile(&quot;rules.lang&quot;)\n    vm.Call(&quot;updateRules&quot;)\n    // discounts.VIPPercent is now 0.15\n    // discounts.HolidayPercent is now 0.10\n    // discounts.MinOrder is now 50 \u2014 no redeploy needed\n</code></pre>\nrules.lang (ops team edits this):<p><pre><code>    fun updateRules() {\n        // Black Friday campaign\n        discounts.SetVIP(0.15)\n        discounts.SetHoliday(0.10)\n        discounts.SetMinOrder(50.0)\n    }\n</code></pre>\nRepo: <a href=\"https://github.com/funvibe/funxy\" rel=\"nofollow\">https://github.com/funvibe/funxy</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Funxy \u2013 A typed scripting language that embeds into Go apps"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/funvibe/funxy"}}, "_tags": ["story", "author_funbitty", "story_46976253", "show_hn"], "author": "funbitty", "children": [46976398], "created_at": "2026-02-11T15:35:54Z", "created_at_i": 1770824154, "num_comments": 0, "objectID": "46976253", "points": 2, "story_id": 46976253, "story_text": "Hi HN, I built Funxy because I needed a scripting layer for Go services \u2014 \nsomething between hardcoding business logic in Go and shipping a full dynamic \nlanguage with no safety guarantees.<p>What it does:\n- Embed in Go with 3 lines: New(), Bind(), Call(), Eval()\n- Bind Go structs and functions directly \u2014 scripts see fields and call methods\n- Type inference catches errors before runtime, but you rarely write annotations\n- Option&#x2F;Result instead of nil \u2014 no surprise panics in production\n- Stdlib covers HTTP, WebSocket, gRPC, SQLite, JSON, CSV, async tasks out of the box<p>Use cases I built it for:\n- Business rules that change without redeploying the Go host\n- Data pipelines and ETL scripts\n- Configuration that&#x27;s more than YAML but less than a full app<p>Example embedding:<p>Use case: discount rules your ops team can change without redeploy<p>Go side:<p><pre><code>    type Discounts struct {\n        VIPPercent     float64\n        HolidayPercent float64\n        MinOrder       float64\n    }\n    func (d *Discounts) SetVIP(pct float64)     { d.VIPPercent = pct }\n    func (d *Discounts) SetHoliday(pct float64)  { d.HolidayPercent = pct }\n    func (d *Discounts) SetMinOrder(min float64)  { d.MinOrder = min }\n\n    discounts := &amp;Discounts{VIPPercent: 0.05, HolidayPercent: 0.0, MinOrder: 100}\n    vm := funxy.New()\n    vm.Bind(&quot;discounts&quot;, discounts)\n    vm.LoadFile(&quot;rules.lang&quot;)\n    vm.Call(&quot;updateRules&quot;)\n    &#x2F;&#x2F; discounts.VIPPercent is now 0.15\n    &#x2F;&#x2F; discounts.HolidayPercent is now 0.10\n    &#x2F;&#x2F; discounts.MinOrder is now 50 \u2014 no redeploy needed\n</code></pre>\nrules.lang (ops team edits this):<p><pre><code>    fun updateRules() {\n        &#x2F;&#x2F; Black Friday campaign\n        discounts.SetVIP(0.15)\n        discounts.SetHoliday(0.10)\n        discounts.SetMinOrder(50.0)\n    }\n</code></pre>\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;funvibe&#x2F;funxy\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;funvibe&#x2F;funxy</a>", "title": "Show HN: Funxy \u2013 A typed scripting language that embeds into Go apps", "updated_at": "2026-02-11T15:46:43Z", "url": "https://github.com/funvibe/funxy"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "vtalwar"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grpc"], "value": "<em>gRPC</em>: Internet-scale RPC framework is now 1.0"}, "url": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "https://cloudplatform.googleblog.com/2016/08/<em>gRPC</em>-a-true-Internet-scale-RPC-framework-is-now-1-and-ready-for-<em>production</em>-deployments.html"}}, "_tags": ["story", "author_vtalwar", "story_12344995"], "author": "vtalwar", "children": [12345154, 12345156, 12345223, 12345298, 12345335, 12345353, 12345358, 12345361, 12345486, 12345548, 12345563, 12345580, 12345704, 12345708, 12345781, 12345812, 12346071, 12346243, 12346778, 12346918, 12347453, 12349522, 12350313], "created_at": "2016-08-23T16:27:54Z", "created_at_i": 1471969674, "num_comments": 108, "objectID": "12344995", "points": 266, "story_id": 12344995, "title": "gRPC: Internet-scale RPC framework is now 1.0", "updated_at": "2024-09-19T23:36:37Z", "url": "https://cloudplatform.googleblog.com/2016/08/gRPC-a-true-Internet-scale-RPC-framework-is-now-1-and-ready-for-production-deployments.html"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ymz_ncnk"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "I created cmd-stream-go, a high-performance client-server library based on the Command Pattern, where Commands are first-class citizens.<p>Why build around Commands? As serializable objects, they can be sent over the network and persisted. They also provide a clean way to model distributed transactions through composition, and naturally support features like Undo and Redo. These qualities make them a great fit for implementing consistency patterns like Saga in distributed systems.<p>On the performance side, sending a Command involves minimal overhead \u2014 only its type and data need to be transmitted. In benchmarks focused on raw throughput (measured using 1, 2, 4, 8, and 16 clients in a simple request/response scenario), <i>cmd-stream/MUS</i> (<i>cmd-stream/Protobuf</i>) is about 3x (2.8x) faster than <i><em>gRPC</em>/Protobuf</i>, where <i>MUS</i> is a serialization format optimized for low byte usage. This kind of speedup can make a real difference in high-throughput systems or when you're trying to squeeze more out of limited resources.<p>By putting Commands at the transport layer, cmd-stream-go avoids the extra complexity of layering Command logic on top of generic RPC or REST.<p>The trade-offs: it\u2019s currently Go-only and maintained by a single developer.<p>If you\u2019re curious to explore more, you can check out the cmd-stream-go repository (<a href=\"https://github.com/cmd-stream/cmd-stream-go\">https://github.com/cmd-stream/cmd-stream-go</a>), see performance benchmarks (<a href=\"https://github.com/ymz-ncnk/go-client-server-benchmarks\">https://github.com/ymz-ncnk/go-client-server-benchmarks</a>), or read the series of posts on Command Pattern and how it can be applied over the network (<a href=\"https://medium.com/p/f9e53442c85d\" rel=\"nofollow\">https://medium.com/p/f9e53442c85d</a>).<p>I\u2019d love to hear your thoughts \u2014 especially where you think this model could shine, any <em>production</em> concerns, similar patterns or tools you\u2019ve seen in practice.<p>Feel free to reach me as <i>ymz-ncnk</i> on the Gophers Slack or follow <a href=\"https://x.com/cmdstream_lib\" rel=\"nofollow\">https://x.com/cmdstream_lib</a> for project updates."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grpc"], "value": "Show HN: Go Command-streaming lib for distributed systems (3x faster than <em>gRPC</em>)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/cmd-stream/cmd-stream-go"}}, "_tags": ["story", "author_ymz_ncnk", "story_44647772", "show_hn"], "author": "ymz_ncnk", "children": [44652076, 44652598], "created_at": "2025-07-22T14:57:50Z", "created_at_i": 1753196270, "num_comments": 4, "objectID": "44647772", "points": 12, "story_id": 44647772, "story_text": "I created cmd-stream-go, a high-performance client-server library based on the Command Pattern, where Commands are first-class citizens.<p>Why build around Commands? As serializable objects, they can be sent over the network and persisted. They also provide a clean way to model distributed transactions through composition, and naturally support features like Undo and Redo. These qualities make them a great fit for implementing consistency patterns like Saga in distributed systems.<p>On the performance side, sending a Command involves minimal overhead \u2014 only its type and data need to be transmitted. In benchmarks focused on raw throughput (measured using 1, 2, 4, 8, and 16 clients in a simple request&#x2F;response scenario), <i>cmd-stream&#x2F;MUS</i> (<i>cmd-stream&#x2F;Protobuf</i>) is about 3x (2.8x) faster than <i>gRPC&#x2F;Protobuf</i>, where <i>MUS</i> is a serialization format optimized for low byte usage. This kind of speedup can make a real difference in high-throughput systems or when you&#x27;re trying to squeeze more out of limited resources.<p>By putting Commands at the transport layer, cmd-stream-go avoids the extra complexity of layering Command logic on top of generic RPC or REST.<p>The trade-offs: it\u2019s currently Go-only and maintained by a single developer.<p>If you\u2019re curious to explore more, you can check out the cmd-stream-go repository (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;cmd-stream&#x2F;cmd-stream-go\">https:&#x2F;&#x2F;github.com&#x2F;cmd-stream&#x2F;cmd-stream-go</a>), see performance benchmarks (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;ymz-ncnk&#x2F;go-client-server-benchmarks\">https:&#x2F;&#x2F;github.com&#x2F;ymz-ncnk&#x2F;go-client-server-benchmarks</a>), or read the series of posts on Command Pattern and how it can be applied over the network (<a href=\"https:&#x2F;&#x2F;medium.com&#x2F;p&#x2F;f9e53442c85d\" rel=\"nofollow\">https:&#x2F;&#x2F;medium.com&#x2F;p&#x2F;f9e53442c85d</a>).<p>I\u2019d love to hear your thoughts \u2014 especially where you think this model could shine, any production concerns, similar patterns or tools you\u2019ve seen in practice.<p>Feel free to reach me as <i>ymz-ncnk</i> on the Gophers Slack or follow <a href=\"https:&#x2F;&#x2F;x.com&#x2F;cmdstream_lib\" rel=\"nofollow\">https:&#x2F;&#x2F;x.com&#x2F;cmdstream_lib</a> for project updates.", "title": "Show HN: Go Command-streaming lib for distributed systems (3x faster than gRPC)", "updated_at": "2025-07-24T04:18:30Z", "url": "https://github.com/cmd-stream/cmd-stream-go"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "edouardb"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "Hi HN!<p>We\u2019re Yann, Edouard, and Bastien from Koyeb (<a href=\"https://www.koyeb.com/\" rel=\"nofollow noreferrer\">https://www.koyeb.com/</a>). We\u2019re building a platform to let you push code to <em>production</em>, everywhere, and on high-performance hardware, in minutes. We aim to provide a \u201cglobal serverless feeling\u201d, without the hassle of re-writing all your apps or managing k8s complexity [1].<p>We built Scaleway, a cloud service provider where we designed ARM servers and provided them as cloud servers. During our time there, we saw customers struggle with the same issues while trying to deploy full-stack applications and APIs resiliently. As it turns out, deploying applications and managing networking across a multi-data center fleet of machines (virtual or physical) requires an overwhelming amount of orchestration and configuration. At the time, that complexity meant that multi-region deployments were simply out-of-reach for most businesses.<p>When thinking about how we wanted to solve those problems, we tried several solutions. We briefly explored offering a FaaS experience [2], but from our first steps, user feedback made us reconsider whether it was the correct abstraction. In most cases, it seemed that functions simply added complexity and required learning how to engineer using provider-specific primitives. In many ways, developing with functions felt like abandoning all of the benefits of frameworks.<p>Another popular option these days is to go with Kubernetes. From an engineering perspective, Kubernetes is extremely powerful, but it also involves massive amounts of overhead. Building software, managing networking, and deploying across regions involves integrating many different components and maintaining them over time. It can be tough to justify the level of effort and investment it takes to keep it all running rather than work on building out your product.<p>We believe you should be able to write your apps and run them without modification with simple scaling, global distribution transparently managed by the provider, and no infrastructure or orchestration management.<p>Koyeb is a cloud platform where you come with a git repository or a Docker image, we build the code into a container (when needed), run the container inside of Firecracker microVMs, and deploy it to multiple regions on top of bare metal servers. There is an edge network in front to accelerate delivery and a global networking layer for inter-service communication (service mesh/discovery) [3].<p>We took a few steps to get the Koyeb platform to where it is today: we built our own serverless engine [4]. We use Nomad and Firecracker for orchestration, and Kuma for the networking layer. In the last year, we spawned two regions in Washington, DC and Frankfurt, added support for native workers, <em>gRPC</em>, HTTP/2 [5], WebSockets, and custom health checks. We are working next on databases, autoscaling, and adding four new regions (US West, Singapore, Tokyo, and Paris).<p>We\u2019re super excited to show you Koyeb today and we\u2019d love to hear your thoughts on the platform and what we are building in the comments. To make getting started easy, we provide $5.50 in free credits every month so you can run up to two services for free.<p>P.S. A payment method is required to access the platform to prevent abuse (we had hard months last year dealing with that). If you\u2019d like to try the platform without adding a card, reach out at support@koyeb.com or @gokoyeb on Twitter.<p>[1] <a href=\"https://www.koyeb.com/blog/the-true-cost-of-kubernetes-people-time-and-productivity\" rel=\"nofollow noreferrer\">https://www.koyeb.com/blog/the-true-cost-of-kubernetes-peopl...</a>\n[2] <a href=\"https://www.koyeb.com/blog/the-koyeb-serverless-engine-docker-containers-and-continuous-deployment-of-functions\" rel=\"nofollow noreferrer\">https://www.koyeb.com/blog/the-koyeb-serverless-engine-docke...</a>\n[3] <a href=\"https://www.koyeb.com/blog/building-a-multi-region-service-mesh-with-kuma-envoy-anycast-bgp-and-mtls\" rel=\"nofollow noreferrer\">https://www.koyeb.com/blog/building-a-multi-region-service-m...</a>\n[4] <a href=\"https://www.koyeb.com/blog/the-koyeb-serverless-engine-from-kubernetes-to-nomad-firecracker-and-kuma\" rel=\"nofollow noreferrer\">https://www.koyeb.com/blog/the-koyeb-serverless-engine-from-...</a>\n[5] <a href=\"https://www.koyeb.com/blog/enabling-grpc-and-http2-support-at-edge-with-kuma-and-envoy\" rel=\"nofollow noreferrer\">https://www.koyeb.com/blog/enabling-<em>grpc</em>-and-http2-support-a...</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Koyeb \u2013 Deploy code to <em>production</em>, everywhere, in minutes"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.koyeb.com"}}, "_tags": ["story", "author_edouardb", "story_36815923", "show_hn"], "author": "edouardb", "created_at": "2023-07-21T16:33:38Z", "created_at_i": 1689957218, "num_comments": 0, "objectID": "36815923", "points": 6, "story_id": 36815923, "story_text": "Hi HN!<p>We\u2019re Yann, Edouard, and Bastien from Koyeb (<a href=\"https:&#x2F;&#x2F;www.koyeb.com&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.koyeb.com&#x2F;</a>). We\u2019re building a platform to let you push code to production, everywhere, and on high-performance hardware, in minutes. We aim to provide a \u201cglobal serverless feeling\u201d, without the hassle of re-writing all your apps or managing k8s complexity [1].<p>We built Scaleway, a cloud service provider where we designed ARM servers and provided them as cloud servers. During our time there, we saw customers struggle with the same issues while trying to deploy full-stack applications and APIs resiliently. As it turns out, deploying applications and managing networking across a multi-data center fleet of machines (virtual or physical) requires an overwhelming amount of orchestration and configuration. At the time, that complexity meant that multi-region deployments were simply out-of-reach for most businesses.<p>When thinking about how we wanted to solve those problems, we tried several solutions. We briefly explored offering a FaaS experience [2], but from our first steps, user feedback made us reconsider whether it was the correct abstraction. In most cases, it seemed that functions simply added complexity and required learning how to engineer using provider-specific primitives. In many ways, developing with functions felt like abandoning all of the benefits of frameworks.<p>Another popular option these days is to go with Kubernetes. From an engineering perspective, Kubernetes is extremely powerful, but it also involves massive amounts of overhead. Building software, managing networking, and deploying across regions involves integrating many different components and maintaining them over time. It can be tough to justify the level of effort and investment it takes to keep it all running rather than work on building out your product.<p>We believe you should be able to write your apps and run them without modification with simple scaling, global distribution transparently managed by the provider, and no infrastructure or orchestration management.<p>Koyeb is a cloud platform where you come with a git repository or a Docker image, we build the code into a container (when needed), run the container inside of Firecracker microVMs, and deploy it to multiple regions on top of bare metal servers. There is an edge network in front to accelerate delivery and a global networking layer for inter-service communication (service mesh&#x2F;discovery) [3].<p>We took a few steps to get the Koyeb platform to where it is today: we built our own serverless engine [4]. We use Nomad and Firecracker for orchestration, and Kuma for the networking layer. In the last year, we spawned two regions in Washington, DC and Frankfurt, added support for native workers, gRPC, HTTP&#x2F;2 [5], WebSockets, and custom health checks. We are working next on databases, autoscaling, and adding four new regions (US West, Singapore, Tokyo, and Paris).<p>We\u2019re super excited to show you Koyeb today and we\u2019d love to hear your thoughts on the platform and what we are building in the comments. To make getting started easy, we provide $5.50 in free credits every month so you can run up to two services for free.<p>P.S. A payment method is required to access the platform to prevent abuse (we had hard months last year dealing with that). If you\u2019d like to try the platform without adding a card, reach out at support@koyeb.com or @gokoyeb on Twitter.<p>[1] <a href=\"https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;the-true-cost-of-kubernetes-people-time-and-productivity\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;the-true-cost-of-kubernetes-peopl...</a>\n[2] <a href=\"https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;the-koyeb-serverless-engine-docker-containers-and-continuous-deployment-of-functions\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;the-koyeb-serverless-engine-docke...</a>\n[3] <a href=\"https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;building-a-multi-region-service-mesh-with-kuma-envoy-anycast-bgp-and-mtls\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;building-a-multi-region-service-m...</a>\n[4] <a href=\"https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;the-koyeb-serverless-engine-from-kubernetes-to-nomad-firecracker-and-kuma\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;the-koyeb-serverless-engine-from-...</a>\n[5] <a href=\"https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;enabling-grpc-and-http2-support-at-edge-with-kuma-and-envoy\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.koyeb.com&#x2F;blog&#x2F;enabling-grpc-and-http2-support-a...</a>", "title": "Show HN: Koyeb \u2013 Deploy code to production, everywhere, in minutes", "updated_at": "2024-09-20T14:34:35Z", "url": "https://www.koyeb.com"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "propeller_head"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "Hi there, the organisation I work for is betting heavily on <em>gRPC</em>. We did a few things on it, learned some, won a few battles and lost some.<p>The general take is that people are somewhat unsure if it's the right way to go.<p>We like the &quot;developing by contract&quot; aspect when using protocol buffers but we don't like the workflow as much.<p>I'm interested in hearing stories from people who have used successfully (maintaining a <em>production</em> product) and unsuccessfully (back to rest or whatever you were doing before.<p>Thanks"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grpc"], "value": "Ask HN: Why do you use <em>gRPC</em>?"}}, "_tags": ["story", "author_propeller_head", "story_22346381", "ask_hn"], "author": "propeller_head", "children": [22346547], "created_at": "2020-02-17T08:46:39Z", "created_at_i": 1581929199, "num_comments": 1, "objectID": "22346381", "points": 3, "story_id": 22346381, "story_text": "Hi there, the organisation I work for is betting heavily on gRPC. We did a few things on it, learned some, won a few battles and lost some.<p>The general take is that people are somewhat unsure if it&#x27;s the right way to go.<p>We like the &quot;developing by contract&quot; aspect when using protocol buffers but we don&#x27;t like the workflow as much.<p>I&#x27;m interested in hearing stories from people who have used successfully (maintaining a production product) and unsuccessfully (back to rest or whatever you were doing before.<p>Thanks", "title": "Ask HN: Why do you use gRPC?", "updated_at": "2024-09-20T05:41:22Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "rey12rey"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "<em>GRPC</em> releases Beta, opening door for use in <em>production</em> environments"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grpc"], "value": "http://googledevelopers.blogspot.com/2015/10/<em>grpc</em>-releases-beta-opening-door-for-use.html?m=0"}}, "_tags": ["story", "author_rey12rey", "story_10453887"], "author": "rey12rey", "created_at": "2015-10-26T19:39:56Z", "created_at_i": 1445888396, "num_comments": 0, "objectID": "10453887", "points": 3, "story_id": 10453887, "title": "GRPC releases Beta, opening door for use in production environments", "updated_at": "2023-09-06T23:56:31Z", "url": "http://googledevelopers.blogspot.com/2015/10/grpc-releases-beta-opening-door-for-use.html?m=0"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "didierbreedt"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "We've spent the last year building RunOS, a platform that spins up <em>production</em>-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via <em>gRPC</em> bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: <em>gRPC</em> streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu<p>- Solid Go bindings via libvirt<p>- Excellent GPU passthrough for AI workloads like Ollama<p>- Good isolation/performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;<p>2. Backend selects available server agents<p>3. <em>gRPC</em> commands sent to provision VMs<p>4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)<p>5. Node agents install and connect<p>6. Kubernetes bootstrap with kubeadm + Cilium<p>7. WireGuard mesh established between nodes<p>8. Storage configured (OpenEBS + Longhorn)<p>9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access<p>- Nodes communicate securely even if Kubernetes fails<p>- Simpler troubleshooting with separated layers<p>- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>The platform supports one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>Managed option: Dedicated servers with fixed 8 CPU/16GB instances. KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it's early access.<p>Self-hosted option: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Working on: Self-managed VM hosts with custom sizing.<p>What's Next<p>The agent code will be open source eventually. One company runs three <em>production</em> clusters already. Common feedback: &quot;I can't believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We're planning weekly updates here on HackerNews about new features, technical challenges, and <em>production</em> lessons learned building RunOS.<p>Questions? Happy to discuss architecture in the comments."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grpc"], "value": "Show HN: We built instant Kubernetes provisioning with KVM and <em>gRPC</em>"}}, "_tags": ["story", "author_didierbreedt", "story_45936611", "show_hn"], "author": "didierbreedt", "children": [45940027], "created_at": "2025-11-15T11:01:12Z", "created_at_i": 1763204472, "num_comments": 2, "objectID": "45936611", "points": 2, "story_id": 45936611, "story_text": "We&#x27;ve spent the last year building RunOS, a platform that spins up production-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu<p>- Solid Go bindings via libvirt<p>- Excellent GPU passthrough for AI workloads like Ollama<p>- Good isolation&#x2F;performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;<p>2. Backend selects available server agents<p>3. gRPC commands sent to provision VMs<p>4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)<p>5. Node agents install and connect<p>6. Kubernetes bootstrap with kubeadm + Cilium<p>7. WireGuard mesh established between nodes<p>8. Storage configured (OpenEBS + Longhorn)<p>9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access<p>- Nodes communicate securely even if Kubernetes fails<p>- Simpler troubleshooting with separated layers<p>- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>The platform supports one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>Managed option: Dedicated servers with fixed 8 CPU&#x2F;16GB instances. KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it&#x27;s early access.<p>Self-hosted option: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Working on: Self-managed VM hosts with custom sizing.<p>What&#x27;s Next<p>The agent code will be open source eventually. One company runs three production clusters already. Common feedback: &quot;I can&#x27;t believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We&#x27;re planning weekly updates here on HackerNews about new features, technical challenges, and production lessons learned building RunOS.<p>Questions? Happy to discuss architecture in the comments.", "title": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC", "updated_at": "2025-11-16T05:21:40Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "dib85"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["grpc", "production"], "value": "We've spent the last year building RunOS, a platform that spins up <em>production</em>-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via <em>gRPC</em> bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: <em>gRPC</em> streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu\n- Solid Go bindings via libvirt\n- Excellent GPU passthrough for AI workloads like Ollama\n- Good isolation/performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;\n2. Backend selects available server agents\n3. <em>gRPC</em> commands sent to provision VMs\n4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)\n5. Node agents install and connect\n6. Kubernetes bootstrap with kubeadm + Cilium\n7. WireGuard mesh established between nodes\n8. Storage configured (OpenEBS + Longhorn)\n9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access\n- Nodes communicate securely even if Kubernetes fails\n- Simpler troubleshooting with separated layers\n- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>We offer one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>RunOS Cloud: Managed dedicated servers with fixed 8 CPU/16GB instances (free trial credits available). KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it's early access.<p>Bring Your Own Node: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Coming soon: Self-managed VM hosts with custom sizing.<p>What's Next<p>Agent code will be open source. One company runs three <em>production</em> clusters already. Common feedback: &quot;I can't believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We're planning weekly updates here on HackerNews about new features, technical challenges, and <em>production</em> lessons.<p>Try it at runos.com - free trial credits for 8 CPU threads and 16GB memory.<p>Questions? Happy to discuss architecture in the comments."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["grpc"], "value": "Show HN: We built instant Kubernetes provisioning with KVM and <em>gRPC</em>"}}, "_tags": ["story", "author_dib85", "story_45927988", "show_hn"], "author": "dib85", "created_at": "2025-11-14T15:50:14Z", "created_at_i": 1763135414, "num_comments": 0, "objectID": "45927988", "points": 2, "story_id": 45927988, "story_text": "We&#x27;ve spent the last year building RunOS, a platform that spins up production-ready Kubernetes clusters in 5-10 minutes with databases, message queues, observability, and AI tooling configured.<p>The Problem<p>Every team rebuilds the same Kubernetes infrastructure: networking, certificates, monitoring, databases, storage. The existing solutions either lock you into a vendor ecosystem or dump you into raw Kubernetes complexity. We wanted the control of self-hosting without weeks of setup.<p>Architecture<p>Our system uses two agent types:<p>Server agents run on VM hosts and communicate with our backend via gRPC bidirectional streams. When users request a cluster node, the agent provisions a KVM-based VM and bootstraps it.<p>Node agents run on each Kubernetes node and handle cluster operations, monitoring, and service installations.<p>Key insight: gRPC streams initiated by agents eliminate firewall configuration and public IP requirements. Agents reach out to our backend, not vice versa.<p>Why KVM?<p>- Battle-tested, works great with Ubuntu\n- Solid Go bindings via libvirt\n- Excellent GPU passthrough for AI workloads like Ollama\n- Good isolation&#x2F;performance balance<p>Sometimes boring technology is the right choice.<p>Provisioning Flow<p>1. User clicks &quot;Create Cluster&quot;\n2. Backend selects available server agents\n3. gRPC commands sent to provision VMs\n4. KVM VMs spin up (Ubuntu Cloud 24.04, 30-60 seconds)\n5. Node agents install and connect\n6. Kubernetes bootstrap with kubeadm + Cilium\n7. WireGuard mesh established between nodes\n8. Storage configured (OpenEBS + Longhorn)\n9. Cluster ready (5-10 minutes total)<p>The WireGuard Decision<p>We manage WireGuard at the OS level, not Kubernetes level. Why?<p>- Same VPN secures both K8s traffic and SSH access\n- Nodes communicate securely even if Kubernetes fails\n- Simpler troubleshooting with separated layers\n- Easier multi-cluster peering (coming soon)<p>Our backend orchestrates WireGuard configs across nodes via the agents. Centrally coordinated, locally executed.<p>Version Management Hell<p>The hardest problem? Keeping 20+ services compatible across updates.<p>We offer one-click installation of: PostgreSQL, MySQL, ClickHouse, Kafka, RabbitMQ, MinIO, Longhorn, Harbor, Traefik, Grafana, Prometheus, Ollama, LiteLLM, Open WebUI, and more.<p>Each has opinions about K8s versions, storage, and networking. We use Helm charts, operators, and custom YAML as appropriate. The real work is maintaining compatibility matrices and testing every combination.<p>Deployment Models<p>RunOS Cloud: Managed dedicated servers with fixed 8 CPU&#x2F;16GB instances (free trial credits available). KVM handles VM provisioning with GPU passthrough for AI workloads. Strict security since it&#x27;s early access.<p>Bring Your Own Node: Run node agents on any hardware. Complete tenant isolation since you control infrastructure.<p>Coming soon: Self-managed VM hosts with custom sizing.<p>What&#x27;s Next<p>Agent code will be open source. One company runs three production clusters already. Common feedback: &quot;I can&#x27;t believe how fast I went from zero to a working cluster with Postgres, Kafka, and monitoring.&quot;<p>We&#x27;re planning weekly updates here on HackerNews about new features, technical challenges, and production lessons.<p>Try it at runos.com - free trial credits for 8 CPU threads and 16GB memory.<p>Questions? Happy to discuss architecture in the comments.", "title": "Show HN: We built instant Kubernetes provisioning with KVM and gRPC", "updated_at": "2025-11-14T18:11:05Z"}], "hitsPerPage": 15, "nbHits": 67, "nbPages": 5, "page": 0, "params": "query=grpc+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 10, "processingTimingsMS": {"_request": {"roundTrip": 20}, "afterFetch": {"format": {"highlighting": 1, "total": 1}}, "fetch": {"query": 5, "scanning": 2, "total": 8}, "total": 10}, "query": "grpc production", "serverTimeMS": 12}}