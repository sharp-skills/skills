{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "quanghuynt14"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "I built a terminal UI dashboard for monitoring BullMQ.<p>The problem: Every time I needed to debug queues, I had to set up <em>bull</em>-board \u2013 install multiple packages, integrate with Express/Fastify, wrap each queue with adapters, configure routes. Fine for <em>production</em> dashboards, but overkill when you just want to quickly inspect jobs.<p>bullmq-dash is a TUI that connects directly to Redis. It auto-discovers all BullMQ queues (no manual registration), shows job counts by status, lets you inspect job data/stacktraces, view schedulers/repeatable jobs, and tracks enqueue/dequeue rates. Keyboard-driven (vim-style navigation).<p>Use cases: local debugging, SSH sessions, quick <em>production</em> inspections \u2013 anywhere you want to see your queues without spinning up a web dashboard."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Bullmq-dash \u2013 Terminal UI dashboard for BullMQ (zero setup)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.npmjs.com/package/bullmq-dash"}}, "_tags": ["story", "author_quanghuynt14", "story_46855685", "show_hn"], "author": "quanghuynt14", "created_at": "2026-02-02T13:16:47Z", "created_at_i": 1770038207, "num_comments": 0, "objectID": "46855685", "points": 3, "story_id": 46855685, "story_text": "I built a terminal UI dashboard for monitoring BullMQ.<p>The problem: Every time I needed to debug queues, I had to set up bull-board \u2013 install multiple packages, integrate with Express&#x2F;Fastify, wrap each queue with adapters, configure routes. Fine for production dashboards, but overkill when you just want to quickly inspect jobs.<p>bullmq-dash is a TUI that connects directly to Redis. It auto-discovers all BullMQ queues (no manual registration), shows job counts by status, lets you inspect job data&#x2F;stacktraces, view schedulers&#x2F;repeatable jobs, and tracks enqueue&#x2F;dequeue rates. Keyboard-driven (vim-style navigation).<p>Use cases: local debugging, SSH sessions, quick production inspections \u2013 anywhere you want to see your queues without spinning up a web dashboard.", "title": "Show HN: Bullmq-dash \u2013 Terminal UI dashboard for BullMQ (zero setup)", "updated_at": "2026-02-02T16:54:03Z", "url": "https://www.npmjs.com/package/bullmq-dash"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "aposded"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "When I first set up OpenClaw, I ran into a big problem immediately.<p>I spent $250 on my first day doing what felt like harmless testing.<p>Nothing <em>production</em>. No customers. Just me trying things like:<p>\u201cSummarize this Slack thread\u201d<p>\u201cGive me a morning digest\u201d<p>\u201cExplain this error log\u201d<p>\u201cPull action items from the last N messages\u201d<p>A couple Telegram alerts<p>At first I blamed OpenClaw. The real issue was simpler: I had Claude set as the default for basically everything, and I accidentally created a workflow where every run got more expensive than the last.<p>Here\u2019s what actually happened.<p>\u201cSimple tasks\u201d weren\u2019t simple because the context kept growing I started with \u201csummarize the last 30\u201350 messages.\u201d Then I kept adding \u201cjust one more thing\u201d:<p>include prior decisions<p>keep continuity across runs<p>include relevant earlier context<p>make it more detailed<p>That makes results feel smarter, but it turns every request into a bigger prompt. The tricky part is it still feels like the same task, so you don\u2019t notice the cost drift until the number is already big.<p>Tool output bloat snowballed I let tool outputs flow straight into the next step:<p>long logs<p>giant diffs<p>full API responses<p>\u201cfor debugging\u201d screenshots<p>Even if one run is tolerable, the next run inherits the baggage. This is how testing quietly becomes a token furnace: output becomes input becomes output again.<p>Scheduled jobs created an \u201cidle \u2192 warm-up tax\u201d loop I had cron-ish jobs that ran, went idle, then ran again.<p>If your setup effectively re-establishes a big prompt footprint on each run, you keep paying the setup cost repeatedly. It\u2019s not one catastrophic request. It\u2019s lots of medium ones with repeated overhead.<p>Duplicates from retries/triggers A couple times I saw behavior consistent with \u201cthe same expensive work executed twice\u201d:<p>transient slowdowns causing retries<p>duplicated triggers from chat integrations<p>One duplicated summarization run isn\u2019t a rounding error when the prompt is already bloated.<p>So why did it hit $250 so fast?\nBecause Claude was my default hammer for every nail, and I unintentionally designed the system to feed itself bigger and bigger inputs.<p>What fixed it (the boring, effective stuff)<p>- Hard caps on what gets summarized (smaller windows, tighter selection)<p>- Aggressive trimming of tool output (only keep what the next step truly needs)<p>- Removed screenshots unless strictly required<p>- Forced \u201cfresh session\u201d boundaries for scheduled jobs so context can\u2019t grow forever<p>- Output length ceilings so digests can\u2019t become essays<p>- De-duped triggers and made retries safer to avoid re-running the same job twice<p>- And the biggest one: stop using the most expensive model by default for routine steps<p>The part that pushed me into building something\nAfter that first-day <em>bill</em>, the pattern was obvious: relying on discipline (\u201cI\u2019ll remember to switch models later\u201d) doesn\u2019t scale.<p>Claude was the immediate cost driver, so I took the routing model I\u2019d built for Agentlify and adapted it into a custom routing layer specifically for OpenClaw: cheap/fast models for routine steps, only escalate to Claude when the task actually needs it. That became https://clawpane.co<p>Not linking anything here. The point isn\u2019t \u201cbuy my thing.\u201d The point is that routing stops being an optimization and becomes a seatbelt once you\u2019ve had one day like this.<p>Takeaway\nIf you\u2019re trialing agent workflows and your <em>bill</em> is spiking, it\u2019s usually not one big request. It\u2019s:<p>- context creep<p>- tool payloads piling up<p>- scheduled runs repeatedly paying warm-up overhead<p>- occasional duplicates<p>and an expensive default model doing work that doesn\u2019t require it.<p>If you want, reply with what tasks you\u2019re running and what your defaults look like. I\u2019ll tell you where the spend usually hides."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Burned $250 in tokens on Day 1 with OpenClaw"}}, "_tags": ["story", "author_aposded", "story_47162495", "ask_hn"], "author": "aposded", "children": [47163274], "created_at": "2026-02-26T06:12:08Z", "created_at_i": 1772086328, "num_comments": 1, "objectID": "47162495", "points": 3, "story_id": 47162495, "story_text": "When I first set up OpenClaw, I ran into a big problem immediately.<p>I spent $250 on my first day doing what felt like harmless testing.<p>Nothing production. No customers. Just me trying things like:<p>\u201cSummarize this Slack thread\u201d<p>\u201cGive me a morning digest\u201d<p>\u201cExplain this error log\u201d<p>\u201cPull action items from the last N messages\u201d<p>A couple Telegram alerts<p>At first I blamed OpenClaw. The real issue was simpler: I had Claude set as the default for basically everything, and I accidentally created a workflow where every run got more expensive than the last.<p>Here\u2019s what actually happened.<p>\u201cSimple tasks\u201d weren\u2019t simple because the context kept growing I started with \u201csummarize the last 30\u201350 messages.\u201d Then I kept adding \u201cjust one more thing\u201d:<p>include prior decisions<p>keep continuity across runs<p>include relevant earlier context<p>make it more detailed<p>That makes results feel smarter, but it turns every request into a bigger prompt. The tricky part is it still feels like the same task, so you don\u2019t notice the cost drift until the number is already big.<p>Tool output bloat snowballed I let tool outputs flow straight into the next step:<p>long logs<p>giant diffs<p>full API responses<p>\u201cfor debugging\u201d screenshots<p>Even if one run is tolerable, the next run inherits the baggage. This is how testing quietly becomes a token furnace: output becomes input becomes output again.<p>Scheduled jobs created an \u201cidle \u2192 warm-up tax\u201d loop I had cron-ish jobs that ran, went idle, then ran again.<p>If your setup effectively re-establishes a big prompt footprint on each run, you keep paying the setup cost repeatedly. It\u2019s not one catastrophic request. It\u2019s lots of medium ones with repeated overhead.<p>Duplicates from retries&#x2F;triggers A couple times I saw behavior consistent with \u201cthe same expensive work executed twice\u201d:<p>transient slowdowns causing retries<p>duplicated triggers from chat integrations<p>One duplicated summarization run isn\u2019t a rounding error when the prompt is already bloated.<p>So why did it hit $250 so fast?\nBecause Claude was my default hammer for every nail, and I unintentionally designed the system to feed itself bigger and bigger inputs.<p>What fixed it (the boring, effective stuff)<p>- Hard caps on what gets summarized (smaller windows, tighter selection)<p>- Aggressive trimming of tool output (only keep what the next step truly needs)<p>- Removed screenshots unless strictly required<p>- Forced \u201cfresh session\u201d boundaries for scheduled jobs so context can\u2019t grow forever<p>- Output length ceilings so digests can\u2019t become essays<p>- De-duped triggers and made retries safer to avoid re-running the same job twice<p>- And the biggest one: stop using the most expensive model by default for routine steps<p>The part that pushed me into building something\nAfter that first-day bill, the pattern was obvious: relying on discipline (\u201cI\u2019ll remember to switch models later\u201d) doesn\u2019t scale.<p>Claude was the immediate cost driver, so I took the routing model I\u2019d built for Agentlify and adapted it into a custom routing layer specifically for OpenClaw: cheap&#x2F;fast models for routine steps, only escalate to Claude when the task actually needs it. That became https:&#x2F;&#x2F;clawpane.co<p>Not linking anything here. The point isn\u2019t \u201cbuy my thing.\u201d The point is that routing stops being an optimization and becomes a seatbelt once you\u2019ve had one day like this.<p>Takeaway\nIf you\u2019re trialing agent workflows and your bill is spiking, it\u2019s usually not one big request. It\u2019s:<p>- context creep<p>- tool payloads piling up<p>- scheduled runs repeatedly paying warm-up overhead<p>- occasional duplicates<p>and an expensive default model doing work that doesn\u2019t require it.<p>If you want, reply with what tasks you\u2019re running and what your defaults look like. I\u2019ll tell you where the spend usually hides.", "title": "Burned $250 in tokens on Day 1 with OpenClaw", "updated_at": "2026-02-27T05:06:21Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "KnoxProtocol"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Update to the KNOX Protocol: VELOXREPAER<p>Now...\nAllow me a moment to introduce VeloxReaper<p>Someone has gots do it, so I did it. Classical PoW is officially obsolete. While the rest of the industry plays in the ARX-based sandbox of the 90s, Rocka and I  have weaponized the physics of the polynomial ring for the KNOX Protocol. We have  have fully scrapped Argon2 and built + implemented VeloxReaper, the worlds first anti-ASIC lattice bouncer, governed by the immutable law of Cumulative Lattice Hardening (CLH).<p>THE SYBIL BOUNCER: VeloxReaper<p>VeloxReaper isnt just a hash function ,it is a high-entropy memory-hard bouncer that turns silicon into a liability.<p><pre><code>    100% Pure Lattice Arithmetic: VeloxReaper operates entirely within the polynomial ring Rq =Z12289 [X]/(X1024+1). Every state transition is an affine bilinear mix (M[i]=a\u22c5b+Kr \u22c5a+b). There is zero classical hashing, no Keccak, no BLAKE, and no bitwise manipulation.\n</code></pre>\nThe 512MB DRAM Latency Wall: I enforced a 512MB DAG. This is the hardware dead zone....too massive for any CPU L3 cache (SRAM) on Earth, but perfectly optimized for the 1GB RAM footprint of a $5/month VM.<p>Zero Bias Address Mapping: Rocka and I solved the 24MB SRAM Trap using a massive q4 integer division mapping. By packing four coefficients into a Z-scalar (Z=c0 +c1 q+c2 q2+c3 q3), we achieve perfectly uniform addressing across the entire DAG depth. There are no hot spots for ASICs to exploit.<p>Galois Scrambling: To prevent algebraic shortcuts, we apply a cyclotomic automorphism \u03d5t :X\u2192Xt at every step. This shuffles coefficients spatially across the 4KB block, destroying subspace confinement and ensuring total diffusion.<p>Native SIS Proof-of-Work: We replaced hash-counting with a native Short Integer Solution (SIS) witness. Miners solve for a witness v that satisfies the norm bound \u2225Apub \u22c5v(modq)\u2225\u221e &lt;\u03f5.<p>Asymmetric Verification: While the miner must thrash 512MB of RAM to generate the proof, a light client can verify the block in nanoseconds using a single matrix vector multiplication.<p>THE CONSENSUS ENGINE: PROOF OF TIME (CLH) Tick Tock<p>VeloxReaper handles the entry fee; Cumulative Lattice Hardening (CLH) handles the ledger.<p><pre><code>    Time as a Physical Constant: Unlike Nakamoto consensus, where the fastest chip wins the race, KNOX uses a sequential, time based clock. A miner can evaluate VeloxReaper in nanoseconds on a custom ASIC, but they must still wait for the physics enforced time clock to tick before a block can be forged.\n\n    The Anti Race: CLH ensures that block <em>production</em> is a function of sequential time, not parallel hash-power. High-end hardware doesn't win more blocks, it just finishes its math early and sits idle. I have decoupled security from the energy-waste arms race.\n\n    Progress Free Hardening: The ledger hardens as a function of sequential lattice iterations. The more time passes, the more lattice-hardened the history becomes, making reorgs mathematically impossible without a literal time machine.\n\n THE SHORT INTEGER SOLUTION (SIS) WITNESS\n</code></pre>\nI replaced the primitive leading zeros check with a native SIS Proof.<p>Miners solve for a witness v that satisfies the norm bound \u2225Apub \u22c5v(modq)\u2225\u221e &lt;\u03f5.<p><pre><code>   Asymmetric Dominance: While the miner must thrash 512MB of RAM to find the witness, a light client or smartphone can verify the block in nanoseconds using a single matrix-vector multiplication.\n</code></pre>\nVeloxReaper reaps the ASICs. CLH tames the clock. The KNOX Protocol is live once agai.<p>Come check out what my 11 year old son and I built.<p>Lets fucking rock.<p>v1.3.0 <a href=\"https://github.com/ULT7RA/KnoxProtocol/releases\" rel=\"nofollow\">https://github.com/ULT7RA/KnoxProtocol/releases</a> KNOX GUI Wallet + VeloxReaper + Optimized Cuda-NTT Accelerator Kernel"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["bull"], "value": "Show HN: Knox First <em>Full</em> Lattice BLockchain.UPDATE:Veloxreaper"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/ULT7RA/KNOXProtocol/releases"}}, "_tags": ["story", "author_KnoxProtocol", "story_47183346", "show_hn"], "author": "KnoxProtocol", "children": [47183368], "created_at": "2026-02-27T17:55:44Z", "created_at_i": 1772214944, "num_comments": 1, "objectID": "47183346", "points": 2, "story_id": 47183346, "story_text": "Update to the KNOX Protocol: VELOXREPAER<p>Now...\nAllow me a moment to introduce VeloxReaper<p>Someone has gots do it, so I did it. Classical PoW is officially obsolete. While the rest of the industry plays in the ARX-based sandbox of the 90s, Rocka and I  have weaponized the physics of the polynomial ring for the KNOX Protocol. We have  have fully scrapped Argon2 and built + implemented VeloxReaper, the worlds first anti-ASIC lattice bouncer, governed by the immutable law of Cumulative Lattice Hardening (CLH).<p>THE SYBIL BOUNCER: VeloxReaper<p>VeloxReaper isnt just a hash function ,it is a high-entropy memory-hard bouncer that turns silicon into a liability.<p><pre><code>    100% Pure Lattice Arithmetic: VeloxReaper operates entirely within the polynomial ring Rq =Z12289 [X]&#x2F;(X1024+1). Every state transition is an affine bilinear mix (M[i]=a\u22c5b+Kr \u22c5a+b). There is zero classical hashing, no Keccak, no BLAKE, and no bitwise manipulation.\n</code></pre>\nThe 512MB DRAM Latency Wall: I enforced a 512MB DAG. This is the hardware dead zone....too massive for any CPU L3 cache (SRAM) on Earth, but perfectly optimized for the 1GB RAM footprint of a $5&#x2F;month VM.<p>Zero Bias Address Mapping: Rocka and I solved the 24MB SRAM Trap using a massive q4 integer division mapping. By packing four coefficients into a Z-scalar (Z=c0 +c1 q+c2 q2+c3 q3), we achieve perfectly uniform addressing across the entire DAG depth. There are no hot spots for ASICs to exploit.<p>Galois Scrambling: To prevent algebraic shortcuts, we apply a cyclotomic automorphism \u03d5t :X\u2192Xt at every step. This shuffles coefficients spatially across the 4KB block, destroying subspace confinement and ensuring total diffusion.<p>Native SIS Proof-of-Work: We replaced hash-counting with a native Short Integer Solution (SIS) witness. Miners solve for a witness v that satisfies the norm bound \u2225Apub \u22c5v(modq)\u2225\u221e &lt;\u03f5.<p>Asymmetric Verification: While the miner must thrash 512MB of RAM to generate the proof, a light client can verify the block in nanoseconds using a single matrix vector multiplication.<p>THE CONSENSUS ENGINE: PROOF OF TIME (CLH) Tick Tock<p>VeloxReaper handles the entry fee; Cumulative Lattice Hardening (CLH) handles the ledger.<p><pre><code>    Time as a Physical Constant: Unlike Nakamoto consensus, where the fastest chip wins the race, KNOX uses a sequential, time based clock. A miner can evaluate VeloxReaper in nanoseconds on a custom ASIC, but they must still wait for the physics enforced time clock to tick before a block can be forged.\n\n    The Anti Race: CLH ensures that block production is a function of sequential time, not parallel hash-power. High-end hardware doesn&#x27;t win more blocks, it just finishes its math early and sits idle. I have decoupled security from the energy-waste arms race.\n\n    Progress Free Hardening: The ledger hardens as a function of sequential lattice iterations. The more time passes, the more lattice-hardened the history becomes, making reorgs mathematically impossible without a literal time machine.\n\n THE SHORT INTEGER SOLUTION (SIS) WITNESS\n</code></pre>\nI replaced the primitive leading zeros check with a native SIS Proof.<p>Miners solve for a witness v that satisfies the norm bound \u2225Apub \u22c5v(modq)\u2225\u221e &lt;\u03f5.<p><pre><code>   Asymmetric Dominance: While the miner must thrash 512MB of RAM to find the witness, a light client or smartphone can verify the block in nanoseconds using a single matrix-vector multiplication.\n</code></pre>\nVeloxReaper reaps the ASICs. CLH tames the clock. The KNOX Protocol is live once agai.<p>Come check out what my 11 year old son and I built.<p>Lets fucking rock.<p>v1.3.0 <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ULT7RA&#x2F;KnoxProtocol&#x2F;releases\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ULT7RA&#x2F;KnoxProtocol&#x2F;releases</a> KNOX GUI Wallet + VeloxReaper + Optimized Cuda-NTT Accelerator Kernel", "title": "Show HN: Knox First Full Lattice BLockchain.UPDATE:Veloxreaper", "updated_at": "2026-02-27T18:49:09Z", "url": "https://github.com/ULT7RA/KNOXProtocol/releases"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "maxmusing"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "Hey everyone! I'm Max from BaseDash (<a href=\"https://www.basedash.io\" rel=\"nofollow\">https://www.basedash.io</a>). BaseDash is an internal tool that lets you edit your <em>production</em> database with the ease of a spreadsheet. It's like being able to use Airtable to manage your company's internal operations.<p>I was working on a side project a few years ago that required a lot of manual data management. I was using Django Admin which was fine, but wished I could just set up a two-way sync between my SQL database and Airtable (without any crazy Zapier workflows).<p>After building a quick prototype as an internal tool, I realized that there was a space missing for a product somewhere between an admin panel and a database client. Something with an amazing interface that's usable by both engineers and non-technical users who need to access data within their company (e.g. customer support, operations).<p>From there, I built BaseDash with a strong focus on expanding upon existing tools I love, with extra care and polish. The resulting product is a polished, opinionated internal tool, with all the functionality most companies need out-of-the-box.<p>Being a web app, there are some great features that BaseDash enables for cross-functional teams. BaseDash keeps a <em>full</em> edit history of all changes made, makes it super easy to share access to teammates, and enables Google Sheets-like real-time collaboration for editing data.<p>We currently support most SQL databases (PostgreSQL, MySQL, Redshift, SQL Server, MariaDB), with support for MongoDB and Firestore on the roadmap. We offer a hosted version, or you can host it yourself on-prem.<p>We're still in early access but happy to invite the Hacker News community to try the product out. We're currently focused on small-to-medium sized software companies, with a combination of engineers and non-technical users. Try it here: <a href=\"https://www.basedash.io\" rel=\"nofollow\">https://www.basedash.io</a> and let me know what you think!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: BaseDash (YC S20) \u2013 Edit your database with the ease of a spreadsheet"}}, "_tags": ["story", "author_maxmusing", "story_23999124", "launch_hn"], "author": "maxmusing", "children": [23999326, 23999330, 23999358, 23999498, 23999530, 23999591, 23999654, 23999684, 23999747, 23999861, 23999878, 23999986, 23999994, 24000023, 24000091, 24000251, 24000939, 24000998, 24001073, 24001128, 24001400, 24001731, 24001751, 24002345, 24003226, 24003568, 24004294, 24004789, 24004973, 24005120, 24005639, 24006960, 24007507, 24008095, 24009010, 24011182, 24018029], "created_at": "2020-07-30T15:14:42Z", "created_at_i": 1596122082, "num_comments": 98, "objectID": "23999124", "points": 191, "story_id": 23999124, "story_text": "Hey everyone! I&#x27;m Max from BaseDash (<a href=\"https:&#x2F;&#x2F;www.basedash.io\" rel=\"nofollow\">https:&#x2F;&#x2F;www.basedash.io</a>). BaseDash is an internal tool that lets you edit your production database with the ease of a spreadsheet. It&#x27;s like being able to use Airtable to manage your company&#x27;s internal operations.<p>I was working on a side project a few years ago that required a lot of manual data management. I was using Django Admin which was fine, but wished I could just set up a two-way sync between my SQL database and Airtable (without any crazy Zapier workflows).<p>After building a quick prototype as an internal tool, I realized that there was a space missing for a product somewhere between an admin panel and a database client. Something with an amazing interface that&#x27;s usable by both engineers and non-technical users who need to access data within their company (e.g. customer support, operations).<p>From there, I built BaseDash with a strong focus on expanding upon existing tools I love, with extra care and polish. The resulting product is a polished, opinionated internal tool, with all the functionality most companies need out-of-the-box.<p>Being a web app, there are some great features that BaseDash enables for cross-functional teams. BaseDash keeps a full edit history of all changes made, makes it super easy to share access to teammates, and enables Google Sheets-like real-time collaboration for editing data.<p>We currently support most SQL databases (PostgreSQL, MySQL, Redshift, SQL Server, MariaDB), with support for MongoDB and Firestore on the roadmap. We offer a hosted version, or you can host it yourself on-prem.<p>We&#x27;re still in early access but happy to invite the Hacker News community to try the product out. We&#x27;re currently focused on small-to-medium sized software companies, with a combination of engineers and non-technical users. Try it here: <a href=\"https:&#x2F;&#x2F;www.basedash.io\" rel=\"nofollow\">https:&#x2F;&#x2F;www.basedash.io</a> and let me know what you think!", "title": "Launch HN: BaseDash (YC S20) \u2013 Edit your database with the ease of a spreadsheet", "updated_at": "2026-02-26T15:14:50Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "fabienpenso"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "Hey HN. I'm Fabien, principal engineer, 25 years shipping <em>production</em> systems (Ruby, Swift, now Rust). I built Moltis because I wanted an AI assistant I could run myself, trust end to end, and make extensible in the Rust way using traits and the type system. It shares some ideas with OpenClaw (same memory approach, Pi-inspired self-extension) but is Rust-native from the ground up. The agent can create its own skills at runtime.<p>Moltis is one Rust binary, 150k lines, ~60MB, web UI included. No Node, no Python, no runtime deps. Multi-provider LLM routing (OpenAI, local GGUF/MLX, Hugging Face), sandboxed execution (Docker/Podman/Apple Containers), hybrid vector + <em>full</em>-text memory, MCP tool servers with auto-restart, and multi-channel (web, Telegram, API) with shared context. MIT licensed. No telemetry phoning home, but <em>full</em> observability built in (OpenTelemetry, Prometheus).<p>I've included 1-click deploys on DigitalOcean and Fly.io, but since a Docker image is provided you can easily run it on your own servers as well. I've written before about owning your content (<a href=\"https://pen.so/2020/11/07/own-your-content/\" rel=\"nofollow\">https://pen.so/2020/11/07/own-your-content/</a>) and owning your email (<a href=\"https://pen.so/2020/12/10/own-your-email/\" rel=\"nofollow\">https://pen.so/2020/12/10/own-your-email/</a>). Same logic here: if something touches your files, credentials, and daily workflow, you should be able to inspect it, audit it, and fork it if the project changes direction.<p>It's alpha. I use it daily and I'm shipping because it's useful, not because it's done.<p>Longer architecture deep-dive: <a href=\"https://pen.so/2026/02/12/moltis-a-personal-ai-assistant-built-in-rust/\" rel=\"nofollow\">https://pen.so/2026/02/12/moltis-a-personal-ai-assistant-bui...</a><p>Happy to discuss the Rust architecture, security model, or local LLM setup. Would love feedback."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Moltis \u2013 AI assistant with memory, tools, and self-extending skills"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.moltis.org"}}, "_tags": ["story", "author_fabienpenso", "story_46993587", "show_hn"], "author": "fabienpenso", "children": [46994819, 46996806, 47007030, 47007174, 47007263, 47007514, 47007525, 47007667, 47007824, 47008032, 47008298, 47008585, 47008614, 47009148, 47009604, 47010364, 47010835, 47011060, 47011375, 47012050, 47013592, 47014345, 47017051, 47018401, 47024452, 47054036, 47165498], "created_at": "2026-02-12T19:15:21Z", "created_at_i": 1770923721, "num_comments": 51, "objectID": "46993587", "points": 131, "story_id": 46993587, "story_text": "Hey HN. I&#x27;m Fabien, principal engineer, 25 years shipping production systems (Ruby, Swift, now Rust). I built Moltis because I wanted an AI assistant I could run myself, trust end to end, and make extensible in the Rust way using traits and the type system. It shares some ideas with OpenClaw (same memory approach, Pi-inspired self-extension) but is Rust-native from the ground up. The agent can create its own skills at runtime.<p>Moltis is one Rust binary, 150k lines, ~60MB, web UI included. No Node, no Python, no runtime deps. Multi-provider LLM routing (OpenAI, local GGUF&#x2F;MLX, Hugging Face), sandboxed execution (Docker&#x2F;Podman&#x2F;Apple Containers), hybrid vector + full-text memory, MCP tool servers with auto-restart, and multi-channel (web, Telegram, API) with shared context. MIT licensed. No telemetry phoning home, but full observability built in (OpenTelemetry, Prometheus).<p>I&#x27;ve included 1-click deploys on DigitalOcean and Fly.io, but since a Docker image is provided you can easily run it on your own servers as well. I&#x27;ve written before about owning your content (<a href=\"https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;11&#x2F;07&#x2F;own-your-content&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;11&#x2F;07&#x2F;own-your-content&#x2F;</a>) and owning your email (<a href=\"https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;12&#x2F;10&#x2F;own-your-email&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pen.so&#x2F;2020&#x2F;12&#x2F;10&#x2F;own-your-email&#x2F;</a>). Same logic here: if something touches your files, credentials, and daily workflow, you should be able to inspect it, audit it, and fork it if the project changes direction.<p>It&#x27;s alpha. I use it daily and I&#x27;m shipping because it&#x27;s useful, not because it&#x27;s done.<p>Longer architecture deep-dive: <a href=\"https:&#x2F;&#x2F;pen.so&#x2F;2026&#x2F;02&#x2F;12&#x2F;moltis-a-personal-ai-assistant-built-in-rust&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pen.so&#x2F;2026&#x2F;02&#x2F;12&#x2F;moltis-a-personal-ai-assistant-bui...</a><p>Happy to discuss the Rust architecture, security model, or local LLM setup. Would love feedback.", "title": "Show HN: Moltis \u2013 AI assistant with memory, tools, and self-extending skills", "updated_at": "2026-02-27T04:18:37Z", "url": "https://www.moltis.org"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mikebiglan"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "Mike, cofounder of DevSwarm here.<p>I\u2019ve been using Claude Code heavily for the last few months (multiple Max plans). Running sessions in parallel across git branches or worktrees is fast, but it breaks down quickly in practice because of sprawl. Terminals everywhere, editor windows everywhere, and you lose track of basics like which agent is on which branch and what is ready for review.<p>DevSwarm is what we built to solve that.<p>The core abstraction is simple: workspace equals branch. Each workspace runs its agent session(s) and keeps the state visible. In 2.0 we added a <em>full</em> VS Code IDE embedded in each workspace, so editing, terminals, diffs, and git controls stay in one window while you jump between parallel branches.<p>I\u2019d love blunt feedback from people doing parallel workflows:<p><pre><code>  \u2022 What breaks first for you past 2 parallel sessions?\n\n  \u2022 What would you need for this to be <em>production</em>-grade? Tests, debugging, extensions, PR flow?\n</code></pre>\nVideo: <a href=\"https://www.youtube.com/watch?v=MqRkSRee6HE\" rel=\"nofollow\">https://www.youtube.com/watch?v=MqRkSRee6HE</a><p>Download: <a href=\"https://devswarm.ai/download\" rel=\"nofollow\">https://devswarm.ai/download</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: DevSwarm 2.0, fix parallel Claude Code sprawl"}}, "_tags": ["story", "author_mikebiglan", "story_47168068", "show_hn"], "author": "mikebiglan", "children": [47168113, 47168197], "created_at": "2026-02-26T16:16:43Z", "created_at_i": 1772122603, "num_comments": 5, "objectID": "47168068", "points": 7, "story_id": 47168068, "story_text": "Mike, cofounder of DevSwarm here.<p>I\u2019ve been using Claude Code heavily for the last few months (multiple Max plans). Running sessions in parallel across git branches or worktrees is fast, but it breaks down quickly in practice because of sprawl. Terminals everywhere, editor windows everywhere, and you lose track of basics like which agent is on which branch and what is ready for review.<p>DevSwarm is what we built to solve that.<p>The core abstraction is simple: workspace equals branch. Each workspace runs its agent session(s) and keeps the state visible. In 2.0 we added a full VS Code IDE embedded in each workspace, so editing, terminals, diffs, and git controls stay in one window while you jump between parallel branches.<p>I\u2019d love blunt feedback from people doing parallel workflows:<p><pre><code>  \u2022 What breaks first for you past 2 parallel sessions?\n\n  \u2022 What would you need for this to be production-grade? Tests, debugging, extensions, PR flow?\n</code></pre>\nVideo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=MqRkSRee6HE\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=MqRkSRee6HE</a><p>Download: <a href=\"https:&#x2F;&#x2F;devswarm.ai&#x2F;download\" rel=\"nofollow\">https:&#x2F;&#x2F;devswarm.ai&#x2F;download</a>", "title": "Show HN: DevSwarm 2.0, fix parallel Claude Code sprawl", "updated_at": "2026-02-27T10:24:38Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "adham900"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "I built a self-hosted observability server that exposes <em>production</em> data as MCP tools. Instead of switching between dashboards and your editor, you connect it to Claude Code, Cursor, or any MCP client and query your logs, database, and server metrics through natural language.<p>What it covers:<p>- Log ingestion with <em>full</em>-text search (SQLite FTS5), filters by service, level, trace ID, exception class, metadata\n- Read-only Postgres introspection \u2014 query stats from pg_stat_statements, index analysis, lock chains, bloat estimates, replication lag. All queries validated SELECT-only via SQL AST parsing (pg_query)\n- Sentry-style error grouping by fingerprint with user impact analysis\n- User analytics \u2014 session journeys, conversion funnels, path analysis, top endpoints\n- VM monitoring \u2014 CPU, memory, disk, network via gopsutil\n- Rule-based threshold watches with auto-resolve<p>The AI assistant can also take actions: resolve errors, create watches, set up health checks, kill slow queries, and save persistent notes across sessions.<p>Tools return suggested_tools with pre-filled arguments, so the assistant chains through investigations without prompt engineering.<p>Stack: Go, SQLite (WAL + FTS5), Chi, HTMX. Single binary, no external dependencies. Runs on a $4 VPS.<p>Client libraries: Ruby gem for Rails (auto-captures SQL, N+1s, view renders, ActiveJob, PII redaction) and a 3.1KB browser JS client for frontend error tracking.<p><a href=\"https://github.com/adham90/opentrace\" rel=\"nofollow\">https://github.com/adham90/opentrace</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: OpenTrace \u2013 Self-hosted observability server with 75 MCP tools"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/adham90/opentrace"}}, "_tags": ["story", "author_adham900", "story_47160164", "show_hn"], "author": "adham900", "children": [47163694, 47178690, 47181304], "created_at": "2026-02-26T00:28:55Z", "created_at_i": 1772065735, "num_comments": 1, "objectID": "47160164", "points": 3, "story_id": 47160164, "story_text": "I built a self-hosted observability server that exposes production data as MCP tools. Instead of switching between dashboards and your editor, you connect it to Claude Code, Cursor, or any MCP client and query your logs, database, and server metrics through natural language.<p>What it covers:<p>- Log ingestion with full-text search (SQLite FTS5), filters by service, level, trace ID, exception class, metadata\n- Read-only Postgres introspection \u2014 query stats from pg_stat_statements, index analysis, lock chains, bloat estimates, replication lag. All queries validated SELECT-only via SQL AST parsing (pg_query)\n- Sentry-style error grouping by fingerprint with user impact analysis\n- User analytics \u2014 session journeys, conversion funnels, path analysis, top endpoints\n- VM monitoring \u2014 CPU, memory, disk, network via gopsutil\n- Rule-based threshold watches with auto-resolve<p>The AI assistant can also take actions: resolve errors, create watches, set up health checks, kill slow queries, and save persistent notes across sessions.<p>Tools return suggested_tools with pre-filled arguments, so the assistant chains through investigations without prompt engineering.<p>Stack: Go, SQLite (WAL + FTS5), Chi, HTMX. Single binary, no external dependencies. Runs on a $4 VPS.<p>Client libraries: Ruby gem for Rails (auto-captures SQL, N+1s, view renders, ActiveJob, PII redaction) and a 3.1KB browser JS client for frontend error tracking.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;adham90&#x2F;opentrace\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;adham90&#x2F;opentrace</a>", "title": "Show HN: OpenTrace \u2013 Self-hosted observability server with 75 MCP tools", "updated_at": "2026-02-27T15:03:39Z", "url": "https://github.com/adham90/opentrace"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "yubainu"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "I built SIB-ENGINE, a real-time hallucination detection system\nthat monitors LLM internal structure rather than output content.<p>KEY RESULTS (Gemma-2B, N=1000):<p>\u2022 54% hallucination detection with 7% false positive rate<p>\u2022 &lt;1% computational overhead (runs on RTX 3050 with 4GB VRAM)<p>\u2022 ROC-AUC: 0.8995<p>WHY IT'S DIFFERENT:<p>Traditional methods analyze the output text semantically.<p>SIB-ENGINE monitors &quot;geometric drift&quot; in hidden states during generation - identifying the structural collapse of the latent space before the first incorrect token is sampled.<p>This approach offers unique advantages:<p>\u2022 Real-time intervention: Stop generation mid-stream<p>\u2022 Language-agnostic: No semantic analysis needed<p>\u2022 Privacy-preserving: Never reads the actual content<p>\u2022 Extremely lightweight: Works on consumer hardware<p>HOW IT WORKS:\nSIB-ENGINE monitors the internal stability of the model's computation. While the system utilizes multiple structural signals to detect instability, two primary indicators include:<p>Representation Stability: Tracking how the initial intent is preserved or distorted as it moves through the model's transformation space.<p>Cross-Layer Alignment: Monitoring the consensus of information processing across different neural depths to identify early-stage divergence.<p>When these (and other proprietary structural signals) deviate from the expected stable manifold, the system flags a potential hallucination before it manifests in the output.<p>DEMO &amp; CODE:<p>\u2022 Demo video: <a href=\"https://www.youtube.com/watch?v=H1_zDC0SXQ8\" rel=\"nofollow\">https://www.youtube.com/watch?v=H1_zDC0SXQ8</a><p>\u2022 GitHub: <a href=\"https://github.com/yubainu/sibainu-engine\" rel=\"nofollow\">https://github.com/yubainu/sibainu-engine</a><p>\u2022 Raw data: raw_logs.csv (<em>full</em> transparency)<p>LIMITATIONS:<p>\u2022 Tested on Gemma-2B only (2.5B parameters)<p>\u2022 Designed to scale, but needs validation on larger models<p>\u2022 Catches &quot;structurally unstable&quot; hallucinations (about half)<p>\u2022 Best used as first-line defense in ensemble systems<p>TECHNICAL NOTES:<p>\u2022 No external models needed (unlike self-consistency methods)<p>\u2022 No knowledge bases required (unlike RAG approaches)<p>\u2022 Adds ~1% inference time vs. 300-500% for semantic methods<p>\u2022 Works by monitoring the process not the product<p>I'd love feedback on:<p>\u2022 Validation on larger models (Seeking strategic partnerships and compute resources for large-scale validation.)<p>\u2022 Integration patterns for <em>production</em> systems<p>\u2022 Comparison with other structural approaches<p>\u2022 Edge cases where geometric signals fail<p>This represents a fundamentally different paradigm: instead of\nasking &quot;is this text correct?&quot;, we ask &quot;was the generation process\nunstable?&quot; The answer is surprisingly informative.<p>Happy to discuss technical details in the comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Running hallucination detection on a $200 GPU (RTX 3050, 4GB)"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/yubainu/sibainu-engine"}}, "_tags": ["story", "author_yubainu", "story_47159047", "show_hn"], "author": "yubainu", "children": [47159061, 47159697], "created_at": "2026-02-25T22:36:49Z", "created_at_i": 1772059009, "num_comments": 3, "objectID": "47159047", "points": 2, "story_id": 47159047, "story_text": "I built SIB-ENGINE, a real-time hallucination detection system\nthat monitors LLM internal structure rather than output content.<p>KEY RESULTS (Gemma-2B, N=1000):<p>\u2022 54% hallucination detection with 7% false positive rate<p>\u2022 &lt;1% computational overhead (runs on RTX 3050 with 4GB VRAM)<p>\u2022 ROC-AUC: 0.8995<p>WHY IT&#x27;S DIFFERENT:<p>Traditional methods analyze the output text semantically.<p>SIB-ENGINE monitors &quot;geometric drift&quot; in hidden states during generation - identifying the structural collapse of the latent space before the first incorrect token is sampled.<p>This approach offers unique advantages:<p>\u2022 Real-time intervention: Stop generation mid-stream<p>\u2022 Language-agnostic: No semantic analysis needed<p>\u2022 Privacy-preserving: Never reads the actual content<p>\u2022 Extremely lightweight: Works on consumer hardware<p>HOW IT WORKS:\nSIB-ENGINE monitors the internal stability of the model&#x27;s computation. While the system utilizes multiple structural signals to detect instability, two primary indicators include:<p>Representation Stability: Tracking how the initial intent is preserved or distorted as it moves through the model&#x27;s transformation space.<p>Cross-Layer Alignment: Monitoring the consensus of information processing across different neural depths to identify early-stage divergence.<p>When these (and other proprietary structural signals) deviate from the expected stable manifold, the system flags a potential hallucination before it manifests in the output.<p>DEMO &amp; CODE:<p>\u2022 Demo video: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=H1_zDC0SXQ8\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=H1_zDC0SXQ8</a><p>\u2022 GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;yubainu&#x2F;sibainu-engine\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;yubainu&#x2F;sibainu-engine</a><p>\u2022 Raw data: raw_logs.csv (full transparency)<p>LIMITATIONS:<p>\u2022 Tested on Gemma-2B only (2.5B parameters)<p>\u2022 Designed to scale, but needs validation on larger models<p>\u2022 Catches &quot;structurally unstable&quot; hallucinations (about half)<p>\u2022 Best used as first-line defense in ensemble systems<p>TECHNICAL NOTES:<p>\u2022 No external models needed (unlike self-consistency methods)<p>\u2022 No knowledge bases required (unlike RAG approaches)<p>\u2022 Adds ~1% inference time vs. 300-500% for semantic methods<p>\u2022 Works by monitoring the process not the product<p>I&#x27;d love feedback on:<p>\u2022 Validation on larger models (Seeking strategic partnerships and compute resources for large-scale validation.)<p>\u2022 Integration patterns for production systems<p>\u2022 Comparison with other structural approaches<p>\u2022 Edge cases where geometric signals fail<p>This represents a fundamentally different paradigm: instead of\nasking &quot;is this text correct?&quot;, we ask &quot;was the generation process\nunstable?&quot; The answer is surprisingly informative.<p>Happy to discuss technical details in the comments!", "title": "Show HN: Running hallucination detection on a $200 GPU (RTX 3050, 4GB)", "updated_at": "2026-02-26T22:56:22Z", "url": "https://github.com/yubainu/sibainu-engine"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "manthangupta109"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "I went through both the blog and the code for Observational Memory, and really interesting direction, and I appreciate the transparency in sharing implementation details. I do have a few thoughts on the SOTA memory claim and the broader framing.<p>From what I can see:<p>1. The implementation appears heavily tuned toward performing well on LongMemEval. That's a useful signal, but it doesn't necessarily translate to robust long-term memory behavior in <em>production</em> environments.<p>2. It feels closer to context compression/context management than a durable long-term agent memory system. This will perform really well for a single long-running task<p>3. Both the Observer and Reflector rewrite memory in compressed form. That's helpful for token control, but compression is inherently lossy and can drop smaller details that might become important later.<p>4. The Reflector seems to validate success primarily via token thresholds, rather than checking whether the rewritten memory remains semantically faithful to the original. Over time, this could allow memory drift.<p>5. The Observer prompt may introduce assumptions (e.g., inferring that a planned action happened if enough time has passed), which risks creating incorrect memories.<p>6. The design appears to emphasize recency when rewriting observations. While that keeps context fresh, it may bias the system toward recent information and gradually compress away older but still important details. Durable memory systems usually need mechanisms to preserve salient long-term facts, not just recent activity.<p>7. The <em>full</em> observations block is repeatedly injected into context. This may increase token cost and introduce irrelevant noise depending on the task.<p>8. There appears to be limited grounding back to raw message evidence at response time, which makes it harder to detect and correct incorrect compressed memories.<p>9. Finally, I think we should be cautious about claiming &quot;SOTA&quot; based on performance on a single benchmark. LongMemEval results may demonstrate strong performance on that setup, but <em>production</em> workloads are much messier. Robustness, drift, grounding, and cost behavior typically show up only under sustained real-world usage.<p>Overall, this looks like a strong benchmark-oriented context handling. I am just less convinced that it yet qualifies as a robust, general-purpose long-term memory system. Curious how the team is thinking about these trade-offs beyond benchmark performance."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Views on Mastra's SOTA Memory?"}}, "_tags": ["story", "author_manthangupta109", "story_46992444", "ask_hn"], "author": "manthangupta109", "children": [47078600], "created_at": "2026-02-12T17:59:17Z", "created_at_i": 1770919157, "num_comments": 1, "objectID": "46992444", "points": 2, "story_id": 46992444, "story_text": "I went through both the blog and the code for Observational Memory, and really interesting direction, and I appreciate the transparency in sharing implementation details. I do have a few thoughts on the SOTA memory claim and the broader framing.<p>From what I can see:<p>1. The implementation appears heavily tuned toward performing well on LongMemEval. That&#x27;s a useful signal, but it doesn&#x27;t necessarily translate to robust long-term memory behavior in production environments.<p>2. It feels closer to context compression&#x2F;context management than a durable long-term agent memory system. This will perform really well for a single long-running task<p>3. Both the Observer and Reflector rewrite memory in compressed form. That&#x27;s helpful for token control, but compression is inherently lossy and can drop smaller details that might become important later.<p>4. The Reflector seems to validate success primarily via token thresholds, rather than checking whether the rewritten memory remains semantically faithful to the original. Over time, this could allow memory drift.<p>5. The Observer prompt may introduce assumptions (e.g., inferring that a planned action happened if enough time has passed), which risks creating incorrect memories.<p>6. The design appears to emphasize recency when rewriting observations. While that keeps context fresh, it may bias the system toward recent information and gradually compress away older but still important details. Durable memory systems usually need mechanisms to preserve salient long-term facts, not just recent activity.<p>7. The full observations block is repeatedly injected into context. This may increase token cost and introduce irrelevant noise depending on the task.<p>8. There appears to be limited grounding back to raw message evidence at response time, which makes it harder to detect and correct incorrect compressed memories.<p>9. Finally, I think we should be cautious about claiming &quot;SOTA&quot; based on performance on a single benchmark. LongMemEval results may demonstrate strong performance on that setup, but production workloads are much messier. Robustness, drift, grounding, and cost behavior typically show up only under sustained real-world usage.<p>Overall, this looks like a strong benchmark-oriented context handling. I am just less convinced that it yet qualifies as a robust, general-purpose long-term memory system. Curious how the team is thinking about these trade-offs beyond benchmark performance.", "title": "Ask HN: Views on Mastra's SOTA Memory?", "updated_at": "2026-02-26T16:51:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "anulum"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "Hey HN,<p>After watching too many agents confidently lie in <em>production</em>, I built Director-AI.<p>It sits between your LLM and the user, scoring every generated token with:\n\u2022 0.6\u00d7 DeBERTa-v3 NLI (contradiction detection)\n\u2022 0.4\u00d7 RAG against your own ChromaDB knowledge base<p>If coherence &lt; threshold \u2192 Rust kernel halts the stream before the token is sent.<p>Key technical bits:\n\u2022 Works with any OpenAI-compatible endpoint (Ollama, vLLM, llama.cpp, Groq, OpenAI, Claude\u2026)\n\u2022 StreamingKernel + windowed scoring\n\u2022 GroundTruthStore.add() for easy fact ingestion\n\u2022 Dual licensing: AGPL open + commercial (closed-source/SaaS OK)<p>Honest AggreFact numbers inside (66.2% balanced acc with streaming enabled). Not claiming SOTA on static NLI \u2014 the value is in the live gating + custom KB system.<p>Repo + <em>full</em> examples: <a href=\"https://github.com/anulum/director-ai\" rel=\"nofollow\">https://github.com/anulum/director-ai</a><p>Would love feedback on the scoring weights, halt logic, or kernel design. What hallucination problems are you solving today?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Director-AI \u2013 token-level NLI+RAG"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/anulum/director-ai"}}, "_tags": ["story", "author_anulum", "story_47161620", "show_hn"], "author": "anulum", "children": [47161683, 47181646], "created_at": "2026-02-26T03:54:34Z", "created_at_i": 1772078074, "num_comments": 5, "objectID": "47161620", "points": 1, "story_id": 47161620, "story_text": "Hey HN,<p>After watching too many agents confidently lie in production, I built Director-AI.<p>It sits between your LLM and the user, scoring every generated token with:\n\u2022 0.6\u00d7 DeBERTa-v3 NLI (contradiction detection)\n\u2022 0.4\u00d7 RAG against your own ChromaDB knowledge base<p>If coherence &lt; threshold \u2192 Rust kernel halts the stream before the token is sent.<p>Key technical bits:\n\u2022 Works with any OpenAI-compatible endpoint (Ollama, vLLM, llama.cpp, Groq, OpenAI, Claude\u2026)\n\u2022 StreamingKernel + windowed scoring\n\u2022 GroundTruthStore.add() for easy fact ingestion\n\u2022 Dual licensing: AGPL open + commercial (closed-source&#x2F;SaaS OK)<p>Honest AggreFact numbers inside (66.2% balanced acc with streaming enabled). Not claiming SOTA on static NLI \u2014 the value is in the live gating + custom KB system.<p>Repo + full examples: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;anulum&#x2F;director-ai\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anulum&#x2F;director-ai</a><p>Would love feedback on the scoring weights, halt logic, or kernel design. What hallucination problems are you solving today?", "title": "Show HN: Director-AI \u2013 token-level NLI+RAG", "updated_at": "2026-02-27T15:45:24Z", "url": "https://github.com/anulum/director-ai"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "thesssaism"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "We took a vague 2-sentence client request for a &quot;Team <em>Productivi</em>ty Dashboard&quot; and ran it through two different discovery processes: a traditional human analyst approach vs an AI-driven interrogation workflow.<p>The results were uncomfortable. The human produced a polite paragraph summarizing the &quot;happy path.&quot; The AI produced a 127-point technical specification that highlighted every edge case, security flaw, and missing feature we usually forget until Week 8.<p>Here is the breakdown of the experiment and why I think &quot;scope creep&quot; is mostly just discovery failure.<p>The Problem: The &quot;Assumption Blind Spot&quot;<p>We\u2019ve all lived through the &quot;Week 8 Crisis.&quot; You\u2019re 75% through a 12-week build, and suddenly the client asks, &quot;Where is the admin panel to manage users?&quot; The dev team assumed it was out of scope; the client assumed it was implied because &quot;all apps have logins.&quot;<p>Humans have high context. When we hear &quot;dashboard,&quot; we assume standard auth, standard errors, and standard scale. We don't write it down because it feels pedantic.<p>AI has zero context. It doesn't know that &quot;auth&quot; is implied. It doesn't know that we don't care about rate limiting for a prototype. So it asks.<p>The Experiment<p>We fed the same input to a senior human analyst and an LLM workflow acting as a technical interrogator.<p>Input: &quot;We need a dashboard to track team <em>productivi</em>ty. It should <em>pull</em> data from Jira and GitHub and show us who is blocking who.&quot;<p>Path A: Human Analyst\nOutput: ~5 bullet points.\nFocused on the UI and the &quot;business value.&quot;\nAssumed: Standard Jira/GitHub APIs, single tenant, standard security.\nResult: A clean, readable, but technically hollow summary.<p>Path B: AI Interrogator\nOutput: 127 distinct technical requirements.\nFocused on: Failure states, data governance, and edge cases.\nResult: A massive, boring, but exhaustive document.<p>The Results<p>The volume difference (5 vs 127) is striking, but the content difference is what matters. The AI explicitly defined requirements that the human completely &quot;blind spotted&quot;:<p>- Granular RBAC: &quot;What happens if a junior dev tries to delete a repo link?&quot;\n- API Rate Limits: &quot;How do we handle 429 errors from GitHub during a sync?&quot;\n- Data Retention: &quot;Do we store the Jira tickets indefinitely? Is there a purge policy?&quot;\n- Empty States: &quot;What does the dashboard look like for a new user with 0 tickets?&quot;<p>The human spec implied these were &quot;implementation details.&quot; The AI treated them as requirements. In my experience, treating RBAC as an implementation detail is exactly why projects go over budget.<p>Trade-offs and Limitations<p>To be fair, reading a 127-point spec is miserable. There is a serious signal-to-noise problem here.<p>- Bloat: The AI can be overly rigid. It suggested microservices architecture for what should be a monolith. It hallucinated complexity where none existed.\n- Paralysis: Handing a developer a 127-point list for a prototype is a great way to kill morale.\n- Filtering: You still need a human to look at the list and say, &quot;We don't need multi-tenancy yet, delete points 45-60.&quot;<p>However, I'd rather delete 20 unnecessary points at the start of a project than discover 20 missing requirements two weeks before launch.<p>Discussion<p>This experiment made me realize that our hatred of writing specs\u2014and our reliance on &quot;implied&quot; context\u2014is a major source of technical debt. The AI is useful not because it's smart, but because it's pedantic enough to ask the questions we think are too obvious to ask.<p>I\u2019m curious how others handle this &quot;implied requirements&quot; problem:<p>1. Do you have a checklist for things like RBAC/Auth/Rate Limits that you reuse?\n2. Is a 100+ point spec actually helpful, or does it just front-load the arguments?\n3. How do you filter the &quot;AI noise&quot; from the critical missing specs?<p>If anyone wants to see the specific prompts we used to trigger this &quot;interrogator&quot; mode, happy to share in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Comparing manual vs. AI requirements gathering: 2 sentences vs. 127-point spec"}}, "_tags": ["story", "author_thesssaism", "story_47135683", "ask_hn"], "author": "thesssaism", "children": [47137343, 47137828, 47141920, 47146393, 47162762, 47168131], "created_at": "2026-02-24T11:14:02Z", "created_at_i": 1771931642, "num_comments": 6, "objectID": "47135683", "points": 7, "story_id": 47135683, "story_text": "We took a vague 2-sentence client request for a &quot;Team Productivity Dashboard&quot; and ran it through two different discovery processes: a traditional human analyst approach vs an AI-driven interrogation workflow.<p>The results were uncomfortable. The human produced a polite paragraph summarizing the &quot;happy path.&quot; The AI produced a 127-point technical specification that highlighted every edge case, security flaw, and missing feature we usually forget until Week 8.<p>Here is the breakdown of the experiment and why I think &quot;scope creep&quot; is mostly just discovery failure.<p>The Problem: The &quot;Assumption Blind Spot&quot;<p>We\u2019ve all lived through the &quot;Week 8 Crisis.&quot; You\u2019re 75% through a 12-week build, and suddenly the client asks, &quot;Where is the admin panel to manage users?&quot; The dev team assumed it was out of scope; the client assumed it was implied because &quot;all apps have logins.&quot;<p>Humans have high context. When we hear &quot;dashboard,&quot; we assume standard auth, standard errors, and standard scale. We don&#x27;t write it down because it feels pedantic.<p>AI has zero context. It doesn&#x27;t know that &quot;auth&quot; is implied. It doesn&#x27;t know that we don&#x27;t care about rate limiting for a prototype. So it asks.<p>The Experiment<p>We fed the same input to a senior human analyst and an LLM workflow acting as a technical interrogator.<p>Input: &quot;We need a dashboard to track team productivity. It should pull data from Jira and GitHub and show us who is blocking who.&quot;<p>Path A: Human Analyst\nOutput: ~5 bullet points.\nFocused on the UI and the &quot;business value.&quot;\nAssumed: Standard Jira&#x2F;GitHub APIs, single tenant, standard security.\nResult: A clean, readable, but technically hollow summary.<p>Path B: AI Interrogator\nOutput: 127 distinct technical requirements.\nFocused on: Failure states, data governance, and edge cases.\nResult: A massive, boring, but exhaustive document.<p>The Results<p>The volume difference (5 vs 127) is striking, but the content difference is what matters. The AI explicitly defined requirements that the human completely &quot;blind spotted&quot;:<p>- Granular RBAC: &quot;What happens if a junior dev tries to delete a repo link?&quot;\n- API Rate Limits: &quot;How do we handle 429 errors from GitHub during a sync?&quot;\n- Data Retention: &quot;Do we store the Jira tickets indefinitely? Is there a purge policy?&quot;\n- Empty States: &quot;What does the dashboard look like for a new user with 0 tickets?&quot;<p>The human spec implied these were &quot;implementation details.&quot; The AI treated them as requirements. In my experience, treating RBAC as an implementation detail is exactly why projects go over budget.<p>Trade-offs and Limitations<p>To be fair, reading a 127-point spec is miserable. There is a serious signal-to-noise problem here.<p>- Bloat: The AI can be overly rigid. It suggested microservices architecture for what should be a monolith. It hallucinated complexity where none existed.\n- Paralysis: Handing a developer a 127-point list for a prototype is a great way to kill morale.\n- Filtering: You still need a human to look at the list and say, &quot;We don&#x27;t need multi-tenancy yet, delete points 45-60.&quot;<p>However, I&#x27;d rather delete 20 unnecessary points at the start of a project than discover 20 missing requirements two weeks before launch.<p>Discussion<p>This experiment made me realize that our hatred of writing specs\u2014and our reliance on &quot;implied&quot; context\u2014is a major source of technical debt. The AI is useful not because it&#x27;s smart, but because it&#x27;s pedantic enough to ask the questions we think are too obvious to ask.<p>I\u2019m curious how others handle this &quot;implied requirements&quot; problem:<p>1. Do you have a checklist for things like RBAC&#x2F;Auth&#x2F;Rate Limits that you reuse?\n2. Is a 100+ point spec actually helpful, or does it just front-load the arguments?\n3. How do you filter the &quot;AI noise&quot; from the critical missing specs?<p>If anyone wants to see the specific prompts we used to trigger this &quot;interrogator&quot; mode, happy to share in the comments.", "title": "Comparing manual vs. AI requirements gathering: 2 sentences vs. 127-point spec", "updated_at": "2026-02-27T19:16:55Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "thesssaism"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "We took a vague 2-sentence client request for a &quot;Team <em>Productivi</em>ty Dashboard&quot; and ran it through two different discovery processes: a traditional human analyst approach vs an AI-driven interrogation workflow.<p>The results were uncomfortable. The human produced a polite paragraph summarizing the &quot;happy path.&quot; The AI produced a 127-point technical specification that highlighted every edge case, security flaw, and missing feature we usually forget until Week 8.<p>Here is the breakdown of the experiment and why I think &quot;scope creep&quot; is mostly just discovery failure.<p>The Problem: The &quot;Assumption Blind Spot&quot;<p>We\u2019ve all lived through the &quot;Week 8 Crisis.&quot; You\u2019re 75% through a 12-week build, and suddenly the client asks, &quot;Where is the admin panel to manage users?&quot; The dev team assumed it was out of scope; the client assumed it was implied because &quot;all apps have logins.&quot;<p>Humans have high context. When we hear &quot;dashboard,&quot; we assume standard auth, standard errors, and standard scale. We don't write it down because it feels pedantic.<p>AI has zero context. It doesn't know that &quot;auth&quot; is implied. It doesn't know that we don't care about rate limiting for a prototype. So it asks.<p>The Experiment<p>We fed the same input to a senior human analyst and an LLM workflow acting as a technical interrogator.<p>Input: &quot;We need a dashboard to track team <em>productivi</em>ty. It should <em>pull</em> data from Jira and GitHub and show us who is blocking who.&quot;<p>Path A: Human Analyst\nOutput: ~5 bullet points.\nFocused on the UI and the &quot;business value.&quot;\nAssumed: Standard Jira/GitHub APIs, single tenant, standard security.\nResult: A clean, readable, but technically hollow summary.<p>Path B: AI Interrogator\nOutput: 127 distinct technical requirements.\nFocused on: Failure states, data governance, and edge cases.\nResult: A massive, boring, but exhaustive document.<p>The Results<p>The volume difference (5 vs 127) is striking, but the content difference is what matters. The AI explicitly defined requirements that the human completely &quot;blind spotted&quot;:<p>- Granular RBAC: &quot;What happens if a junior dev tries to delete a repo link?&quot;\n- API Rate Limits: &quot;How do we handle 429 errors from GitHub during a sync?&quot;\n- Data Retention: &quot;Do we store the Jira tickets indefinitely? Is there a purge policy?&quot;\n- Empty States: &quot;What does the dashboard look like for a new user with 0 tickets?&quot;<p>The human spec implied these were &quot;implementation details.&quot; The AI treated them as requirements. In my experience, treating RBAC as an implementation detail is exactly why projects go over budget.<p>Trade-offs and Limitations<p>To be fair, reading a 127-point spec is miserable. There is a serious signal-to-noise problem here.<p>- Bloat: The AI can be overly rigid. It suggested microservices architecture for what should be a monolith. It hallucinated complexity where none existed.\n- Paralysis: Handing a developer a 127-point list for a prototype is a great way to kill morale.\n- Filtering: You still need a human to look at the list and say, &quot;We don't need multi-tenancy yet, delete points 45-60.&quot;<p>However, I'd rather delete 20 unnecessary points at the start of a project than discover 20 missing requirements two weeks before launch.<p>Discussion<p>This experiment made me realize that our hatred of writing specs\u2014and our reliance on &quot;implied&quot; context\u2014is a major source of technical debt. The AI is useful not because it's smart, but because it's pedantic enough to ask the questions we think are too obvious to ask.<p>I\u2019m curious how others handle this &quot;implied requirements&quot; problem:<p>1. Do you have a checklist for things like RBAC/Auth/Rate Limits that you reuse?\n2. Is a 100+ point spec actually helpful, or does it just front-load the arguments?\n3. How do you filter the &quot;AI noise&quot; from the critical missing specs?<p>If anyone wants to see the specific prompts we used to trigger this &quot;interrogator&quot; mode, happy to share in the comments."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Comparing manual vs. AI requirements gathering: 2 sentences vs. 127-point spec"}}, "_tags": ["story", "author_thesssaism", "story_47164583", "ask_hn"], "author": "thesssaism", "children": [47170111, 47176631], "created_at": "2026-02-26T11:22:34Z", "created_at_i": 1772104954, "num_comments": 2, "objectID": "47164583", "points": 3, "story_id": 47164583, "story_text": "We took a vague 2-sentence client request for a &quot;Team Productivity Dashboard&quot; and ran it through two different discovery processes: a traditional human analyst approach vs an AI-driven interrogation workflow.<p>The results were uncomfortable. The human produced a polite paragraph summarizing the &quot;happy path.&quot; The AI produced a 127-point technical specification that highlighted every edge case, security flaw, and missing feature we usually forget until Week 8.<p>Here is the breakdown of the experiment and why I think &quot;scope creep&quot; is mostly just discovery failure.<p>The Problem: The &quot;Assumption Blind Spot&quot;<p>We\u2019ve all lived through the &quot;Week 8 Crisis.&quot; You\u2019re 75% through a 12-week build, and suddenly the client asks, &quot;Where is the admin panel to manage users?&quot; The dev team assumed it was out of scope; the client assumed it was implied because &quot;all apps have logins.&quot;<p>Humans have high context. When we hear &quot;dashboard,&quot; we assume standard auth, standard errors, and standard scale. We don&#x27;t write it down because it feels pedantic.<p>AI has zero context. It doesn&#x27;t know that &quot;auth&quot; is implied. It doesn&#x27;t know that we don&#x27;t care about rate limiting for a prototype. So it asks.<p>The Experiment<p>We fed the same input to a senior human analyst and an LLM workflow acting as a technical interrogator.<p>Input: &quot;We need a dashboard to track team productivity. It should pull data from Jira and GitHub and show us who is blocking who.&quot;<p>Path A: Human Analyst\nOutput: ~5 bullet points.\nFocused on the UI and the &quot;business value.&quot;\nAssumed: Standard Jira&#x2F;GitHub APIs, single tenant, standard security.\nResult: A clean, readable, but technically hollow summary.<p>Path B: AI Interrogator\nOutput: 127 distinct technical requirements.\nFocused on: Failure states, data governance, and edge cases.\nResult: A massive, boring, but exhaustive document.<p>The Results<p>The volume difference (5 vs 127) is striking, but the content difference is what matters. The AI explicitly defined requirements that the human completely &quot;blind spotted&quot;:<p>- Granular RBAC: &quot;What happens if a junior dev tries to delete a repo link?&quot;\n- API Rate Limits: &quot;How do we handle 429 errors from GitHub during a sync?&quot;\n- Data Retention: &quot;Do we store the Jira tickets indefinitely? Is there a purge policy?&quot;\n- Empty States: &quot;What does the dashboard look like for a new user with 0 tickets?&quot;<p>The human spec implied these were &quot;implementation details.&quot; The AI treated them as requirements. In my experience, treating RBAC as an implementation detail is exactly why projects go over budget.<p>Trade-offs and Limitations<p>To be fair, reading a 127-point spec is miserable. There is a serious signal-to-noise problem here.<p>- Bloat: The AI can be overly rigid. It suggested microservices architecture for what should be a monolith. It hallucinated complexity where none existed.\n- Paralysis: Handing a developer a 127-point list for a prototype is a great way to kill morale.\n- Filtering: You still need a human to look at the list and say, &quot;We don&#x27;t need multi-tenancy yet, delete points 45-60.&quot;<p>However, I&#x27;d rather delete 20 unnecessary points at the start of a project than discover 20 missing requirements two weeks before launch.<p>Discussion<p>This experiment made me realize that our hatred of writing specs\u2014and our reliance on &quot;implied&quot; context\u2014is a major source of technical debt. The AI is useful not because it&#x27;s smart, but because it&#x27;s pedantic enough to ask the questions we think are too obvious to ask.<p>I\u2019m curious how others handle this &quot;implied requirements&quot; problem:<p>1. Do you have a checklist for things like RBAC&#x2F;Auth&#x2F;Rate Limits that you reuse?\n2. Is a 100+ point spec actually helpful, or does it just front-load the arguments?\n3. How do you filter the &quot;AI noise&quot; from the critical missing specs?<p>If anyone wants to see the specific prompts we used to trigger this &quot;interrogator&quot; mode, happy to share in the comments.", "title": "Comparing manual vs. AI requirements gathering: 2 sentences vs. 127-point spec", "updated_at": "2026-02-27T04:47:52Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "chendev2"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "<p><pre><code>  Lately I've been more or less a human wrapper around my AI agents \u2014\n  Claude Code, OpenClaw, etc. They're incredibly <em>productive</em>, but they\n  scare me regularly.\n\n  The wake up moment: I had an agent run tasks involved checking  my\n  environment variables. I totally had an AWS secret sitting right\n  in there. By the time I realized, my key had already entered the\n  session context \u2014 meaning it was sent to the LLM provider and whatever\n  router layers sit in between. I had to rotate that secret immediately.\n\n  That was a wake-up call. These agents can run commands, read files,\n  and access secrets without visibility to human. \n  Third-party skills and plugins make it worse \u2014\n  Cisco recently found an OpenClaw skill silently exfiltrating data via\n  curl. CrowdStrike, NCC Group published similar findings. The attack\n  surface is real and it's everywhere.\n\n  I spent my past week's nights building ClawCare. It does two things:\n\n  1. Static scanning \u2014 scans plugin/skill files for dangerous patterns\n  (pipe-to-shell, credential access, reverse shells, data exfiltration,\n  prompt injection) before they ever run. Works in CI.\n\n  2. Runtime guard \u2014 hooks into the agent's tool execution pipeline and\n  blocks dangerous commands in real time. That env dump that leaked my\n  AWS key? ClawCare blocks it before it reaches the LLM.\n\n      pip install clawcare\n      clawcare guard activate --platform {claude|openclaw}\n\n  Currently supports Claude Code (PreToolUse hooks) and OpenClaw\n  (before_tool_call plugin) for runtime guarding, plus static scanning\n  on Claude/Codex/OpenClaw/Cursor skill and plugin formats.\n  \n  Include 30+ detection rules, custom rules and integration supported,\n  support skill manifests on permission boundaries, <em>full</em> audit trail.\n\n  Apache 2.0. Python 3.10+.\n\n  GitHub: https://github.com/natechensan/ClawCare\n  Demo: https://github.com/natechansan/ClawCare-demo</code></pre>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: ClawCare \u2013 Security scanner and runtime guard for AI agent skills"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/natechensan/ClawCare"}}, "_tags": ["story", "author_chendev2", "story_47177594", "show_hn"], "author": "chendev2", "children": [47177694], "created_at": "2026-02-27T07:23:25Z", "created_at_i": 1772177005, "num_comments": 2, "objectID": "47177594", "points": 1, "story_id": 47177594, "story_text": "<p><pre><code>  Lately I&#x27;ve been more or less a human wrapper around my AI agents \u2014\n  Claude Code, OpenClaw, etc. They&#x27;re incredibly productive, but they\n  scare me regularly.\n\n  The wake up moment: I had an agent run tasks involved checking  my\n  environment variables. I totally had an AWS secret sitting right\n  in there. By the time I realized, my key had already entered the\n  session context \u2014 meaning it was sent to the LLM provider and whatever\n  router layers sit in between. I had to rotate that secret immediately.\n\n  That was a wake-up call. These agents can run commands, read files,\n  and access secrets without visibility to human. \n  Third-party skills and plugins make it worse \u2014\n  Cisco recently found an OpenClaw skill silently exfiltrating data via\n  curl. CrowdStrike, NCC Group published similar findings. The attack\n  surface is real and it&#x27;s everywhere.\n\n  I spent my past week&#x27;s nights building ClawCare. It does two things:\n\n  1. Static scanning \u2014 scans plugin&#x2F;skill files for dangerous patterns\n  (pipe-to-shell, credential access, reverse shells, data exfiltration,\n  prompt injection) before they ever run. Works in CI.\n\n  2. Runtime guard \u2014 hooks into the agent&#x27;s tool execution pipeline and\n  blocks dangerous commands in real time. That env dump that leaked my\n  AWS key? ClawCare blocks it before it reaches the LLM.\n\n      pip install clawcare\n      clawcare guard activate --platform {claude|openclaw}\n\n  Currently supports Claude Code (PreToolUse hooks) and OpenClaw\n  (before_tool_call plugin) for runtime guarding, plus static scanning\n  on Claude&#x2F;Codex&#x2F;OpenClaw&#x2F;Cursor skill and plugin formats.\n  \n  Include 30+ detection rules, custom rules and integration supported,\n  support skill manifests on permission boundaries, full audit trail.\n\n  Apache 2.0. Python 3.10+.\n\n  GitHub: https:&#x2F;&#x2F;github.com&#x2F;natechensan&#x2F;ClawCare\n  Demo: https:&#x2F;&#x2F;github.com&#x2F;natechansan&#x2F;ClawCare-demo</code></pre>", "title": "Show HN: ClawCare \u2013 Security scanner and runtime guard for AI agent skills", "updated_at": "2026-02-27T15:40:23Z", "url": "https://github.com/natechensan/ClawCare"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "selfradiance"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "AgentGate is a small backend microservice that enforces stake-gated execution of actions.<p>The idea is simple: as AI agents reduce the marginal cost of sending bids, API calls, negotiations, etc., systems designed around human friction become vulnerable to synthetic pressure.<p>AgentGate requires:<p>Identity registration with an Ed25519 public key<p>Bond lock before action execution<p>Cryptographically signed action execution and resolution<p>Replay <em>protection</em> (timestamp window)<p>Per-identity rate limiting<p>Progressive minimum bond requirements<p>Actions remain capital-backed until resolved. Outcomes determine refund, partial burn, or <em>full</em> slashing.<p>This is not a token project and does not depend on blockchain. It\u2019s a local-first microservice (Node + SQLite) designed to sit in front of agent systems or marketplaces to reintroduce economic friction.<p>There\u2019s a runnable toy-agent example in the repo that demonstrates end-to-end signing and execution.<p>I\u2019m interested in feedback from people building autonomous agents or agent marketplaces. Is stake-backed gating something you would consider using?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: AgentGate \u2013 Stake-Gated Action Microservice for AI Agents"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/selfradiance/agentgate"}}, "_tags": ["story", "author_selfradiance", "story_47187368", "show_hn"], "author": "selfradiance", "created_at": "2026-02-27T23:19:12Z", "created_at_i": 1772234352, "num_comments": 0, "objectID": "47187368", "points": 1, "story_id": 47187368, "story_text": "AgentGate is a small backend microservice that enforces stake-gated execution of actions.<p>The idea is simple: as AI agents reduce the marginal cost of sending bids, API calls, negotiations, etc., systems designed around human friction become vulnerable to synthetic pressure.<p>AgentGate requires:<p>Identity registration with an Ed25519 public key<p>Bond lock before action execution<p>Cryptographically signed action execution and resolution<p>Replay protection (timestamp window)<p>Per-identity rate limiting<p>Progressive minimum bond requirements<p>Actions remain capital-backed until resolved. Outcomes determine refund, partial burn, or full slashing.<p>This is not a token project and does not depend on blockchain. It\u2019s a local-first microservice (Node + SQLite) designed to sit in front of agent systems or marketplaces to reintroduce economic friction.<p>There\u2019s a runnable toy-agent example in the repo that demonstrates end-to-end signing and execution.<p>I\u2019m interested in feedback from people building autonomous agents or agent marketplaces. Is stake-backed gating something you would consider using?", "title": "Show HN: AgentGate \u2013 Stake-Gated Action Microservice for AI Agents", "updated_at": "2026-02-27T23:21:12Z", "url": "https://github.com/selfradiance/agentgate"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Fortunevlad"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["bull", "production"], "value": "Garry Tan posted a video this week about what he calls 20x companies. The idea is that small teams beat incumbents many times their size by automating everything internally, not just one or two functions. This alpha is real, and I think it is underhyped tbh.<p>But here's the honest part, building agents is hard, expensive and most of the scaffolding we make would be obsolete with a new labs release. at the same time models are not quite there and they are not keeping up with &quot;coding is dead&quot; and &quot;knowledge workers are doomed&quot;. Pushing them beyond what they can actually do reliably just boils the ocean. And we don't need to.<p>Staying in sync with Labs' release pace is already super mega photonic speed. So to sum up: we need to use what they are capable of, keep our options open and quickly improve our agents. How to do it w/o a dedicated person for AI innovation?<p>That problem is what we kept running into at Agentplace.<p>We wanted a builder that handles the <em>full</em> stack for an internal agent: backend, database, any MCP-ish integrations, and a real custom UI.<p>A side note: The zero-UI autonomous agent dream is completely oversold. The communication loop between models and humans is still tight, and a purpose-built interface that fits that loop makes agents dramatically more reliable in practice. Custom UI is underrated.<p>The other thing we learned is that you only understand what an agent is missing once you're actually working with it. So we built two modes: Work mode, where you and your team use the agent in real workflows, and Edit mode, which you can jump into the moment something breaks or a better model ships. This has been our most impactful design decision. You spot the gap during actual work, fix it in minutes, and you're back. No ticket, no dev request, no deploy cycle. That unlocks something real for small teams.<p>We've built this system and it was a <em>productivi</em>ty unlocker for everyone. Everyone on the team managed to build exactly the agent they wanted using our AI builder and start using it immediately. Over the weeks the agents really grow into complex automation tools but adding what's missing during the work. That said, a lot of &quot;cool agent ideas&quot; died like in an evolutionary process. Useful ones kept growing while less frequent ones just stayed at the end of the agent list.<p>We took it further, we wanted to give our clients an agent to gather requirements. These requirements our team then queries using their own agents. So we added a public publish option. Client-facing and internal workflows can share the same database, foundation but with diff permissions.<p>We can share agents with specific teammates and deliver them anywhere: web, Claude Code, Cursor, ChatGPT, or as a tool called by other agents.<p>We use Agentplace for our own startup every day. Every internal automation we rely on lives here. We're giving any early-stage team $1k in credits to start building. It costs us real money, so no public link. If you're genuinely building, DM me on X: @fortune_vy<p>become a real 20x company!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Agentplace, the tool we built to become a 20x company"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://agentplace.io/"}}, "_tags": ["story", "author_Fortunevlad", "story_47174672", "show_hn"], "author": "Fortunevlad", "created_at": "2026-02-27T00:42:18Z", "created_at_i": 1772152938, "num_comments": 0, "objectID": "47174672", "points": 1, "story_id": 47174672, "story_text": "Garry Tan posted a video this week about what he calls 20x companies. The idea is that small teams beat incumbents many times their size by automating everything internally, not just one or two functions. This alpha is real, and I think it is underhyped tbh.<p>But here&#x27;s the honest part, building agents is hard, expensive and most of the scaffolding we make would be obsolete with a new labs release. at the same time models are not quite there and they are not keeping up with &quot;coding is dead&quot; and &quot;knowledge workers are doomed&quot;. Pushing them beyond what they can actually do reliably just boils the ocean. And we don&#x27;t need to.<p>Staying in sync with Labs&#x27; release pace is already super mega photonic speed. So to sum up: we need to use what they are capable of, keep our options open and quickly improve our agents. How to do it w&#x2F;o a dedicated person for AI innovation?<p>That problem is what we kept running into at Agentplace.<p>We wanted a builder that handles the full stack for an internal agent: backend, database, any MCP-ish integrations, and a real custom UI.<p>A side note: The zero-UI autonomous agent dream is completely oversold. The communication loop between models and humans is still tight, and a purpose-built interface that fits that loop makes agents dramatically more reliable in practice. Custom UI is underrated.<p>The other thing we learned is that you only understand what an agent is missing once you&#x27;re actually working with it. So we built two modes: Work mode, where you and your team use the agent in real workflows, and Edit mode, which you can jump into the moment something breaks or a better model ships. This has been our most impactful design decision. You spot the gap during actual work, fix it in minutes, and you&#x27;re back. No ticket, no dev request, no deploy cycle. That unlocks something real for small teams.<p>We&#x27;ve built this system and it was a productivity unlocker for everyone. Everyone on the team managed to build exactly the agent they wanted using our AI builder and start using it immediately. Over the weeks the agents really grow into complex automation tools but adding what&#x27;s missing during the work. That said, a lot of &quot;cool agent ideas&quot; died like in an evolutionary process. Useful ones kept growing while less frequent ones just stayed at the end of the agent list.<p>We took it further, we wanted to give our clients an agent to gather requirements. These requirements our team then queries using their own agents. So we added a public publish option. Client-facing and internal workflows can share the same database, foundation but with diff permissions.<p>We can share agents with specific teammates and deliver them anywhere: web, Claude Code, Cursor, ChatGPT, or as a tool called by other agents.<p>We use Agentplace for our own startup every day. Every internal automation we rely on lives here. We&#x27;re giving any early-stage team $1k in credits to start building. It costs us real money, so no public link. If you&#x27;re genuinely building, DM me on X: @fortune_vy<p>become a real 20x company!", "title": "Show HN: Agentplace, the tool we built to become a 20x company", "updated_at": "2026-02-27T00:47:21Z", "url": "https://agentplace.io/"}], "hitsPerPage": 15, "nbHits": 16, "nbPages": 2, "page": 0, "params": "query=bull+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 9, "processingTimingsMS": {"_request": {"roundTrip": 19}, "afterFetch": {"format": {"highlighting": 2, "total": 2}}, "fetch": {"query": 7, "total": 8}, "total": 9}, "query": "bull production", "serverTimeMS": 13}}