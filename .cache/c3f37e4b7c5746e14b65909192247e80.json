{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "larrykubin"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["fastapi"], "value": "I'm currently evaluating Python frameworks for a new project. I've built a number of applications with Flask / Django over the years, but also recently discovered <em>FastAPI</em>, which is built on Starlette. I tried it out for a small hobby app locally and enjoyed the experience so far. I liked how you could define types with Pydantic, the dependency injection, and how it seemed a little easier to bootstrap and form a project structure  vs. flask. The auto API docs are great as well. Also the <em>FastAPI</em> documentation itself seemed very thorough and had great examples for authentication, docker images, boilerplates, etc. The project and docs feel mature.<p>That said, I don't see as much written about <em>FastAPI</em> and Starlette. I want to recommend it for a major project. Is there a large community around Starlette/<em>FastAPI</em>? Have you used it for large mission critical projects with many users? Would love to hear more about real world usage."}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Ask HN: What are your experiences with using Starlette or <em>FastAPI</em> in <em>production</em>?"}}, "_tags": ["story", "author_larrykubin", "story_22776339", "ask_hn"], "author": "larrykubin", "children": [22778862, 22779231, 22779949], "created_at": "2020-04-04T05:15:34Z", "created_at_i": 1585977334, "num_comments": 11, "objectID": "22776339", "points": 19, "story_id": 22776339, "story_text": "I&#x27;m currently evaluating Python frameworks for a new project. I&#x27;ve built a number of applications with Flask &#x2F; Django over the years, but also recently discovered FastAPI, which is built on Starlette. I tried it out for a small hobby app locally and enjoyed the experience so far. I liked how you could define types with Pydantic, the dependency injection, and how it seemed a little easier to bootstrap and form a project structure  vs. flask. The auto API docs are great as well. Also the FastAPI documentation itself seemed very thorough and had great examples for authentication, docker images, boilerplates, etc. The project and docs feel mature.<p>That said, I don&#x27;t see as much written about FastAPI and Starlette. I want to recommend it for a major project. Is there a large community around Starlette&#x2F;FastAPI? Have you used it for large mission critical projects with many users? Would love to hear more about real world usage.", "title": "Ask HN: What are your experiences with using Starlette or FastAPI in production?", "updated_at": "2026-01-25T12:39:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "surferdude"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Show HN: Generate customizable <em>production</em> grade <em>FastAPI</em> projects"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["fastapi"], "value": "https://github.com/rszamszur/<em>fastapi</em>-mvc"}}, "_tags": ["story", "author_surferdude", "story_30165261", "show_hn"], "author": "surferdude", "children": [30165284], "created_at": "2022-02-01T16:47:43Z", "created_at_i": 1643734063, "num_comments": 1, "objectID": "30165261", "points": 4, "story_id": 30165261, "title": "Show HN: Generate customizable production grade FastAPI projects", "updated_at": "2024-09-20T10:26:36Z", "url": "https://github.com/rszamszur/fastapi-mvc"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "niklasdev"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Hello HN<p>After building numerous <em>FastAPI</em> backends, I consistently found myself repeating the same configuration, including auth, email, payments, migrations, etc.<p>In order to handle all the boilerplate, I developed FastLaunchAPI, a <em>production</em>-ready startup package.<p>Included are:<p><pre><code>    JWT authentication with social login and email\n\n    Webhooks and Stripe billing together\n\n    Alembic, PostgreSQL, and SQLAlchemy\n\n    Background work for celebrities\n\n    SMTP email that includes templates\n\n    Integration of LangChain and OpenAI\n\n    Setup for Pytest + API documents\n</code></pre>\nIt is designed to bring you to <em>production</em> in less than half an hour and is modular.<p>There is a comprehensive documentation, which can be viewed before pruchase.<p>I would be happy to hear any comments or inquiries!"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Show HN: FastLaunchAPI \u2013 A <em>production</em>-ready <em>FastAPI</em> template batteries included"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://fastlaunchapi.dev/"}}, "_tags": ["story", "author_niklasdev", "story_44716785", "show_hn"], "author": "niklasdev", "children": [44719492, 44721484], "created_at": "2025-07-28T22:59:25Z", "created_at_i": 1753743565, "num_comments": 4, "objectID": "44716785", "points": 2, "story_id": 44716785, "story_text": "Hello HN<p>After building numerous FastAPI backends, I consistently found myself repeating the same configuration, including auth, email, payments, migrations, etc.<p>In order to handle all the boilerplate, I developed FastLaunchAPI, a production-ready startup package.<p>Included are:<p><pre><code>    JWT authentication with social login and email\n\n    Webhooks and Stripe billing together\n\n    Alembic, PostgreSQL, and SQLAlchemy\n\n    Background work for celebrities\n\n    SMTP email that includes templates\n\n    Integration of LangChain and OpenAI\n\n    Setup for Pytest + API documents\n</code></pre>\nIt is designed to bring you to production in less than half an hour and is modular.<p>There is a comprehensive documentation, which can be viewed before pruchase.<p>I would be happy to hear any comments or inquiries!", "title": "Show HN: FastLaunchAPI \u2013 A production-ready FastAPI template batteries included", "updated_at": "2026-02-01T13:26:44Z", "url": "https://fastlaunchapi.dev/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "justvugg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Hi HN,<p>I built *LLM-Use*, an open-source intelligent router that helps reduce LLM API costs by automatically selecting the most appropriate model for each prompt.<p>I created it after realizing I was using GPT-4 for everything \u2014 including simple prompts like \u201ctranslate hello to Spanish\u201d \u2014 which cost $0.03 per call. Models like Mixtral can do the same for $0.0003.<p>### How it works:\n- Uses NLP (spaCy + transformers) to analyze prompt complexity\n- Routes to the optimal model (GPT-4, Claude, LLaMA, Mixtral, etc.)\n- Uses semantic similarity scoring to preserve output quality\n- Falls back gracefully if a model fails or gives poor results<p>### Key features:\n- Real-time streaming support for all providers\n- A/B testing with statistical significance\n- Response caching (LRU + TTL)\n- Circuit breakers for <em>production</em> stability\n- <em>FastAPI</em> backend with Prometheus metrics<p>### Early results:\n- Personal tests show up to 80% cost reduction\n- Output quality preserved (verified via internal A/B testing)<p>### Technical notes:\n- 2000+ lines of Python\n- Supports OpenAI, Anthropic, Google, Groq, Ollama\n- Complexity scoring: lexical diversity, prompt length, semantic analysis\n- Quality checks: relevance, coherence, grammar<p>Repo: [<a href=\"https://github.com/JustVugg/llm-use\" rel=\"nofollow\">https://github.com/JustVugg/llm-use</a>](<a href=\"https://github.com/JustVugg/llm-use\" rel=\"nofollow\">https://github.com/JustVugg/llm-use</a>)<p>Thanks! Happy to answer questions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LLM-Use \u2013 An LLM router that chooses the right model for each prompt"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/JustVugg/llm-use"}}, "_tags": ["story", "author_justvugg", "story_45504149", "show_hn"], "author": "justvugg", "children": [45505532], "created_at": "2025-10-07T15:13:39Z", "created_at_i": 1759850019, "num_comments": 2, "objectID": "45504149", "points": 3, "story_id": 45504149, "story_text": "Hi HN,<p>I built *LLM-Use*, an open-source intelligent router that helps reduce LLM API costs by automatically selecting the most appropriate model for each prompt.<p>I created it after realizing I was using GPT-4 for everything \u2014 including simple prompts like \u201ctranslate hello to Spanish\u201d \u2014 which cost $0.03 per call. Models like Mixtral can do the same for $0.0003.<p>### How it works:\n- Uses NLP (spaCy + transformers) to analyze prompt complexity\n- Routes to the optimal model (GPT-4, Claude, LLaMA, Mixtral, etc.)\n- Uses semantic similarity scoring to preserve output quality\n- Falls back gracefully if a model fails or gives poor results<p>### Key features:\n- Real-time streaming support for all providers\n- A&#x2F;B testing with statistical significance\n- Response caching (LRU + TTL)\n- Circuit breakers for production stability\n- FastAPI backend with Prometheus metrics<p>### Early results:\n- Personal tests show up to 80% cost reduction\n- Output quality preserved (verified via internal A&#x2F;B testing)<p>### Technical notes:\n- 2000+ lines of Python\n- Supports OpenAI, Anthropic, Google, Groq, Ollama\n- Complexity scoring: lexical diversity, prompt length, semantic analysis\n- Quality checks: relevance, coherence, grammar<p>Repo: [<a href=\"https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;llm-use\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;llm-use</a>](<a href=\"https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;llm-use\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;llm-use</a>)<p>Thanks! Happy to answer questions.", "title": "Show HN: LLM-Use \u2013 An LLM router that chooses the right model for each prompt", "updated_at": "2025-10-07T21:59:51Z", "url": "https://github.com/JustVugg/llm-use"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "dm118"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "The core idea is that no single LLM is best at everything, so we built orchestration primitives that let you combine them intelligently via a single API.<p>Mixture-of-Agents (MoA): Our /blend endpoint implements multi-layer MoA. You send a prompt to 2-6 models in parallel, then each model refines its answer using the other models' outputs as reference material. This runs for 1-3 configurable layers before a synthesizer model produces the final response. We also built a Self-MoA variant: a single model generates 2-8 diverse candidates using temperature variation and distinct agent prompts (&quot;prioritize correctness&quot;, &quot;anticipate edge cases&quot;, &quot;be skeptical&quot;), then synthesizes the best parts. Six blend strategies total: consensus, council, best_of, chain, moa, and self_moa.<p>Circuit breakers: Every model has a health tracker with a classic closed to open to half-open state machine. Three consecutive failures trips the circuit for 30 seconds. When a model is down, mesh routing automatically skips it and tries the fallback chain, so no wasted latency on providers that are having a bad day. The SSE stream emits route events so you can see exactly what happened: trying, failed, skipped(circuit_open), trying, success. OpenRouter gets its own tuned thresholds (6 consecutive 429s, 20s cooldown) because rate limits there behave differently than hard failures.<p>Auto-router: model: &quot;auto&quot; does zero-overhead heuristic routing, pure regex classification, no LLM call. Code goes to GPT, math/creative goes to Claude, translation goes to Gemini Flash, etc. Simple, fast, and surprisingly effective for common queries.<p>Other things that were fun to build:<p>- Credit settlement with margin targeting: we reserve credits upfront, then reconcile against actual provider cost after the response completes\n- Per-user semantic memory via pgvector: conversations build retrievable context across sessions\n- BYOK encryption (Fernet/AES-128) so you can bring your own API keys and skip our billing entirely<p>The whole backend is async Python (<em>FastAPI</em> + asyncpg + LiteLLM), frontend is static Next.js served by the same <em>FastAPI</em> process in <em>production</em>. Single Docker image on Railway.<p>For the technically curious: <a href=\"https://llmwise.ai/llms-full.txt\" rel=\"nofollow\">https://llmwise.ai/llms-full.txt</a> has the complete platform documentation in plain text, and there's also a machine-readable view at <a href=\"https://llmwise.ai/ai\" rel=\"nofollow\">https://llmwise.ai/ai</a> designed for AI agents to consume."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: LLMWise \u2013 Compare, Blend, and Judge LLM Outputs from One API"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://llmwise.ai/"}}, "_tags": ["story", "author_dm118", "story_47091997", "show_hn"], "author": "dm118", "created_at": "2026-02-20T18:42:10Z", "created_at_i": 1771612930, "num_comments": 0, "objectID": "47091997", "points": 1, "story_id": 47091997, "story_text": "The core idea is that no single LLM is best at everything, so we built orchestration primitives that let you combine them intelligently via a single API.<p>Mixture-of-Agents (MoA): Our &#x2F;blend endpoint implements multi-layer MoA. You send a prompt to 2-6 models in parallel, then each model refines its answer using the other models&#x27; outputs as reference material. This runs for 1-3 configurable layers before a synthesizer model produces the final response. We also built a Self-MoA variant: a single model generates 2-8 diverse candidates using temperature variation and distinct agent prompts (&quot;prioritize correctness&quot;, &quot;anticipate edge cases&quot;, &quot;be skeptical&quot;), then synthesizes the best parts. Six blend strategies total: consensus, council, best_of, chain, moa, and self_moa.<p>Circuit breakers: Every model has a health tracker with a classic closed to open to half-open state machine. Three consecutive failures trips the circuit for 30 seconds. When a model is down, mesh routing automatically skips it and tries the fallback chain, so no wasted latency on providers that are having a bad day. The SSE stream emits route events so you can see exactly what happened: trying, failed, skipped(circuit_open), trying, success. OpenRouter gets its own tuned thresholds (6 consecutive 429s, 20s cooldown) because rate limits there behave differently than hard failures.<p>Auto-router: model: &quot;auto&quot; does zero-overhead heuristic routing, pure regex classification, no LLM call. Code goes to GPT, math&#x2F;creative goes to Claude, translation goes to Gemini Flash, etc. Simple, fast, and surprisingly effective for common queries.<p>Other things that were fun to build:<p>- Credit settlement with margin targeting: we reserve credits upfront, then reconcile against actual provider cost after the response completes\n- Per-user semantic memory via pgvector: conversations build retrievable context across sessions\n- BYOK encryption (Fernet&#x2F;AES-128) so you can bring your own API keys and skip our billing entirely<p>The whole backend is async Python (FastAPI + asyncpg + LiteLLM), frontend is static Next.js served by the same FastAPI process in production. Single Docker image on Railway.<p>For the technically curious: <a href=\"https:&#x2F;&#x2F;llmwise.ai&#x2F;llms-full.txt\" rel=\"nofollow\">https:&#x2F;&#x2F;llmwise.ai&#x2F;llms-full.txt</a> has the complete platform documentation in plain text, and there&#x27;s also a machine-readable view at <a href=\"https:&#x2F;&#x2F;llmwise.ai&#x2F;ai\" rel=\"nofollow\">https:&#x2F;&#x2F;llmwise.ai&#x2F;ai</a> designed for AI agents to consume.", "title": "Show HN: LLMWise \u2013 Compare, Blend, and Judge LLM Outputs from One API", "updated_at": "2026-02-20T18:45:35Z", "url": "https://llmwise.ai/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "giorgiop"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Lanarky: Deploy LLM applications in <em>production</em>, built on <em>FastAPI</em>"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/ajndkr/lanarky"}}, "_tags": ["story", "author_giorgiop", "story_36270313"], "author": "giorgiop", "created_at": "2023-06-10T13:31:15Z", "created_at_i": 1686403875, "num_comments": 0, "objectID": "36270313", "points": 3, "story_id": 36270313, "title": "Lanarky: Deploy LLM applications in production, built on FastAPI", "updated_at": "2024-09-20T14:17:13Z", "url": "https://github.com/ajndkr/lanarky"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "victorveloso"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Show HN: Define a <em>production</em>-ready backend CRUD <em>FastAPI</em> using a JSON schema"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://rationalbloks.com/"}}, "_tags": ["story", "author_victorveloso", "story_46647160", "show_hn"], "author": "victorveloso", "children": [46647178], "created_at": "2026-01-16T15:13:15Z", "created_at_i": 1768576395, "num_comments": 1, "objectID": "46647160", "points": 1, "story_id": 46647160, "title": "Show HN: Define a production-ready backend CRUD FastAPI using a JSON schema", "updated_at": "2026-01-16T15:21:02Z", "url": "https://rationalbloks.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jianna_777"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Most memory solutions just do RAG search on chat logs. That's not real memory. We want a real memory that could understand a user and grow with them, not just search their past chats.<p>Memobase works like ChatGPT  memory, specially designed for chatbot agents. It's personalized and scales to millions. Retrieval latency is under 80ms. ( 85% score on LOCOMO Temporal Benchmark <a href=\"https://github.com/memodb-io/memobase/tree/main/docs/experiments/locomo-benchmark\uff09\" rel=\"nofollow\">https://github.com/memodb-io/memobase/tree/main/docs/experim...</a><p>- Profile + Timeline schema \u2014 not complex embedding.\n- Context API \u2014 full system prompts for evolving &amp; personalized LLM responses.\n- Built with <em>FastAPI</em>, Postgres, Redis \u2014 ready for <em>production</em>.<p>Check it out on GitHub: <a href=\"https://github.com/memodb-io/memobase\" rel=\"nofollow\">https://github.com/memodb-io/memobase</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: An Open-Source Structured Memory Back End (Like ChatGPT Memory)"}}, "_tags": ["story", "author_jianna_777", "story_44917214", "show_hn"], "author": "jianna_777", "created_at": "2025-08-15T21:02:54Z", "created_at_i": 1755291774, "num_comments": 0, "objectID": "44917214", "points": 3, "story_id": 44917214, "story_text": "Most memory solutions just do RAG search on chat logs. That&#x27;s not real memory. We want a real memory that could understand a user and grow with them, not just search their past chats.<p>Memobase works like ChatGPT  memory, specially designed for chatbot agents. It&#x27;s personalized and scales to millions. Retrieval latency is under 80ms. ( 85% score on LOCOMO Temporal Benchmark <a href=\"https:&#x2F;&#x2F;github.com&#x2F;memodb-io&#x2F;memobase&#x2F;tree&#x2F;main&#x2F;docs&#x2F;experiments&#x2F;locomo-benchmark\uff09\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;memodb-io&#x2F;memobase&#x2F;tree&#x2F;main&#x2F;docs&#x2F;experim...</a><p>- Profile + Timeline schema \u2014 not complex embedding.\n- Context API \u2014 full system prompts for evolving &amp; personalized LLM responses.\n- Built with FastAPI, Postgres, Redis \u2014 ready for production.<p>Check it out on GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;memodb-io&#x2F;memobase\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;memodb-io&#x2F;memobase</a>", "title": "Show HN: An Open-Source Structured Memory Back End (Like ChatGPT Memory)", "updated_at": "2025-08-19T00:03:04Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "suiyang03"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Hey HN!<p>I created <em>fastapi</em>-injectable to solve a common pain point in <em>FastAPI</em>: using Depends() outside of route handlers.\nThis has been a long-standing issue in the <em>FastAPI</em> community with a discussion post around it.<p>Install with: pip install <em>fastapi</em>-injectable<p>The package lets you use <em>FastAPI</em>'s dependency injection in:<p>- CLI tools<p>- Background workers<p>- Test fixtures<p>- Any non-HTTP context<p>Basic example:<p><pre><code>    from typing import Annotated\n\n    from <em>fastapi</em> import Depends\n    from <em>fastapi</em>_injectable import injectable\n\n    class Database:\n        def query(self) -&gt; str:\n            return &quot;data&quot;\n\n    def get_db() -&gt; Database:\n        return Database()\n\n    @injectable\n    def process_data(db: Annotated[Database, Depends(get_db)]) -&gt; str:\n        return db.query()\n\n    # Use it anywhere!\n    result = process_data()\n    print(result) # Output: 'data'\n\n</code></pre>\nThis is a <em>production</em>-ready package for <em>FastAPI</em> developers who need to use dependency injection outside of HTTP routes.<p>The common suggestion for handling dependencies outside <em>FastAPI</em> routes is to use separate DI frameworks like dependency-injector. However, this means:<p>1. Managing two different DI systems in your codebase<p>2. Learning another framework's patterns<p>3. Potentially duplicating dependency logic<p>Currently, there's no package that extends <em>FastAPI</em>'s native Depends() beyond routes.<p><em>fastapi</em>-injectable solves this by providing a clean API that lets you use <em>FastAPI</em>'s dependency injection anywhere.<p>Links:<p>GitHub: <a href=\"https://github.com/your-username/fastapi-injectable\">https://github.com/your-username/<em>fastapi</em>-injectable</a><p>Docs: <a href=\"https://fastapi-injectable.readthedocs.io\" rel=\"nofollow\">https://<em>fastapi</em>-injectable.readthedocs.io</a><p>PyPI: <a href=\"https://pypi.org/project/fastapi-injectable\" rel=\"nofollow\">https://pypi.org/project/<em>fastapi</em>-injectable</a><p>I'd love to hear your feedback and contributions to make this package better! Feel free to open issues or PRs if you have ideas for improvements."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["fastapi"], "value": "Show HN: <em>fastapi</em>-injectable \u2013 Use <em>FastAPI</em> Depends() anywhere even outside routes"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["fastapi"], "value": "https://github.com/JasperSui/<em>fastapi</em>-injectable"}}, "_tags": ["story", "author_suiyang03", "story_42472219", "show_hn"], "author": "suiyang03", "created_at": "2024-12-20T16:01:53Z", "created_at_i": 1734710513, "num_comments": 0, "objectID": "42472219", "points": 1, "story_id": 42472219, "story_text": "Hey HN!<p>I created fastapi-injectable to solve a common pain point in FastAPI: using Depends() outside of route handlers.\nThis has been a long-standing issue in the FastAPI community with a discussion post around it.<p>Install with: pip install fastapi-injectable<p>The package lets you use FastAPI&#x27;s dependency injection in:<p>- CLI tools<p>- Background workers<p>- Test fixtures<p>- Any non-HTTP context<p>Basic example:<p><pre><code>    from typing import Annotated\n\n    from fastapi import Depends\n    from fastapi_injectable import injectable\n\n    class Database:\n        def query(self) -&gt; str:\n            return &quot;data&quot;\n\n    def get_db() -&gt; Database:\n        return Database()\n\n    @injectable\n    def process_data(db: Annotated[Database, Depends(get_db)]) -&gt; str:\n        return db.query()\n\n    # Use it anywhere!\n    result = process_data()\n    print(result) # Output: &#x27;data&#x27;\n\n</code></pre>\nThis is a production-ready package for FastAPI developers who need to use dependency injection outside of HTTP routes.<p>The common suggestion for handling dependencies outside FastAPI routes is to use separate DI frameworks like dependency-injector. However, this means:<p>1. Managing two different DI systems in your codebase<p>2. Learning another framework&#x27;s patterns<p>3. Potentially duplicating dependency logic<p>Currently, there&#x27;s no package that extends FastAPI&#x27;s native Depends() beyond routes.<p>fastapi-injectable solves this by providing a clean API that lets you use FastAPI&#x27;s dependency injection anywhere.<p>Links:<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;your-username&#x2F;fastapi-injectable\">https:&#x2F;&#x2F;github.com&#x2F;your-username&#x2F;fastapi-injectable</a><p>Docs: <a href=\"https:&#x2F;&#x2F;fastapi-injectable.readthedocs.io\" rel=\"nofollow\">https:&#x2F;&#x2F;fastapi-injectable.readthedocs.io</a><p>PyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;fastapi-injectable\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;fastapi-injectable</a><p>I&#x27;d love to hear your feedback and contributions to make this package better! Feel free to open issues or PRs if you have ideas for improvements.", "title": "Show HN: fastapi-injectable \u2013 Use FastAPI Depends() anywhere even outside routes", "updated_at": "2025-04-13T20:30:08Z", "url": "https://github.com/JasperSui/fastapi-injectable"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "chokoswitch"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "I have released a new Python ASGI/WSGI application server. This may be particularly interesting to<p>- Python developers interested in server tech, especially if already using Envoy<p>- Envoy users to see use cases enabled by dynamic modules<p>- Rust fans to see an example of it bridging two non-Rust ecosystems, Envoy and Python<p>Note, this is my first real project in Rust - while I tried to study idioms while working on it, I doubt it is an example of a fully idiomatic Rust project.<p>I was interested in a Python app server with support for HTTP/2 trailers to be able to serve gRPC as a normal application, together with non-gRPC endpoints. When looking at existing options, I noticed a lot of complexity with wiring up sockets, flow control, and similar. Coming from Go, I am used to net/http providing fully featured, <em>production</em>-ready HTTP servers with very little work. But for many reasons, it's not realistic to drive Python apps from Go.<p>Coincidentally, Envoy released support for dynamic modules which allow running arbitrary code in Envoy, along with a Rust SDK. I thought it would be a fun experiment to see if this could actually drive a full Python server - I thought it wouldn't. But after exposing some more knobs in dynamic modules - it actually worked and pyvoy was born, a dynamic module that loads the Python interpreter to run ASGI and WSGI apps, marshaling from Envoy's HTTP filter. There's also a CLI which takes care of running Envoy with the module pointed to an app - this is definitely not net/http level of convenience, but I appreciate that complexity is only on the startup side. There is nothing needed to handle HTTP, TLS, etc in pyvoy, it is all taken care of by the battle-hardened Envoy stack, and we get everything from HTTP, including trailers and HTTP/3.<p>With support for trailers, pyvoy drives the gRPC protocol support on the server for connect-python (<a href=\"https://github.com/connectrpc/connect-python\" rel=\"nofollow\">https://github.com/connectrpc/connect-python</a>), allowing them to be served along an existing Flask or <em>FastAPI</em> application as needed. Notably, it is the only server that passes all of connect's conformance tests with no flakiness. It's important to note that uvicorn also passes reliably when disabling features that require HTTP/2. It's a great server when bidirectional streaming or gRPC aren't needed - unfortunately others we tried would have unreliable behavior handling client disconnects, keepalive, and such. This doesn't surprise me as I have seen quite some time ago how hard it is to implement especially HTTP/2 reliably, and I appreciate pyvoy can rely on Envoy to just take care of it.<p>It seems that pyvoy is a fast (always benchmark your own workload), reliable server not just for gRPC but any workload. It also can directly use any Envoy feature, and could replace a pair of Envoy + Python app server. I currently use it in <em>production</em> at low scale serving Django, <em>FastAPI</em>, and connect-python.<p>Happy to hear any thoughts on this project. Thanks for reading!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Pyvoy \u2013 a modern Python application server built in Envoy"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/curioswitch/pyvoy"}}, "_tags": ["story", "author_chokoswitch", "story_46675907", "show_hn"], "author": "chokoswitch", "created_at": "2026-01-19T07:24:38Z", "created_at_i": 1768807478, "num_comments": 0, "objectID": "46675907", "points": 2, "story_id": 46675907, "story_text": "I have released a new Python ASGI&#x2F;WSGI application server. This may be particularly interesting to<p>- Python developers interested in server tech, especially if already using Envoy<p>- Envoy users to see use cases enabled by dynamic modules<p>- Rust fans to see an example of it bridging two non-Rust ecosystems, Envoy and Python<p>Note, this is my first real project in Rust - while I tried to study idioms while working on it, I doubt it is an example of a fully idiomatic Rust project.<p>I was interested in a Python app server with support for HTTP&#x2F;2 trailers to be able to serve gRPC as a normal application, together with non-gRPC endpoints. When looking at existing options, I noticed a lot of complexity with wiring up sockets, flow control, and similar. Coming from Go, I am used to net&#x2F;http providing fully featured, production-ready HTTP servers with very little work. But for many reasons, it&#x27;s not realistic to drive Python apps from Go.<p>Coincidentally, Envoy released support for dynamic modules which allow running arbitrary code in Envoy, along with a Rust SDK. I thought it would be a fun experiment to see if this could actually drive a full Python server - I thought it wouldn&#x27;t. But after exposing some more knobs in dynamic modules - it actually worked and pyvoy was born, a dynamic module that loads the Python interpreter to run ASGI and WSGI apps, marshaling from Envoy&#x27;s HTTP filter. There&#x27;s also a CLI which takes care of running Envoy with the module pointed to an app - this is definitely not net&#x2F;http level of convenience, but I appreciate that complexity is only on the startup side. There is nothing needed to handle HTTP, TLS, etc in pyvoy, it is all taken care of by the battle-hardened Envoy stack, and we get everything from HTTP, including trailers and HTTP&#x2F;3.<p>With support for trailers, pyvoy drives the gRPC protocol support on the server for connect-python (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;connectrpc&#x2F;connect-python\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;connectrpc&#x2F;connect-python</a>), allowing them to be served along an existing Flask or FastAPI application as needed. Notably, it is the only server that passes all of connect&#x27;s conformance tests with no flakiness. It&#x27;s important to note that uvicorn also passes reliably when disabling features that require HTTP&#x2F;2. It&#x27;s a great server when bidirectional streaming or gRPC aren&#x27;t needed - unfortunately others we tried would have unreliable behavior handling client disconnects, keepalive, and such. This doesn&#x27;t surprise me as I have seen quite some time ago how hard it is to implement especially HTTP&#x2F;2 reliably, and I appreciate pyvoy can rely on Envoy to just take care of it.<p>It seems that pyvoy is a fast (always benchmark your own workload), reliable server not just for gRPC but any workload. It also can directly use any Envoy feature, and could replace a pair of Envoy + Python app server. I currently use it in production at low scale serving Django, FastAPI, and connect-python.<p>Happy to hear any thoughts on this project. Thanks for reading!", "title": "Show HN: Pyvoy \u2013 a modern Python application server built in Envoy", "updated_at": "2026-01-19T15:30:13Z", "url": "https://github.com/curioswitch/pyvoy"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "homarp"}, "title": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "<em>FastAPI</em> framework, high perf, easy to learn, fast to code, ready for <em>production</em>"}, "url": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["fastapi"], "value": "https://<em>fastapi</em>.tiangolo.com/"}}, "_tags": ["story", "author_homarp", "story_25990702"], "author": "homarp", "children": [25991217, 25991267, 25991402, 25991624, 25991724, 25991847, 25991849, 25991859, 25991932, 25992078, 25992112, 25992297, 25992458, 25992561, 25992582, 25992645, 25992674, 25992685, 25992783, 25992802, 25992843, 25992861, 25992891, 25992949, 25993159, 25993283, 25993336, 25993787, 25994472, 25994578, 25994588, 25995019, 25995846, 25995950, 25996564, 25997203, 25997465, 25997500, 25998364, 25999435, 26005304], "created_at": "2021-02-01T17:20:19Z", "created_at_i": 1612200019, "num_comments": 151, "objectID": "25990702", "points": 508, "story_id": 25990702, "title": "FastAPI framework, high perf, easy to learn, fast to code, ready for production", "updated_at": "2026-01-16T10:21:46Z", "url": "https://fastapi.tiangolo.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ashvardanian"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "A few months ago, I benchmarked <em>FastAPI</em> on an i9 MacBook Pro. I couldn't believe my eyes. A primary REST endpoint to `sum` two integers took 6 milliseconds to evaluate. It is okay if you are targeting a server in another city, but it should be less when your client and server apps are running on the same machine.<p><em>FastAPI</em> would have bottleneck-ed the inference of our lightweight UForm neural networks recently trending on HN under the title &quot;Beating OpenAI CLIP with 100x less data and compute&quot;. (Thank you all for the kind words!) So I wrote another library.<p>It has been a while since I have written networking libraries, so I was eager to try the newer io_uring networking functionality added by Jens Axboe in kernel 5.19. TLDR: It's excellent! We used pre-registered buffers and re-allocated file descriptors from a managed pool. Some other parts, like multi-shot requests, also look intriguing, but we couldn't see a flawless way to integrate them into UJRPC. Maybe next time.<p>Like a parent with two kids, we tell everyone we love Kernel Bypass and SIMD equally. So I decided to combine the two, potentially implementing one of the fastest implementations of the most straightforward RPC protocol - JSON-RPC. ~~Healthy and Fun~~ Efficient and Simple, what can be better?<p>By now, you may already guess at least one of the dependencies - `simdjson` by Daniel Lemiere, that has become the industry standard. io_uring is generally very fast, even with a single core. Adding more polling threads may only increase congestion. We needed to continue using no more than one thread, but parsing messages may involve more work than just invoking a JSON parser.<p>JSON-RPC is transport agnostic. The incoming requests can be sent over HTTP, pre-pended by rows of headers. Those would have to be POSTs and generally contain Content-Length and Content-Type. There is a SIMD-accelerated library for that as well. It is called `picohttpparser`, uses SSE, and is maintained by H2O.<p>The story doesn't end there. JSON is limited. Passing binary strings is a nightmare. The most common approach is to encode them with base-64. So we took the Turbo-Base64 from the PowTurbo project to decode those binary strings.<p>The core implementation of UJRPC is under 2000 lines of C++. Knowing that those lines connect 3 great libraries with the newest and coolest parts of Linux is enough to put a smile on my face. Most people are more rational, so here is another reason to be cheerful.<p>- <em>FastAPI</em> throughput: 3'184 rps.\n- Python gRPC throughput: 9'849 rps.\n- UJRPC throughput:\n-- Python server with io_uring: 43'000 rps.\n-- C server with POSIX: 79'000 rps.\n-- C server with io_uring: 231'000 rps.<p>Granted, this is yet to be your batteries-included server. It can't balance the load, manage threads, spell S in HTTPS, or call parents when you misbehave in school. But at least part of it you shouldn't expect from a web server.<p>After following the standardization process of executors in C++ for the last N+1 years, we adapted the &quot;bring your runtime&quot; and &quot;bring your thread-pool&quot; policies. HTTPS support, however, is our next primary objective.<p>---<p>Of course, it is a pre-<em>production</em> project and must have a lot of bugs. Don't hesitate to report them. We have huge plans for this tiny package and will potentially make it the default transport of UKV: <a href=\"https://github.com/unum-cloud/ukv\">https://github.com/unum-cloud/ukv</a>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["fastapi"], "value": "Show HN: Faster <em>FastAPI</em> with simdjson and io_uring on Linux 5.19"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/unum-cloud/ujrpc"}}, "_tags": ["story", "author_ashvardanian", "story_35042316", "show_hn"], "author": "ashvardanian", "children": [35042609, 35042699, 35042709, 35042733, 35042807, 35042895, 35043092, 35043349, 35044940, 35045476, 35046029, 35049669], "created_at": "2023-03-06T15:41:23Z", "created_at_i": 1678117283, "num_comments": 90, "objectID": "35042316", "points": 290, "story_id": 35042316, "story_text": "A few months ago, I benchmarked FastAPI on an i9 MacBook Pro. I couldn&#x27;t believe my eyes. A primary REST endpoint to `sum` two integers took 6 milliseconds to evaluate. It is okay if you are targeting a server in another city, but it should be less when your client and server apps are running on the same machine.<p>FastAPI would have bottleneck-ed the inference of our lightweight UForm neural networks recently trending on HN under the title &quot;Beating OpenAI CLIP with 100x less data and compute&quot;. (Thank you all for the kind words!) So I wrote another library.<p>It has been a while since I have written networking libraries, so I was eager to try the newer io_uring networking functionality added by Jens Axboe in kernel 5.19. TLDR: It&#x27;s excellent! We used pre-registered buffers and re-allocated file descriptors from a managed pool. Some other parts, like multi-shot requests, also look intriguing, but we couldn&#x27;t see a flawless way to integrate them into UJRPC. Maybe next time.<p>Like a parent with two kids, we tell everyone we love Kernel Bypass and SIMD equally. So I decided to combine the two, potentially implementing one of the fastest implementations of the most straightforward RPC protocol - JSON-RPC. ~~Healthy and Fun~~ Efficient and Simple, what can be better?<p>By now, you may already guess at least one of the dependencies - `simdjson` by Daniel Lemiere, that has become the industry standard. io_uring is generally very fast, even with a single core. Adding more polling threads may only increase congestion. We needed to continue using no more than one thread, but parsing messages may involve more work than just invoking a JSON parser.<p>JSON-RPC is transport agnostic. The incoming requests can be sent over HTTP, pre-pended by rows of headers. Those would have to be POSTs and generally contain Content-Length and Content-Type. There is a SIMD-accelerated library for that as well. It is called `picohttpparser`, uses SSE, and is maintained by H2O.<p>The story doesn&#x27;t end there. JSON is limited. Passing binary strings is a nightmare. The most common approach is to encode them with base-64. So we took the Turbo-Base64 from the PowTurbo project to decode those binary strings.<p>The core implementation of UJRPC is under 2000 lines of C++. Knowing that those lines connect 3 great libraries with the newest and coolest parts of Linux is enough to put a smile on my face. Most people are more rational, so here is another reason to be cheerful.<p>- FastAPI throughput: 3&#x27;184 rps.\n- Python gRPC throughput: 9&#x27;849 rps.\n- UJRPC throughput:\n-- Python server with io_uring: 43&#x27;000 rps.\n-- C server with POSIX: 79&#x27;000 rps.\n-- C server with io_uring: 231&#x27;000 rps.<p>Granted, this is yet to be your batteries-included server. It can&#x27;t balance the load, manage threads, spell S in HTTPS, or call parents when you misbehave in school. But at least part of it you shouldn&#x27;t expect from a web server.<p>After following the standardization process of executors in C++ for the last N+1 years, we adapted the &quot;bring your runtime&quot; and &quot;bring your thread-pool&quot; policies. HTTPS support, however, is our next primary objective.<p>---<p>Of course, it is a pre-production project and must have a lot of bugs. Don&#x27;t hesitate to report them. We have huge plans for this tiny package and will potentially make it the default transport of UKV: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;ukv\">https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;ukv</a>", "title": "Show HN: Faster FastAPI with simdjson and io_uring on Linux 5.19", "updated_at": "2025-04-27T23:00:46Z", "url": "https://github.com/unum-cloud/ujrpc"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "calebtv"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Hey HN! We\u2019re Caleb and Josh, the founders of BuildFlow (<a href=\"https://www.buildflow.dev\" rel=\"nofollow\">https://www.buildflow.dev</a>). We provide an open source framework for building your entire data pipeline quickly using Python. You can think of us as an easy alternative to Apache Beam or Google Cloud Dataflow.<p>The problem we're trying to solve is simple: building data pipelines can be a real pain. You often need to deal with complex frameworks, manage external cloud resources, and wire everything together into a single deployment (you\u2019re probably drowning in Yaml by this point in the dev cycle). This can be a burden on both data scientists and engineering teams.<p>Data pipelines is a broad term, but we generally mean any kind of processing that happens outside of the user facing path. This can be things like: processing file uploads, syncing data to a data warehouse, or ingesting data from IoT devices.<p>BuildFlow, our open-source framework, lets you build a data pipeline by simply attaching a decorator to a Python function. All you need to do is describe where your input is coming from and where your output should be written, and BuildFlow handles the rest. No configuration outside of the code is required. See our docs for some examples: <a href=\"https://www.buildflow.dev/docs/intro\" rel=\"nofollow\">https://www.buildflow.dev/docs/intro</a>.<p>When you attach the decorator to your function, the BuildFlow runtime creates your referenced cloud resources, spins up replicas of your processor, and wires up everything needed to efficiently scale out the reads from your source and then writes to your sink. This lets you focus on writing logic as opposed to interacting with your external dependencies.<p>BuildFlow aims to hide as much complexity as possible in the sources / sinks so that your processing logic can remain simple. The framework provides generic I/O connectors for popular cloud services and storage systems, in addition to &quot;use case driven\u201d I/O connectors that chain together multiple I/O steps required by common use cases. An example \u201cuse case driven\u201d source that chains together GCS pubsub notifications &amp; fetching GCS blobs can be seen here: <a href=\"https://www.buildflow.dev/docs/io-connectors/gcs_notifications\" rel=\"nofollow\">https://www.buildflow.dev/docs/io-connectors/gcs_notificatio...</a><p>BuildFlow was inspired by our time at Verily (Google Life Sciences) where we designed an internal platform to help data scientists build and deploy ML infra / data pipelines using Apache Beam. Using a complex framework was a burden on our data science team because they had to learn a whole new paradigm to write their Python code in, and our engineering team was left with the operational load of helping folks learn Apache Beam while also managing / deploying <em>production</em> pipelines. From this pain, BuildFlow was born.<p>Our design is based around two observations we made from that experience:<p>(1) The hardest thing to get right is I/O. Efficiently fanning out I/O to workers, concurrently reading / processing input data, catching schema mismatches before runtime, and configuring cloud resources is where most of the pain is. BuildFlow attempts to abstract away all of these bits.<p>(2) Most use cases are large scale but not (overly) complex. Existing frameworks give you scalability and a complicated programming model that supports every use case under the sun. BuildFlow provides the same scalability but focuses on common use cases so that the API can remain lightweight &amp; easy to use.<p>BuildFlow is open source, but we offer a managed cloud offering that allows you to easily deploy your pipelines to the cloud. We provide a CLI that deploys your pipeline to a managed kubernetes cluster, and you can optionally opt in to letting us manage your resources / terraform as well. Ultimately this will feed into our VS Code Extension which will allow users to visually build their data pipelines directly from VS Code (see <a href=\"https://launchflow.com\">https://launchflow.com</a> for a preview). The extension will be free to use and will come packaged with a bunch of nice-to-haves (code generation, fuzzing, tracing, and arcade games (yep!) just to name a few in the works).<p>Our managed offering is still in private beta but we\u2019re hoping to release our CLI in the next couple weeks. Pricing for this service is still being ironed out but we expect it to be based on usage.<p>We\u2019d love for you to try BuildFlow and would love any feedback. You can get started right away by installing the python package: pip install buildflow. Check out our docs (<a href=\"https://buildflow.dev/docs/intro\" rel=\"nofollow\">https://buildflow.dev/docs/intro</a>) and GitHub (<a href=\"https://github.com/launchflow/buildflow\">https://github.com/launchflow/buildflow</a>) to see examples on how to use the API.<p>This project is very new, so we\u2019d love to gather some specific feedback from you, the community. How do you feel about a framework managing your cloud resources? We\u2019re considering adding a module that would let BuildFlow create / manage your terraform for you (terraform state would be dumped to disk). What are some common I/O operations you find yourself rewriting? What are some operational tasks that require you to leave your code editor? We\u2019d like to bring as many tasks into BuildFlow and our VSCode extension so you can avoid context switches."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["fastapi"], "value": "Launch HN: BuildFlow (YC W23) \u2013 The <em>FastAPI</em> of data pipelines"}}, "_tags": ["story", "author_calebtv", "story_35169256", "launch_hn"], "author": "calebtv", "children": [35169391, 35169471, 35170102, 35170615, 35173464, 35174051, 35174549, 35175249, 35179804], "created_at": "2023-03-15T14:55:01Z", "created_at_i": 1678892101, "num_comments": 35, "objectID": "35169256", "points": 104, "story_id": 35169256, "story_text": "Hey HN! We\u2019re Caleb and Josh, the founders of BuildFlow (<a href=\"https:&#x2F;&#x2F;www.buildflow.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;www.buildflow.dev</a>). We provide an open source framework for building your entire data pipeline quickly using Python. You can think of us as an easy alternative to Apache Beam or Google Cloud Dataflow.<p>The problem we&#x27;re trying to solve is simple: building data pipelines can be a real pain. You often need to deal with complex frameworks, manage external cloud resources, and wire everything together into a single deployment (you\u2019re probably drowning in Yaml by this point in the dev cycle). This can be a burden on both data scientists and engineering teams.<p>Data pipelines is a broad term, but we generally mean any kind of processing that happens outside of the user facing path. This can be things like: processing file uploads, syncing data to a data warehouse, or ingesting data from IoT devices.<p>BuildFlow, our open-source framework, lets you build a data pipeline by simply attaching a decorator to a Python function. All you need to do is describe where your input is coming from and where your output should be written, and BuildFlow handles the rest. No configuration outside of the code is required. See our docs for some examples: <a href=\"https:&#x2F;&#x2F;www.buildflow.dev&#x2F;docs&#x2F;intro\" rel=\"nofollow\">https:&#x2F;&#x2F;www.buildflow.dev&#x2F;docs&#x2F;intro</a>.<p>When you attach the decorator to your function, the BuildFlow runtime creates your referenced cloud resources, spins up replicas of your processor, and wires up everything needed to efficiently scale out the reads from your source and then writes to your sink. This lets you focus on writing logic as opposed to interacting with your external dependencies.<p>BuildFlow aims to hide as much complexity as possible in the sources &#x2F; sinks so that your processing logic can remain simple. The framework provides generic I&#x2F;O connectors for popular cloud services and storage systems, in addition to &quot;use case driven\u201d I&#x2F;O connectors that chain together multiple I&#x2F;O steps required by common use cases. An example \u201cuse case driven\u201d source that chains together GCS pubsub notifications &amp; fetching GCS blobs can be seen here: <a href=\"https:&#x2F;&#x2F;www.buildflow.dev&#x2F;docs&#x2F;io-connectors&#x2F;gcs_notifications\" rel=\"nofollow\">https:&#x2F;&#x2F;www.buildflow.dev&#x2F;docs&#x2F;io-connectors&#x2F;gcs_notificatio...</a><p>BuildFlow was inspired by our time at Verily (Google Life Sciences) where we designed an internal platform to help data scientists build and deploy ML infra &#x2F; data pipelines using Apache Beam. Using a complex framework was a burden on our data science team because they had to learn a whole new paradigm to write their Python code in, and our engineering team was left with the operational load of helping folks learn Apache Beam while also managing &#x2F; deploying production pipelines. From this pain, BuildFlow was born.<p>Our design is based around two observations we made from that experience:<p>(1) The hardest thing to get right is I&#x2F;O. Efficiently fanning out I&#x2F;O to workers, concurrently reading &#x2F; processing input data, catching schema mismatches before runtime, and configuring cloud resources is where most of the pain is. BuildFlow attempts to abstract away all of these bits.<p>(2) Most use cases are large scale but not (overly) complex. Existing frameworks give you scalability and a complicated programming model that supports every use case under the sun. BuildFlow provides the same scalability but focuses on common use cases so that the API can remain lightweight &amp; easy to use.<p>BuildFlow is open source, but we offer a managed cloud offering that allows you to easily deploy your pipelines to the cloud. We provide a CLI that deploys your pipeline to a managed kubernetes cluster, and you can optionally opt in to letting us manage your resources &#x2F; terraform as well. Ultimately this will feed into our VS Code Extension which will allow users to visually build their data pipelines directly from VS Code (see <a href=\"https:&#x2F;&#x2F;launchflow.com\">https:&#x2F;&#x2F;launchflow.com</a> for a preview). The extension will be free to use and will come packaged with a bunch of nice-to-haves (code generation, fuzzing, tracing, and arcade games (yep!) just to name a few in the works).<p>Our managed offering is still in private beta but we\u2019re hoping to release our CLI in the next couple weeks. Pricing for this service is still being ironed out but we expect it to be based on usage.<p>We\u2019d love for you to try BuildFlow and would love any feedback. You can get started right away by installing the python package: pip install buildflow. Check out our docs (<a href=\"https:&#x2F;&#x2F;buildflow.dev&#x2F;docs&#x2F;intro\" rel=\"nofollow\">https:&#x2F;&#x2F;buildflow.dev&#x2F;docs&#x2F;intro</a>) and GitHub (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;launchflow&#x2F;buildflow\">https:&#x2F;&#x2F;github.com&#x2F;launchflow&#x2F;buildflow</a>) to see examples on how to use the API.<p>This project is very new, so we\u2019d love to gather some specific feedback from you, the community. How do you feel about a framework managing your cloud resources? We\u2019re considering adding a module that would let BuildFlow create &#x2F; manage your terraform for you (terraform state would be dumped to disk). What are some common I&#x2F;O operations you find yourself rewriting? What are some operational tasks that require you to leave your code editor? We\u2019d like to bring as many tasks into BuildFlow and our VSCode extension so you can avoid context switches.", "title": "Launch HN: BuildFlow (YC W23) \u2013 The FastAPI of data pipelines", "updated_at": "2024-09-20T13:38:29Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "Aherontas"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["fastapi", "production"], "value": "Hey all!\nI recently gave a workshop talk at PyCon Greece 2025 about building <em>production</em>-ready agent systems.<p>To check the workshop, I put together a demo repo: (I will add the slides too soon in my blog: <a href=\"https://www.petrostechchronicles.com/\" rel=\"nofollow\">https://www.petrostechchronicles.com/</a>)\n<a href=\"https://github.com/Aherontas/Pycon_Greece_2025_Presentation_Agents\" rel=\"nofollow\">https://github.com/Aherontas/Pycon_Greece_2025_Presentation_...</a><p>The idea was to show how multiple AI agents can collaborate using <em>FastAPI</em> + Pydantic-AI, with protocols like MCP (Model Context Protocol) and A2A (Agent-to-Agent) for safe communication and orchestration.<p>Features:<p>- Multiple agents running in containers<p>- MCP servers (Brave search, GitHub, filesystem, etc.) as tools<p>- A2A communication between services<p>- Minimal UI for experimentation for Tech Trend - repo analysis<p>I built this repo because most agent frameworks look great in isolated demos, but fall apart when you try to glue agents together into a real application. My goal was to help people experiment with these patterns and move closer to real-world use cases.<p>It\u2019s not <em>production</em>-grade, but would love feedback, criticism, or war stories from anyone who\u2019s tried building actual multi-agent systems.\nBig questions:<p>Do you think agent-to-agent protocols like MCP/A2A will stick?<p>Or will the future be mostly single powerful LLMs with plugin stacks?<p>Thanks \u2014 excited to hear what the HN crowd thinks!"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["fastapi"], "value": "Show HN: AI-powered web service combining <em>FastAPI</em>, Pydantic-AI, and MCP servers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/Aherontas/Pycon_Greece_2025_Presentation_Agents"}}, "_tags": ["story", "author_Aherontas", "story_45243320", "show_hn"], "author": "Aherontas", "children": [45128860, 45243325, 45253092, 45253248, 45253999, 45254599, 45257839, 45260699, 45288417, 45332320], "created_at": "2025-09-14T21:17:56Z", "created_at_i": 1757884676, "num_comments": 24, "objectID": "45243320", "points": 46, "story_id": 45243320, "story_text": "Hey all!\nI recently gave a workshop talk at PyCon Greece 2025 about building production-ready agent systems.<p>To check the workshop, I put together a demo repo: (I will add the slides too soon in my blog: <a href=\"https:&#x2F;&#x2F;www.petrostechchronicles.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.petrostechchronicles.com&#x2F;</a>)\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;Aherontas&#x2F;Pycon_Greece_2025_Presentation_Agents\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Aherontas&#x2F;Pycon_Greece_2025_Presentation_...</a><p>The idea was to show how multiple AI agents can collaborate using FastAPI + Pydantic-AI, with protocols like MCP (Model Context Protocol) and A2A (Agent-to-Agent) for safe communication and orchestration.<p>Features:<p>- Multiple agents running in containers<p>- MCP servers (Brave search, GitHub, filesystem, etc.) as tools<p>- A2A communication between services<p>- Minimal UI for experimentation for Tech Trend - repo analysis<p>I built this repo because most agent frameworks look great in isolated demos, but fall apart when you try to glue agents together into a real application. My goal was to help people experiment with these patterns and move closer to real-world use cases.<p>It\u2019s not production-grade, but would love feedback, criticism, or war stories from anyone who\u2019s tried building actual multi-agent systems.\nBig questions:<p>Do you think agent-to-agent protocols like MCP&#x2F;A2A will stick?<p>Or will the future be mostly single powerful LLMs with plugin stacks?<p>Thanks \u2014 excited to hear what the HN crowd thinks!", "title": "Show HN: AI-powered web service combining FastAPI, Pydantic-AI, and MCP servers", "updated_at": "2025-09-23T17:02:14Z", "url": "https://github.com/Aherontas/Pycon_Greece_2025_Presentation_Agents"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "macinjosh"}, "story_text": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "For some of my work with LLMs and RAG I needed to be able to fetch content from my customer's support site. Basic cURL calls weren't working because the site is an SPA and is rendered completely through javascript.<p>The trouble it took me to figure out how to capture the rendered HTML from a headless browser and how to do it reliably in <em>production</em> inspired me to create LinkGrabs.com so others would not have to go through the same pain."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["fastapi"], "value": "Show HN: Linkgrabs.com the Simple and <em>Fast API</em> to Fetch JavaScript Web Pages"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://linkgrabs.com"}}, "_tags": ["story", "author_macinjosh", "story_40825997", "show_hn"], "author": "macinjosh", "children": [40843065, 40843108], "created_at": "2024-06-28T22:29:44Z", "created_at_i": 1719613784, "num_comments": 2, "objectID": "40825997", "points": 7, "story_id": 40825997, "story_text": "For some of my work with LLMs and RAG I needed to be able to fetch content from my customer&#x27;s support site. Basic cURL calls weren&#x27;t working because the site is an SPA and is rendered completely through javascript.<p>The trouble it took me to figure out how to capture the rendered HTML from a headless browser and how to do it reliably in production inspired me to create LinkGrabs.com so others would not have to go through the same pain.", "title": "Show HN: Linkgrabs.com the Simple and Fast API to Fetch JavaScript Web Pages", "updated_at": "2024-09-20T17:20:58Z", "url": "https://linkgrabs.com"}], "hitsPerPage": 15, "nbHits": 69, "nbPages": 5, "page": 0, "params": "query=fastapi+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 12, "processingTimingsMS": {"_request": {"roundTrip": 13}, "afterFetch": {"format": {"highlighting": 1, "total": 2}}, "fetch": {"query": 8, "scanning": 2, "total": 11}, "total": 12}, "query": "fastapi production", "serverTimeMS": 14}}