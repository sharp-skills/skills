{"d": {"exhaustive": {"nbHits": false, "typo": false}, "exhaustiveNbHits": false, "exhaustiveTypo": false, "hits": [{"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "RobTheFrog"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hey HN,<p>I built a REST API for capturing screenshots and generating PDFs from URLs.<p>Why: I got tired of managing Puppeteer/<em>Playwright</em> in <em>production</em>. Memory leaks, zombie processes, Docker issues. So I wrapped it in an API.<p><pre><code>  Stack:\n  - Node.js + Fastify\n  - <em>Playwright</em> (more stable than Puppeteer in my experience)\n  - Self-hosted on Hetzner\n\n  Features:\n  - Screenshots: PNG, JPEG, WebP\n  - PDFs: A4, Letter, custom sizes\n  - Full-page capture, custom viewports\n  - Cookie consent auto-accept\n  - Lazy-load handling\n</code></pre>\nAPI example:<p><pre><code>      curl -X POST &quot;https://www.screencraftapi.com/api/v1/screenshots&quot; \\\n        -H &quot;Authorization: Bearer YOUR_KEY&quot; \\\n        -d '{&quot;url&quot;: &quot;https://example.com&quot;}'\n\n  Free tier: 250 requests/month\n  Docs: https://www.screencraftapi.com/docs\n</code></pre>\nWould love technical feedback. What's missing? What would make this useful for your projects?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: ScreenCraft \u2013 Screenshot and PDF API Without the Puppeteer Headaches"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://screencraftapi.com/"}}, "_tags": ["story", "author_RobTheFrog", "story_46428944", "show_hn"], "author": "RobTheFrog", "children": [46428966], "created_at": "2025-12-30T02:46:02Z", "created_at_i": 1767062762, "num_comments": 4, "objectID": "46428944", "points": 5, "story_id": 46428944, "story_text": "Hey HN,<p>I built a REST API for capturing screenshots and generating PDFs from URLs.<p>Why: I got tired of managing Puppeteer&#x2F;Playwright in production. Memory leaks, zombie processes, Docker issues. So I wrapped it in an API.<p><pre><code>  Stack:\n  - Node.js + Fastify\n  - Playwright (more stable than Puppeteer in my experience)\n  - Self-hosted on Hetzner\n\n  Features:\n  - Screenshots: PNG, JPEG, WebP\n  - PDFs: A4, Letter, custom sizes\n  - Full-page capture, custom viewports\n  - Cookie consent auto-accept\n  - Lazy-load handling\n</code></pre>\nAPI example:<p><pre><code>      curl -X POST &quot;https:&#x2F;&#x2F;www.screencraftapi.com&#x2F;api&#x2F;v1&#x2F;screenshots&quot; \\\n        -H &quot;Authorization: Bearer YOUR_KEY&quot; \\\n        -d &#x27;{&quot;url&quot;: &quot;https:&#x2F;&#x2F;example.com&quot;}&#x27;\n\n  Free tier: 250 requests&#x2F;month\n  Docs: https:&#x2F;&#x2F;www.screencraftapi.com&#x2F;docs\n</code></pre>\nWould love technical feedback. What&#x27;s missing? What would make this useful for your projects?", "title": "Show HN: ScreenCraft \u2013 Screenshot and PDF API Without the Puppeteer Headaches", "updated_at": "2026-01-08T23:26:04Z", "url": "https://screencraftapi.com/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ricardodevelop"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hello everyone,<p>I\u2019ve created this starter project for creating <em>production</em> ready web apps in Vite and React that I hope some might find useful.<p>This template came about as a necessity to provide some standardization across new projects at work. A few of the initial goals when creating this project were to:<p>- Reduce setup time<p>- Standardize codebase with ESLint and Prettier<p>- Improve commit messages with tools like husky, commitizen and commitlint<p>- Improve codebase maintainability and scalability by providing a reasonable folder structure<p>- Simplify React Component development through use of tools like Storybook<p>- Improving codebase stability with unit and E2E tests via Vitest + React Testing Library and <em>Playwright</em> respectively<p>- Ease the deployment process by providing a simple starter Dockerfile<p>In addition to all the aforementioned goals, I also wanted to use modern tools such as React Query + Zustand for state management, React Hook Form + Zod for creating and validating forms, Tailwind CSS for building out UI\u2019s, etc.<p>I tried to cover everything I, and others, might need but recognize that everyones requirements are different. Luckily, this isn\u2019t a framework so removing unneeded packages or adding new ones is as simple it would normally be. The project itself doesn\u2019t come with a demo as its purpose is to simply provide a foundation for any new projects you might have in mind.<p>Feedback is always welcome and I appreciate anyone willing to checkout this project.<p>Thank you and have a great day."}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["production"], "value": "Show HN: Vite React Boilerplate \u2013 A <em>Production</em> Ready, Scalable Starter Template"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/RicardoValdovinos/vite-react-boilerplate"}}, "_tags": ["story", "author_ricardodevelop", "story_36926095", "show_hn"], "author": "ricardodevelop", "children": [36927690, 36928003], "created_at": "2023-07-29T23:49:15Z", "created_at_i": 1690674555, "num_comments": 5, "objectID": "36926095", "points": 18, "story_id": 36926095, "story_text": "Hello everyone,<p>I\u2019ve created this starter project for creating production ready web apps in Vite and React that I hope some might find useful.<p>This template came about as a necessity to provide some standardization across new projects at work. A few of the initial goals when creating this project were to:<p>- Reduce setup time<p>- Standardize codebase with ESLint and Prettier<p>- Improve commit messages with tools like husky, commitizen and commitlint<p>- Improve codebase maintainability and scalability by providing a reasonable folder structure<p>- Simplify React Component development through use of tools like Storybook<p>- Improving codebase stability with unit and E2E tests via Vitest + React Testing Library and Playwright respectively<p>- Ease the deployment process by providing a simple starter Dockerfile<p>In addition to all the aforementioned goals, I also wanted to use modern tools such as React Query + Zustand for state management, React Hook Form + Zod for creating and validating forms, Tailwind CSS for building out UI\u2019s, etc.<p>I tried to cover everything I, and others, might need but recognize that everyones requirements are different. Luckily, this isn\u2019t a framework so removing unneeded packages or adding new ones is as simple it would normally be. The project itself doesn\u2019t come with a demo as its purpose is to simply provide a foundation for any new projects you might have in mind.<p>Feedback is always welcome and I appreciate anyone willing to checkout this project.<p>Thank you and have a great day.", "title": "Show HN: Vite React Boilerplate \u2013 A Production Ready, Scalable Starter Template", "updated_at": "2024-09-20T14:45:12Z", "url": "https://github.com/RicardoValdovinos/vite-react-boilerplate"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "acenji"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "I kept running into the same problems across web projects \u2014 JS errors silently breaking pages, no visibility into \n  per-page SEO or performance, and Lighthouse only auditing one URL at a time. So I built ScaleLighthouse.<p><pre><code>  It's a config-driven platform that runs Lighthouse audits, <em>Playwright</em> smoke tests (JS errors, missing DOM         \n  elements, broken flows), and CrUX real-user metrics across your entire site. Works on localhost, staging, or      \n  <em>production</em> \u2014 including authenticated pages. No test code to write.                                                \n                                                                                                                    \n  pnpm monorepo, Vue 3 dashboard, MIT licensed.</code></pre>"}, "title": {"fullyHighlighted": false, "matchLevel": "partial", "matchedWords": ["playwright"], "value": "Show HN: ScaleLighthouse \u2013 Bulk Lighthouse, <em>Playwright</em> smoke tests, CrUX metrics"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/acenji/lighthouse"}}, "_tags": ["story", "author_acenji", "story_46770358", "show_hn"], "author": "acenji", "created_at": "2026-01-26T19:30:21Z", "created_at_i": 1769455821, "num_comments": 0, "objectID": "46770358", "points": 1, "story_id": 46770358, "story_text": "I kept running into the same problems across web projects \u2014 JS errors silently breaking pages, no visibility into \n  per-page SEO or performance, and Lighthouse only auditing one URL at a time. So I built ScaleLighthouse.<p><pre><code>  It&#x27;s a config-driven platform that runs Lighthouse audits, Playwright smoke tests (JS errors, missing DOM         \n  elements, broken flows), and CrUX real-user metrics across your entire site. Works on localhost, staging, or      \n  production \u2014 including authenticated pages. No test code to write.                                                \n                                                                                                                    \n  pnpm monorepo, Vue 3 dashboard, MIT licensed.</code></pre>", "title": "Show HN: ScaleLighthouse \u2013 Bulk Lighthouse, Playwright smoke tests, CrUX metrics", "updated_at": "2026-01-26T19:35:11Z", "url": "https://github.com/acenji/lighthouse"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "jancurn"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hey HN,<p>This is Jan, founder of Apify, a web scraping and automation platform. Drawing on our team's years of experience, today we're launching Crawlee [1], the web scraping and browser automation library for Node.js that's designed for the fastest development and maximum reliability in <em>production</em>.<p>For details, see the short video [2] or read the announcement blog post [3].<p>Main features:<p>-  Supports headless browsers with <em>Playwright</em> or Puppeteer<p>-  Supports raw HTTP crawling with Cheerio or JSDOM<p>-  Automated parallelization and scaling of crawlers for best performance<p>-  Avoids blocking using smart sessions, proxies, and browser fingerprints<p>-  Simple management and persistence of queues of URLs to crawl<p>-  Written completely in TypeScript for type safety and code autocompletion<p>-  Comprehensive documentation, code examples, and tutorials<p>-  Actively maintained and developed by Apify\u2014we use it ourselves!<p>-  Lively community on Discord<p>To get started, visit <a href=\"https://crawlee.dev\" rel=\"nofollow\">https://crawlee.dev</a> or run the following command: npx crawlee create my-crawler<p>If you have any questions or comments, our team will be happy to answer them here.<p>[1] <a href=\"https://crawlee.dev/\" rel=\"nofollow\">https://crawlee.dev/</a><p>[2] <a href=\"https://www.youtube.com/watch?v=g1Ll9OlFwEQ\" rel=\"nofollow\">https://www.youtube.com/watch?v=g1Ll9OlFwEQ</a><p>[3] <a href=\"https://blog.apify.com/announcing-crawlee-the-web-scraping-and-browser-automation-library/\" rel=\"nofollow\">https://blog.apify.com/announcing-crawlee-the-web-scraping-a...</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Crawlee \u2013 Web scraping and browser automation library for Node.js"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://crawlee.dev/"}}, "_tags": ["story", "author_jancurn", "story_32561127", "show_hn"], "author": "jancurn", "children": [32561219, 32561435, 32561621, 32561804, 32561840, 32561982, 32562035, 32562129, 32562206, 32562216, 32562238, 32562400, 32562887, 32563071, 32563337, 32564004, 32564084, 32564711, 32565174, 32566021, 32566243, 32567025, 32567953, 32568318, 32569492, 32570745, 32583079], "created_at": "2022-08-23T06:25:39Z", "created_at_i": 1661235939, "num_comments": 80, "objectID": "32561127", "points": 282, "story_id": 32561127, "story_text": "Hey HN,<p>This is Jan, founder of Apify, a web scraping and automation platform. Drawing on our team&#x27;s years of experience, today we&#x27;re launching Crawlee [1], the web scraping and browser automation library for Node.js that&#x27;s designed for the fastest development and maximum reliability in production.<p>For details, see the short video [2] or read the announcement blog post [3].<p>Main features:<p>-  Supports headless browsers with Playwright or Puppeteer<p>-  Supports raw HTTP crawling with Cheerio or JSDOM<p>-  Automated parallelization and scaling of crawlers for best performance<p>-  Avoids blocking using smart sessions, proxies, and browser fingerprints<p>-  Simple management and persistence of queues of URLs to crawl<p>-  Written completely in TypeScript for type safety and code autocompletion<p>-  Comprehensive documentation, code examples, and tutorials<p>-  Actively maintained and developed by Apify\u2014we use it ourselves!<p>-  Lively community on Discord<p>To get started, visit <a href=\"https:&#x2F;&#x2F;crawlee.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;crawlee.dev</a> or run the following command: npx crawlee create my-crawler<p>If you have any questions or comments, our team will be happy to answer them here.<p>[1] <a href=\"https:&#x2F;&#x2F;crawlee.dev&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;crawlee.dev&#x2F;</a><p>[2] <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=g1Ll9OlFwEQ\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=g1Ll9OlFwEQ</a><p>[3] <a href=\"https:&#x2F;&#x2F;blog.apify.com&#x2F;announcing-crawlee-the-web-scraping-and-browser-automation-library&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;blog.apify.com&#x2F;announcing-crawlee-the-web-scraping-a...</a>", "title": "Show HN: Crawlee \u2013 Web scraping and browser automation library for Node.js", "updated_at": "2025-07-24T17:40:33Z", "url": "https://crawlee.dev/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "mpapazian"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hi HN, we're Marc and Matt, and we're building Propolis (app.propolis.tech/#/launch). We use browser agents to simulate users in order to report bugs and write e2e tests. Today, you can launch 10s-100s of agents that collaboratively explore a website and report back on pain points + propose e2e tests that can run as part of your CI.<p>You can try an initial run (two minute set up) to get a feel for the product for free here: app.propolis.tech/#/launch. Or watch our demo video: <a href=\"https://www.tella.tv/video/autonomous-qa-system-walkthrough-3s4e\">https://www.tella.tv/video/autonomous-qa-system-walkthrough-...</a><p>The Problem<p>Both Matt and I have been thinking about software quality for the last 10 years. While at Airtable Matt worked on the infrastructure team responsible for deploys and thought a lot about how to catch bugs before users did. Deterministic tests are incredibly effective at ensuring pre-defined behavior continues to function, but it's hard to get meaningful coverage &amp; easy to &quot;stub/mock&quot; so much that it's no longer representative of real usage.<p>I like to pitch what we're building now as a set of \u201cusers\u201d you can treat like a canary group without worrying about impacting real users.<p>What we do: \nPropolis runs &quot;swarms&quot; of browser agents that collaborate to come up with user journeys, flag points of friction, and propose e2e tests that can then be run more cheaply on any trigger you'd like. We have customers from public companies to startups running &quot;swarms&quot; regularly to massively increase the breadth of their automated testing + running the produced tests as part of their CI pipeline to ensure that more specific flows stay working without needing to worry about updating <em>playwright</em>/selenium tests.<p>One thing that really excites me about this approach is how flexible &quot;checks&quot; can be since they're evaluated partially via LLM, for example we've caught bugs related to the quality of non-deterministic output (think a shopping assistant recommending a product that the user then searches for and can\u2019t find).<p>Pricing and Availability<p>It's <em>production</em>-ready today at $1000/month unlimited-use + active support for early users willing to give feedback and request features. We're also happy to work with you for capped-use / hobby plans at lower prices if you'd like to use it for smaller or personal projects.<p>We'd love to hear from the HN community - especially curious if folks have thoughts on what else autonomous agents could validate beyond bugs and functional correctness. Try it out and let us know what you think!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Propolis (YC X25) \u2013 Browser agents that QA your web app autonomously"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://app.propolis.tech/#/launch"}}, "_tags": ["story", "author_mpapazian", "story_45762012", "launch_hn"], "author": "mpapazian", "children": [45762067, 45762218, 45762662, 45762883, 45763608, 45763872, 45763992, 45764103, 45764531, 45765302, 45765480, 45766803, 45767669, 45768089, 45771454, 45837173], "created_at": "2025-10-30T16:40:02Z", "created_at_i": 1761842402, "num_comments": 37, "objectID": "45762012", "points": 116, "story_id": 45762012, "story_text": "Hi HN, we&#x27;re Marc and Matt, and we&#x27;re building Propolis (app.propolis.tech&#x2F;#&#x2F;launch). We use browser agents to simulate users in order to report bugs and write e2e tests. Today, you can launch 10s-100s of agents that collaboratively explore a website and report back on pain points + propose e2e tests that can run as part of your CI.<p>You can try an initial run (two minute set up) to get a feel for the product for free here: app.propolis.tech&#x2F;#&#x2F;launch. Or watch our demo video: <a href=\"https:&#x2F;&#x2F;www.tella.tv&#x2F;video&#x2F;autonomous-qa-system-walkthrough-3s4e\">https:&#x2F;&#x2F;www.tella.tv&#x2F;video&#x2F;autonomous-qa-system-walkthrough-...</a><p>The Problem<p>Both Matt and I have been thinking about software quality for the last 10 years. While at Airtable Matt worked on the infrastructure team responsible for deploys and thought a lot about how to catch bugs before users did. Deterministic tests are incredibly effective at ensuring pre-defined behavior continues to function, but it&#x27;s hard to get meaningful coverage &amp; easy to &quot;stub&#x2F;mock&quot; so much that it&#x27;s no longer representative of real usage.<p>I like to pitch what we&#x27;re building now as a set of \u201cusers\u201d you can treat like a canary group without worrying about impacting real users.<p>What we do: \nPropolis runs &quot;swarms&quot; of browser agents that collaborate to come up with user journeys, flag points of friction, and propose e2e tests that can then be run more cheaply on any trigger you&#x27;d like. We have customers from public companies to startups running &quot;swarms&quot; regularly to massively increase the breadth of their automated testing + running the produced tests as part of their CI pipeline to ensure that more specific flows stay working without needing to worry about updating playwright&#x2F;selenium tests.<p>One thing that really excites me about this approach is how flexible &quot;checks&quot; can be since they&#x27;re evaluated partially via LLM, for example we&#x27;ve caught bugs related to the quality of non-deterministic output (think a shopping assistant recommending a product that the user then searches for and can\u2019t find).<p>Pricing and Availability<p>It&#x27;s production-ready today at $1000&#x2F;month unlimited-use + active support for early users willing to give feedback and request features. We&#x27;re also happy to work with you for capped-use &#x2F; hobby plans at lower prices if you&#x27;d like to use it for smaller or personal projects.<p>We&#x27;d love to hear from the HN community - especially curious if folks have thoughts on what else autonomous agents could validate beyond bugs and functional correctness. Try it out and let us know what you think!", "title": "Launch HN: Propolis (YC X25) \u2013 Browser agents that QA your web app autonomously", "updated_at": "2025-11-13T15:00:16Z", "url": "https://app.propolis.tech/#/launch"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "adam_gyroscope"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Here\u2019s how we\u2019re working with LLMs at my startup.<p>We have a monorepo with scheduled Python data workflows, two Next.js apps, and a small engineering team. We use GitHub for SCM and CI/CD, deploy to GCP and Vercel, and lean heavily on automation.<p>Local development:\nEvery engineer gets Cursor Pro (plus Bugbot), Gemini Pro, OpenAI Pro, and optionally Claude Pro. We don\u2019t really care which model people use. In practice, LLMs are worth about 1.5 excellent junior/mid-level engineers per engineer, so paying for multiple models is easily worth it.<p>We rely heavily on pre-commit hooks: ty, ruff, TypeScript checks, tests across all languages, formatting, and other guards. Everything is auto-formatted. LLMs make types and tests much easier to write, though complex typing still needs some hand-holding.<p>GitHub + Copilot workflow:\nWe pay for GitHub Enterprise primarily because it allows assigning issues to Copilot, which then opens a PR. Our rule is simple: if you open an issue, you assign it to Copilot. Every issue gets a code attempt attached to it.<p>There\u2019s no stigma around lots of PRs. We frequently delete ones we don\u2019t use.<p>We use Turborepo for the monorepo and are fully uv on the Python side.<p>All coding practices are encoded in .cursor/rules files. For example: \u201cIf you are doing database work, only edit Drizzle\u2019s schema.ts and don\u2019t hand-write SQL.\u201d Cursor generally respects this, but other tools struggle to consistently read or follow these rules no matter how many agent.md-style files we add.<p>My personal dev loop:\nIf I\u2019m on the go and see a bug or have an idea, I open a GitHub issue (via Slack, mobile, or web) and assign it to Copilot. Sometimes the issue is detailed; sometimes a single sentence. Copilot opens a PR, and I review it later.<p>If I\u2019m at the keyboard, I start in Cursor as an agent in a Git worktree, using whatever the best model is. I iterate until I\u2019m happy, ask the LLM to write tests, review everything, and push to GitHub. Before a human review, I let Cursor Bugbot, Copilot, and GitHub CodeQL review the code, and ask Copilot to fix anything they flag.<p>Things that are still painful:\nTo really know if code works, I need to run Temporal, two Next.js apps, several Python workers, and a Node worker. Some of this is Dockerized, some isn\u2019t. Then I need a browser to run manual checks.<p>AFAICT, there\u2019s no service that lets me: give a prompt, write the code, spin up all this infra, run <em>Playwright</em>, handle database migrations, and let me manually poke at the system. We approximate this with GitHub Actions, but that doesn\u2019t help with manual verification or DB work.<p>Copilot doesn\u2019t let you choose a model when assigning an issue or during code review. The model it uses is generally bad. You can pick a model in Copilot chat, but not in issues, PRs or reviews.<p>Cursor + worktrees + agents suck. Worktrees clone from the source repo including unstaged files, so if you want a clean agent environment, your main repo has to be clean. At times it feels simpler to just clone the repo into a new directory instead of using worktrees.<p>What\u2019s working well:\nBecause we constantly spin up agents, our monorepo setup scripts are well-tested and reliable. They also translate cleanly into CI/CD.<p>Roughly 25% of \u201copen issue \u2192 Copilot PR\u201d results are mergeable as-is. That\u2019s not amazing, but better than zero, and it gets to ~50% with a few comments. This would be higher if Copilot followed our setup instructions more reliably or let us use stronger models.<p>Overall, for roughly $1k/month, we\u2019re getting the equivalent of 1.5 additional junior/mid engineers per engineer. Those \u201cLLM engineers\u201d always write tests, follow standards, produce good commit messages, and work 24/7. There\u2019s friction in reviewing and context-switching across agents, but it\u2019s manageable.<p>What are you doing for vibe coding in a <em>production</em> system?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: How are you LLM-coding in an established code base?"}}, "_tags": ["story", "author_adam_gyroscope", "story_46292682", "ask_hn"], "author": "adam_gyroscope", "children": [46294747, 46294779, 46295422, 46307422, 46330999, 46331023, 46331073, 46331080, 46331132, 46331161, 46331170, 46331255, 46331263, 46331367, 46331493, 46331549, 46331691, 46331805, 46331868, 46331893, 46331919, 46331988, 46332013, 46332047, 46332068, 46332139, 46334444, 46334841, 46335104], "created_at": "2025-12-16T18:54:37Z", "created_at_i": 1765911277, "num_comments": 66, "objectID": "46292682", "points": 70, "story_id": 46292682, "story_text": "Here\u2019s how we\u2019re working with LLMs at my startup.<p>We have a monorepo with scheduled Python data workflows, two Next.js apps, and a small engineering team. We use GitHub for SCM and CI&#x2F;CD, deploy to GCP and Vercel, and lean heavily on automation.<p>Local development:\nEvery engineer gets Cursor Pro (plus Bugbot), Gemini Pro, OpenAI Pro, and optionally Claude Pro. We don\u2019t really care which model people use. In practice, LLMs are worth about 1.5 excellent junior&#x2F;mid-level engineers per engineer, so paying for multiple models is easily worth it.<p>We rely heavily on pre-commit hooks: ty, ruff, TypeScript checks, tests across all languages, formatting, and other guards. Everything is auto-formatted. LLMs make types and tests much easier to write, though complex typing still needs some hand-holding.<p>GitHub + Copilot workflow:\nWe pay for GitHub Enterprise primarily because it allows assigning issues to Copilot, which then opens a PR. Our rule is simple: if you open an issue, you assign it to Copilot. Every issue gets a code attempt attached to it.<p>There\u2019s no stigma around lots of PRs. We frequently delete ones we don\u2019t use.<p>We use Turborepo for the monorepo and are fully uv on the Python side.<p>All coding practices are encoded in .cursor&#x2F;rules files. For example: \u201cIf you are doing database work, only edit Drizzle\u2019s schema.ts and don\u2019t hand-write SQL.\u201d Cursor generally respects this, but other tools struggle to consistently read or follow these rules no matter how many agent.md-style files we add.<p>My personal dev loop:\nIf I\u2019m on the go and see a bug or have an idea, I open a GitHub issue (via Slack, mobile, or web) and assign it to Copilot. Sometimes the issue is detailed; sometimes a single sentence. Copilot opens a PR, and I review it later.<p>If I\u2019m at the keyboard, I start in Cursor as an agent in a Git worktree, using whatever the best model is. I iterate until I\u2019m happy, ask the LLM to write tests, review everything, and push to GitHub. Before a human review, I let Cursor Bugbot, Copilot, and GitHub CodeQL review the code, and ask Copilot to fix anything they flag.<p>Things that are still painful:\nTo really know if code works, I need to run Temporal, two Next.js apps, several Python workers, and a Node worker. Some of this is Dockerized, some isn\u2019t. Then I need a browser to run manual checks.<p>AFAICT, there\u2019s no service that lets me: give a prompt, write the code, spin up all this infra, run Playwright, handle database migrations, and let me manually poke at the system. We approximate this with GitHub Actions, but that doesn\u2019t help with manual verification or DB work.<p>Copilot doesn\u2019t let you choose a model when assigning an issue or during code review. The model it uses is generally bad. You can pick a model in Copilot chat, but not in issues, PRs or reviews.<p>Cursor + worktrees + agents suck. Worktrees clone from the source repo including unstaged files, so if you want a clean agent environment, your main repo has to be clean. At times it feels simpler to just clone the repo into a new directory instead of using worktrees.<p>What\u2019s working well:\nBecause we constantly spin up agents, our monorepo setup scripts are well-tested and reliable. They also translate cleanly into CI&#x2F;CD.<p>Roughly 25% of \u201copen issue \u2192 Copilot PR\u201d results are mergeable as-is. That\u2019s not amazing, but better than zero, and it gets to ~50% with a few comments. This would be higher if Copilot followed our setup instructions more reliably or let us use stronger models.<p>Overall, for roughly $1k&#x2F;month, we\u2019re getting the equivalent of 1.5 additional junior&#x2F;mid engineers per engineer. Those \u201cLLM engineers\u201d always write tests, follow standards, produce good commit messages, and work 24&#x2F;7. There\u2019s friction in reviewing and context-switching across agents, but it\u2019s manageable.<p>What are you doing for vibe coding in a production system?", "title": "Ask HN: How are you LLM-coding in an established code base?", "updated_at": "2026-01-27T04:05:40Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "marcon680"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hi HN! We\u2019re Marco and Shreya, founders of Simplex (<a href=\"https://www.simplex.sh/\">https://www.simplex.sh/</a>). We\u2019re building all the infrastructure you need for modern browser automation \u2013 including remote browsers, steerable web agents, and more.<p>Here\u2019s a demo: <a href=\"https://youtu.be/7KpWJbOcm1Y\" rel=\"nofollow\">https://youtu.be/7KpWJbOcm1Y</a><p>We\u2019re excited to be posting on HN again! Back in January, we Show HN\u2019d the earliest version of Simplex (<a href=\"https://news.ycombinator.com/item?id=42704160\">https://news.ycombinator.com/item?id=42704160</a>). We\u2019ve now spent close to a year working with real customers, forward-deploying into their codebases, and building web agent systems for them from the ground up to understand what it takes to get agents working in <em>production</em>.<p>We built Simplex because we started seeing a pattern: companies would initially roll their own <em>Playwright</em>/Stagehand web automation solutions. This worked fine in the early prototype stages, but they\u2019d quickly get overwhelmed with technical challenges as they productionized automations across all the websites their customers use.<p>As they scaled, they\u2019d have to build and manage:<p>- Chrome infrastructure: You'll need remote browsers, extension support, browser settings for anti-bot detection/stealth, and a hundred more small fixes.<p>- DOM parsing: We\u2019ve seen many web portals have really weird quirks (nested iframes, shadow DOM elements, dynamic loading, popups, unstable selectors, etc..) that are hard to parse with traditional/existing browser agents.<p>- Agent context engineering: Website state, user prompts, system prompts, past actions all take up a massive amount of context. Without managing this, agents can get caught in loops or take wrong actions.<p>- Caching/reliability: No matter how perfect your prompts are, it\u2019s hard to guarantee consistency without caching/deterministic actions.<p>- Login/2FA: Solve captcha, fetch 2FA from email/text/Google Auth, encrypt/decrypt credentials to access portals blocked by login.<p>- Automation management: You\u2019ll have to store all your prompts, scrapers, and agents, and find a way to make them reusable if you have the same workflows across different portals.<p>- User interface: Creating new workflows + debugging can take time. You\u2019ll have to find easy ways to expose this to your engineers to make the process more efficient when you have hundreds of automations to build.<p>Simplex is a proper solution that handles all of the above for you. We offer both an UI/dashboard (which is what we use even as technical developers) and an extensive API for customers who are using Simplex in their existing AI agents. Our dashboard/API docs are here: <a href=\"https://simplex.sh/docs\">https://simplex.sh/docs</a>. We\u2019d love for you to check them out!<p>You can get started for free with Simplex at (<a href=\"https://www.simplex.sh/\">https://www.simplex.sh/</a>) (you have to register to prevent abuse since we\u2019re giving you a remote browser that connects to the internet).<p>Our first users have been AI companies across different industries like accounting, logistics/transportation, customer service, and healthtech. We\u2019ve seen them:<p>- Fill out prior authorization forms on medical provider portals<p>- Download hundreds of PDFs from grocer portals across the US<p>- Automate and scrape structured data from traditional ERPs like NetSuite<p>- Submit bids/shipments on logistics/TMS portals<p>- Scrape lawyer/doctor license information across public government portals<p>- And more!<p>We\u2019re excited to see more use cases as we open up the platform \u2013 this is our first time doing self-serve.<p>Wanted to end with a quick thank you to HN. The feedback on our first Show HN gave us confidence to steer our product in this direction, and has deeply shaped the last year of our lives. We\u2019d love feedback, especially from anyone who\u2019s tried solving this problem or built similar tools.<p>Happy to answer questions and looking forward to your comments!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Launch HN: Simplex (YC S24) \u2013 Browser automation platform for developers"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.simplex.sh/"}}, "_tags": ["story", "author_marcon680", "story_45451547", "launch_hn"], "author": "marcon680", "children": [45452081, 45452146, 45452473, 45452726, 45452751, 45452839, 45452932, 45454555, 45456113, 45459596, 45487214], "created_at": "2025-10-02T16:07:23Z", "created_at_i": 1759421243, "num_comments": 28, "objectID": "45451547", "points": 54, "story_id": 45451547, "story_text": "Hi HN! We\u2019re Marco and Shreya, founders of Simplex (<a href=\"https:&#x2F;&#x2F;www.simplex.sh&#x2F;\">https:&#x2F;&#x2F;www.simplex.sh&#x2F;</a>). We\u2019re building all the infrastructure you need for modern browser automation \u2013 including remote browsers, steerable web agents, and more.<p>Here\u2019s a demo: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;7KpWJbOcm1Y\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;7KpWJbOcm1Y</a><p>We\u2019re excited to be posting on HN again! Back in January, we Show HN\u2019d the earliest version of Simplex (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42704160\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42704160</a>). We\u2019ve now spent close to a year working with real customers, forward-deploying into their codebases, and building web agent systems for them from the ground up to understand what it takes to get agents working in production.<p>We built Simplex because we started seeing a pattern: companies would initially roll their own Playwright&#x2F;Stagehand web automation solutions. This worked fine in the early prototype stages, but they\u2019d quickly get overwhelmed with technical challenges as they productionized automations across all the websites their customers use.<p>As they scaled, they\u2019d have to build and manage:<p>- Chrome infrastructure: You&#x27;ll need remote browsers, extension support, browser settings for anti-bot detection&#x2F;stealth, and a hundred more small fixes.<p>- DOM parsing: We\u2019ve seen many web portals have really weird quirks (nested iframes, shadow DOM elements, dynamic loading, popups, unstable selectors, etc..) that are hard to parse with traditional&#x2F;existing browser agents.<p>- Agent context engineering: Website state, user prompts, system prompts, past actions all take up a massive amount of context. Without managing this, agents can get caught in loops or take wrong actions.<p>- Caching&#x2F;reliability: No matter how perfect your prompts are, it\u2019s hard to guarantee consistency without caching&#x2F;deterministic actions.<p>- Login&#x2F;2FA: Solve captcha, fetch 2FA from email&#x2F;text&#x2F;Google Auth, encrypt&#x2F;decrypt credentials to access portals blocked by login.<p>- Automation management: You\u2019ll have to store all your prompts, scrapers, and agents, and find a way to make them reusable if you have the same workflows across different portals.<p>- User interface: Creating new workflows + debugging can take time. You\u2019ll have to find easy ways to expose this to your engineers to make the process more efficient when you have hundreds of automations to build.<p>Simplex is a proper solution that handles all of the above for you. We offer both an UI&#x2F;dashboard (which is what we use even as technical developers) and an extensive API for customers who are using Simplex in their existing AI agents. Our dashboard&#x2F;API docs are here: <a href=\"https:&#x2F;&#x2F;simplex.sh&#x2F;docs\">https:&#x2F;&#x2F;simplex.sh&#x2F;docs</a>. We\u2019d love for you to check them out!<p>You can get started for free with Simplex at (<a href=\"https:&#x2F;&#x2F;www.simplex.sh&#x2F;\">https:&#x2F;&#x2F;www.simplex.sh&#x2F;</a>) (you have to register to prevent abuse since we\u2019re giving you a remote browser that connects to the internet).<p>Our first users have been AI companies across different industries like accounting, logistics&#x2F;transportation, customer service, and healthtech. We\u2019ve seen them:<p>- Fill out prior authorization forms on medical provider portals<p>- Download hundreds of PDFs from grocer portals across the US<p>- Automate and scrape structured data from traditional ERPs like NetSuite<p>- Submit bids&#x2F;shipments on logistics&#x2F;TMS portals<p>- Scrape lawyer&#x2F;doctor license information across public government portals<p>- And more!<p>We\u2019re excited to see more use cases as we open up the platform \u2013 this is our first time doing self-serve.<p>Wanted to end with a quick thank you to HN. The feedback on our first Show HN gave us confidence to steer our product in this direction, and has deeply shaped the last year of our lives. We\u2019d love feedback, especially from anyone who\u2019s tried solving this problem or built similar tools.<p>Happy to answer questions and looking forward to your comments!", "title": "Launch HN: Simplex (YC S24) \u2013 Browser automation platform for developers", "updated_at": "2025-10-09T11:55:42Z", "url": "https://www.simplex.sh/"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "ogandreakiro"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hey HN,<p>We\u2019ve been building browser agents for a while. In <em>production</em>, we kept converging on the same pattern: deterministic scripts for the happy path, agents only for edge cases. So we built Demonstrate Mode.<p>The idea is simple: You perform your workflow once in a remote browser. Notte records the interactions and generates deterministic automation code.<p>How it works:\n- Record clicks, inputs, navigations in a cloud browser\n- Compile them into deterministic code (no LLM at runtime)\n- Run and deploy on managed browser infrastructure<p>Closest analog is <em>Playwright</em> codegen but:\n- Infrastructure is handled (remote browsers, proxies, auth state)\n- Code runs in a deployable runtime with logs, retries, and optional agent fallback<p>Agents are great for prototyping and dynamic steps, but for <em>production</em> we usually want versioned code and predictable cost/behavior. Happy to dive into implementation details in the comments.<p>Demo: <a href=\"https://www.loom.com/share/f83cb83ecd5e48188dd9741724cde49a\" rel=\"nofollow\">https://www.loom.com/share/f83cb83ecd5e48188dd9741724cde49a</a><p>--\nAndrea &amp; Lucas,\nNotte Founders"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Build Web Automations via Demonstration"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://www.notte.cc/launch-week-i/demonstrate-mode"}}, "_tags": ["story", "author_ogandreakiro", "story_46779864", "show_hn"], "author": "ogandreakiro", "children": [46796780, 46797065, 46797385, 46797545, 46797624, 46798756, 46798834, 46801140], "created_at": "2026-01-27T13:48:30Z", "created_at_i": 1769521710, "num_comments": 20, "objectID": "46779864", "points": 36, "story_id": 46779864, "story_text": "Hey HN,<p>We\u2019ve been building browser agents for a while. In production, we kept converging on the same pattern: deterministic scripts for the happy path, agents only for edge cases. So we built Demonstrate Mode.<p>The idea is simple: You perform your workflow once in a remote browser. Notte records the interactions and generates deterministic automation code.<p>How it works:\n- Record clicks, inputs, navigations in a cloud browser\n- Compile them into deterministic code (no LLM at runtime)\n- Run and deploy on managed browser infrastructure<p>Closest analog is Playwright codegen but:\n- Infrastructure is handled (remote browsers, proxies, auth state)\n- Code runs in a deployable runtime with logs, retries, and optional agent fallback<p>Agents are great for prototyping and dynamic steps, but for production we usually want versioned code and predictable cost&#x2F;behavior. Happy to dive into implementation details in the comments.<p>Demo: <a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;f83cb83ecd5e48188dd9741724cde49a\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;f83cb83ecd5e48188dd9741724cde49a</a><p>--\nAndrea &amp; Lucas,\nNotte Founders", "title": "Show HN: Build Web Automations via Demonstration", "updated_at": "2026-02-01T11:56:44Z", "url": "https://www.notte.cc/launch-week-i/demonstrate-mode"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "pranav9"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hey HN,<p>I've been building Aether, a background agent that takes <em>production</em> errors from Sentry and attempts to turn them into verified pull requests.<p>When a new error hits your Sentry project:<p>1. Sentry webhook fires with the stack trace, breadcrumbs, and context\n2. Aether spins up an isolated Fly.io VM and clones the repo at the relevant commit\n3. Agent analyzes the stack trace, reproduces the issue, proposes a fix\n4. Starts the dev server, re-runs tests, and can verify the running app with <em>Playwright</em> (headless Chromium is pre-installed in every VM)\n5. A review pass evaluates the diff before a PR is opened\n6. Pushes to a feature branch and opens a GitHub PR, but only if verification succeeds\n7. If CI fails, it retries once with the failure logs. If it fails again, the task is marked failed. No infinite loops.<p>Why full VMs instead of worktrees? Each task runs in its own isolated machine with a real filesystem, real process model, real network stack. It can `npm install`, run a dev server on port 3000, and <em>Playwright</em> can hit `localhost:3000` because it's an actual environment, not a sandbox. Since each task is its own VM, preview URLs are exposed per task via a gateway proxy so you can inspect the running app while the agent works. VMs shut down shortly after the task completes.<p>There's a simple multi-agent setup: a solver proposes the fix, a review agent evaluates the diff, and the fix has to survive re-execution in a clean isolated environment before a PR gets opened. Not claiming formal guarantees here, just requiring the fix to actually execute successfully in a reproducible environment before it touches your repo.<p>Limitations:<p>- Works best on well-tested codebases where &quot;reproduce and verify&quot; is meaningful\n- If reproduction isn't deterministic, results degrade\n- CI retry is capped at one automatic attempt\n- Code review is model-driven, not an architectural enforcement layer\n- BYOK only, you bring your own API key via OpenRouter. No markup on model costs but it's not super cheap to run\n- Sentry integration is built but waiting on approval from Sentry, coming soon\n- CLI is also coming soon<p>Bug fixing is the main focus but it's built on top of a general-purpose background agents system that works today. The agent is still great at general coding tasks. You can give the agent tasks from a full web IDE with a code editor, terminal, file tree, and agent chat panel. CLI is coming soon too (`aether run &quot;add auth to the API&quot;`). Each task gets its own isolated VM with shareable preview URLs so you can hand someone a link to see exactly what the agent built. Similar to Cursor background agents but running in the cloud with full environment isolation instead of local worktrees.<p>Stack: Go API (Chi), Fly.io VMs, React 19 + Vite frontend, Bun workspace service inside each VM, Supabase for auth/db/realtime, <em>Playwright</em> + Chromium preinstalled on each VM.<p>Self-serve right now: GitHub OAuth, connect a repo, and go via the web IDE. Sentry and CLI coming soon.<p>Would value feedback from engineers who deal with <em>production</em> debugging regularly, or frequently use background agents. Where would this break, and what would make you trust it?<p>Landing page: <a href=\"https://www.runaether.dev\" rel=\"nofollow\">https://www.runaether.dev</a>\nTry it: <a href=\"https://app.runaether.dev\" rel=\"nofollow\">https://app.runaether.dev</a>"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Aether \u2013 Background agents that fix bugs in isolated VMs, opens PRs"}}, "_tags": ["story", "author_pranav9", "story_47081472", "show_hn"], "author": "pranav9", "children": [47081704, 47082242, 47082789, 47084861, 47094474], "created_at": "2026-02-19T23:43:54Z", "created_at_i": 1771544634, "num_comments": 6, "objectID": "47081472", "points": 8, "story_id": 47081472, "story_text": "Hey HN,<p>I&#x27;ve been building Aether, a background agent that takes production errors from Sentry and attempts to turn them into verified pull requests.<p>When a new error hits your Sentry project:<p>1. Sentry webhook fires with the stack trace, breadcrumbs, and context\n2. Aether spins up an isolated Fly.io VM and clones the repo at the relevant commit\n3. Agent analyzes the stack trace, reproduces the issue, proposes a fix\n4. Starts the dev server, re-runs tests, and can verify the running app with Playwright (headless Chromium is pre-installed in every VM)\n5. A review pass evaluates the diff before a PR is opened\n6. Pushes to a feature branch and opens a GitHub PR, but only if verification succeeds\n7. If CI fails, it retries once with the failure logs. If it fails again, the task is marked failed. No infinite loops.<p>Why full VMs instead of worktrees? Each task runs in its own isolated machine with a real filesystem, real process model, real network stack. It can `npm install`, run a dev server on port 3000, and Playwright can hit `localhost:3000` because it&#x27;s an actual environment, not a sandbox. Since each task is its own VM, preview URLs are exposed per task via a gateway proxy so you can inspect the running app while the agent works. VMs shut down shortly after the task completes.<p>There&#x27;s a simple multi-agent setup: a solver proposes the fix, a review agent evaluates the diff, and the fix has to survive re-execution in a clean isolated environment before a PR gets opened. Not claiming formal guarantees here, just requiring the fix to actually execute successfully in a reproducible environment before it touches your repo.<p>Limitations:<p>- Works best on well-tested codebases where &quot;reproduce and verify&quot; is meaningful\n- If reproduction isn&#x27;t deterministic, results degrade\n- CI retry is capped at one automatic attempt\n- Code review is model-driven, not an architectural enforcement layer\n- BYOK only, you bring your own API key via OpenRouter. No markup on model costs but it&#x27;s not super cheap to run\n- Sentry integration is built but waiting on approval from Sentry, coming soon\n- CLI is also coming soon<p>Bug fixing is the main focus but it&#x27;s built on top of a general-purpose background agents system that works today. The agent is still great at general coding tasks. You can give the agent tasks from a full web IDE with a code editor, terminal, file tree, and agent chat panel. CLI is coming soon too (`aether run &quot;add auth to the API&quot;`). Each task gets its own isolated VM with shareable preview URLs so you can hand someone a link to see exactly what the agent built. Similar to Cursor background agents but running in the cloud with full environment isolation instead of local worktrees.<p>Stack: Go API (Chi), Fly.io VMs, React 19 + Vite frontend, Bun workspace service inside each VM, Supabase for auth&#x2F;db&#x2F;realtime, Playwright + Chromium preinstalled on each VM.<p>Self-serve right now: GitHub OAuth, connect a repo, and go via the web IDE. Sentry and CLI coming soon.<p>Would value feedback from engineers who deal with production debugging regularly, or frequently use background agents. Where would this break, and what would make you trust it?<p>Landing page: <a href=\"https:&#x2F;&#x2F;www.runaether.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;www.runaether.dev</a>\nTry it: <a href=\"https:&#x2F;&#x2F;app.runaether.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;app.runaether.dev</a>", "title": "Show HN: Aether \u2013 Background agents that fix bugs in isolated VMs, opens PRs", "updated_at": "2026-02-21T21:50:06Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "galaxyeye"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hi HN,<p>I\u2019d like to share an open-source project we\u2019ve been working on for a while: <i>Browser4</i>.<p>The motivation came from a recurring frustration: most browser automation tools (<em>Playwright</em>, Selenium, Puppeteer) are excellent for <i>human-written scripts</i>, but start to show friction when used as a <i>core execution layer for AI agents</i> or at very high concurrency.<p>So instead of building \u201canother wrapper around <em>Playwright</em>\u201d, we experimented with a different direction:\n<i>designing a browser engine where AI agents are first-class citizens.</i><p>### What Browser4 is<p>Browser4 is a browser automation engine built on <i>native Chrome DevTools Protocol (CDP)</i>, with a focus on:<p>* <i>Coroutine-safe concurrency</i> (designed to run many browser sessions in parallel)<p>* <i>Agent-oriented APIs</i> (navigation, interaction, extraction as composable actions)<p>* <i>Hybrid extraction</i>: ML agent driven extraction + LLM extraction + structured selectors + an SQL-like DOM query language (X-SQL)<p>* <i>Low-level control</i> without <em>Playwright</em>-style abstraction overhead<p>It\u2019s written in <i>Kotlin/JVM</i>, mainly because we needed predictable concurrency behavior and long-running stability under load.<p>The project is fully open-source (Apache 2.0).<p>### What it\u2019s <i>not</i><p>* It\u2019s not a drop-in <em>Playwright</em> replacement.<p>* It\u2019s not a no-code RPA tool.<p>* It\u2019s not \u201cLLM magic\u201d \u2014 LLMs sit <i>outside</i> the browser engine.<p>Browser4 intentionally stays close to the browser execution layer and leaves planning/reasoning to external agent loops.<p>### Current use cases we\u2019re testing<p>* Large-scale web data extraction<p>* Agentic workflows (search \u2192 navigate \u2192 extract \u2192 summarize)<p>* Price / content monitoring with frequent revisits<p>* High-concurrency crawling where browser startup and context switching are bottlenecks<p>On a single machine, we can sustain <i>very high daily page visits</i>, though we\u2019re still validating benchmarks across different workloads.<p>### Open questions (where I\u2019d love feedback)<p>* For agentic systems, does it make sense to bypass <em>Playwright</em> entirely and work closer to CDP?<p>* Where do you see the biggest pain points when combining LLMs with browser automation today?<p>* Is JVM a reasonable choice here, or is Python still the better tradeoff despite concurrency limits<p>* What abstractions would <i>you</i> want in a browser engine built for AI agents?<p>### Links<p>* GitHub: <a href=\"https://github.com/platonai/browser4\" rel=\"nofollow\">https://github.com/platonai/browser4</a><p>* Website (light overview): <a href=\"https://browser4.io\" rel=\"nofollow\">https://browser4.io</a><p>Happy to answer technical questions or hear criticism \u2014 especially from people running browser automation or agent systems in <em>production</em>.<p>Thanks for reading."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Browser4 \u2013 an open-source browser engine for agents and concurrency"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/platonai/Browser4"}}, "_tags": ["story", "author_galaxyeye", "story_46252251", "show_hn"], "author": "galaxyeye", "children": [46252924, 46254130, 46271047], "created_at": "2025-12-13T05:25:54Z", "created_at_i": 1765603554, "num_comments": 6, "objectID": "46252251", "points": 7, "story_id": 46252251, "story_text": "Hi HN,<p>I\u2019d like to share an open-source project we\u2019ve been working on for a while: <i>Browser4</i>.<p>The motivation came from a recurring frustration: most browser automation tools (Playwright, Selenium, Puppeteer) are excellent for <i>human-written scripts</i>, but start to show friction when used as a <i>core execution layer for AI agents</i> or at very high concurrency.<p>So instead of building \u201canother wrapper around Playwright\u201d, we experimented with a different direction:\n<i>designing a browser engine where AI agents are first-class citizens.</i><p>### What Browser4 is<p>Browser4 is a browser automation engine built on <i>native Chrome DevTools Protocol (CDP)</i>, with a focus on:<p>* <i>Coroutine-safe concurrency</i> (designed to run many browser sessions in parallel)<p>* <i>Agent-oriented APIs</i> (navigation, interaction, extraction as composable actions)<p>* <i>Hybrid extraction</i>: ML agent driven extraction + LLM extraction + structured selectors + an SQL-like DOM query language (X-SQL)<p>* <i>Low-level control</i> without Playwright-style abstraction overhead<p>It\u2019s written in <i>Kotlin&#x2F;JVM</i>, mainly because we needed predictable concurrency behavior and long-running stability under load.<p>The project is fully open-source (Apache 2.0).<p>### What it\u2019s <i>not</i><p>* It\u2019s not a drop-in Playwright replacement.<p>* It\u2019s not a no-code RPA tool.<p>* It\u2019s not \u201cLLM magic\u201d \u2014 LLMs sit <i>outside</i> the browser engine.<p>Browser4 intentionally stays close to the browser execution layer and leaves planning&#x2F;reasoning to external agent loops.<p>### Current use cases we\u2019re testing<p>* Large-scale web data extraction<p>* Agentic workflows (search \u2192 navigate \u2192 extract \u2192 summarize)<p>* Price &#x2F; content monitoring with frequent revisits<p>* High-concurrency crawling where browser startup and context switching are bottlenecks<p>On a single machine, we can sustain <i>very high daily page visits</i>, though we\u2019re still validating benchmarks across different workloads.<p>### Open questions (where I\u2019d love feedback)<p>* For agentic systems, does it make sense to bypass Playwright entirely and work closer to CDP?<p>* Where do you see the biggest pain points when combining LLMs with browser automation today?<p>* Is JVM a reasonable choice here, or is Python still the better tradeoff despite concurrency limits<p>* What abstractions would <i>you</i> want in a browser engine built for AI agents?<p>### Links<p>* GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;platonai&#x2F;browser4\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;platonai&#x2F;browser4</a><p>* Website (light overview): <a href=\"https:&#x2F;&#x2F;browser4.io\" rel=\"nofollow\">https:&#x2F;&#x2F;browser4.io</a><p>Happy to answer technical questions or hear criticism \u2014 especially from people running browser automation or agent systems in production.<p>Thanks for reading.", "title": "Show HN: Browser4 \u2013 an open-source browser engine for agents and concurrency", "updated_at": "2025-12-23T06:00:18Z", "url": "https://github.com/platonai/Browser4"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "smother_mate68"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hey everyone,<p>We're excited to introduce MarinaBox, an open-source toolkit for creating isolated desktop/browser sandboxes tailored for AI agents.<p>Over the past few months, we've worked on various projects involving:<p>1.   AI agents interacting with computers (think Claude computer-use scenarios).<p>2.   Browser automation for AI agents using tools like <em>Playwright</em> and Selenium.<p>3.   Applications that need a live-session view to monitor AI agents' actions, with the ability for human-in-the-loop intervention.<p>What we learned: All these scenarios share a common need for robust infrastructure. So, we built MarinaBox to provide:<p>\u2022    Containerized Desktops/Browsers: Easily start and manage desktop/browser sessions in a containerized environment.<p>\u2022    Seamless Transition: Develop locally and host effortlessly on your cloud in <em>production</em>.<p>\u2022    SDK/CLI for Control: Native support for computer use, browser automation (<em>Playwright</em>/Selenium), and session management.<p>\u2022    Live-Session Embedding: Integrate a live view directly into your app, enabling human-in-the-loop interactions.<p>\u2022    Session Replays: Record and replay sessions with ease.\nCheck it out:<p>\u2022  Documentation:<a href=\"https://marinabox.mintlify.app/get-started/introduction\" rel=\"nofollow\">https://marinabox.mintlify.app/get-started/introduction</a>\nMain Repo:<a href=\"https://github.com/marinabox/marinabox\">https://github.com/marinabox/marinabox</a>\nSandbox Infra:<a href=\"https://github.com/marinabox/marinabox-sandbox\">https://github.com/marinabox/marinabox-sandbox</a><p>We\u2019ve worked hard to make the documentation detailed and developer-friendly. For any questions, feedback, or contributions:<p>\u2022    Discord: <a href=\"https://discord.gg/nAyFBSSU87\" rel=\"nofollow\">https://discord.gg/nAyFBSSU87</a>\n\u2022    Email: askmarinabox@gmail.com<p>Let us know what you think, and feel free to contribute or suggest ideas!<p>We built this in about 10 days and a large part of the code and docs were generated using AI. Let us know if something is wrong. We would love your feedback.<p>PS: The above version allows you to run locally. We are soon releasing self hosting on cloud."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: MarinaBox: Open-Source Sandbox Infra for AI Agents"}, "url": {"matchLevel": "none", "matchedWords": [], "value": "https://github.com/marinabox/marinabox"}}, "_tags": ["story", "author_smother_mate68", "story_42503627", "show_hn"], "author": "smother_mate68", "created_at": "2024-12-24T18:05:50Z", "created_at_i": 1735063550, "num_comments": 0, "objectID": "42503627", "points": 6, "story_id": 42503627, "story_text": "Hey everyone,<p>We&#x27;re excited to introduce MarinaBox, an open-source toolkit for creating isolated desktop&#x2F;browser sandboxes tailored for AI agents.<p>Over the past few months, we&#x27;ve worked on various projects involving:<p>1.   AI agents interacting with computers (think Claude computer-use scenarios).<p>2.   Browser automation for AI agents using tools like Playwright and Selenium.<p>3.   Applications that need a live-session view to monitor AI agents&#x27; actions, with the ability for human-in-the-loop intervention.<p>What we learned: All these scenarios share a common need for robust infrastructure. So, we built MarinaBox to provide:<p>\u2022    Containerized Desktops&#x2F;Browsers: Easily start and manage desktop&#x2F;browser sessions in a containerized environment.<p>\u2022    Seamless Transition: Develop locally and host effortlessly on your cloud in production.<p>\u2022    SDK&#x2F;CLI for Control: Native support for computer use, browser automation (Playwright&#x2F;Selenium), and session management.<p>\u2022    Live-Session Embedding: Integrate a live view directly into your app, enabling human-in-the-loop interactions.<p>\u2022    Session Replays: Record and replay sessions with ease.\nCheck it out:<p>\u2022  Documentation:<a href=\"https:&#x2F;&#x2F;marinabox.mintlify.app&#x2F;get-started&#x2F;introduction\" rel=\"nofollow\">https:&#x2F;&#x2F;marinabox.mintlify.app&#x2F;get-started&#x2F;introduction</a>\nMain Repo:<a href=\"https:&#x2F;&#x2F;github.com&#x2F;marinabox&#x2F;marinabox\">https:&#x2F;&#x2F;github.com&#x2F;marinabox&#x2F;marinabox</a>\nSandbox Infra:<a href=\"https:&#x2F;&#x2F;github.com&#x2F;marinabox&#x2F;marinabox-sandbox\">https:&#x2F;&#x2F;github.com&#x2F;marinabox&#x2F;marinabox-sandbox</a><p>We\u2019ve worked hard to make the documentation detailed and developer-friendly. For any questions, feedback, or contributions:<p>\u2022    Discord: <a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;nAyFBSSU87\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.gg&#x2F;nAyFBSSU87</a>\n\u2022    Email: askmarinabox@gmail.com<p>Let us know what you think, and feel free to contribute or suggest ideas!<p>We built this in about 10 days and a large part of the code and docs were generated using AI. Let us know if something is wrong. We would love your feedback.<p>PS: The above version allows you to run locally. We are soon releasing self hosting on cloud.", "title": "Show HN: MarinaBox: Open-Source Sandbox Infra for AI Agents", "updated_at": "2024-12-26T22:38:14Z", "url": "https://github.com/marinabox/marinabox"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "anon23432343"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "I got hired by a new big company in Sweden and I started to work for them last year.<p>I'm a senior front end developer working on a project that is the heart of the company. Everything is managed and done through the product we are developing.<p>So when I joined the team I was warned that its hard to work with some of the people and that other developers shortly left after they joined the team. For me this was okay since even if someone is an a-hole I can handle that. Thats not a problem.<p>One of the developers that is working for 5+ years on that project basically told me that they do whatever they feel like to do and that's why its fun for him/her to work on this project. He/She is an consultant. I thought to my self okay that's strange but then I thought okay maybe its fine because they also told me that the code is tested and commented and so on.<p>Then we had our first planning meeting for the next iteration. To say that nothing was clear to anybody except 3 developers who are working on that project for the longest time is an understatement. Nothing was clear not even how and when the handover to devops should happen. For a project that was running on <em>production</em> for a long time now.<p>I was assigned some minor bugs and tasks because I was told that the system is to complex to understand and it would take me years to understand it. When I asked what was so complex. I got the answer that everything is generic and the system is super flexible and that you can model anything in it.<p>So i started to fix bugs and look into the code more. The code base is a mess. The code is not readable at all. We are using Angular and we are working against the framework. We also have a mixed mono repo with angularjs and angular 15. I showed some code snippets to a friend if I'm the crazy one or is the code that bad? He and some other developers said its not me the code is a mess. We have no architecture. Everything is kind of half finished and self written. I pointed that out and some of them did not see a problem.<p>I then also started to push to work in an organized way and have better tickets in Jira and also clear requirements instead of making them up as we go. One developer who is also a freelancer then said to my manager that he will quit if we start to working in that way since he does not see the point but management is to scared to push back since this person has so much knowledge about the system in his had that they fear what would happen if he/she would go.<p>We now have also a record high of bugs and even this is cheated because some real bugs are moved to feature requests.<p>Everything is tested in e2e test with <em>playwright</em> because we don't need unit tests because with e2e test we are also testing the functions. So it takes forever to do anything.<p>The UX persons are thought of as people who waste time instead of getting things done. So the product without training for several weeks is not usable by any person even if you have the business knowledge.<p>Its hard to get anything done without new bugs.<p>Management does not know what to do because they are scared that the old developers will leave and then the system will break. New developers are leaving because they don't want to work on a broken system and team like that and I'm in the middle of it and can't push because then they will leave.<p>So I don't know what to do.<p>Maybe someone can share the experience and what he/she did?"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Ask HN: Joined a company and the project/team is splintered"}}, "_tags": ["story", "author_anon23432343", "story_34824806", "ask_hn"], "author": "anon23432343", "children": [34825282, 34825544, 34829869], "created_at": "2023-02-16T19:52:28Z", "created_at_i": 1676577148, "num_comments": 9, "objectID": "34824806", "points": 5, "story_id": 34824806, "story_text": "I got hired by a new big company in Sweden and I started to work for them last year.<p>I&#x27;m a senior front end developer working on a project that is the heart of the company. Everything is managed and done through the product we are developing.<p>So when I joined the team I was warned that its hard to work with some of the people and that other developers shortly left after they joined the team. For me this was okay since even if someone is an a-hole I can handle that. Thats not a problem.<p>One of the developers that is working for 5+ years on that project basically told me that they do whatever they feel like to do and that&#x27;s why its fun for him&#x2F;her to work on this project. He&#x2F;She is an consultant. I thought to my self okay that&#x27;s strange but then I thought okay maybe its fine because they also told me that the code is tested and commented and so on.<p>Then we had our first planning meeting for the next iteration. To say that nothing was clear to anybody except 3 developers who are working on that project for the longest time is an understatement. Nothing was clear not even how and when the handover to devops should happen. For a project that was running on production for a long time now.<p>I was assigned some minor bugs and tasks because I was told that the system is to complex to understand and it would take me years to understand it. When I asked what was so complex. I got the answer that everything is generic and the system is super flexible and that you can model anything in it.<p>So i started to fix bugs and look into the code more. The code base is a mess. The code is not readable at all. We are using Angular and we are working against the framework. We also have a mixed mono repo with angularjs and angular 15. I showed some code snippets to a friend if I&#x27;m the crazy one or is the code that bad? He and some other developers said its not me the code is a mess. We have no architecture. Everything is kind of half finished and self written. I pointed that out and some of them did not see a problem.<p>I then also started to push to work in an organized way and have better tickets in Jira and also clear requirements instead of making them up as we go. One developer who is also a freelancer then said to my manager that he will quit if we start to working in that way since he does not see the point but management is to scared to push back since this person has so much knowledge about the system in his had that they fear what would happen if he&#x2F;she would go.<p>We now have also a record high of bugs and even this is cheated because some real bugs are moved to feature requests.<p>Everything is tested in e2e test with playwright because we don&#x27;t need unit tests because with e2e test we are also testing the functions. So it takes forever to do anything.<p>The UX persons are thought of as people who waste time instead of getting things done. So the product without training for several weeks is not usable by any person even if you have the business knowledge.<p>Its hard to get anything done without new bugs.<p>Management does not know what to do because they are scared that the old developers will leave and then the system will break. New developers are leaving because they don&#x27;t want to work on a broken system and team like that and I&#x27;m in the middle of it and can&#x27;t push because then they will leave.<p>So I don&#x27;t know what to do.<p>Maybe someone can share the experience and what he&#x2F;she did?", "title": "Ask HN: Joined a company and the project/team is splintered", "updated_at": "2024-09-20T13:21:47Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "prism_insight"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hey HN! I built PRISM-INSIGHT, a multi-agent system where 13 specialized AI agents collaborate to analyze Korean stocks (KOSPI/KOSDAQ). It's completely open source and has been running live since March 2025.<p>[What it does]\nThe system automatically detects surging stocks twice daily, generates analyst-level reports, and executes trading strategies. Each agent specializes in something different \u2013 technical analysis, trading flows, financials, news, market conditions, etc. They work together like a real research team.<p>[Why I built this]\nI wanted to see if GPT-4 and GPT-5 could genuinely replicate what human analysts do, but without the typical single-agent limitations. So I split the work across multiple specialized agents that collaborate. The trading simulation has been running for 8 months now with real Korean market data.<p>[How to try it]<p>Join the live Telegram channel!\n<a href=\"https://t.me/prism_insight_global_en\" rel=\"nofollow\">https://t.me/prism_insight_global_en</a> (gets daily alerts and reports)<p>Check the real-time dashboard!\n<a href=\"https://analysis.stocksimulation.kr\" rel=\"nofollow\">https://analysis.stocksimulation.kr</a> (all trades, performance, AI reasoning)<p>Clone and run it yourself!\n<a href=\"https://github.com/dragon1086/prism-insight\" rel=\"nofollow\">https://github.com/dragon1086/prism-insight</a><p>[The interesting parts]\nThe system uses MCP (Model Context Protocol) servers to give agents access to live market data, web search, and financial APIs. I'm using GPT-4.1 for analysis, GPT-5 for trading decisions, and Claude Sonnet 4.5 for the conversational bot.<p>The first trading simulation (Season 1, Mar-Sep 2025) returned 408% across 51 trades. Current season(2) is at +11% realized returns vs KOSPI's +16%. Also running it with real money now ($10k account, up 9.35% since late September).<p>[Tech stack]\nPython 3.10+, async/await throughout, SQLite for trade history, <em>Playwright</em> for PDF reports, matplotlib for charts. The whole thing is about 8,400 lines of Python across 56 files.<p>[What makes it different]\nMost AI trading projects are either single-agent or black boxes. This one uses a multi-agent architecture where you can see exactly what each agent is analyzing and why. Everything is transparent \u2013 the dashboard shows every trade, every decision, and all the reasoning.<p>It's MIT licensed and runs entirely on your machine if you want. I'm covering the API costs (~$200/month) to keep the public Telegram channel free for 450+ users(Korean channel + Global channel).<p>Would love feedback on the multi-agent approach or questions about running AI agents in <em>production</em>!"}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Multi-agent AI stock analyzer \u2013 408% return trading Korean market"}}, "_tags": ["story", "author_prism_insight", "story_45946056", "show_hn"], "author": "prism_insight", "children": [45951650, 45953848], "created_at": "2025-11-16T16:02:10Z", "created_at_i": 1763308930, "num_comments": 4, "objectID": "45946056", "points": 5, "story_id": 45946056, "story_text": "Hey HN! I built PRISM-INSIGHT, a multi-agent system where 13 specialized AI agents collaborate to analyze Korean stocks (KOSPI&#x2F;KOSDAQ). It&#x27;s completely open source and has been running live since March 2025.<p>[What it does]\nThe system automatically detects surging stocks twice daily, generates analyst-level reports, and executes trading strategies. Each agent specializes in something different \u2013 technical analysis, trading flows, financials, news, market conditions, etc. They work together like a real research team.<p>[Why I built this]\nI wanted to see if GPT-4 and GPT-5 could genuinely replicate what human analysts do, but without the typical single-agent limitations. So I split the work across multiple specialized agents that collaborate. The trading simulation has been running for 8 months now with real Korean market data.<p>[How to try it]<p>Join the live Telegram channel!\n<a href=\"https:&#x2F;&#x2F;t.me&#x2F;prism_insight_global_en\" rel=\"nofollow\">https:&#x2F;&#x2F;t.me&#x2F;prism_insight_global_en</a> (gets daily alerts and reports)<p>Check the real-time dashboard!\n<a href=\"https:&#x2F;&#x2F;analysis.stocksimulation.kr\" rel=\"nofollow\">https:&#x2F;&#x2F;analysis.stocksimulation.kr</a> (all trades, performance, AI reasoning)<p>Clone and run it yourself!\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;dragon1086&#x2F;prism-insight\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;dragon1086&#x2F;prism-insight</a><p>[The interesting parts]\nThe system uses MCP (Model Context Protocol) servers to give agents access to live market data, web search, and financial APIs. I&#x27;m using GPT-4.1 for analysis, GPT-5 for trading decisions, and Claude Sonnet 4.5 for the conversational bot.<p>The first trading simulation (Season 1, Mar-Sep 2025) returned 408% across 51 trades. Current season(2) is at +11% realized returns vs KOSPI&#x27;s +16%. Also running it with real money now ($10k account, up 9.35% since late September).<p>[Tech stack]\nPython 3.10+, async&#x2F;await throughout, SQLite for trade history, Playwright for PDF reports, matplotlib for charts. The whole thing is about 8,400 lines of Python across 56 files.<p>[What makes it different]\nMost AI trading projects are either single-agent or black boxes. This one uses a multi-agent architecture where you can see exactly what each agent is analyzing and why. Everything is transparent \u2013 the dashboard shows every trade, every decision, and all the reasoning.<p>It&#x27;s MIT licensed and runs entirely on your machine if you want. I&#x27;m covering the API costs (~$200&#x2F;month) to keep the public Telegram channel free for 450+ users(Korean channel + Global channel).<p>Would love feedback on the multi-agent approach or questions about running AI agents in production!", "title": "Show HN: Multi-agent AI stock analyzer \u2013 408% return trading Korean market", "updated_at": "2025-11-26T08:32:34Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "marvinified"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "Hi Hackernews, I\u2019m Marvin from playrun.dev (<a href=\"https://playrun.dev\" rel=\"nofollow\">https://playrun.dev</a>), a platform for generating end-to-end tests for your application using just the URL.<p>I recently worked on a project rewrite with a small team on a tight deadline. We needed to keep shipping while also writing tests for the application. We believed unit tests were a waste of time and decided e2e tests were the way to go, but doing so manually was still time-consuming and made shipping slower.<p>Testing is one of those things we do as engineers that don\u2019t have immediate ROI, and it was almost impossible to make the business team understand why we needed to do it and why we were spending so much time on it.<p>Augmenting testing with AI helps reduce the dev hours we invest in testing so we can focus on shipping, which is very important to the business and users.<p>Basically, playrun.dev crawls through your application and discovers possible user flows. Once a flow is found, it is mapped step by step, and each step is run and evaluated to ensure it\u2019s correct before the next step is mapped until it can be determined that the flow has ended.<p>Under the hood, we use multi-agent LLM to discover flows and generate tests.<p>All that is needed is to provide the public URL to a testing version of your application (please don\u2019t use <em>production</em> URLs, lots of dummy data will be generated). We will navigate the whole application and generate <em>Playwright</em> tests. We also provide a CLI tool to export generated tests directly to your <em>Playwright</em> project.<p>Although this isn\u2019t perfect (we still had to review and manually fix a few tests), it was able to generate almost perfect tests for about 70% - 80% of flows in our applications (&gt;90% on some runs, we hope to reproduce that consistently).<p>We still have some limitations and gotchas around Captchas and 3rd party authentication (Google and some other providers block testing browsers as bots and the only way we are able to get around Captchas for now is by disabling it on our testing server).<p>Additionally we are working on ability to generate, manage and run you e2e tests on a schedule in one place. So regressions can be detected faster on autopilot and if application flows changes, they can be easily be remapped without writing a line of code.<p>You can try it today at <a href=\"https://playrun.dev\" rel=\"nofollow\">https://playrun.dev</a>.<p>I\u2019m really happy to show this to you all, thanks for reading about it, please let me know your thoughts and questions in the comment."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Generate E2E tests for your applications from its URL"}}, "_tags": ["story", "author_marvinified", "story_41648961", "show_hn"], "author": "marvinified", "created_at": "2024-09-25T16:09:50Z", "created_at_i": 1727280590, "num_comments": 0, "objectID": "41648961", "points": 5, "story_id": 41648961, "story_text": "Hi Hackernews, I\u2019m Marvin from playrun.dev (<a href=\"https:&#x2F;&#x2F;playrun.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;playrun.dev</a>), a platform for generating end-to-end tests for your application using just the URL.<p>I recently worked on a project rewrite with a small team on a tight deadline. We needed to keep shipping while also writing tests for the application. We believed unit tests were a waste of time and decided e2e tests were the way to go, but doing so manually was still time-consuming and made shipping slower.<p>Testing is one of those things we do as engineers that don\u2019t have immediate ROI, and it was almost impossible to make the business team understand why we needed to do it and why we were spending so much time on it.<p>Augmenting testing with AI helps reduce the dev hours we invest in testing so we can focus on shipping, which is very important to the business and users.<p>Basically, playrun.dev crawls through your application and discovers possible user flows. Once a flow is found, it is mapped step by step, and each step is run and evaluated to ensure it\u2019s correct before the next step is mapped until it can be determined that the flow has ended.<p>Under the hood, we use multi-agent LLM to discover flows and generate tests.<p>All that is needed is to provide the public URL to a testing version of your application (please don\u2019t use production URLs, lots of dummy data will be generated). We will navigate the whole application and generate Playwright tests. We also provide a CLI tool to export generated tests directly to your Playwright project.<p>Although this isn\u2019t perfect (we still had to review and manually fix a few tests), it was able to generate almost perfect tests for about 70% - 80% of flows in our applications (&gt;90% on some runs, we hope to reproduce that consistently).<p>We still have some limitations and gotchas around Captchas and 3rd party authentication (Google and some other providers block testing browsers as bots and the only way we are able to get around Captchas for now is by disabling it on our testing server).<p>Additionally we are working on ability to generate, manage and run you e2e tests on a schedule in one place. So regressions can be detected faster on autopilot and if application flows changes, they can be easily be remapped without writing a line of code.<p>You can try it today at <a href=\"https:&#x2F;&#x2F;playrun.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;playrun.dev</a>.<p>I\u2019m really happy to show this to you all, thanks for reading about it, please let me know your thoughts and questions in the comment.", "title": "Show HN: Generate E2E tests for your applications from its URL", "updated_at": "2024-09-26T09:09:37Z"}, {"_highlightResult": {"author": {"matchLevel": "none", "matchedWords": [], "value": "creativedg"}, "story_text": {"fullyHighlighted": false, "matchLevel": "full", "matchedWords": ["playwright", "production"], "value": "I started this boilerplate in July 2020 and I\u2019ve been maintaining it for 5 years. It began on Next.js 9 and kept upgrading to Next.js 15+ (App Router), while upgrading the stack over time (Tailwind 1 \u2192 4, ESLint 8, swapping Cypress \u2192 <em>Playwright</em>, etc.). The goal is simple: I kept rebuilding the same setup, so I packaged it and kept it updated.<p>What you get (preconfigured, keep only what you need):<p>- Next.js 15 (App Router) + TypeScript + Tailwind 4<p>- Auth with Clerk (magic links, MFA, social, passkeys)<p>- I18n via next-intl<p>- DB with Drizzle ORM (PGlite locally)<p>- Forms with React Hook Form + Zod validation<p>- Testing: Vitest (unit), <em>Playwright</em> (integration/E2E)<p>- CI with GitHub Actions; Storybook for UI work<p>- SEO (Open Graph, JSON-LD, sitemap, robots)<p>- Observability: Sentry, logging with LogTape, log management &amp; uptime/monitoring<p>- Security: Arcjet (bot detection, rate limiting, shield rules)<p>- DX details: ESLint/Prettier, Lefthook + lint-staged, Commitlint, absolute imports, bundle analyzer<p>- AI code review<p>It\u2019s free and open source (MIT). Today the project sits around 11.8k GitHub stars and 2.2k forks. I\u2019m still actively maintaining it and adding features.<p>Repo: <a href=\"https://github.com/ixartz/Next-js-Boilerplate\" rel=\"nofollow\">https://github.com/ixartz/Next-js-Boilerplate</a><p>Why I built it<p>Spinning up auth, a DB, i18n, tests, and lint/format/CI for each new app was repetitive. This gives me (and hopefully you) a <em>production</em>-ready base in minutes, with opinionated defaults you can start.<p>I\u2019m open to suggestions and feedback, what would you like to see next? I\u2019ll hang around in the comments to answer questions."}, "title": {"matchLevel": "none", "matchedWords": [], "value": "Show HN: Open-source Next.js 15 boilerplate \u2013 auth, DB, intl, tests, monitoring"}}, "_tags": ["story", "author_creativedg", "story_45052744", "show_hn"], "author": "creativedg", "created_at": "2025-08-28T14:38:31Z", "created_at_i": 1756391911, "num_comments": 0, "objectID": "45052744", "points": 4, "story_id": 45052744, "story_text": "I started this boilerplate in July 2020 and I\u2019ve been maintaining it for 5 years. It began on Next.js 9 and kept upgrading to Next.js 15+ (App Router), while upgrading the stack over time (Tailwind 1 \u2192 4, ESLint 8, swapping Cypress \u2192 Playwright, etc.). The goal is simple: I kept rebuilding the same setup, so I packaged it and kept it updated.<p>What you get (preconfigured, keep only what you need):<p>- Next.js 15 (App Router) + TypeScript + Tailwind 4<p>- Auth with Clerk (magic links, MFA, social, passkeys)<p>- I18n via next-intl<p>- DB with Drizzle ORM (PGlite locally)<p>- Forms with React Hook Form + Zod validation<p>- Testing: Vitest (unit), Playwright (integration&#x2F;E2E)<p>- CI with GitHub Actions; Storybook for UI work<p>- SEO (Open Graph, JSON-LD, sitemap, robots)<p>- Observability: Sentry, logging with LogTape, log management &amp; uptime&#x2F;monitoring<p>- Security: Arcjet (bot detection, rate limiting, shield rules)<p>- DX details: ESLint&#x2F;Prettier, Lefthook + lint-staged, Commitlint, absolute imports, bundle analyzer<p>- AI code review<p>It\u2019s free and open source (MIT). Today the project sits around 11.8k GitHub stars and 2.2k forks. I\u2019m still actively maintaining it and adding features.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ixartz&#x2F;Next-js-Boilerplate\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ixartz&#x2F;Next-js-Boilerplate</a><p>Why I built it<p>Spinning up auth, a DB, i18n, tests, and lint&#x2F;format&#x2F;CI for each new app was repetitive. This gives me (and hopefully you) a production-ready base in minutes, with opinionated defaults you can start.<p>I\u2019m open to suggestions and feedback, what would you like to see next? I\u2019ll hang around in the comments to answer questions.", "title": "Show HN: Open-source Next.js 15 boilerplate \u2013 auth, DB, intl, tests, monitoring", "updated_at": "2025-08-29T18:59:31Z"}], "hitsPerPage": 15, "nbHits": 25, "nbPages": 2, "page": 0, "params": "query=playwright+production&tags=story&hitsPerPage=15&advancedSyntax=true&analyticsTags=backend", "processingTimeMS": 12, "processingTimingsMS": {"_request": {"roundTrip": 14}, "afterFetch": {"format": {"highlighting": 2, "total": 2}}, "fetch": {"query": 9, "scanning": 1, "total": 11}, "total": 12}, "query": "playwright production", "serverTimeMS": 15}}